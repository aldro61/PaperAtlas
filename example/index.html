<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NeurIPS 2025 - My Papers</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: #f4f6f9;
            color: #2c3e50;
            padding: 0;
            min-height: 100vh;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 20px;
        }

        header {
            background: linear-gradient(135deg, #1c3664 0%, #0a1f44 100%);
            padding: 60px 20px;
            margin-bottom: 40px;
            text-align: center;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        h1 {
            color: #ffffff;
            font-size: 2.8em;
            margin-bottom: 15px;
            font-weight: 600;
            letter-spacing: -0.5px;
        }

        .subtitle {
            color: #b8c5d6;
            font-size: 1.15em;
            font-weight: 400;
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }

        .stat-card {
            background: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.08);
            text-align: center;
            transition: all 0.3s ease;
            border: 1px solid #e8ecef;
        }

        .stat-card:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.12);
        }

        .stat-value {
            font-size: 2.8em;
            font-weight: 700;
            color: #1c3664;
            margin: 10px 0;
        }

        .stat-label {
            color: #5d6d7e;
            font-size: 0.85em;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            font-weight: 600;
        }

        .chart-section {
            background: white;
            padding: 35px;
            border-radius: 8px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.08);
            margin-bottom: 30px;
            border: 1px solid #e8ecef;
        }

        .chart-section h2 {
            color: #1c3664;
            margin-bottom: 25px;
            font-size: 1.75em;
            font-weight: 600;
        }

        .chart-container {
            position: relative;
            height: 300px;
            margin-bottom: 20px;
        }

        .papers-section {
            background: white;
            padding: 35px;
            border-radius: 8px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.08);
            border: 1px solid #e8ecef;
        }

        .papers-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 20px;
            flex-wrap: wrap;
            gap: 15px;
        }

        .papers-header h2 {
            color: #667eea;
            font-size: 1.8em;
        }

        .sort-controls {
            display: flex;
            gap: 10px;
            align-items: center;
        }

        .sort-controls label {
            color: #666;
            font-weight: 500;
        }

        select {
            padding: 10px 15px;
            border: 2px solid #667eea;
            border-radius: 8px;
            background: white;
            color: #333;
            font-size: 1em;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        select:hover {
            background: #f8f9ff;
        }

        .paper-card {
            background: #ffffff;
            padding: 24px;
            border-radius: 6px;
            margin-bottom: 16px;
            border-left: 4px solid #00c781;
            transition: all 0.2s ease;
            border: 1px solid #e8ecef;
            border-left: 4px solid #00c781;
        }

        .paper-card:hover {
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
            border-left-color: #1c3664;
        }

        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: start;
            margin-bottom: 10px;
            gap: 15px;
        }

        .paper-title {
            font-size: 1.2em;
            font-weight: 600;
            color: #333;
            flex: 1;
        }

        .paper-score {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 8px 16px;
            border-radius: 20px;
            font-weight: bold;
            font-size: 1.1em;
            min-width: 60px;
            text-align: center;
        }

        .paper-authors {
            color: #666;
            margin-bottom: 10px;
            font-size: 0.95em;
        }

        .paper-details {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 10px;
            font-size: 0.9em;
        }

        .detail-item {
            display: flex;
            align-items: center;
            gap: 5px;
            color: #666;
        }

        .detail-item strong {
            color: #667eea;
        }

        .paper-stats {
            display: flex;
            gap: 15px;
            margin-top: 10px;
        }

        .stat-badge {
            background: white;
            padding: 5px 12px;
            border-radius: 15px;
            font-size: 0.85em;
            display: flex;
            align-items: center;
            gap: 5px;
        }

        .stat-badge strong {
            color: #667eea;
        }

        .paper-link {
            margin-top: 10px;
        }

        .paper-link a {
            color: #667eea;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }

        .paper-link a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .tabs {
            background: white;
            border-radius: 20px;
            padding: 10px;
            margin-bottom: 30px;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
            display: flex;
            gap: 10px;
        }

        .tab {
            flex: 1;
            padding: 16px 30px;
            background: transparent;
            border: none;
            border-bottom: 3px solid transparent;
            border-radius: 0;
            font-size: 1em;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.2s ease;
            color: #5d6d7e;
        }

        .tab:hover {
            color: #1c3664;
            background: #f8f9fb;
        }

        .tab.active {
            background: transparent;
            color: #1c3664;
            border-bottom: 3px solid #00c781;
        }

        .tab-content {
            display: none;
        }

        .tab-content.active {
            display: block;
        }

        .author-card {
            background: #ffffff;
            padding: 24px;
            border-radius: 6px;
            margin-bottom: 16px;
            border-left: 4px solid #00c781;
            transition: all 0.2s ease;
            border: 1px solid #e8ecef;
        }

        .author-card:hover {
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
            border-left-color: #1c3664;
        }

        .author-header {
            display: flex;
            flex-direction: row;
            gap: 15px;
            margin-bottom: 15px;
            align-items: flex-start;
        }

        .author-photo {
            width: 80px;
            height: 80px;
            border-radius: 50%;
            object-fit: cover;
            border: 3px solid #667eea;
            flex-shrink: 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .author-photo-placeholder {
            width: 80px;
            height: 80px;
            border-radius: 50%;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            flex-shrink: 0;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 2em;
            color: white;
            font-weight: 600;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .author-info {
            flex: 1;
            min-width: 0;
        }

        .author-name {
            font-size: 1.3em;
            font-weight: 600;
            color: #333;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .author-profile-link {
            display: inline-flex;
            align-items: center;
            gap: 5px;
            padding: 4px 10px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            text-decoration: none;
            border-radius: 8px;
            font-size: 0.7em;
            font-weight: 500;
            transition: all 0.2s ease;
        }

        .author-profile-link:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
        }

        .author-affiliation {
            display: flex;
            gap: 8px;
            flex-wrap: wrap;
            align-items: center;
        }

        .affiliation-badge {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.85em;
            font-weight: 500;
        }

        .role-badge {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.85em;
            font-weight: 500;
        }

        .author-stats-badges {
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
        }

        .author-badge {
            background: white;
            padding: 6px 12px;
            border-radius: 15px;
            font-size: 0.85em;
            display: flex;
            align-items: center;
            gap: 5px;
            white-space: nowrap;
        }

        .pagination-btn {
            padding: 8px 16px;
            border: 1px solid #ddd;
            background: white;
            border-radius: 8px;
            cursor: pointer;
            font-size: 0.9em;
            transition: all 0.2s ease;
        }

        .pagination-btn:hover:not(:disabled) {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-color: transparent;
        }

        .pagination-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }

        .pagination-btn.active {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-color: transparent;
            font-weight: 600;
        }

        .pagination-info {
            color: #666;
            font-size: 0.9em;
        }

        .category-pill {
            display: inline-block;
            padding: 6px 14px;
            margin: 4px;
            border-radius: 20px;
            font-size: 0.85em;
            cursor: pointer;
            transition: all 0.2s ease;
            border: 2px solid #ddd;
            background: white;
            color: #555;
        }

        .category-pill:hover {
            border-color: #667eea;
            background: #f0f4ff;
        }

        .category-pill.selected {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-color: transparent;
        }

        .paper-category-badge {
            display: inline-block;
            padding: 4px 10px;
            margin: 2px 4px 2px 0;
            border-radius: 12px;
            font-size: 0.75em;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            font-weight: 500;
        }

        .paper-card {
            cursor: pointer;
        }

        .paper-card.expanded {
            background: #f8f9fa;
        }

        .paper-expandable {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease;
        }

        .paper-card.expanded .paper-expandable {
            max-height: 500px;
            padding-top: 15px;
        }

        .paper-key-info {
            background: white;
            padding: 15px;
            border-radius: 8px;
            margin-bottom: 10px;
        }

        .paper-key-info h4 {
            margin: 0 0 8px 0;
            color: #667eea;
            font-size: 0.9em;
        }

        .paper-key-info p {
            margin: 0;
            color: #555;
            font-size: 0.9em;
            line-height: 1.5;
        }

        .view-details-btn {
            display: inline-block;
            padding: 8px 16px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 8px;
            border: none;
            cursor: pointer;
            font-size: 0.9em;
            margin-top: 10px;
            transition: transform 0.2s ease;
        }

        .view-details-btn:hover {
            transform: translateY(-2px);
        }

        .modal {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.5);
            z-index: 1000;
            align-items: center;
            justify-content: center;
        }

        .modal.active {
            display: flex;
        }

        .modal-content {
            background: white;
            border-radius: 15px;
            padding: 30px;
            max-width: 800px;
            max-height: 90vh;
            overflow-y: auto;
            position: relative;
            box-shadow: 0 10px 40px rgba(0,0,0,0.3);
        }

        .modal-close {
            position: absolute;
            top: 15px;
            right: 15px;
            font-size: 28px;
            cursor: pointer;
            color: #999;
            background: none;
            border: none;
            padding: 0;
            width: 32px;
            height: 32px;
            line-height: 28px;
            text-align: center;
        }

        .modal-close:hover {
            color: #333;
        }

        .modal-section {
            margin-bottom: 20px;
        }

        .modal-section h3 {
            color: #667eea;
            margin: 0 0 10px 0;
            font-size: 1.1em;
        }

        .modal-section p {
            color: #555;
            line-height: 1.6;
            margin: 0;
        }

        .author-badge strong {
            color: #764ba2;
        }

        .author-papers-list {
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #ddd;
        }

        .author-paper-item {
            padding: 8px 0;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .author-paper-score {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 4px 10px;
            border-radius: 12px;
            font-weight: bold;
            font-size: 0.9em;
            min-width: 40px;
            text-align: center;
        }

        .author-paper-title {
            flex: 1;
            color: #333;
            font-size: 0.95em;
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 2em;
            }

            .stats-grid {
                grid-template-columns: 1fr;
            }

            .papers-header {
                flex-direction: column;
                align-items: stretch;
            }

            .sort-controls {
                flex-direction: column;
                width: 100%;
            }

            select {
                width: 100%;
            }

            .tabs {
                flex-direction: column;
            }

            .author-header {
                flex-direction: column;
                align-items: flex-start;
            }
        }

        /* Paper reference tooltips */
        .paper-ref {
            color: #00c781;
            font-weight: 600;
            cursor: help;
            position: relative;
            text-decoration: underline dotted;
            transition: all 0.2s ease;
        }

        .paper-ref:hover {
            color: #1c3664;
        }

        .paper-ref::after {
            content: attr(data-tooltip);
            position: absolute;
            bottom: 125%;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(0, 0, 0, 0.95);
            color: white;
            padding: 12px 16px;
            border-radius: 8px;
            white-space: normal;
            width: 320px;
            font-size: 0.85em;
            font-weight: normal;
            line-height: 1.5;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
            z-index: 1000;
            opacity: 0;
            visibility: hidden;
            transition: opacity 0.3s ease, visibility 0.3s ease;
            pointer-events: none;
            text-align: left;
        }

        .paper-ref:hover::after {
            opacity: 1;
            visibility: visible;
        }

        /* Tooltip arrow */
        .paper-ref::before {
            content: '';
            position: absolute;
            bottom: 115%;
            left: 50%;
            transform: translateX(-50%);
            border: 6px solid transparent;
            border-top-color: rgba(0, 0, 0, 0.95);
            opacity: 0;
            visibility: hidden;
            transition: opacity 0.3s ease, visibility 0.3s ease;
        }

        .paper-ref:hover::before {
            opacity: 1;
            visibility: visible;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>üéì NeurIPS 2025 Papers</h1>
            <p class="subtitle">Your personalized conference guide</p>
        </header>

        <div class="tabs">
            <button class="tab active" onclick="switchTab('papers')">üìÑ Papers</button>
            <button class="tab" onclick="switchTab('authors')">üë• Authors</button>
            <button class="tab" onclick="switchTab('synthesis')">üî¨ Synthesis</button>
        </div>

        <div id="papersTab" class="tab-content active">
            <div class="stats-grid">
                <div class="stat-card">
                    <div class="stat-label">Total Papers</div>
                    <div class="stat-value" id="totalPapers">-</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Average Score</div>
                    <div class="stat-value" id="avgScore">-</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Top Score</div>
                    <div class="stat-value" id="topScore">-</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Most Popular</div>
                    <div class="stat-value" id="mostPopular">-</div>
                </div>
            </div>

            <div class="chart-section">
                <h2>üìä Score Distribution</h2>
                <div class="chart-container">
                    <canvas id="scoreChart"></canvas>
                </div>
            </div>

            <div class="papers-section">
                <div class="papers-header">
                    <h2>üìÑ Your Papers</h2>
                    <div class="sort-controls">
                        <label for="sortBy">Sort by:</label>
                        <select id="sortBy">
                            <option value="score">Score (highest first)</option>
                            <option value="relevant">Most Relevant</option>
                            <option value="reads">Most Read</option>
                            <option value="title">Title (A-Z)</option>
                        </select>
                    </div>
                </div>

                <div id="categoryFilters" style="margin-bottom: 25px;"></div>

                <div id="papersList"></div>

                <div id="papersPagination" style="display: flex; justify-content: center; align-items: center; gap: 10px; margin-top: 30px;">
                </div>
            </div>
        </div>

        <div id="authorsTab" class="tab-content">
            <div class="papers-section">
                <div class="papers-header">
                    <h2>üë• Key Authors to Meet</h2>
                </div>
                <div style="background: #f8f9fa; padding: 15px; border-radius: 10px; margin-bottom: 20px; font-size: 0.95em; color: #555;">
                    <strong>Ranking by Research Alignment:</strong> Authors are ranked by their number of <strong>highly relevant papers (score ‚â• 85)</strong>.
                    Showing <strong>first, second, and last authors</strong> (primary contributors, key collaborators, and senior researchers) with at least <strong>1 highly relevant paper</strong> ‚Äî these are the must-meet researchers whose work is most aligned with your interests.
                    <span style="opacity: 0.8;">(Focusing on these key positions helps prioritize important contributors)</span>
                </div>

                <div style="background: white; padding: 25px; border-radius: 15px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); margin-bottom: 30px;">
                    <h3 style="margin-top: 0; margin-bottom: 20px; color: #333;">üèõÔ∏è Top Institutions</h3>
                    <canvas id="affiliationChart" style="max-height: 400px;"></canvas>
                </div>

                <div id="authorsList"></div>

                <div id="authorsPagination" style="display: flex; justify-content: center; align-items: center; gap: 10px; margin-top: 30px;">
                </div>
            </div>
        </div>

        <div id="synthesisTab" class="tab-content">
            <div class="papers-section">
                <div class="papers-header">
                    <h2>üî¨ Research Synthesis</h2>
                </div>
                <div style="background: #f8f9fa; padding: 15px; border-radius: 10px; margin-bottom: 20px; font-size: 0.95em; color: #555;">
                    <strong>Critical Analysis:</strong> A synthesized overview of major trends, surprising findings, and impactful work across all papers.
                </div>

                <div id="synthesisContent" style="background: white; padding: 30px; border-radius: 15px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); line-height: 1.8; max-width: 900px; margin: 0 auto;">
                    <div style="font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;"><div style="font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; line-height: 1.8; color: #2c3e50;">
<div style="text-align: center; margin-bottom: 30px; padding: 30px; background: linear-gradient(135deg, #1c3664 0%, #0a1f44 100%); color: white; border-radius: 8px;">
<h1 style="margin: 0; color: white; font-weight: 600;">NeurIPS 2025: Agent Systems Research Synthesis</h1>
<p style="margin: 10px 0 0 0; color: #b8c5d6;">Analysis of 367 papers across 15 research areas</p>
</div>
<h1 style="color: #1c3664; font-weight: 600; margin-top: 20px;">NeurIPS 2025: A Critical Synthesis of Agent Systems, Benchmarking, and Reasoning Research</h1>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">Executive Summary</h2>

<p>NeurIPS 2025 represents a pivotal moment in AI agent research, marked by a dramatic shift from capability demonstration to critical evaluation, from monolithic models to collaborative systems, and from static benchmarks to dynamic, contamination-resistant evaluation frameworks. Across 367 papers spanning agent benchmarking, tool use, reasoning, multi-agent collaboration, and safety, several patterns emerge: <strong>test-time compute scaling is reaching theoretical and practical limits</strong>, <strong>multi-agent systems show promise but lack fundamental social intelligence</strong>, <strong>current benchmarks systematically overestimate real-world capabilities</strong>, and <strong>the safety-capability tension in autonomous systems remains fundamentally unresolved</strong>. Perhaps most critically, the conference reveals a field grappling with reproducibility crises, evaluation validity concerns, and the sobering realization that scaling alone cannot solve reasoning limitations.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">I. The Evaluation Crisis: Benchmarking Under Scrutiny</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">1.1 Contamination, Validity, and the Death of Static Benchmarks</h3>

<p>The conference mounted a comprehensive assault on traditional evaluation methodologies, with over 30 papers directly challenging benchmark validity. <strong>The contamination problem has reached crisis levels</strong>: <span class="paper-ref" data-paper-id="127" data-tooltip="MathArena: Evaluating LLMs on Uncontaminated Math Competitions">[Paper 127]</span> demonstrates that models show declining performance on newer test sets despite high scores on established benchmarks, suggesting widespread memorization rather than genuine reasoning. <span class="paper-ref" data-paper-id="218" data-tooltip="ThinkBench: Dynamic Out-of-Distribution Evaluation for Robust LLM Reasoning">[Paper 218]</span> introduces dynamic out-of-distribution generation specifically to combat this, while <span class="paper-ref" data-paper-id="72" data-tooltip="SWE-bench Goes Live!">[Paper 72]</span> launches "SWE-bench Goes Live" with continuous automated issue collection to prevent dataset staleness.</p>

<p>More fundamentally, multiple papers expose that <strong>benchmark performance dramatically overestimates real-world capabilities</strong>. <span class="paper-ref" data-paper-id="2" data-tooltip="REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites">[Paper 2]</span> shows that deterministic simulation reveals capabilities invisible in live-environment testing, while <span class="paper-ref" data-paper-id="4" data-tooltip="TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks">[Paper 4]</span> demonstrates success rates below 50% on realistic business tasks despite models achieving 80%+ on academic benchmarks. <span class="paper-ref" data-paper-id="74" data-tooltip="Measuring AI Ability to Complete Long Software Tasks">[Paper 74]</span> reveals that even advanced models take 4-10√ó longer to complete real software tasks than benchmark metrics suggest. The gap between "laboratory performance" and "production readiness" has never been more apparent.</p>

<p><strong>The psychometric critique</strong> introduced by <span class="paper-ref" data-paper-id="178" data-tooltip="Neither Valid nor Reliable? Investigating the Use of LLMs as Judges">[Paper 178]</span> fundamentally questions whether LLM-as-judge has construct validity, revealing that position bias, prompt sensitivity, and systematic artifacts persist despite optimization attempts. <span class="paper-ref" data-paper-id="191" data-tooltip="Measuring what Matters: Construct Validity in Large Language Model Benchmarks">[Paper 191]</span> provides the first rigorous framework for assessing construct validity in LLM benchmarks, while <span class="paper-ref" data-paper-id="94" data-tooltip="BenchmarkCards: Standardized Documentation for Large Language Model Benchmarks">[Paper 94]</span> establishes standardized documentation requirements through BenchmarkCards. These methodological papers signal a field maturing beyond raw performance chasing toward scientific rigor.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">1.2 Task-Specific Limitations and Domain Gaps</h3>

<p>Domain-specific evaluation reveals systematic blind spots. <strong>Agent systems struggle with consequential, long-horizon tasks</strong>: <span class="paper-ref" data-paper-id="4" data-tooltip="TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks">[Paper 4]</span> shows leading models fail on authentic business workflows, <span class="paper-ref" data-paper-id="16" data-tooltip="MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?">[Paper 16]</span> demonstrates near-random performance on ML research challenges, <span class="paper-ref" data-paper-id="32" data-tooltip="MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research">[Paper 32]</span> reveals struggles with open-ended research methodology, and <span class="paper-ref" data-paper-id="75" data-tooltip="MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents">[Paper 75]</span> exposes severe limitations in spatial planning for embodied tasks. The pattern is clear: <strong>models excel at isolated, short-horizon tasks with clear success criteria but collapse when facing open-ended, multi-step challenges requiring sustained reasoning and adaptation</strong>.</p>

<p><strong>Spatial and temporal reasoning remain fundamental bottlenecks</strong>: <span class="paper-ref" data-paper-id="33" data-tooltip="LTD-Bench: Evaluating Large Language Models by Letting Them Draw">[Paper 33]</span> shows models achieve only 30-60% accuracy on spatial reasoning despite strong general performance, <span class="paper-ref" data-paper-id="41" data-tooltip="OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding">[Paper 41]</span> demonstrates performance degradation as context length increases in video understanding, <span class="paper-ref" data-paper-id="138" data-tooltip="Scientists&#39; First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning">[Paper 138]</span> reveals that even state-of-the-art models fail at basic mental visualization tasks, and <span class="paper-ref" data-paper-id="262" data-tooltip="Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges">[Paper 262]</span> shows models struggling with combined spatiotemporal reasoning despite handling either dimension independently. <span class="paper-ref" data-paper-id="23" data-tooltip="LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language">[Paper 23]</span> introduces temporal constraints to planning evaluation and finds current models fundamentally unprepared for time-bounded reasoning.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">1.3 The Multi-Modal Reality Check</h3>

<p>Vision-language-action models face particularly harsh scrutiny. <span class="paper-ref" data-paper-id="59" data-tooltip="Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence">[Paper 59]</span> reveals 66.6% of failures stem from cross-domain integration (transitioning between physical and digital contexts) rather than individual domain incompetence, achieving only 6.4% on cooking tasks versus 77% human performance. <span class="paper-ref" data-paper-id="106" data-tooltip="ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models">[Paper 106]</span> exposes dramatic 35-55% performance drops on visual reasoning questions compared to text-heavy questions, while <span class="paper-ref" data-paper-id="201" data-tooltip="VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs">[Paper 201]</span> demonstrates that leading VLMs fail at fundamental nonlocal visual reasoning tasks that are trivial for humans (99.5-100% accuracy).</p>

<p>The <strong>vision-language capability tension</strong> identified by <span class="paper-ref" data-paper-id="279" data-tooltip="Caption This, Reason That: VLMs Caught in the Middle">[Paper 279]</span> reveals that architectural choices optimal for captioning degrade reasoning performance and vice versa‚Äîsuggesting current unified architectures may be fundamentally limited. <span class="paper-ref" data-paper-id="280" data-tooltip="Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders">[Paper 280]</span> shows this extends to temporal understanding, where models struggle despite architectural claims of video processing capability.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">II. Test-Time Compute Scaling: Promise and Limits</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">2.1 The Scaling Paradigm and Its Discontents</h3>

<p>Over 50 papers explore test-time compute scaling, revealing both potential and fundamental limitations. <strong>The basic premise works</strong>: <span class="paper-ref" data-paper-id="10" data-tooltip="$\texttt{G1}$: Teaching LLMs to Reason on Graphs with Reinforcement Learning">[Paper 10]</span> demonstrates 3B models outperforming 72B models through RL on synthetic graph tasks, <span class="paper-ref" data-paper-id="24" data-tooltip="Agentic RL Scaling Law: Spontaneous Code Execution for Mathematical Problem Solving">[Paper 24]</span> shows spontaneous code execution emerging through pure RL, and <span class="paper-ref" data-paper-id="18" data-tooltip="Bag of Tricks for Inference-time Computation of LLM Reasoning">[Paper 18]</span> provides comprehensive empirical validation across 1,000+ experiments. <span class="paper-ref" data-paper-id="50" data-tooltip="Language Models can Self-Improve at State-Value Estimation for Better Search">[Paper 50]</span> enables models to learn value estimation without ground-truth rewards, achieving 39% success rate improvements.</p>

<p>However, <strong>critical failure modes emerge at scale</strong>. <span class="paper-ref" data-paper-id="30" data-tooltip="Large Language Models Think Too Fast To Explore Effectively">[Paper 30]</span> reveals LLMs "think too fast," making premature decisions before empowerment considerations can influence behavior‚Äîtest-time compute helps only when models use extended reasoning at inference (like o1). <span class="paper-ref" data-paper-id="78" data-tooltip="The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity">[Paper 78]</span> exposes performance collapse beyond specific complexity thresholds despite available token budgets, while <span class="paper-ref" data-paper-id="168" data-tooltip="Does Thinking More Always Help? Mirage of Test-Time Scaling in Reasoning Models">[Paper 168]</span> demonstrates "overthinking" where extended reasoning actually degrades performance, with drops up to 17 percentage points.</p>

<p><strong>The efficiency crisis</strong> is stark: <span class="paper-ref" data-paper-id="170" data-tooltip="Thinkless: LLM Learns When to Think">[Paper 170]</span> shows models use 2√ó more tokens than needed, <span class="paper-ref" data-paper-id="149" data-tooltip="Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning">[Paper 149]</span> finds optimal CoT length varies by domain with excessive computation hurting performance, and <span class="paper-ref" data-paper-id="303" data-tooltip="Efficiently Scaling LLM Reasoning Programs with Certaindex">[Paper 303]</span> reveals models generate 4.5√ó more tokens than necessary due to "self-doubt." <span class="paper-ref" data-paper-id="259" data-tooltip="Don√¢¬Ä¬ôt Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models">[Paper 259]</span> cuts token usage by 47% with only 4.9% accuracy loss, demonstrating massive waste in current approaches.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">2.2 Theoretical Limits and Architectural Constraints</h3>

<p>Several papers provide theoretical grounding for observed limitations. <span class="paper-ref" data-paper-id="238" data-tooltip="Let Me Think! A Long Chain of Thought Can Be Worth Exponentially Many Short Ones">[Paper 238]</span> proves formal separations between sequential and parallel test-time compute strategies, establishing that certain reasoning tasks require long chains that cannot be efficiently substituted by ensembles of short chains. <span class="paper-ref" data-paper-id="130" data-tooltip="L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models">[Paper 130]</span> demonstrates that longer reasoning without structure doesn't help‚Äîleap-based prediction patterns that break from adjacent tokens achieve superior results.</p>

<p><strong>The "thinking vs. doing" distinction</strong> introduced by <span class="paper-ref" data-paper-id="79" data-tooltip="Thinking vs. Doing: Improving Agent Reasoning by  Scaling Test-Time Interaction">[Paper 79]</span> reveals that scaling interaction horizons (environmental feedback loops) provides orthogonal benefits to per-step reasoning compute, with curriculum-based horizon scheduling enabling models to learn when to think versus when to act. <span class="paper-ref" data-paper-id="270" data-tooltip="MindJourney: Test-Time Scaling with World Models for Spatial Reasoning">[Paper 270]</span> shows controllable video generation enables test-time spatial reasoning improvements without fine-tuning, achieving 7-8% gains by granting VLMs missing 3D reasoning capabilities through external world modeling.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">2.3 Adaptive and Efficient Scaling Strategies</h3>

<p>The field is converging on <strong>adaptive, difficulty-aware compute allocation</strong> rather than uniform scaling. <span class="paper-ref" data-paper-id="13" data-tooltip="AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks">[Paper 13]</span> introduces agents that autonomously discover optimal model-budget allocations across heterogeneous subtasks, <span class="paper-ref" data-paper-id="111" data-tooltip="AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking">[Paper 111]</span> trains models to adaptively decide when to use chain-of-thought versus direct inference, and <span class="paper-ref" data-paper-id="179" data-tooltip="Think Only When You Need with Large Hybrid-Reasoning Models">[Paper 179]</span> enables reasoning models to dynamically adjust thinking effort based on problem difficulty. <span class="paper-ref" data-paper-id="308" data-tooltip="Know What You Don&#39;t Know: Uncertainty Calibration of Process Reward Models">[Paper 308]</span> achieves 60-75% computational savings through instance-adaptive scaling using calibrated uncertainty estimates.</p>

<p><strong>Speculative and parallel approaches</strong> offer practical efficiency gains: <span class="paper-ref" data-paper-id="48" data-tooltip="Hogwild! Inference: Parallel LLM Generation via Concurrent Attention">[Paper 48]</span> enables multiple LLM instances to run in parallel with shared attention cache, <span class="paper-ref" data-paper-id="161" data-tooltip="Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation">[Paper 161]</span> breaks autoregressive constraints through learned parallelization decisions achieving 2√ó speedup, and <span class="paper-ref" data-paper-id="304" data-tooltip="SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning">[Paper 304]</span> operates at semantic similarity level rather than token-level for reasoning-specific speculation achieving 1.5-2.5√ó speedup.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">III. The Multi-Agent Reality: Capability and Coordination Challenges</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">3.1 The Social Intelligence Gap</h3>

<p>A sobering theme emerges: <strong>multi-agent systems built from capable individual LLMs systematically fail at coordination</strong>. <span class="paper-ref" data-paper-id="35" data-tooltip="Large Language Models Miss the Multi-agent Mark">[Paper 35]</span> provides the definitive diagnosis: LLMs fundamentally lack native social intelligence capabilities required for multi-agent collaboration, struggling with communication protocols, theory of mind, and cooperative behavior despite strong individual performance. <span class="paper-ref" data-paper-id="25" data-tooltip="Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia">[Paper 25]</span> shows agents exhibit 90%+ agreement rates in collaborative reasoning, limiting their ability to challenge incorrect solutions‚Äîprecisely the opposite of effective collaboration.</p>

<p><strong>Communication architecture matters profoundly</strong>: <span class="paper-ref" data-paper-id="29" data-tooltip="AutoData: A Multi-Agent System for Open Web Data Collection">[Paper 29]</span> reduces costs by 77% through oriented message hypergraph communication that enables targeted multi-recipient messaging rather than broadcast, <span class="paper-ref" data-paper-id="156" data-tooltip="AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems">[Paper 156]</span> introduces decentralized evolutionary coordination eliminating centralized bottlenecks, and <span class="paper-ref" data-paper-id="19" data-tooltip="Multi-Agent Collaboration via Evolving Orchestration">[Paper 19]</span> uses reinforcement learning to train centralized orchestrators that dynamically direct collaboration, achieving simultaneous effectiveness and efficiency improvements.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">3.2 Coordination Patterns and Specialization</h3>

<p>The <strong>puppeteer-style paradigm</strong> <span class="paper-ref" data-paper-id="19" data-tooltip="Multi-Agent Collaboration via Evolving Orchestration">[Paper 19]</span> achieves superior performance with reduced computational costs by using centralized orchestrators trained via RL to dynamically direct agents, evolving toward compact, cyclic graph structures. <span class="paper-ref" data-paper-id="17" data-tooltip="AI-Researcher: Autonomous Scientific Innovation">[Paper 17]</span> demonstrates complete pipeline automation from literature review to publication through hierarchical mentor-student collaboration patterns, achieving 93.8% implementation success with Claude models.</p>

<p>However, <span class="paper-ref" data-paper-id="96" data-tooltip="Why Do Multi-Agent LLM Systems Fail?">[Paper 96]</span> reveals systematic failure modes across diverse scenarios: communication breakdowns, role confusion, premature termination, and conversation history loss plague current systems. <span class="paper-ref" data-paper-id="70" data-tooltip="AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement">[Paper 70]</span> demonstrates that safety optimization must occur at the architectural level‚Äîmulti-agent systems harbor previously unexplored vulnerabilities that emerge during capability optimization.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">3.3 Practical Multi-Agent Applications</h3>

<p>Domain-specific applications show promise. <span class="paper-ref" data-paper-id="12" data-tooltip="AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems">[Paper 12]</span> establishes that agentic recommenders substantially outperform traditional methods when workflows integrate user history, candidate items, and platform-specific features. <span class="paper-ref" data-paper-id="51" data-tooltip="OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation">[Paper 51]</span> achieves 69.70% on GAIA by training domain-agnostic planners through RL on real-world feedback, demonstrating cross-domain transferability. <span class="paper-ref" data-paper-id="242" data-tooltip="Learning √¢¬Ä¬úPartner-Aware√¢¬Ä¬ù Collaborators in Multi-Party Collaboration">[Paper 242]</span> learns partner-awareness through prompt-based counterfactual construction, achieving 47% improvement on collaborative decision-making tasks.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">IV. Reasoning: Progress, Limitations, and Fundamental Questions</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">4.1 The Reinforcement Learning Revolution</h3>

<p><strong>RLVR (Reinforcement Learning with Verifiable Rewards) dominates mathematical and logical reasoning improvements</strong>. <span class="paper-ref" data-paper-id="95" data-tooltip="Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?">[Paper 95]</span> enables generalization beyond mathematics to diverse domains, achieving GPT-4o-level performance while being 12√ó faster. <span class="paper-ref" data-paper-id="167" data-tooltip="ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models">[Paper 167]</span> shows extended RL training (2000+ steps) genuinely expands capabilities beyond base models, achieving perfect performance on tasks where base models completely fail. <span class="paper-ref" data-paper-id="254" data-tooltip="Reinforcement Learning for Reasoning in Large Language Models with One Training Example">[Paper 254]</span> demonstrates one-shot RL can boost performance from 36% to 73.6% using just a single training example.</p>

<p>However, <strong>fundamental questions about what RL actually learns remain</strong>. <span class="paper-ref" data-paper-id="95" data-tooltip="Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?">[Paper 95]</span> challenges the assumption that RLVR develops genuinely new reasoning strategies, showing base models achieve higher pass@k at large k values, suggesting RL optimizes existing capacities rather than discovering novel reasoning patterns. <span class="paper-ref" data-paper-id="230" data-tooltip="On Reasoning Strength Planning in Large Reasoning Models">[Paper 230]</span> reveals RL activates latent reasoning by adjusting only ~4% of reasoning-critical tokens, with these adjustments transferable across model scales‚Äîsuggesting reasoning capabilities already exist from pretraining.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">4.2 Search, Planning, and World Models</h3>

<p><strong>Classical AI techniques make surprising comebacks</strong>. <span class="paper-ref" data-paper-id="20" data-tooltip="Classical Planning with LLM-Generated Heuristics: Challenging the State of the Art with Python Code">[Paper 20]</span> shows LLM-generated heuristics for classical planning outperform traditional approaches, solving 373/720 test tasks versus 243 for widely-used heuristics while generating reusable domain-level knowledge. <span class="paper-ref" data-paper-id="63" data-tooltip="WALL-E: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents">[Paper 63]</span> proves world model alignment without retraining through executable code constraints achieves 16-98% higher success rates than RL-based methods like PPO.</p>

<p><strong>Model-based planning struggles with long horizons</strong>: <span class="paper-ref" data-paper-id="14" data-tooltip="Wide-Horizon Thinking and Simulation-Based Evaluation for Real-World LLM Planning with Multifaceted Constraints">[Paper 14]</span> introduces "wide-horizon thinking" for parallel constraint consideration, achieving 5-40% improvements over sequential methods but revealing that current approaches fail when plans exceed certain complexity thresholds. <span class="paper-ref" data-paper-id="251" data-tooltip="DMWM: Dual-Mind World Model with Long-Term Imagination">[Paper 251]</span> shows dual-mind architectures combining neural dynamics with logical reasoning address error accumulation in long-term imagination, achieving 120% improvement on extended horizons.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">4.3 The Self-Verification Challenge</h3>

<p>Multiple papers reveal <strong>models struggle to self-verify their reasoning</strong>. <span class="paper-ref" data-paper-id="329" data-tooltip="Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards">[Paper 329]</span> exposes "superficial self-reflection" where models trained with traditional RL can generate correct answers but cannot reliably verify outputs, achieving only 26.8% verification accuracy without explicit verification training. <span class="paper-ref" data-paper-id="113" data-tooltip="Incentivizing LLMs to Self-Verify Their Answers">[Paper 113]</span> shows unified RL training of both generation and verification achieves 83.60% accuracy, but models still lag behind external verifiers.</p>

<p><strong>The calibration problem</strong> persists: <span class="paper-ref" data-paper-id="308" data-tooltip="Know What You Don&#39;t Know: Uncertainty Calibration of Process Reward Models">[Paper 308]</span> reveals state-of-the-art process reward models systematically overestimate success probabilities, requiring quantile regression calibration to achieve reliable confidence bounds. <span class="paper-ref" data-paper-id="165" data-tooltip="Scalable Best-of-N Selection for Large Language Models via Self-Certainty">[Paper 165]</span> demonstrates lightweight probes can match medium-sized LLM monitors for safety monitoring at million-fold lower computational cost, but generalization across domains remains limited.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">V. Tool Use, Code Generation, and Executable Reasoning</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">5.1 The Code-as-Reasoning Paradigm</h3>

<p><strong>Code generation emerges as a powerful reasoning modality</strong>: <span class="paper-ref" data-paper-id="24" data-tooltip="Agentic RL Scaling Law: Spontaneous Code Execution for Mathematical Problem Solving">[Paper 24]</span> demonstrates agents can spontaneously learn to use code execution tools through pure RL without supervised examples, achieving 52.3% on challenging math benchmarks. <span class="paper-ref" data-paper-id="182" data-tooltip="Teaching Language Models to Reason with Tools">[Paper 182]</span> shows teaching LLMs to strategically integrate tools at optimal reasoning points achieves 4-8% accuracy improvements while reducing token usage by 30-50%. <span class="paper-ref" data-paper-id="43" data-tooltip="Eliciting Reasoning in Language Models with Cognitive Tools">[Paper 43]</span> reveals cognitive tools (modular reasoning operations executed by the LLM itself) increase GPT-4's performance from 32% to 53% on AIME2024.</p>

<p>However, <strong>code reasoning faces severe robustness challenges</strong>: <span class="paper-ref" data-paper-id="275" data-tooltip="CodeCrash: Exposing LLM Fragility to Misleading Natural Language in Code Reasoning">[Paper 275]</span> exposes that LLMs suffer 23.2% performance degradation when code contains misleading comments or documentation, demonstrating over-reliance on textual cues rather than genuine execution tracing. <span class="paper-ref" data-paper-id="235" data-tooltip="Rethinking Verification for LLM Code Generation: From Generation to Testing">[Paper 235]</span> challenges existing benchmarks as insufficiently rigorous, showing that limited test coverage artificially inflates performance metrics while missing subtle bugs.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">5.2 Agentic Tool Selection and Learning</h3>

<p><strong>Tool learning benefits from reinforcement over supervision</strong>: <span class="paper-ref" data-paper-id="199" data-tooltip="ToolRL: Reward is All Tool Learning Needs">[Paper 199]</span> demonstrates RL-based tool learning outperforms supervised fine-tuning by 15%, with fine-grained reward decomposition providing richer learning signals than binary rewards. <span class="paper-ref" data-paper-id="125" data-tooltip="Iterative Tool Usage Exploration for Multimodal Agents via Step-wise Preference Tuning">[Paper 125]</span> enables agents to learn effective tool-usage strategies through completely autonomous exploration without pre-collected annotations, using step-level preference optimization.</p>

<p><strong>The instruction-following gap</strong> identified by <span class="paper-ref" data-paper-id="6" data-tooltip="AGENTIF: Benchmarking Large Language Models Instruction Following Ability in Agentic Scenarios">[Paper 6]</span> shows current LLMs perform poorly at following instructions in agentic contexts (below 50% on novel constraints), particularly struggling with complex constraint structures typical of real-world agent applications. <span class="paper-ref" data-paper-id="152" data-tooltip="Generalizing Verifiable Instruction Following">[Paper 152]</span> extends this finding to show models trained on existing benchmarks cannot generalize to novel constraint types, achieving below 50% despite 80%+ performance on training distributions.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">5.3 Planning with Executable Verification</h3>

<p><strong>Executable verification enables reliable evaluation</strong>: <span class="paper-ref" data-paper-id="11" data-tooltip="Self-Challenging Language Model Agents">[Paper 11]</span> introduces Code-as-Task formulation with automatic verification, achieving 2√ó performance improvement through self-challenging frameworks. <span class="paper-ref" data-paper-id="63" data-tooltip="WALL-E: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents">[Paper 63]</span> shows training-free world alignment through executable code rules outperforms expensive RL fine-tuning. <span class="paper-ref" data-paper-id="256" data-tooltip="Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning">[Paper 256]</span> combines symbolic verification with interactive exploration, improving success rates by 46.2% through exploratory code generation.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">VI. Safety, Security, and Alignment: Fundamental Tensions</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">6.1 The Attack Surface Expands</h3>

<p><strong>Agent systems introduce qualitatively new vulnerabilities</strong>. <span class="paper-ref" data-paper-id="7" data-tooltip="RiOSWorld: Benchmarking the Risk of Multimodal Computer-Use Agents">[Paper 7]</span> reveals 84.93% unsafe rates across 492 risky tasks in real VM environments, with 99.2% unsafe rate for phishing websites and 89.8% for induced text attacks‚Äîdemonstrating that safety alignment from dialogue scenarios doesn't transfer to autonomous computer-use environments. <span class="paper-ref" data-paper-id="8" data-tooltip="WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks">[Paper 8]</span> shows state-of-the-art web agents achieve intermediate attack success rates up to 86% despite low end-to-end goal completion (0-17%), revealing "security by incompetence" rather than robust defenses.</p>

<p><strong>Prompt injection remains effective despite defenses</strong>: <span class="paper-ref" data-paper-id="27" data-tooltip="Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools">[Paper 27]</span> introduces metadata manipulation attacks achieving 81-95% success rates that remain effective even under existing prompt-level defenses and structured tool-selection protocols. <span class="paper-ref" data-paper-id="228" data-tooltip="Memory Injection Attacks on LLM Agents via Query-Only Interaction">[Paper 228]</span> demonstrates query-only memory injection attacks achieving 98.2% injection success without privileged access, revealing critical supply-chain vulnerabilities. <span class="paper-ref" data-paper-id="124" data-tooltip="Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency">[Paper 124]</span> shows task concurrency can be exploited through word-level interleaving of harmful and benign content.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">6.2 Training-Time and Fine-Tuning Attacks</h3>

<p><strong>Fine-tuning emerges as a critical attack vector</strong>: <span class="paper-ref" data-paper-id="255" data-tooltip="Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs">[Paper 255]</span> demonstrates jailbreak attacks achieving 94.84% success rate using only 10 benign QA pairs, with 100% benign content making attacks undetectable by content moderation. <span class="paper-ref" data-paper-id="198" data-tooltip="Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler">[Paper 198]</span> introduces Bayesian data scheduling achieving 74.4% improvement in defending against harmful fine-tuning with 50%+ average improvement across diverse models.</p>

<p><strong>Backdoor attacks</strong> reveal deep vulnerabilities: <span class="paper-ref" data-paper-id="85" data-tooltip="BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks and Defenses on Large Language Models">[Paper 85]</span> provides the first comprehensive benchmark showing backdoor attacks remain feasible across various LLM architectures, with even low-success-rate backdoors significantly amplifying jailbreak vulnerabilities. <span class="paper-ref" data-paper-id="361" data-tooltip="BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization">[Paper 361]</span> demonstrates first systematic backdoor attacks on VLA models achieving near-100% success rates while maintaining clean task accuracy.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">6.3 Defense Mechanisms and Fundamental Limits</h3>

<p><strong>Promising defense directions emerge</strong>: <span class="paper-ref" data-paper-id="64" data-tooltip="Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach">[Paper 64]</span> achieves 31.6% safety improvement through strategic information acquisition using LLM-guided MCTS with only 2.7 queries on average. <span class="paper-ref" data-paper-id="108" data-tooltip="Reasoning as an Adaptive Defense for Safety">[Paper 108]</span> introduces TARS showing smaller reasoning-enabled models can be safer than larger models through adaptive test-time compute for safety evaluation. <span class="paper-ref" data-paper-id="123" data-tooltip="DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents">[Paper 123]</span> reduces attack success from 30.7% to 1.3% through dynamic privilege-based validation with memory stream isolation.</p>

<p>However, <strong>fundamental attribution and detection challenges persist</strong>: <span class="paper-ref" data-paper-id="66" data-tooltip="Predicting the Performance of Black-box Language Models with Follow-up Queries">[Paper 66]</span> shows predicting model correctness from internal activations achieves 0.95-0.96 AUROC but depends heavily on activation selection. <span class="paper-ref" data-paper-id="163" data-tooltip="Abstract Counterfactuals for Language Model Agents">[Paper 163]</span> introduces "abstract counterfactuals" recognizing token-level approaches are inadequate for agent safety evaluation. <span class="paper-ref" data-paper-id="330" data-tooltip="Best-of-N Jailbreaking">[Paper 330]</span> demonstrates Best-of-N jailbreaking achieves 89% ASR on GPT-4o by exploiting sampling randomness, revealing systematic biases that scaling defenses alone cannot address.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">VII. Memory, Context, and Knowledge Management</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">7.1 The Long-Context Challenge</h3>

<p><strong>Extended context reveals fundamental limitations</strong>: <span class="paper-ref" data-paper-id="58" data-tooltip="LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?">[Paper 58]</span> shows performance gaps persist despite expanded context windows, with degradation across lengthy passages indicating context size alone doesn't guarantee effective reasoning. <span class="paper-ref" data-paper-id="237" data-tooltip="Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning">[Paper 237]</span> demonstrates long-context capacity is foundational for reasoning ability, with models showing substantial improvements (85.04% to 88.70%) when context window is extended before reasoning fine-tuning.</p>

<p><strong>Memory architectures evolve toward structure</strong>: <span class="paper-ref" data-paper-id="166" data-tooltip="A-Mem: Agentic Memory for LLM Agents">[Paper 166]</span> introduces agentic memory systems where LLMs autonomously decide memory organization, achieving 80% improvement on multi-hop reasoning with 13√ó fewer tokens. <span class="paper-ref" data-paper-id="98" data-tooltip="3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model">[Paper 98]</span> uses dense 3D representations for episodic memory in embodied agents, achieving 37.6% success rate with 16.5% improvements over baselines. <span class="paper-ref" data-paper-id="195" data-tooltip="G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems">[Paper 195]</span> employs hierarchical graph architecture enabling agent-specific customization, achieving 20.89% increase in embodied action success.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">7.2 Retrieval and Context Integration</h3>

<p><strong>Retrieval-augmented approaches</strong> show mixed results: <span class="paper-ref" data-paper-id="22" data-tooltip="WebThinker: Empowering Large Reasoning Models with Deep Research Capability">[Paper 22]</span> reveals visual reasoning models achieve 60.7 percentage points improvement through world-aware planning narratives, while <span class="paper-ref" data-paper-id="88" data-tooltip="ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning">[Paper 88]</span> demonstrates search-augmented reasoning benefits from RL-learned search policies rather than heuristic triggers. <span class="paper-ref" data-paper-id="289" data-tooltip="Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers">[Paper 289]</span> achieves +7.8% exact match improvement through iterative self-incentivization where LLMs learn from search trajectories.</p>

<p>However, <span class="paper-ref" data-paper-id="335" data-tooltip="KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems">[Paper 335]</span> reveals KV-cache sharing across multi-agent contexts faces offset variance problems, with KVCOMM achieving up to 7.8√ó speedup through anchor-based online learning. <span class="paper-ref" data-paper-id="288" data-tooltip="LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions">[Paper 288]</span> introduces self-synthesis for long-context instructions achieving 10-13√ó token efficiency versus existing methods.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">VIII. Domain-Specific Applications and Transfer</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">8.1 Scientific and Technical Domains</h3>

<p><strong>Specialized domains reveal severe capability gaps</strong>: <span class="paper-ref" data-paper-id="138" data-tooltip="Scientists&#39; First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning">[Paper 138]</span> shows only 63.0% accuracy for best models on scientific reasoning versus 93% human performance, <span class="paper-ref" data-paper-id="260" data-tooltip="STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models">[Paper 260]</span> exposes that models achieve near-random accuracy on atmospheric science tasks, and <span class="paper-ref" data-paper-id="221" data-tooltip="PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models">[Paper 221]</span> demonstrates frontier models achieve only 36.9% on physics problems versus 61.9% human expert performance.</p>

<p><strong>Theorem proving and formal reasoning</strong> show promise: <span class="paper-ref" data-paper-id="176" data-tooltip="MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation">[Paper 176]</span> introduces step-wise theorem proving with multi-perspective search achieving state-of-the-art performance, while <span class="paper-ref" data-paper-id="140" data-tooltip="Logic.py: Bridging the Gap between LLMs and Constraint Solvers">[Paper 140]</span> bridges LLMs and constraint solvers through specialized DSLs achieving 91.4% accuracy on logic grid puzzles (65% absolute improvement).</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">8.2 Robotics and Embodied AI</h3>

<p><strong>Vision-language-action models face reality</strong>: <span class="paper-ref" data-paper-id="54" data-tooltip="InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction">[Paper 54]</span> achieves 35.3% success on OSWorld through modular architecture with specialized models for different stages, but <span class="paper-ref" data-paper-id="318" data-tooltip="LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for Scientific Embodied Agents">[Paper 318]</span> reveals models achieve >70% success on atomic actions with sharp degradation on long-horizon tasks and near-zero out-of-domain generalization. <span class="paper-ref" data-paper-id="284" data-tooltip="OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis">[Paper 284]</span> demonstrates unified VLM architecture for mobile manipulation achieves 97.85% decision-making success in simulation but faces substantial sim-to-real gaps.</p>

<p><strong>Spatial reasoning remains fundamental bottleneck</strong>: <span class="paper-ref" data-paper-id="187" data-tooltip="SURDS: Benchmarking Spatial Understanding and Reasoning in Driving Scenarios with Vision Language Models">[Paper 187]</span> shows models struggle dramatically with fine-grained spatial reasoning in driving scenarios despite safety-critical requirements, while <span class="paper-ref" data-paper-id="220" data-tooltip="Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models">[Paper 220]</span> introduces activation-based spatial reasoning achieving 88.5% on CV-Bench versus GPT-4o's 62.7%.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">8.3 Business and Production Systems</h3>

<p><strong>The deployment readiness gap yawns wide</strong>: <span class="paper-ref" data-paper-id="4" data-tooltip="TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks">[Paper 4]</span> shows success rates below 50% on realistic business workflows, <span class="paper-ref" data-paper-id="185" data-tooltip="Can Agent Fix Agent Issues?">[Paper 185]</span> demonstrates agents achieve only 0.67-4.67% resolution rates on agent system issues (versus 23-50% on traditional software), and <span class="paper-ref" data-paper-id="59" data-tooltip="Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence">[Paper 59]</span> reveals only 6.4% success on cooking tasks requiring physical-digital integration.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">IX. Model Efficiency, Architecture, and Training</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">9.1 The Efficiency Imperative</h3>

<p><strong>Computational waste pervades current systems</strong>: <span class="paper-ref" data-paper-id="170" data-tooltip="Thinkless: LLM Learns When to Think">[Paper 170]</span> identifies models use 2√ó necessary compute, <span class="paper-ref" data-paper-id="259" data-tooltip="Don√¢¬Ä¬ôt Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models">[Paper 259]</span> achieves 47% attention FLOPs reduction while improving accuracy, and <span class="paper-ref" data-paper-id="303" data-tooltip="Efficiently Scaling LLM Reasoning Programs with Certaindex">[Paper 303]</span> reveals 4.5√ó unnecessary token generation. <span class="paper-ref" data-paper-id="367" data-tooltip="Transcending Cost-Quality Tradeoff in Agent Serving via Session-Awareness">[Paper 367]</span> reduces costs by 83.5% through session-aware serving, while <span class="paper-ref" data-paper-id="172" data-tooltip="Fast Inference for Augmented Large Language Models">[Paper 172]</span> achieves 10√ó inference speedup through token-level routing between small and large models.</p>

<p><strong>Speculative approaches</strong> offer practical gains: <span class="paper-ref" data-paper-id="315" data-tooltip="EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test">[Paper 315]</span> achieves 6.5√ó speedup through training-time test enabling multi-step prediction alignment, <span class="paper-ref" data-paper-id="216" data-tooltip="SpecEM: Training-Free LLM Ensembling via Iterative Drafting, Verification, and Online Feedback">[Paper 216]</span> provides 15-40% sequence reduction through online compression-based tokenization, and <span class="paper-ref" data-paper-id="283" data-tooltip="zip2zip: Inference-Time Adaptive Tokenization via Online Compression">[Paper 283]</span> reduces token consumption by 48-70% in specialized domains through dynamic vocabulary adaptation.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">9.2 Scaling Laws and Predictability</h3>

<p><strong>Scaling behavior proves more nuanced than expected</strong>: <span class="paper-ref" data-paper-id="155" data-tooltip="Sloth: scaling laws for LLM skills to predict multi-benchmark performance across families">[Paper 155]</span> introduces latent skills framework showing reasoning scales primarily with model size while knowledge depends heavily on both size and training tokens. <span class="paper-ref" data-paper-id="281" data-tooltip="Predictable Scale (Part II) --- Farseer: A Refined Scaling Law in LLMs">[Paper 281]</span> provides refined scaling laws improving extrapolation accuracy by 433%, predicting optimal data-to-parameter ratios increase with compute budgets (contradicting Chinchilla's static recommendations).</p>

<p><strong>Architecture and training insights</strong>: <span class="paper-ref" data-paper-id="82" data-tooltip="Language Modeling by Language Models">[Paper 82]</span> demonstrates LLMs can autonomously discover competitive architectures through genetic programming and adversarial self-play, while <span class="paper-ref" data-paper-id="146" data-tooltip="Revolutionizing Training-Free NAS: Towards Efficient Automatic Proxy Discovery via Large Language Models">[Paper 146]</span> shows training-free NAS via LLM-guided discovery outperforms manually-designed proxies. <span class="paper-ref" data-paper-id="325" data-tooltip="Zero-Shot Performance Prediction for Probabilistic Scaling Laws">[Paper 325]</span> enables zero-shot learning curve prediction using multi-output Gaussian Processes, reducing scaling law determination costs substantially.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">9.3 Training Dynamics and Data Quality</h3>

<p><strong>Data quality trumps quantity</strong>: <span class="paper-ref" data-paper-id="269" data-tooltip="ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking">[Paper 269]</span> reduces performance gap to <2% while saving 70-90% annotation costs through human-LLM collaboration, <span class="paper-ref" data-paper-id="340" data-tooltip="SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning">[Paper 340]</span> reveals weakness-driven problem synthesis achieves 10% (7B) and 7.7% (32B) improvements through targeted self-improvement, and <span class="paper-ref" data-paper-id="297" data-tooltip="GRIP: A Graph-Based Reasoning Instruction Producer">[Paper 297]</span> generates 2.1M instructions from 7.5K seeds through graph-based systematic exploration.</p>

<p><strong>Training stability and optimization</strong> receive attention: <span class="paper-ref" data-paper-id="91" data-tooltip="DAPO: An Open-Source LLM Reinforcement Learning System at Scale">[Paper 91]</span> introduces decoupled clipping bounds and dynamic sampling addressing entropy collapse in large-scale RL, while <span class="paper-ref" data-paper-id="150" data-tooltip="AREAL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning">[Paper 150]</span> demonstrates asynchronous RL eliminates straggler delays achieving 2.77√ó speedup. <span class="paper-ref" data-paper-id="320" data-tooltip="Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning">[Paper 320]</span> reveals standard cross-entropy fine-tuning misaligns with test-time compute scaling, requiring direct coverage optimization.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">X. Emerging Directions and Future Challenges</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">10.1 Reasoning About Reasoning</h3>

<p><strong>Meta-level reasoning capabilities emerge</strong>: <span class="paper-ref" data-paper-id="21" data-tooltip="ReMA: Learning to Meta-Think for LLMs with Multi-agent Reinforcement Learning">[Paper 21]</span> introduces hierarchical decomposition with strategic meta-thinking agents showing improved generalization through learned collaboration. <span class="paper-ref" data-paper-id="328" data-tooltip="AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling">[Paper 328]</span> achieves 82.43% accuracy on Theory of Mind benchmarks through automated agent modeling, substantially outperforming reasoning models. <span class="paper-ref" data-paper-id="230" data-tooltip="On Reasoning Strength Planning in Large Reasoning Models">[Paper 230]</span> reveals models pre-plan reasoning strengths in activations, with directional vectors causally controlling reasoning allocation.</p>

<p><strong>Self-improvement without supervision</strong> shows promise: <span class="paper-ref" data-paper-id="224" data-tooltip="Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization">[Paper 224]</span> minimizes entropy in latent semantic space achieving 48.1% accuracy on MATH (from 30.7%) without labeled data, <span class="paper-ref" data-paper-id="231" data-tooltip="Absolute Zero: Reinforced Self-play Reasoning with Zero Data">[Paper 231]</span> achieves 50.4% combined code+math performance through self-play without expert examples, and <span class="paper-ref" data-paper-id="225" data-tooltip="Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning">[Paper 225]</span> uses trajectory consistency patterns for fully self-supervised learning.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">10.2 Multimodal Integration and Grounding</h3>

<p><strong>Vision-language reasoning</strong> advances slowly: <span class="paper-ref" data-paper-id="287" data-tooltip="MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM">[Paper 287]</span> introduces first metric for reasoning-induced hallucinations versus perception errors, revealing spatial hallucinations remain unaffected by scaling. <span class="paper-ref" data-paper-id="309" data-tooltip="VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception">[Paper 309]</span> demonstrates visual test-time scaling through iterative perception achieves 5%+ improvements, while <span class="paper-ref" data-paper-id="331" data-tooltip="Multi-step Visual Reasoning with Visual Tokens Scaling and Verification">[Paper 331]</span> enables models to dynamically generate visual information during reasoning achieving 6.9% improvements on compositional tasks.</p>

<p><strong>Grounding challenges persist</strong>: <span class="paper-ref" data-paper-id="301" data-tooltip="ESCA: Contextualizing Embodied Agents via Scene-Graph Generation">[Paper 301]</span> achieves spatial-temporal scene understanding through graph generation but requires 87K+ training videos. <span class="paper-ref" data-paper-id="341" data-tooltip="ChatVLA-2: Vision-Language-Action Model with Open-World Reasoning">[Paper 341]</span> preserves VLM capabilities during robotics fine-tuning achieving 82.7% on open-world reasoning versus 0% for baselines, demonstrating path toward genuine transfer.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">10.3 Unresolved Fundamental Questions</h3>

<p><strong>What is reasoning?</strong> <span class="paper-ref" data-paper-id="78" data-tooltip="The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity">[Paper 78]</span> exposes performance collapse beyond complexity thresholds suggesting fundamental computational limits. <span class="paper-ref" data-paper-id="365" data-tooltip="Thoughts Are All Over the Place: On the Underthinking of Long Reasoning Models">[Paper 365]</span> reveals longer reasoning chains can be exponentially better than ensembles for certain task structures (formal proof via computational complexity theory). <span class="paper-ref" data-paper-id="101" data-tooltip="Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study">[Paper 101]</span> challenges whether LLMs possess "general learning ability" versus task-specific memorization.</p>

<p><strong>The generalization puzzle</strong>: <span class="paper-ref" data-paper-id="141" data-tooltip="OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization">[Paper 141]</span> disaggregates mathematical generalization into exploratory, compositional, and transformative types, revealing brittleness across all three dimensions. <span class="paper-ref" data-paper-id="52" data-tooltip="TimE: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios">[Paper 52]</span> introduces time-based evaluation showing temporal retrieval correlates with all higher-level temporal reasoning capabilities. <span class="paper-ref" data-paper-id="236" data-tooltip="On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study">[Paper 236]</span> demonstrates training-free methods severely underperform on research-level sequential reasoning.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">XI. Critical Assessment and Future Directions</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">11.1 The Field's Maturation Crisis</h3>

<p>The conference reveals a field in transition from <strong>capability demonstration to scientific rigor</strong>. The proliferation of benchmarks (30+ papers), systematic evaluation critiques [Papers 34, 94, 178, 191], and reproducibility concerns [Papers 28, 72, 233] signal growing pains. <strong>Publication incentives misalign with progress</strong>: researchers chase benchmark metrics while real-world deployment remains distant [Papers 4, 74, 185].</p>

<p><strong>The contamination problem</strong> threatens scientific validity. With models trained on web-scale data inevitably including benchmark solutions, the field faces a fundamental measurement challenge. Dynamic benchmarks [Papers 72, 127, 218] and contamination-resistant evaluation [Papers 148, 212] offer partial solutions, but <strong>the cat-and-mouse game between dataset creation and model training continues</strong>.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">11.2 What Actually Works?</h3>

<p><strong>High-confidence findings from convergent evidence</strong>:</p>

<p>1. <strong>Reinforcement learning with verifiable rewards consistently improves reasoning</strong> across mathematics, coding, and logic [Papers 10, 24, 95, 167, 254] - but primarily activates latent capabilities rather than teaching new strategies [Papers 95, 230].</p>

<p>2. <strong>Test-time compute scaling helps but faces diminishing returns</strong> beyond problem-specific thresholds [Papers 78, 168, 170] - adaptive allocation based on difficulty is critical [Papers 13, 111, 179, 308].</p>

<p>3. <strong>Multi-agent coordination fails systematically due to social intelligence gaps</strong> [Papers 35, 96] - specialized communication architectures [Papers 29, 156] and trained orchestration <span class="paper-ref" data-paper-id="19" data-tooltip="Multi-Agent Collaboration via Evolving Orchestration">[Paper 19]</span> partially address this.</p>

<p>4. <strong>Current benchmarks systematically overestimate capabilities</strong> [Papers 2, 4, 7, 74] - contamination, distribution shift, and task simplification create false confidence.</p>

<p>5. <strong>Safety alignment doesn't transfer to agentic contexts</strong> [Papers 7, 8, 228] - new paradigms required for autonomous systems with tool access and environmental interaction.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">11.3 What Remains Uncertain?</h3>

<p><strong>Unresolved tensions</strong>:</p>

<p>- <strong>Does extended thinking actually improve reasoning or just redistribute errors?</strong> [Papers 78, 168, 365] provide contradictory evidence depending on task structure.</p>

<p>- <strong>Can multi-agent systems overcome social intelligence limitations through architecture alone</strong>, or do fundamental training paradigm shifts [Papers 35, 242] require new approaches?</p>

<p>- <strong>Will adaptive test-time compute</strong> [Papers 111, 179, 308] solve efficiency problems or merely shift waste to meta-decision overhead?</p>

<p>- <strong>Can evaluation keep pace with capability growth</strong>, or will contamination and benchmark gaming [Papers 127, 178, 218] permanently undermine progress measurement?</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">11.4 Overhyped Approaches</h3>

<p><strong>Critical skepticism warranted for</strong>:</p>

<p>1. <strong>Chain-of-thought as universal solution</strong> - [Papers 78, 168, 358] show it can hurt performance; task-appropriate reasoning matters more than uniform verbosity.</p>

<p>2. <strong>Multi-agent systems without coordination mechanisms</strong> - [Papers 35, 96] demonstrate systematic failures; throwing more agents at problems without addressing social intelligence gaps wastes resources.</p>

<p>3. <strong>Benchmark-driven development without deployment validation</strong> - [Papers 4, 74, 185] reveal massive capability gaps between benchmark and production performance.</p>

<p>4. <strong>Vision-language models as reasoning systems</strong> - [Papers 201, 279, 287] expose fundamental limitations in spatial reasoning and modality integration that architectural unification hasn't solved.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">XII. Practical Recommendations</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">12.1 For Researchers</h3>

<p><strong>Prioritize</strong>:<br>- Contamination-resistant evaluation using continuous benchmarks [Papers 72, 218] and out-of-distribution testing [Papers 127, 141]<br>- Task-appropriate reasoning strategies rather than uniform CoT application [Papers 111, 179, 358]<br>- Systematic ablation studies isolating architectural choices from training effects [Papers 3, 350]<br>- Multi-dimensional evaluation capturing generalization breadth [Papers 52, 141, 148]</p>

<p><strong>Avoid</strong>:<br>- Static benchmarks without contamination monitoring<br>- Claiming reasoning improvements without verifying latent capability activation [Papers 95, 230]<br>- Multi-agent systems without explicit coordination mechanisms [Papers 35, 156]<br>- Aggregate performance metrics obscuring critical failure modes [Papers 78, 287]</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">12.2 For Practitioners</h3>

<p><strong>Deploy with confidence</strong>:<br>- Reinforcement learning for domains with verifiable rewards [Papers 10, 24, 95, 167]<br>- Adaptive test-time compute with difficulty-aware allocation [Papers 13, 179, 308]<br>- Structured memory architectures for long-context applications [Papers 98, 166, 195]<br>- Session-aware serving for cost-efficient agent deployment <span class="paper-ref" data-paper-id="367" data-tooltip="Transcending Cost-Quality Tradeoff in Agent Serving via Session-Awareness">[Paper 367]</span></p>

<p><strong>Approach cautiously</strong>:<br>- Multi-agent systems without proven coordination (expect systematic failures [Papers 35, 96])<br>- Vision-language agents for spatial reasoning tasks (fundamental gaps remain [Papers 33, 201, 220])<br>- Autonomous agents with tool access in safety-critical contexts (alignment gaps persist [Papers 7, 8])<br>- Long-horizon planning without intermediate verification (error accumulation inevitable [Papers 14, 251])</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">12.3 For the Field</h3>

<p><strong>The path forward requires</strong>:</p>

<p>1. <strong>Evaluation reform</strong> - Abandon static benchmarks; embrace continuous evaluation <span class="paper-ref" data-paper-id="72" data-tooltip="SWE-bench Goes Live!">[Paper 72]</span>, contamination-resistant generation [Papers 127, 218], and multi-dimensional metrics [Papers 52, 141].</p>

<p>2. <strong>Architectural innovation beyond scaling</strong> - Papers demonstrate fundamental limits to test-time compute [Papers 78, 168], suggesting qualitative architectural changes needed rather than quantitative scaling.</p>

<p>3. <strong>Social intelligence as first-class problem</strong> - Multi-agent success requires addressing coordination fundamentally [Papers 35, 156, 242], not treating it as emergent from capability scaling.</p>

<p>4. <strong>Safety paradigms for agentic systems</strong> - Dialogue safety doesn't transfer [Papers 7, 8]; new frameworks needed for autonomous tool-using agents [Papers 123, 228, 264].</p>

<p>5. <strong>Bridging simulation and reality</strong> - The deployment gap [Papers 4, 59, 74] demands systematic sim-to-real transfer validation rather than assuming benchmark performance generalizes.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">Conclusion</h2>

<p>NeurIPS 2025 marks a <strong>critical inflection point</strong>: the field confronts uncomfortable truths about evaluation validity, capability generalization, and safety alignment while making genuine progress on reasoning, efficiency, and specialized applications. The conference reveals both <strong>extraordinary capability emergence</strong> (self-play learning <span class="paper-ref" data-paper-id="231" data-tooltip="Absolute Zero: Reinforced Self-play Reasoning with Zero Data">[Paper 231]</span>, spontaneous tool use <span class="paper-ref" data-paper-id="24" data-tooltip="Agentic RL Scaling Law: Spontaneous Code Execution for Mathematical Problem Solving">[Paper 24]</span>, autonomous architecture discovery <span class="paper-ref" data-paper-id="82" data-tooltip="Language Modeling by Language Models">[Paper 82]</span>) and <strong>sobering limitations</strong> (social intelligence gaps <span class="paper-ref" data-paper-id="35" data-tooltip="Large Language Models Miss the Multi-agent Mark">[Paper 35]</span>, spatial reasoning failures [Papers 201, 220], deployment readiness gaps [Papers 4, 74]).</p>

<p>The next phase demands <strong>scientific rigor over capability claims</strong>, <strong>deployment validation over benchmark optimization</strong>, and <strong>fundamental innovation over incremental scaling</strong>. Researchers equipped with insights from these 367 papers have unprecedented understanding of both what works and what remains fundamentally unsolved‚Äîthe critical question is whether incentive structures will enable the field to address root causes rather than pursuing incremental improvements on saturated benchmarks.</p>

<p>The research community stands at a crossroads: continue the benchmark-capability arms race or pivot toward generalizable, deployable, safe agent systems. The evidence from NeurIPS 2025 suggests only the latter path leads to genuine progress.</p>

<details style="margin-top: 40px; padding: 20px; background: #f8f9fa; border-radius: 8px;">
<summary style="cursor: pointer; font-weight: bold; font-size: 1.1em; color: #1c3664;">üìö Paper Reference Index (367 papers)</summary>
<div style="margin-top: 20px;">
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 1]</strong> Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents<br><small style="color: #666;">Score: 95 | Agent Benchmarking and Evaluation, Web and Computer-Use Agents, Vision-Language-Action Models</small> <a href="https://arxiv.org/pdf/2505.24878" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 2]</strong> REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites<br><small style="color: #666;">Score: 93 | Agent Benchmarking and Evaluation, Web and Computer-Use Agents</small> <a href="https://arxiv.org/pdf/2504.11543" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 3]</strong> How to Train Your LLM Web Agent: A Statistical Diagnosis<br><small style="color: #666;">Score: 92 | Agent Benchmarking and Evaluation, Web and Computer-Use Agents, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/0252f1f5697c0153740a7438e473e45964e85102.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 4]</strong> TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks<br><small style="color: #666;">Score: 92 | Agent Benchmarking and Evaluation, Web and Computer-Use Agents, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2412.14161" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 5]</strong> Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge<br><small style="color: #666;">Score: 92 | Agent Benchmarking and Evaluation, Web and Computer-Use Agents, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2506.21506" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 6]</strong> AGENTIF: Benchmarking Large Language Models Instruction Following Ability in Agentic Scenarios<br><small style="color: #666;">Score: 92 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 7]</strong> RiOSWorld: Benchmarking the Risk of Multimodal Computer-Use Agents<br><small style="color: #666;">Score: 91 | Agent Safety and Security, Agent Benchmarking and Evaluation, Web and Computer-Use Agents</small> <a href="https://openreview.net/pdf/11ac207fff4f7439cb345f2e79ef2a6fb5611910.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 8]</strong> WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks<br><small style="color: #666;">Score: 91 | Agent Safety and Security, Web and Computer-Use Agents, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2504.18575" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 9]</strong> T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning<br><small style="color: #666;">Score: 90 | Tool Use and Code Generation, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2505.16986" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 10]</strong> $\texttt{G1}$: Teaching LLMs to Reason on Graphs with Reinforcement Learning<br><small style="color: #666;">Score: 89 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/eb95dfb11f1b0c705f2e368d650f0af1afbef02b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 11]</strong> Self-Challenging Language Model Agents<br><small style="color: #666;">Score: 89 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/d7856a8a4e991dab71814a3f39f28fe17b6b93bd.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 12]</strong> AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems<br><small style="color: #666;">Score: 89 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Domain-Specific Agents and Applications</small> <a href="https://arxiv.org/pdf/2505.19623" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 13]</strong> AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks<br><small style="color: #666;">Score: 89 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/13ea5abf7135c2269571122e956e24009565de1f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 14]</strong> Wide-Horizon Thinking and Simulation-Based Evaluation for Real-World LLM Planning with Multifaceted Constraints<br><small style="color: #666;">Score: 88 | Planning and Decision Making, Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/4fe14c361104913e172e118fa05d358d3b030840.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 15]</strong> General-Reasoner: Advancing LLM Reasoning Across All Domains<br><small style="color: #666;">Score: 87 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/59ca2ebad16c9f4c1d622f0bcd9007d31e1874bc.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 16]</strong> MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?<br><small style="color: #666;">Score: 87 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2504.09702" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 17]</strong> AI-Researcher: Autonomous Scientific Innovation<br><small style="color: #666;">Score: 86 | Multi-Agent Systems and Collaboration, Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/a1c63cdd0495de94664b1513f7d95a3aedcb483a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 18]</strong> Bag of Tricks for Inference-time Computation of LLM Reasoning<br><small style="color: #666;">Score: 86 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2502.07191" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 19]</strong> Multi-Agent Collaboration via Evolving Orchestration<br><small style="color: #666;">Score: 85 | Multi-Agent Systems and Collaboration, Reinforcement Learning for LLMs, Planning and Decision Making</small> <a href="https://openreview.net/pdf/9727f658d788c52f49f12ae4b230baf4cf0d4007.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 20]</strong> Classical Planning with LLM-Generated Heuristics: Challenging the State of the Art with Python Code<br><small style="color: #666;">Score: 85 | Tool Use and Code Generation, World Models and Planning</small> <a href="https://openreview.net/pdf/924624fad0f2d9ea4637686b275593407c20b753.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 21]</strong> ReMA: Learning to Meta-Think for LLMs with Multi-agent Reinforcement Learning<br><small style="color: #666;">Score: 84 | Multi-Agent Systems and Collaboration, Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/4ef88667f9d6e59455e6805cd91df2e8e519f9d0.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 22]</strong> WebThinker: Empowering Large Reasoning Models with Deep Research Capability<br><small style="color: #666;">Score: 84 | Web and Computer-Use Agents, Reasoning and Test-Time Compute, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/4e3ecfb1260a3ed3a6f051cf994b3be14a8f904e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 23]</strong> LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language<br><small style="color: #666;">Score: 84 | Agent Benchmarking and Evaluation, World Models and Planning, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2510.05972" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 24]</strong> Agentic RL Scaling Law: Spontaneous Code Execution for Mathematical Problem Solving<br><small style="color: #666;">Score: 83 | Tool Use and Code Generation, Reasoning and Test-Time Compute, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/6d8a6cfbc785966d9381495a786eea81de48d681.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 25]</strong> Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia<br><small style="color: #666;">Score: 83 | Agent Benchmarking and Evaluation, Multi-Agent Systems and Collaboration</small> <a href="https://arxiv.org/pdf/2512.03318" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 26]</strong> Agentic Plan Caching: Test-Time Memory for Fast and Cost-Efficient LLM Agents<br><small style="color: #666;">Score: 83 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/2362f2d245503d9d209dde5a45fbfbf3fb6a5990.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 27]</strong> Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools<br><small style="color: #666;">Score: 82 | Agent Safety and Security, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/58fd047ac6ca0bee4f61d85fc98df7a5c5b55e31.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 28]</strong> The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements<br><small style="color: #666;">Score: 82 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2506.22419" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 29]</strong> AutoData: A Multi-Agent System for Open Web Data Collection<br><small style="color: #666;">Score: 81 | Multi-Agent Systems and Collaboration, Tool Use and Code Generation, Web and Computer-Use Agents</small> <a href="https://openreview.net/pdf/066a04337fe56fee38b21d8e6b2366fac00856f4.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 30]</strong> Large Language Models Think Too Fast To Explore Effectively<br><small style="color: #666;">Score: 81 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation, Planning and Decision Making</small> <a href="https://openreview.net/pdf/84905da6e0006182e87623ee1b532443c3ca840a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 31]</strong> WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch<br><small style="color: #666;">Score: 81 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Web and Computer-Use Agents</small> <a href="https://arxiv.org/pdf/2505.03733" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 32]</strong> MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research<br><small style="color: #666;">Score: 81 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2505.19955" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 33]</strong> LTD-Bench: Evaluating Large Language Models by Letting Them Draw<br><small style="color: #666;">Score: 81 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2511.02347" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 34]</strong> Establishing Best Practices in Building Rigorous Agentic Benchmarks<br><small style="color: #666;">Score: 81 | Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2507.02825" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 35]</strong> Large Language Models Miss the Multi-agent Mark<br><small style="color: #666;">Score: 80 | Multi-Agent Systems and Collaboration, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2505.21298" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 36]</strong> Factorio Learning Environment<br><small style="color: #666;">Score: 80 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Planning and Decision Making</small> <a href="https://arxiv.org/pdf/2503.09617" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 37]</strong> Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition<br><small style="color: #666;">Score: 80 | Agent Safety and Security, Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2507.20526" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 38]</strong> PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors<br><small style="color: #666;">Score: 79 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2507.15550" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 39]</strong> WebDancer: Towards Autonomous Information Seeking Agency<br><small style="color: #666;">Score: 79 | Web and Computer-Use Agents, Agent Benchmarking and Evaluation, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/7c886fbc63b09377d123254d93907b41820d72d7.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 40]</strong> LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models<br><small style="color: #666;">Score: 79 | Reinforcement Learning for LLMs, Planning and Decision Making, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/64ba671c860d8032b21824e28d00916c3e613964.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 41]</strong> OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding<br><small style="color: #666;">Score: 78 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2507.07984" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 42]</strong> Enigmata:  Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles<br><small style="color: #666;">Score: 78 | Reasoning and Test-Time Compute, Mathematical and Logical Reasoning, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/d16d55cfeff749792ae0b093a3f4c0123aa6c09f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 43]</strong> Eliciting Reasoning in Language Models with Cognitive Tools<br><small style="color: #666;">Score: 77 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/ea576c6461243d27a8102c9dfa554db950d66757.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 44]</strong> MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem<br><small style="color: #666;">Score: 77 | Tool Use and Code Generation, Mathematical and Logical Reasoning, Planning and Decision Making</small> <a href="https://openreview.net/pdf/e01a39be0de98c2f808394f7affa2bfb004c68a5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 45]</strong> AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents<br><small style="color: #666;">Score: 77 | Agent Safety and Security, Agent Benchmarking and Evaluation, Web and Computer-Use Agents</small> <a href="https://arxiv.org/pdf/2503.09780" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 46]</strong> APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay<br><small style="color: #666;">Score: 77 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2504.03601" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 47]</strong> SimWorld: An Open-ended Simulator for Agents in Physical and Social Worlds<br><small style="color: #666;">Score: 77 | Agent Benchmarking and Evaluation, Multi-Agent Systems and Collaboration, Planning and Decision Making</small> <a href="https://openreview.net/pdf/a98dee23d8b37552151336bbac20c838fd5e9ee1.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 48]</strong> Hogwild! Inference: Parallel LLM Generation via Concurrent Attention<br><small style="color: #666;">Score: 76 | Reasoning and Test-Time Compute, Multi-Agent Systems and Collaboration</small> <a href="https://openreview.net/pdf/752690b760cb01f226ba630228b97af81df4d1d1.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 49]</strong> Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective<br><small style="color: #666;">Score: 76 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2506.14965" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 50]</strong> Language Models can Self-Improve at State-Value Estimation for Better Search<br><small style="color: #666;">Score: 76 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation, Web and Computer-Use Agents</small> <a href="https://openreview.net/pdf/ea251516f78a77cbc42dd4a50e2ce4aadfc2226f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 51]</strong> OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation<br><small style="color: #666;">Score: 75 | Multi-Agent Systems and Collaboration, Reinforcement Learning for LLMs, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/9ea2d3d5cf7f874c7669ab5c3f1270eb3bc794d1.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 52]</strong> TimE: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios<br><small style="color: #666;">Score: 75 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2505.12891" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 53]</strong> Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL<br><small style="color: #666;">Score: 74 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/a27fd29bd9a99f5624d5362501cf5a6acc4c4e37.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 54]</strong> InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction<br><small style="color: #666;">Score: 74 | Web and Computer-Use Agents, Tool Use and Code Generation, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/ae7bb751a0d72057590b5907d9d4fa86c412be50.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 55]</strong> AgentAuditor: Human-level Safety and Security Evaluation for LLM Agents<br><small style="color: #666;">Score: 74 | Agent Safety and Security, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/2ceecf4bf50bffa7cff2d145fadb18c23daf7206.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 56]</strong> Collaborative Reasoner: Self-Improving Social Agents with Synthetic Conversations<br><small style="color: #666;">Score: 74 | Multi-Agent Systems and Collaboration, Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/42a338bca40ea896002753679729eb2240bf62b3.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 57]</strong> LogicTree: Improving Complex Reasoning of LLMs via Instantiated Multi-step Synthetic Logical Data<br><small style="color: #666;">Score: 73 | Reasoning and Test-Time Compute, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/30059572044fffae201f1e0daf92fc78a5aef218.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 58]</strong> LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?<br><small style="color: #666;">Score: 73 | Agent Benchmarking and Evaluation, Memory and Context Management</small> <a href="https://arxiv.org/pdf/2510.22548" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 59]</strong> Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence<br><small style="color: #666;">Score: 71 | Web and Computer-Use Agents, Agent Benchmarking and Evaluation, Vision-Language-Action Models</small> <a href="https://arxiv.org/pdf/2506.15677" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 60]</strong> ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code<br><small style="color: #666;">Score: 71 | Tool Use and Code Generation, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2506.02314" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 61]</strong> ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests<br><small style="color: #666;">Score: 71 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2506.04894" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 62]</strong> Wider or Deeper?  Scaling LLM Inference-Time Compute with Adaptive Branching Tree Search<br><small style="color: #666;">Score: 70 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/e6d2b8dcb02f4dadb940b53aefa65032db4fbd89.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 63]</strong> WALL-E: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents<br><small style="color: #666;">Score: 70 | World Models and Planning, Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/4a70c0605cac16a41fee061564452113f886243a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 64]</strong> Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach<br><small style="color: #666;">Score: 70 | Agent Safety and Security, Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/c98e7220e280f1d8bae43d92944840467237a1e7.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 65]</strong> Rollout Roulette: A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods<br><small style="color: #666;">Score: 70 | Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/7ee28149679cef6235a635039365773e8ff846ed.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 66]</strong> Predicting the Performance of Black-box Language Models with Follow-up Queries<br><small style="color: #666;">Score: 70 | Agent Benchmarking and Evaluation, Agent Safety and Security</small> <a href="https://openreview.net/pdf/a9530414a4fd5f326629f3402e2729d04dac678a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 67]</strong> RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning<br><small style="color: #666;">Score: 69 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/df2d115dd7fe7763e94fd35d45007ed71cbcebc7.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 68]</strong> RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for  Complex Task Solving<br><small style="color: #666;">Score: 69 | Tool Use and Code Generation, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/fea6d1106531a6d10e824159b158dcbffbc07cc2.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 69]</strong> SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks<br><small style="color: #666;">Score: 69 | Agent Benchmarking and Evaluation, Agent Safety and Security, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/66d2a1a241b7979f0f8776b51e62c6b1f1bc7db8.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 70]</strong> AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement<br><small style="color: #666;">Score: 69 | Multi-Agent Systems and Collaboration, Agent Safety and Security, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/237ca0099a7a95299cd0bae4b495038b19873e08.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 71]</strong> AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play<br><small style="color: #666;">Score: 69 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/d73f4b0dc08ba840cfc15d8e7e9be6ad4a9b52ce.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 72]</strong> SWE-bench Goes Live!<br><small style="color: #666;">Score: 68 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2505.23419" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 73]</strong> MM-OPERA: Benchmarking Open-ended Association Reasoning for Large Vision-Language Models<br><small style="color: #666;">Score: 68 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2510.26937" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 74]</strong> Measuring AI Ability to Complete Long Software Tasks<br><small style="color: #666;">Score: 68 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/402f09e70f9d9e6a99edbcaaa360692d05eec7ab.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 75]</strong> MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents<br><small style="color: #666;">Score: 68 | Agent Benchmarking and Evaluation, Spatial and Physical Reasoning, Planning and Decision Making</small> <a href="https://arxiv.org/pdf/2505.20148" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 76]</strong> CoP: Agentic Red-teaming for Large Language Models using Composition of Principles<br><small style="color: #666;">Score: 68 | Agent Safety and Security, Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/cb30447780175b5ce7f25d0e0277ddcc32156544.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 77]</strong> Meta-World+: An Improved, Standardized, RL Benchmark<br><small style="color: #666;">Score: 67 | Agent Benchmarking and Evaluation, Reinforcement Learning for LLMs</small> <a href="https://arxiv.org/pdf/2505.11289" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 78]</strong> The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity<br><small style="color: #666;">Score: 66 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/b58069a804c5fad686ceb13a131631201748c264.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 79]</strong> Thinking vs. Doing: Improving Agent Reasoning by  Scaling Test-Time Interaction<br><small style="color: #666;">Score: 66 | Reasoning and Test-Time Compute, Web and Computer-Use Agents, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/20cba89765ebda48a0c183f17ceb68d13f90d0dc.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 80]</strong> Can Large Language Models Master Complex Card Games?<br><small style="color: #666;">Score: 66 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Domain-Specific Agents and Applications</small> <a href="https://openreview.net/pdf/1d7991c179addeedf33ce77dc2ffd0e475f69b2c.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 81]</strong> Benchmarking Large Language Models with Integer Sequence Generation Tasks<br><small style="color: #666;">Score: 65 | Agent Benchmarking and Evaluation, Mathematical and Logical Reasoning</small> <a href="https://arxiv.org/pdf/2411.04372" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 82]</strong> Language Modeling by Language Models<br><small style="color: #666;">Score: 65 | Self-Improvement and Meta-Learning, Multi-Agent Systems and Collaboration</small> <a href="https://openreview.net/pdf/1dda401bc2d6f8ee17a263ac3f358eb51e094d8e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 83]</strong> Let the LLM Stick to Its Strengths: Learning to Route Economical LLM<br><small style="color: #666;">Score: 65 | Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/f8ce3f48c335fa784e0e6aef63e636d13c30d93d.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 84]</strong> AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?<br><small style="color: #666;">Score: 65 | Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2507.15887" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 85]</strong> BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks and Defenses on Large Language Models<br><small style="color: #666;">Score: 65 | Agent Safety and Security, Domain-Specific Agents and Applications</small> <a href="https://arxiv.org/pdf/2408.12798" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 86]</strong> DeepDiver: Adaptive Web-Search Intensity Scaling via Reinforcement Learning<br><small style="color: #666;">Score: 64 | Web and Computer-Use Agents, Reinforcement Learning for LLMs, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/4da779e56e6d170cf4fc07f2af1015f08b35314e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 87]</strong> OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents<br><small style="color: #666;">Score: 64 | Agent Safety and Security, Agent Benchmarking and Evaluation, Web and Computer-Use Agents</small> <a href="https://arxiv.org/pdf/2506.14866" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 88]</strong> ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning<br><small style="color: #666;">Score: 64 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/3a4c411411ec8827c21b081ac099f93878bb8269.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 89]</strong> VIKI√¢¬Ä¬ëR: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning<br><small style="color: #666;">Score: 64 | Multi-Agent Systems and Collaboration, Reinforcement Learning for LLMs, Vision-Language-Action Models</small> <a href="https://arxiv.org/pdf/2506.09049" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 90]</strong> AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model for Atmospheric Science<br><small style="color: #666;">Score: 64 | Agent Benchmarking and Evaluation, Domain-Specific Agents and Applications</small> <a href="https://arxiv.org/pdf/2502.01159" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 91]</strong> DAPO: An Open-Source LLM Reinforcement Learning System at Scale<br><small style="color: #666;">Score: 63 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/a5ca4684c1debe30e4fde4bd063a262d61e13db7.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 92]</strong> World-aware Planning Narratives Enhance Large Vision-Language Model Planner<br><small style="color: #666;">Score: 63 | Planning and Decision Making, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/c4dfdf9e91cc4527c4c879b52d8cc8d352f626ce.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 93]</strong> SeRL: Self-play Reinforcement Learning for Large Language Models with Limited Data<br><small style="color: #666;">Score: 62 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/f20577e67650da84d5c9396fc93fb971782ca925.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 94]</strong> BenchmarkCards: Standardized Documentation for Large Language Model Benchmarks<br><small style="color: #666;">Score: 61 | Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2410.12974" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 95]</strong> Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?<br><small style="color: #666;">Score: 61 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/c3957c2dc397dd6f7bf1e3da21cebaeca53844af.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 96]</strong> Why Do Multi-Agent LLM Systems Fail?<br><small style="color: #666;">Score: 61 | Multi-Agent Systems and Collaboration, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2503.13657" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 97]</strong> GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling<br><small style="color: #666;">Score: 60 | Agent Safety and Security, Multi-Agent Systems and Collaboration</small> <a href="https://openreview.net/pdf/12984819e947af4200e71748ef8d3b07f7b029cd.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 98]</strong> 3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model<br><small style="color: #666;">Score: 60 | Memory and Context Management, Vision-Language-Action Models, Spatial and Physical Reasoning</small> <a href="https://openreview.net/pdf/8e3474ff3b680185f73a397352845c1347b77d73.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 99]</strong> VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents<br><small style="color: #666;">Score: 59 | Vision-Language-Action Models, Planning and Decision Making, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/ed11882b04656406635941c99bb0b5dbf475a088.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 100]</strong> Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs<br><small style="color: #666;">Score: 59 | Agent Benchmarking and Evaluation, Domain-Specific Agents and Applications</small> <a href="https://openreview.net/pdf/13fb7451afdc5c8796e7166332feddd0c8a38f8e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 101]</strong> Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study<br><small style="color: #666;">Score: 59 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/0ced283865c2dd5717208f09588d27256b2fa260.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 102]</strong> Self-Guided Hierarchical Exploration for Generalist Foundation Model Web Agents<br><small style="color: #666;">Score: 59 | Web and Computer-Use Agents, Self-Improvement and Meta-Learning, Planning and Decision Making</small> <a href="https://openreview.net/pdf/23935fe967684b7bdc286739474d07c94da9cc76.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 103]</strong> Predicting Empirical AI Research Outcomes with Language Models<br><small style="color: #666;">Score: 58 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/149b9e435472a05ff9bc8f5c3138e681d73132fd.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 104]</strong> Generating Computational Cognitive models using Large Language Models<br><small style="color: #666;">Score: 58 | Tool Use and Code Generation, Domain-Specific Applications</small> <a href="https://openreview.net/pdf/71914784b067b8a89e5ba9e648c3f3fe0bef8659.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 105]</strong> AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration<br><small style="color: #666;">Score: 58 | Agent Safety and Security, Multi-Agent Systems and Collaboration, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/e53fb762e883d79a783bab07668988038d9e5162.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 106]</strong> ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models<br><small style="color: #666;">Score: 57 | Agent Benchmarking and Evaluation, Vision-Language-Action Models</small> <a href="https://arxiv.org/pdf/2505.13444" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 107]</strong> Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models<br><small style="color: #666;">Score: 57 | Agent Safety and Security, Vision-Language-Action Models, Multi-Agent Systems and Collaboration</small> <a href="https://openreview.net/pdf/8babb592735eb9e46455028c5428dfc816f08161.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 108]</strong> Reasoning as an Adaptive Defense for Safety<br><small style="color: #666;">Score: 57 | Agent Safety and Security, Reasoning and Test-Time Compute, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/c036ebd40ea82ab94b9badc0028faf7e94456ca1.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 109]</strong> Delving into Large Language Models for Effective Time-Series Anomaly Detection<br><small style="color: #666;">Score: 57 | Domain-Specific Agents and Applications, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/ff3e7df135cd4038ebbe31199752645c9946fa1e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 110]</strong> SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models<br><small style="color: #666;">Score: 57 | Reasoning and Test-Time Compute, Tool Use and Code Generation, World Models and Planning</small> <a href="https://openreview.net/pdf/2ac09a3b4dda169b90bb239fc45cb980a8bc3efd.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 111]</strong> AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking<br><small style="color: #666;">Score: 55 | Reasoning and Test-Time Compute, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/dd1128b101a8e529cfa219b76424da7861923e32.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 112]</strong> SiriuS: Self-improving Multi-agent Systems via Bootstrapped Reasoning<br><small style="color: #666;">Score: 55 | Multi-Agent Systems and Collaboration, Reasoning and Test-Time Compute, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/856c35f60eccff17ac8726de9ca5f7fbf9bcf3ee.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 113]</strong> Incentivizing LLMs to Self-Verify Their Answers<br><small style="color: #666;">Score: 55 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/ea6649a9cfb1c181a137632923e930e4e14e6ad3.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 114]</strong> Time Travel is Cheating: Going Live with DeepFund for Real-Time Fund Investment Benchmarking<br><small style="color: #666;">Score: 55 | Agent Benchmarking and Evaluation, Multi-Agent Systems and Collaboration, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2505.11065" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 115]</strong> MLZero: A Multi-Agent System for End-to-end Machine Learning Automation<br><small style="color: #666;">Score: 55 | Multi-Agent Systems and Collaboration, Tool Use and Code Generation, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/55f28109c8ee532fe1c950142c23f6efd636a79e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 116]</strong> Training Language Models to Reason Efficiently<br><small style="color: #666;">Score: 55 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/c169ad531f568624e1f7af8211b9ff6b12391b63.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 117]</strong> Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning<br><small style="color: #666;">Score: 55 | Tool Use and Code Generation, Reinforcement Learning for LLMs, Domain-Specific Agents and Applications</small> <a href="https://openreview.net/pdf/7b8e4f95272ab1e9e3900b30ef9647d058b6b034.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 118]</strong> PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments<br><small style="color: #666;">Score: 55 | Vision-Language-Action Models, Reasoning and Test-Time Compute, Planning and Decision Making</small> <a href="https://openreview.net/pdf/89b1018647a9f5681e3ad0ed184e80229e3997ee.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 119]</strong> LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?<br><small style="color: #666;">Score: 53 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2506.11928" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 120]</strong> RvLLM: LLM Runtime Verification with Domain Knowledge<br><small style="color: #666;">Score: 53 | Agent Safety and Security, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/e99cb11cda9beb3092445ecb739b48abb0369ac9.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 121]</strong> BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems<br><small style="color: #666;">Score: 53 | Agent Benchmarking and Evaluation, Agent Safety and Security, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2505.15216" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 122]</strong> Lookahead Routing for Large Language Models<br><small style="color: #666;">Score: 53 | Model Efficiency and Optimization, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/552fece3eff509f39e0f54fafd8aafc36248c8a8.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 123]</strong> DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents<br><small style="color: #666;">Score: 53 | Agent Safety and Security, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/7e05f767d463d43be3b045378b14be5760ea2fc1.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 124]</strong> Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency<br><small style="color: #666;">Score: 52 | Agent Safety and Security, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/7e13bf73f0b1b45f44c038b630279494b0220174.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 125]</strong> Iterative Tool Usage Exploration for Multimodal Agents via Step-wise Preference Tuning<br><small style="color: #666;">Score: 52 | Tool Use and Code Generation, Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/a3ae43bcdfe712b2361e4ab5254bbce2bcc0dd95.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 126]</strong> DyFlow: Dynamic Workflow Framework for Agentic Reasoning<br><small style="color: #666;">Score: 52 | Reasoning and Test-Time Compute, Multi-Agent Systems and Collaboration, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/705f64f765b412ab6e17c0dc9c9146763c3e63fe.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 127]</strong> MathArena: Evaluating LLMs on Uncontaminated Math Competitions<br><small style="color: #666;">Score: 52 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2505.23281" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 128]</strong> SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond<br><small style="color: #666;">Score: 51 | Reasoning and Test-Time Compute, Mathematical and Logical Reasoning, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/b5b93e7b533135e2ad1d72d22e3a481324c2a73d.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 129]</strong> Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies<br><small style="color: #666;">Score: 51 | Reasoning and Test-Time Compute, Multi-Agent Systems and Collaboration, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/5d59ea40926886752f5ab100ab83a383587e3e1e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 130]</strong> L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models<br><small style="color: #666;">Score: 51 | Model Efficiency and Optimization, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/03edf649a9f42cdf6d921cb59599e7130120540a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 131]</strong> AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench<br><small style="color: #666;">Score: 51 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/a22e34894b1de174131d79168e9a32bf5fefd3a5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 132]</strong> SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines<br><small style="color: #666;">Score: 51 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2502.14739" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 133]</strong> CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections<br><small style="color: #666;">Score: 51 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/bcb703c11665128fb39f34dfd53237d4a3431b28.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 134]</strong> PoE-World: Compositional World Modeling with Products of Programmatic Experts<br><small style="color: #666;">Score: 50 | World Models and Planning, Reasoning and Test-Time Compute, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/1983f2216c20adc421975e0092eb41f2ac1d93fa.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 135]</strong> Web-Shepherd: Advancing PRMs for Reinforcing Web Agents<br><small style="color: #666;">Score: 49 | Web and Computer-Use Agents, Agent Benchmarking and Evaluation, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/db46564195dad40b9b174514d7d03b0336d2a8eb.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 136]</strong> RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation<br><small style="color: #666;">Score: 49 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Planning and Decision Making</small> <a href="https://arxiv.org/pdf/2506.06677" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 137]</strong> MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering<br><small style="color: #666;">Score: 49 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2505.07782" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 138]</strong> Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning<br><small style="color: #666;">Score: 49 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2506.10521" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 139]</strong> ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints<br><small style="color: #666;">Score: 49 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/2bc8b445588504ed12b491f7cc33f033191ae2ad.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 140]</strong> Logic.py: Bridging the Gap between LLMs and Constraint Solvers<br><small style="color: #666;">Score: 49 | Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/a811f4bc76e7ad2fcf67bc0ce62afd3123512b8d.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 141]</strong> OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization<br><small style="color: #666;">Score: 49 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://arxiv.org/pdf/2506.18880" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 142]</strong> TTRL: Test-Time Reinforcement Learning<br><small style="color: #666;">Score: 48 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/3ff432e912f7ed9bbbacf9a7a16d7e5af88d721a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 143]</strong> Atom of Thoughts for Markov LLM Test-Time Scaling<br><small style="color: #666;">Score: 48 | Reasoning and Test-Time Compute, Planning and Decision Making, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/aee7e5e0ac85edb676698e634deccc28c92e1407.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 144]</strong> OmniBench: Towards The Future of  Universal Omni-Language Models<br><small style="color: #666;">Score: 48 | Agent Benchmarking and Evaluation, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/c52922a7cb62ce2d159c41efd473976ed5a5693f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 145]</strong> MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models<br><small style="color: #666;">Score: 48 | Agent Benchmarking and Evaluation, Vision-Language-Action Models</small> <a href="https://arxiv.org/pdf/2306.13394" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 146]</strong> Revolutionizing Training-Free NAS: Towards Efficient Automatic Proxy Discovery via Large Language Models<br><small style="color: #666;">Score: 48 | Self-Improvement and Meta-Learning, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/f5967a55faa0ccebb78581f66e96dad4d59eb767.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 147]</strong> EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths<br><small style="color: #666;">Score: 48 | Tool Use and Code Generation, Planning and Decision Making, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/ec254672c2c5013801b6522a08e51c829a7ef814.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 148]</strong> RealMath: A Continuous Benchmark for Evaluating Language Models on Research-Level Mathematics<br><small style="color: #666;">Score: 47 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2505.12575" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 149]</strong> Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning<br><small style="color: #666;">Score: 47 | Reasoning and Test-Time Compute, Self-Improvement and Meta-Learning, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/a85740fc42f5b0086b031316724925f3d39daa30.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 150]</strong> AREAL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning<br><small style="color: #666;">Score: 47 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/1b2829a8cfd93ca52bca9bf2c38c826016159024.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 151]</strong> RLVR-World: Training World Models with Reinforcement Learning<br><small style="color: #666;">Score: 46 | Reinforcement Learning for LLMs, Planning and Decision Making, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/4b0d2935ed4554453743ecdcf099e0d679355328.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 152]</strong> Generalizing Verifiable Instruction Following<br><small style="color: #666;">Score: 46 | Agent Benchmarking and Evaluation, Reinforcement Learning for LLMs, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2507.02833" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 153]</strong> A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning<br><small style="color: #666;">Score: 46 | Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/73a0607365582aedefc2167e27fb239c96092223.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 154]</strong> SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents<br><small style="color: #666;">Score: 46 | Reasoning and Test-Time Compute, Self-Improvement and Meta-Learning, Planning and Decision Making</small> <a href="https://openreview.net/pdf/814cd78232da3150c1f91b29920f4f2e4d70fb3c.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 155]</strong> Sloth: scaling laws for LLM skills to predict multi-benchmark performance across families<br><small style="color: #666;">Score: 45 | Agent Benchmarking and Evaluation, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/82fec2c6ee0cec1161d232d81bfad1d63de8fd54.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 156]</strong> AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems<br><small style="color: #666;">Score: 45 | Multi-Agent Systems and Collaboration, Tool Use and Code Generation, Memory and Context Management</small> <a href="https://openreview.net/pdf/38bdf6e7191adba7391b1fde1ad37e27887b2bac.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 157]</strong> DynaAct: Large Language Model Reasoning with Dynamic Action Spaces<br><small style="color: #666;">Score: 45 | Planning and Decision Making, Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/45820087f0186edb228a8b251bb2af4a3cca38b5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 158]</strong> GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining<br><small style="color: #666;">Score: 45 | Tool Use and Code Generation, Agent Benchmarking and Evaluation, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/d184d257f33dec6f932171111d977423bd83f8ef.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 159]</strong> Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation<br><small style="color: #666;">Score: 45 | Agent Benchmarking and Evaluation, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2503.05493" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 160]</strong> TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning<br><small style="color: #666;">Score: 45 | Model Efficiency and Optimization, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/1608428ae182161621e0d5c0fae3d6b608a7acaa.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 161]</strong> Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation<br><small style="color: #666;">Score: 44 | Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/5f50a250befd3553dd40112c1a440f86b36737da.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 162]</strong> Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing<br><small style="color: #666;">Score: 44 | Spatial and Physical Reasoning, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/0da039d9be67a74526e2ce4408db4cde8bdca6f5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 163]</strong> Abstract Counterfactuals for Language Model Agents<br><small style="color: #666;">Score: 43 | Agent Benchmarking and Evaluation, Agent Safety and Security</small> <a href="https://openreview.net/pdf/c39308d32f99302b4eb8a97c6d7eae5b2e2c4466.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 164]</strong> OpenCUA: Open Foundations for Computer-Use Agents<br><small style="color: #666;">Score: 43 | Web and Computer-Use Agents, Agent Benchmarking and Evaluation, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/eb1bd0238abbc386303352dba1049a4d5d1fec83.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 165]</strong> Scalable Best-of-N Selection for Large Language Models via Self-Certainty<br><small style="color: #666;">Score: 43 | Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/141a8955a8007083b8b3068692459c467c444619.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 166]</strong> A-Mem: Agentic Memory for LLM Agents<br><small style="color: #666;">Score: 42 | Memory and Context Management, Planning and Decision Making, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/518e506d098a6c821c7e4ae97d1368f374fddac9.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 167]</strong> ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models<br><small style="color: #666;">Score: 42 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/75e49a843f0b6d00c0584a776fad3bea93496c83.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 168]</strong> Does Thinking More Always Help? Mirage of Test-Time Scaling in Reasoning Models<br><small style="color: #666;">Score: 42 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/9a4ddca48299d0fb623da4e9a0093d29392e48a2.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 169]</strong> Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost<br><small style="color: #666;">Score: 42 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/496a103d9eacba8143b7d8a13098936a351a1a81.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 170]</strong> Thinkless: LLM Learns When to Think<br><small style="color: #666;">Score: 41 | Reasoning and Test-Time Compute, Model Efficiency and Optimization, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/69d5f7c0fe7ed54d42a4a1908d50a94bc062b721.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 171]</strong> Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL<br><small style="color: #666;">Score: 41 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/dc89a6b0221240f7e120c7bd39c116d57711eec4.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 172]</strong> Fast Inference for Augmented Large Language Models<br><small style="color: #666;">Score: 40 | Tool Use and Code Generation, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/466ae7c6d881581805533176114b2bde5a683e8d.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 173]</strong> Distilling LLM Agent into Small Models with Retrieval and Code Tools<br><small style="color: #666;">Score: 40 | Tool Use and Code Generation, Model Efficiency and Optimization, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/c3ead64f6c3fd03156d79bf7aa5185204700b2a2.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 174]</strong> Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks<br><small style="color: #666;">Score: 40 | Planning and Decision Making, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/05d0804df1c396da814a29970ae7d37c40a1b84e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 175]</strong> Scaling Up Active Testing to Large Language Models<br><small style="color: #666;">Score: 39 | Agent Benchmarking and Evaluation, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/0e4fdfd137ed152835e93d1c72c71b4217f5dfa0.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 176]</strong> MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation<br><small style="color: #666;">Score: 38 | Mathematical and Logical Reasoning, Reasoning and Test-Time Compute, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/350dc187c2833efc7cd93402a27fd709f790c12b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 177]</strong> Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model<br><small style="color: #666;">Score: 38 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/64ee56c0512b904e0aebcd799494fb96e7bdedba.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 178]</strong> Neither Valid nor Reliable? Investigating the Use of LLMs as Judges<br><small style="color: #666;">Score: 37 | Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2508.18076" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 179]</strong> Think Only When You Need with Large Hybrid-Reasoning Models<br><small style="color: #666;">Score: 37 | Reasoning and Test-Time Compute, Model Efficiency and Optimization, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/33fb886d1cda050a0c29d0bdee85176c1c3f7f31.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 180]</strong> Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees<br><small style="color: #666;">Score: 37 | Agent Benchmarking and Evaluation, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/42cf5e1bbaeccd15c151a00a71cb2d9ecef2aa6f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 181]</strong> A Controllable Examination for Long-Context Language Models<br><small style="color: #666;">Score: 37 | Agent Benchmarking and Evaluation, Memory and Context Management</small> <a href="https://arxiv.org/pdf/2506.02921" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 182]</strong> Teaching Language Models to Reason with Tools<br><small style="color: #666;">Score: 37 | Tool Use and Code Generation, Mathematical and Logical Reasoning, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/406d2cc67059647e62684368005578debbc188e9.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 183]</strong> SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts<br><small style="color: #666;">Score: 37 | Agent Safety and Security, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2505.21828" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 184]</strong> DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios<br><small style="color: #666;">Score: 36 | Agent Safety and Security, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2510.15501" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 185]</strong> Can Agent Fix Agent Issues?<br><small style="color: #666;">Score: 36 | Agent Benchmarking and Evaluation, Self-Improvement and Meta-Learning, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/ab4c234adba0d835dbb64828e87b465d8983cecd.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 186]</strong> Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs<br><small style="color: #666;">Score: 36 | Agent Benchmarking and Evaluation, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2507.11932" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 187]</strong> SURDS: Benchmarking Spatial Understanding and Reasoning in Driving Scenarios with Vision Language Models<br><small style="color: #666;">Score: 35 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2411.13112" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 188]</strong> Reward Reasoning Models<br><small style="color: #666;">Score: 35 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/1b460f75d98849e58bc2a9cf1b6673536d0898d5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 189]</strong> PolyGuard: Massive Multi-Domain Safety Policy-Grounded Guardrail Dataset<br><small style="color: #666;">Score: 35 | Agent Safety and Security, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2506.19054" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 190]</strong> Chain of Execution Supervision Promotes General Reasoning in Large Language Models<br><small style="color: #666;">Score: 35 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/81876043b2c12b524b3d7cebe4d6ef2b4a940143.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 191]</strong> Measuring what Matters: Construct Validity in Large Language Model Benchmarks<br><small style="color: #666;">Score: 35 | Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2511.04703" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 192]</strong> MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants<br><small style="color: #666;">Score: 35 | Agent Benchmarking and Evaluation, Memory and Context Management</small> <a href="https://openreview.net/pdf/96e1b7b1eeb53a530580aff14cf9527fe3e3d1ac.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 193]</strong> EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving<br><small style="color: #666;">Score: 35 | Agent Benchmarking and Evaluation, Self-Improvement and Meta-Learning, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/78058118e972a34b3ab22aa5b6b0c000ab083f58.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 194]</strong> RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video<br><small style="color: #666;">Score: 35 | Agent Benchmarking and Evaluation, Vision-Language-Action Models</small> <a href="https://arxiv.org/pdf/2505.02064" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 195]</strong> G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems<br><small style="color: #666;">Score: 34 | Multi-Agent Systems and Collaboration, Memory and Context Management</small> <a href="https://openreview.net/pdf/52f961783a3212459f228b4ec297f523ba2d0c95.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 196]</strong> Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning<br><small style="color: #666;">Score: 34 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute, Multi-Agent Systems and Collaboration</small> <a href="https://openreview.net/pdf/363710ed9012131f837da723473810ad47f9d2c9.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 197]</strong> LIFEBENCH: Evaluating Length Instruction Following in Large Language Models<br><small style="color: #666;">Score: 34 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2505.16234" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 198]</strong> Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler<br><small style="color: #666;">Score: 34 | Agent Safety and Security, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/2b29a4e872717270eff659096ceb805ef86a1735.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 199]</strong> ToolRL: Reward is All Tool Learning Needs<br><small style="color: #666;">Score: 33 | Tool Use and Code Generation, Reinforcement Learning for LLMs, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/097ae4a34c2eb2b82b2bb8fccc279fb0e3585304.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 200]</strong> OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles<br><small style="color: #666;">Score: 33 | Reasoning and Test-Time Compute, Vision-Language-Action Models, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/62ceb097c643e0416c764c187ebf4f4d6d1ba9c3.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 201]</strong> VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs<br><small style="color: #666;">Score: 33 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/ca79a743ac55c7fce5e100be3b2d695bf84a71ba.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 202]</strong> LLM-PySC2: Starcraft II learning environment for Large Language Models<br><small style="color: #666;">Score: 32 | Agent Benchmarking and Evaluation, Multi-Agent Systems and Collaboration, Planning and Decision Making</small> <a href="https://openreview.net/pdf/ae23b5a1cbb9529359c184cdf3d394c6383add5a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 203]</strong> GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments<br><small style="color: #666;">Score: 32 | Multi-Agent Systems and Collaboration, Tool Use and Code Generation, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/e15247fbacf803da9beef26d39153bd1afd55fe6.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 204]</strong> InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback<br><small style="color: #666;">Score: 32 | Reinforcement Learning for LLMs, Memory and Context Management</small> <a href="https://arxiv.org/pdf/2505.23950" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 205]</strong> Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs<br><small style="color: #666;">Score: 31 | Agent Benchmarking and Evaluation, Planning and Decision Making, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/01546f97c01b32e5c0cf560fc4be7a511b46e042.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 206]</strong> Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models<br><small style="color: #666;">Score: 31 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/bc154619c4195b6a62774c122f1706e4d7b1bb7f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 207]</strong> Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning<br><small style="color: #666;">Score: 31 | Reinforcement Learning for LLMs, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/109c600393cc962e64028e8425eca62778f40ee9.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 208]</strong> ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning<br><small style="color: #666;">Score: 31 | Vision-Language-Action Models, Planning and Decision Making, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/b35b0fc70612e191baced400f754db8ff1fae711.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 209]</strong> QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?<br><small style="color: #666;">Score: 30 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Planning and Decision Making</small> <a href="https://arxiv.org/pdf/2503.22674" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 210]</strong> SeePhys:  Does Seeing Help Thinking? √¢¬Ä¬ì Benchmarking Vision-Based Physics Reasoning<br><small style="color: #666;">Score: 30 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2505.19099" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 211]</strong> The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense<br><small style="color: #666;">Score: 30 | Agent Safety and Security, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/85f3e38bd08668ee901051237edc01d1b8d8823e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 212]</strong> Introducing FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark<br><small style="color: #666;">Score: 30 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2502.19676" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 213]</strong> Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve<br><small style="color: #666;">Score: 29 | Multi-Agent Systems and Collaboration, Tool Use and Code Generation, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/11bc550a62e76a702248f346f36018dce79624fb.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 214]</strong> SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search<br><small style="color: #666;">Score: 29 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/b7f1cf26530dd86e2ae6d7c4a3a2f77a1c33b13b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 215]</strong> MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly<br><small style="color: #666;">Score: 29 | Agent Benchmarking and Evaluation, Vision-Language-Action Models</small> <a href="https://arxiv.org/pdf/2505.10610" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 216]</strong> SpecEM: Training-Free LLM Ensembling via Iterative Drafting, Verification, and Online Feedback<br><small style="color: #666;">Score: 28 | Model Efficiency and Optimization, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/ce36072cf898e279341e1ca31c51626ddead6950.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 217]</strong> SATURN: SAT-based Reinforcement Learning to Unleash LLMs Reasoning<br><small style="color: #666;">Score: 28 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/8c8fdd51e03779f1c757cd48a07127240f2339d4.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 218]</strong> ThinkBench: Dynamic Out-of-Distribution Evaluation for Robust LLM Reasoning<br><small style="color: #666;">Score: 28 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://arxiv.org/pdf/2502.16268" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 219]</strong> VERA: Variational Inference Framework for Jailbreaking Large Language Models<br><small style="color: #666;">Score: 28 | Agent Safety and Security, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/5d8395fee02384e70cded7bf691df4c9976ebb5a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 220]</strong> Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models<br><small style="color: #666;">Score: 28 | Spatial and Physical Reasoning, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/1321dbcd7c95604245189132069ca5ac16d3058c.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 221]</strong> PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models<br><small style="color: #666;">Score: 28 | Agent Benchmarking and Evaluation, Mathematical and Logical Reasoning, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2504.16074" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 222]</strong> Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling<br><small style="color: #666;">Score: 28 | Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/639ca4d7460b732c0c9d399142939d60bcdb29d2.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 223]</strong> RAST: Reasoning Activation in LLMs via Small-model Transfer<br><small style="color: #666;">Score: 28 | Reasoning and Test-Time Compute, Model Efficiency and Optimization, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/4e1cf538a0cee20ba69772f7ede39e11ffddd493.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 224]</strong> Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization<br><small style="color: #666;">Score: 28 | Reasoning and Test-Time Compute, Self-Improvement and Meta-Learning, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/4beac7313c00cacdbfe88ef717756d49edfe75a1.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 225]</strong> Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning<br><small style="color: #666;">Score: 28 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/4c696fcdaaa5b6e8b53a1bf9e94c8993ee0cd433.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 226]</strong> WorldModelBench: Judging Video Generation Models As World Models<br><small style="color: #666;">Score: 27 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2502.20694" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 227]</strong> Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs<br><small style="color: #666;">Score: 27 | Agent Safety and Security</small> <a href="https://openreview.net/pdf/106f56012866f9c2a8f8d433881447430b77c9df.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 228]</strong> Memory Injection Attacks on LLM Agents via Query-Only Interaction<br><small style="color: #666;">Score: 27 | Agent Safety and Security, Memory and Context Management</small> <a href="https://openreview.net/pdf/0fa0fce8cc51e3ae2f36e5aef29236b15bcc35bc.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 229]</strong> World Models Should Prioritize the Unification of Physical and Social Dynamics<br><small style="color: #666;">Score: 26 | Multi-Agent Systems and Collaboration, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2510.21219" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 230]</strong> On Reasoning Strength Planning in Large Reasoning Models<br><small style="color: #666;">Score: 26 | Reasoning and Test-Time Compute, Model Efficiency and Optimization, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/ec0170f131842f1aaee5993a19df22764b470213.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 231]</strong> Absolute Zero: Reinforced Self-play Reasoning with Zero Data<br><small style="color: #666;">Score: 26 | Self-Improvement and Meta-Learning, Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/34a1a72a0a54db41248d9ad8862c78e55ac789d9.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 232]</strong> MetaDefense: Defending Fine-tuning based Jailbreak Attack Before and During Generation<br><small style="color: #666;">Score: 25 | Agent Safety and Security, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/006cba1cf9e75c1f5fafd3bea5ee62d71f204085.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 233]</strong> SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents<br><small style="color: #666;">Score: 25 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2505.20411" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 234]</strong> Common Task Framework For a Critical Evaluation of Scientific Machine Learning Algorithms<br><small style="color: #666;">Score: 25 | Agent Benchmarking and Evaluation, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2510.23166" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 235]</strong> Rethinking Verification for LLM Code Generation: From Generation to Testing<br><small style="color: #666;">Score: 25 | Tool Use and Code Generation, Agent Benchmarking and Evaluation, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/11dbcf6567a9c2d7cfb82938a0ec215c232e6c80.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 236]</strong> On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study<br><small style="color: #666;">Score: 24 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation, Planning and Decision Making</small> <a href="https://openreview.net/pdf/57860c28b21d95678533bc618a0afee1c2a54e47.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 237]</strong> Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning<br><small style="color: #666;">Score: 24 | Reasoning and Test-Time Compute, Memory and Context Management</small> <a href="https://openreview.net/pdf/3aa73c81186850fe236acaa4a81d2af3f1e5c910.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 238]</strong> Let Me Think! A Long Chain of Thought Can Be Worth Exponentially Many Short Ones<br><small style="color: #666;">Score: 24 | Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/3522982c33f5d5199bd558f48909b1bccbf81615.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 239]</strong> FlySearch: Exploring how vision-language models explore<br><small style="color: #666;">Score: 24 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Planning and Decision Making</small> <a href="https://arxiv.org/pdf/2506.02896" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 240]</strong> PARCO: Parallel AutoRegressive Models for Multi-Agent Combinatorial Optimization<br><small style="color: #666;">Score: 23 | Multi-Agent Systems and Collaboration, Planning and Decision Making, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/672bb4ede45dba28f133a8ebb2cfe40fb1f395c8.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 241]</strong> Partner Modelling Emerges in Recurrent Agents (But Only When It Matters)<br><small style="color: #666;">Score: 23 | Multi-Agent Systems and Collaboration, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/ce9e0e5f6bb446a8103ef0fb8f4c67e47b7972d0.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 242]</strong> Learning √¢¬Ä¬úPartner-Aware√¢¬Ä¬ù Collaborators in Multi-Party Collaboration<br><small style="color: #666;">Score: 23 | Multi-Agent Systems and Collaboration, Reinforcement Learning for LLMs, Agent Safety and Security</small> <a href="https://openreview.net/pdf/40d31b0852235d5e98c6f82c41aa70978247a0dd.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 243]</strong> LASeR: Learning to Adaptively Select Reward Models with Multi-Arm Bandits<br><small style="color: #666;">Score: 23 | Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/bf589cfd8356e7e428426d438835ed093caa2e02.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 244]</strong> Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs<br><small style="color: #666;">Score: 23 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/e7ed5c9a866ddc4710624ed2df37d91bce6455d3.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 245]</strong> A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning<br><small style="color: #666;">Score: 23 | Mathematical and Logical Reasoning, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/edfcc94d23a1796e8903435652cc311e00009492.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 246]</strong> REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving<br><small style="color: #666;">Score: 23 | Tool Use and Code Generation, Model Efficiency and Optimization, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/3e257cd601e943e872f777fd88fbaf2a64c6c6ea.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 247]</strong> Reasoning Planning for Language Models<br><small style="color: #666;">Score: 23 | Reasoning and Test-Time Compute, Planning and Decision Making, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/4e2c8f0df4fed5d40292835b8448bea21d28648f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 248]</strong> The World Is Bigger: A Computationally-Embedded Perspective on the Big World Hypothesis<br><small style="color: #666;">Score: 22 | Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/463a93594b9e688fecce37543cf3bf45e7f91310.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 249]</strong> IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering<br><small style="color: #666;">Score: 22 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2506.23329" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 250]</strong> Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs<br><small style="color: #666;">Score: 22 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/de9fc962acefe20dd0d80073eadeb19263afeb06.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 251]</strong> DMWM: Dual-Mind World Model with Long-Term Imagination<br><small style="color: #666;">Score: 21 | Planning and Decision Making, Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/46a97d474ce914d7520303484925b64a33af1b9b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 252]</strong> Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models<br><small style="color: #666;">Score: 21 | Vision-Language-Action Models, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2505.23757" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 253]</strong> Lifelong Safety Alignment for Language Models<br><small style="color: #666;">Score: 21 | Agent Safety and Security, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/8b599899e65f157c99b0a6ac3e8b45afbc4fee1a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 254]</strong> Reinforcement Learning for Reasoning in Large Language Models with One Training Example<br><small style="color: #666;">Score: 21 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/6488aab2ea1a4b2423d232a17c9fcd1545659f8c.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 255]</strong> Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs<br><small style="color: #666;">Score: 20 | Agent Safety and Security, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/2a60bb227d356283cc8b6eddcfeda9a0ef466282.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 256]</strong> Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning<br><small style="color: #666;">Score: 20 | Tool Use and Code Generation, Planning and Decision Making, Spatial and Physical Reasoning</small> <a href="https://openreview.net/pdf/8a135640e90e68e7dd192021ba6a8fdff76f596f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 257]</strong> WritingBench: A Comprehensive Benchmark for Generative Writing<br><small style="color: #666;">Score: 19 | Agent Benchmarking and Evaluation, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2503.05244" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 258]</strong> MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning<br><small style="color: #666;">Score: 19 | Multi-Agent Systems and Collaboration, Planning and Decision Making, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/8e363493ff9d9b3fa837f8d6bb3198fa13ba65f4.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 259]</strong> Don√¢¬Ä¬ôt Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models<br><small style="color: #666;">Score: 19 | Reasoning and Test-Time Compute, Model Efficiency and Optimization, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/1b9aa688f3d241f228fbd9b8694bdffb938d1d5f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 260]</strong> STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models<br><small style="color: #666;">Score: 19 | Agent Benchmarking and Evaluation, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/ce44a9975c0bc8b6ace090a98fc92829bdf5fece.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 261]</strong> SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning<br><small style="color: #666;">Score: 19 | Reasoning and Test-Time Compute, Self-Improvement and Meta-Learning, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/37589115324975c5657483b37546cdd4916aeeed.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 262]</strong> Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges<br><small style="color: #666;">Score: 19 | Agent Benchmarking and Evaluation, Spatial and Physical Reasoning, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2505.11618" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 263]</strong> ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering<br><small style="color: #666;">Score: 19 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2506.09050" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 264]</strong> Contextual Integrity in LLMs via Reasoning and Reinforcement Learning<br><small style="color: #666;">Score: 19 | Agent Safety and Security, Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/b39de1061902f7e2d23191a93a6d94f945bff28e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 265]</strong> MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Cultural Learning<br><small style="color: #666;">Score: 19 | Multi-Agent Systems and Collaboration, Reasoning and Test-Time Compute, Memory and Context Management</small> <a href="https://openreview.net/pdf/aaff34bc5f2395383d58c98d38975df26262971c.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 266]</strong> SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents<br><small style="color: #666;">Score: 19 | Self-Improvement and Meta-Learning, Reinforcement Learning for LLMs, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/d934e1858b206a0ba8fe1fb5281ee9f117238785.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 267]</strong> Detecting High-Stakes Interactions with Activation Probes<br><small style="color: #666;">Score: 18 | Agent Safety and Security, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/6291cf20df7b3579af7a0739c773db78424d18cd.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 268]</strong> LLM Generated Persona is a Promise with a Catch<br><small style="color: #666;">Score: 18 | Agent Benchmarking and Evaluation, Agent Safety and Security</small> <a href="https://arxiv.org/pdf/2503.16527" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 269]</strong> ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking<br><small style="color: #666;">Score: 18 | Agent Benchmarking and Evaluation, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/2d4818decec8bd80bc1421d0101a796c541e0194.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 270]</strong> MindJourney: Test-Time Scaling with World Models for Spatial Reasoning<br><small style="color: #666;">Score: 17 | Reasoning and Test-Time Compute, Spatial and Physical Reasoning, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/5ed386610c8657ff319e5833b8272c6459dd85a4.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 271]</strong> Once Upon an Input: Reasoning via Per-Instance Program Synthesis<br><small style="color: #666;">Score: 17 | Tool Use and Code Generation, Reasoning and Test-Time Compute, Planning and Decision Making</small> <a href="https://openreview.net/pdf/7b4a6ca902e7228c2855e6b864e4699825b54723.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 272]</strong> Can We Infer Confidential Properties of Training Data from LLMs?<br><small style="color: #666;">Score: 17 | Agent Safety and Security, Domain-Specific Applications</small> <a href="https://openreview.net/pdf/cd0fb86716237f2897625fe1354d6af3d7b0ec5b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 273]</strong> Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs<br><small style="color: #666;">Score: 17 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Agent Safety and Security</small> <a href="https://openreview.net/pdf/33aae90c6ae97c2a878758d797cf8196f7ca09de.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 274]</strong> From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction<br><small style="color: #666;">Score: 16 | Planning and Decision Making, Vision-Language-Action Models, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/dbe78cc8fdf28b0340af74010ec4ad766aca831b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 275]</strong> CodeCrash: Exposing LLM Fragility to Misleading Natural Language in Code Reasoning<br><small style="color: #666;">Score: 16 | Tool Use and Code Generation, Agent Safety and Security, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/9b9883a392dca607cfb2b26eccc24136b0e5cf90.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 276]</strong> Learning and Planning Multi-Agent Tasks via an MoE-based World Model<br><small style="color: #666;">Score: 16 | Multi-Agent Systems and Collaboration, Planning and Decision Making, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/09e9ae881c16219816d4e101be5c5634163ae0b6.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 277]</strong> MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection<br><small style="color: #666;">Score: 15 | Vision-Language-Action Models, Domain-Specific Applications</small> <a href="https://openreview.net/pdf/12eb1e4189682fe7b8223e53327f9e2ada20bd59.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 278]</strong> MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks<br><small style="color: #666;">Score: 15 | Agent Benchmarking and Evaluation, Multi-Agent Systems and Collaboration, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2505.12371" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 279]</strong> Caption This, Reason That: VLMs Caught in the Middle<br><small style="color: #666;">Score: 14 | Vision-Language-Action Models, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/756a05a421ed180f3aedfcaa7a55a73f50e83d30.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 280]</strong> Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders<br><small style="color: #666;">Score: 14 | Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/8983a85d88c67728d26c47fa75cdce08379b3561.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 281]</strong> Predictable Scale (Part II) --- Farseer: A Refined Scaling Law in LLMs<br><small style="color: #666;">Score: 14 | Model Efficiency and Optimization, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/b2d2dae1fc87cdd4510e8c2672fcf585d1c653f2.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 282]</strong> Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models<br><small style="color: #666;">Score: 14 | Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/2b62626f959a636707dfb61efe094e4dcd7bd32f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 283]</strong> zip2zip: Inference-Time Adaptive Tokenization via Online Compression<br><small style="color: #666;">Score: 14 | Model Efficiency and Optimization, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/0f10815a6be342f3a8137fe6510088e422bd3a6b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 284]</strong> OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis<br><small style="color: #666;">Score: 14 | Vision-Language-Action Models, Tool Use and Code Generation, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/b83bcc6b13bf3bed81ebb73be9bae7cc2be710e7.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 285]</strong> Temporal Logic-Based Multi-Vehicle Backdoor Attacks against Offline RL Agents in End-to-end Autonomous Driving<br><small style="color: #666;">Score: 14 | Agent Safety and Security, Reinforcement Learning for LLMs, Planning and Decision Making</small> <a href="https://openreview.net/pdf/26313d9a67de648ba12a7315cbcaa87c2787a4f5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 286]</strong> SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models for Time Series<br><small style="color: #666;">Score: 14 | Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2510.20273" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 287]</strong> MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM<br><small style="color: #666;">Score: 14 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/5c8396fbd51956694870ead9c44afcff4e5253cb.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 288]</strong> LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions<br><small style="color: #666;">Score: 14 | Memory and Context Management, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/ed87797f943163aff0337ae4824c26f2e347ea6a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 289]</strong> Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers<br><small style="color: #666;">Score: 14 | Web and Computer-Use Agents, Self-Improvement and Meta-Learning, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/f6244ed69adb4a0f1c375503deb1e23c39a8ac15.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 290]</strong> RADAR: Benchmarking Language Models on Imperfect Tabular Data<br><small style="color: #666;">Score: 13 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2506.08249" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 291]</strong> Bootstrap Off-policy with World Model<br><small style="color: #666;">Score: 13 | Planning and Decision Making, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/41b2f866d682b3c56d82ba0f291b84901efb52d3.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 292]</strong> The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement<br><small style="color: #666;">Score: 13 | Self-Improvement and Meta-Learning, Planning and Decision Making, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/ae3bf83043c88b4e28668b095624b19cc07ed197.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 293]</strong> Imagined Autocurricula<br><small style="color: #666;">Score: 13 | Self-Improvement and Meta-Learning, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/3facb7ccf15168534e07f4990a5bd009d1afea45.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 294]</strong> Reverse Engineering Human Preferences with Reinforcement Learning<br><small style="color: #666;">Score: 12 | Agent Safety and Security, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/e7f264f17bfbaac2f1357558dddf2cceae14cd6e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 295]</strong> SciArena: An Open Evaluation Platform for Non-Verifiable Scientific Literature-Grounded Tasks<br><small style="color: #666;">Score: 12 | Agent Benchmarking and Evaluation, Domain-Specific Applications</small></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 296]</strong> Lost in Transmission: When and Why LLMs Fail to Reason Globally<br><small style="color: #666;">Score: 12 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/93d0795dfa82fdff6b6b121fce6307ed161915ba.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 297]</strong> GRIP: A Graph-Based Reasoning Instruction Producer<br><small style="color: #666;">Score: 12 | Reasoning and Test-Time Compute, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/9ef2967c43fa97f6b4cfdddaf146abf156250794.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 298]</strong> Generalizing Experience for Language Agents with Hierarchical MetaFlows<br><small style="color: #666;">Score: 12 | Memory and Context Management, Self-Improvement and Meta-Learning, Planning and Decision Making</small> <a href="https://openreview.net/pdf/e5e7cd3c074c77adae516f8d392136a8fe8657f5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 299]</strong> PlanU: Large Language Model Reasoning through Planning under Uncertainty<br><small style="color: #666;">Score: 12 | Planning and Decision Making, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/a9771087543396ca7e59296ca5d3d68429e5a708.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 300]</strong> LILO: Learning to Reason at the Frontier of Learnability<br><small style="color: #666;">Score: 12 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/81c8ea42739bd8be2077f7a518eeedc3ad8b7d5c.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 301]</strong> ESCA: Contextualizing Embodied Agents via Scene-Graph Generation<br><small style="color: #666;">Score: 11 | Vision-Language-Action Models, Planning and Decision Making, Spatial and Physical Reasoning</small> <a href="https://openreview.net/pdf/32e384d1b7bff0817b05af22b3999df7789e5b8d.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 302]</strong> ML4CO-Bench-101: Benchmark Machine Learning for Classic Combinatorial Problems on Graphs<br><small style="color: #666;">Score: 11 | Agent Benchmarking and Evaluation, Planning and Decision Making</small></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 303]</strong> Efficiently Scaling LLM Reasoning Programs with Certaindex<br><small style="color: #666;">Score: 11 | Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/4ca50f6ea693aeda7b95d22f1929d6a5d49cf4ff.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 304]</strong> SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning<br><small style="color: #666;">Score: 11 | Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/7c62b5e4703772251c504c1cdc203fa7a96cd873.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 305]</strong> Dependency Matters: Enhancing LLM Reasoning with Explicit Knowledge Grounding<br><small style="color: #666;">Score: 11 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Agent Safety and Security</small> <a href="https://openreview.net/pdf/88669d564fe12a96458ad3ff7025be1f74506a21.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 306]</strong> ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents<br><small style="color: #666;">Score: 11 | Planning and Decision Making, Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/311841e491eca81acc49d5288503e9b7819788d6.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 307]</strong> Retro-R1: LLM-based Agentic Retrosynthesis<br><small style="color: #666;">Score: 11 | Tool Use and Code Generation, Reinforcement Learning for LLMs, Domain-Specific Applications</small> <a href="https://openreview.net/pdf/470a5ef177e20c474a9e358c1bab0c1cf082f1de.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 308]</strong> Know What You Don't Know: Uncertainty Calibration of Process Reward Models<br><small style="color: #666;">Score: 11 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/18533f919a9403854c7955628ad3488bf682b694.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 309]</strong> VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception<br><small style="color: #666;">Score: 11 | Reasoning and Test-Time Compute, Vision-Language-Action Models, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/7cf4479fc77f583f1cdede042be97389be035f9a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 310]</strong> Steering When Necessary: Flexible Steering Large Language Models with Backtracking<br><small style="color: #666;">Score: 10 | Agent Safety and Security, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/795a175c3b3b1e9de873069086e754159a4e855b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 311]</strong> Reasoning Models Hallucinate More: Factuality-Aware Reinforcement Learning for Large Reasoning Models<br><small style="color: #666;">Score: 10 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Agent Safety and Security</small> <a href="https://openreview.net/pdf/31f57f113b7a0a8d53b00020bbcdabe6ac8a82cf.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 312]</strong> GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning<br><small style="color: #666;">Score: 10 | Web and Computer-Use Agents, Reinforcement Learning for LLMs, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/ce35fb684e3b11b9c0f1fcc38598cfb3504c728e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 313]</strong> On Evaluating LLM Alignment by Evaluating LLMs as Judges<br><small style="color: #666;">Score: 10 | Agent Benchmarking and Evaluation, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/cefd5dffe2958e9dbfba77cc4764ee04a8cc5a95.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 314]</strong> Scaling RL to Long Videos<br><small style="color: #666;">Score: 10 | Reinforcement Learning for LLMs, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/086f9eed5c2d342130b7d5c3c1f80a2cc8f3594f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 315]</strong> EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test<br><small style="color: #666;">Score: 9 | Model Efficiency and Optimization, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/28c4c8cf58b0086a2136d73f6059ada87ac33e53.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 316]</strong> How Benchmark Prediction from Fewer Data Misses the Mark<br><small style="color: #666;">Score: 9 | Agent Benchmarking and Evaluation, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/a4097f48740ba588b5ffe5a4fd3f7d88b8eb0a70.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 317]</strong> SpecMAS: A Multi-Agent System for Self-Verifying System Generation via Formal Model Checking<br><small style="color: #666;">Score: 9 | Multi-Agent Systems and Collaboration, Tool Use and Code Generation, Agent Safety and Security</small> <a href="https://openreview.net/pdf/d2baf726e03709dc05beda305bd6ede14d1a9b1b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 318]</strong> LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for Scientific Embodied Agents<br><small style="color: #666;">Score: 9 | Agent Benchmarking and Evaluation, Domain-Specific Applications, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2505.22634" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 319]</strong> R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing<br><small style="color: #666;">Score: 9 | Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/14de671242c3654992d79919aba5d312e05f7347.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 320]</strong> Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning<br><small style="color: #666;">Score: 9 | Reasoning and Test-Time Compute, Mathematical and Logical Reasoning, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/d5b025da459325ca88dbc0f0f8c3dc0c23384640.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 321]</strong> Automated Model Discovery via Multi-modal & Multi-step Pipeline<br><small style="color: #666;">Score: 9 | Tool Use and Code Generation, Self-Improvement and Meta-Learning, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/9677940f4812871bd634d12cccadc3598687a9ac.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 322]</strong> Inference-Time Reward Hacking in Large Language Models<br><small style="color: #666;">Score: 8 | Reinforcement Learning for LLMs, Agent Safety and Security, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/8d477ae3e38043ed738ea2aa59e110eec3f49a44.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 323]</strong> AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning<br><small style="color: #666;">Score: 8 | Vision-Language-Action Models, Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/ac3e0d2216d650bf65be2b1559d68dc79c32c6ed.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 324]</strong> CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs<br><small style="color: #666;">Score: 8 | Agent Safety and Security, Multi-Agent Systems and Collaboration, Web and Computer-Use Agents</small> <a href="https://openreview.net/pdf/5b9c63b9b600fd293798deb51960a50373bb0faf.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 325]</strong> Zero-Shot Performance Prediction for Probabilistic Scaling Laws<br><small style="color: #666;">Score: 8 | Model Efficiency and Optimization, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/5656a3e0c168ce620e714e942c21cc42662e5dcc.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 326]</strong> STSBench: A Spatio-temporal Scenario Benchmark for Multi-modal Large Language Models in Autonomous Driving<br><small style="color: #666;">Score: 7 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2506.06218" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 327]</strong> Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs<br><small style="color: #666;">Score: 7 | Agent Benchmarking and Evaluation, Self-Improvement and Meta-Learning, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2510.16062" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 328]</strong> AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling<br><small style="color: #666;">Score: 7 | Multi-Agent Systems and Collaboration, Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/97d508cdb6040e0326e1f3e82a7473020de10424.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 329]</strong> Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards<br><small style="color: #666;">Score: 7 | Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/f28db2a8d244c8994006bd065afbd5a061c42feb.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 330]</strong> Best-of-N Jailbreaking<br><small style="color: #666;">Score: 7 | Agent Safety and Security, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/214b0cbe5fe5a3a56ddfd1977e1acfb9c721c50a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 331]</strong> Multi-step Visual Reasoning with Visual Tokens Scaling and Verification<br><small style="color: #666;">Score: 7 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/a48656c1e2d8fb5cee742d21b8b328458f483707.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 332]</strong> Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models<br><small style="color: #666;">Score: 7 | Planning and Decision Making, Reinforcement Learning for LLMs, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/5233ea1b8598dfd1eed771e087de919731ced5a1.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 333]</strong> R$^2$ec: Towards Large Recommender Models with Reasoning<br><small style="color: #666;">Score: 6 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/f29b278eccadd4b83c9cca979b6b35476c14e3a8.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 334]</strong> Triplets Better Than Pairs: Towards Stable and Effective Self-Play Fine-Tuning for LLMs<br><small style="color: #666;">Score: 6 | Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/e0596e6e39072aed73b22d2c2c86772c2aae52a0.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 335]</strong> KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems<br><small style="color: #666;">Score: 5 | Multi-Agent Systems and Collaboration, Memory and Context Management, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/81561154949bf17e7f12ee6dc0485c10a2415686.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 336]</strong> Representation Consistency for Accurate and Coherent LLM Answer Aggregation<br><small style="color: #666;">Score: 5 | Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/a3ecdca64c27eaba285b6683681268faca747be5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 337]</strong> Optimizing Anytime Reasoning via Budget Relative Policy Optimization<br><small style="color: #666;">Score: 5 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/8136e4668a09f8c47a2454d9e72728d4fdea055e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 338]</strong> Checklists Are Better Than Reward Models For Aligning Language Models<br><small style="color: #666;">Score: 5 | Reinforcement Learning for LLMs, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/490597cf8f353f8b01b8474e2f98c045eba8f5f4.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 339]</strong> See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model<br><small style="color: #666;">Score: 5 | Vision-Language-Action Models, Spatial and Physical Reasoning, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/bcef03fe8da8c459269e00eeb6626d6584a0ba4c.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 340]</strong> SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning<br><small style="color: #666;">Score: 5 | Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/c3403842de341324f63358f1732f1518761661f2.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 341]</strong> ChatVLA-2: Vision-Language-Action Model with Open-World Reasoning<br><small style="color: #666;">Score: 5 | Vision-Language-Action Models, Reasoning and Test-Time Compute, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/c88d737915ea445cb600d21cb0c7125912b7053b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 342]</strong> Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab<br><small style="color: #666;">Score: 4 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2507.02083" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 343]</strong> MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark<br><small style="color: #666;">Score: 4 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2506.05587" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 344]</strong> MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?<br><small style="color: #666;">Score: 4 | Reasoning and Test-Time Compute, Self-Improvement and Meta-Learning</small> <a href="https://arxiv.org/pdf/2503.09499" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 345]</strong> Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models<br><small style="color: #666;">Score: 4 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/eec1920da8921f9ebab8a70513b7531b0b8281d3.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 346]</strong> Janus-Pro-R1: Advancing Collaborative Visual Comprehension and Generation via Reinforcement Learning<br><small style="color: #666;">Score: 3 | Reinforcement Learning for LLMs, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/4aad34acb1fe54ce3d709604ec7d81ff8ce11b11.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 347]</strong> Dynamic Risk Assessments for Offensive Cybersecurity Agents<br><small style="color: #666;">Score: 3 | Agent Safety and Security, Agent Benchmarking and Evaluation, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2505.18384" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 348]</strong> KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation<br><small style="color: #666;">Score: 3 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/a3f1d4df6324abb0ea9576e5fe2da1d467238283.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 349]</strong> Among Us: A Sandbox for Measuring and Detecting Agentic Deception<br><small style="color: #666;">Score: 3 | Agent Benchmarking and Evaluation, Agent Safety and Security, Multi-Agent Systems and Collaboration</small> <a href="https://openreview.net/pdf/0ff014da0c71915ceb0a84e3977c85eaf1e134dd.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 350]</strong> VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models<br><small style="color: #666;">Score: 3 | Vision-Language-Action Models, Planning and Decision Making, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/05a810d8dce16f520e115b9ee80b8096e6512276.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 351]</strong> ContextAgent: Context-Aware Proactive LLM Agents with Open-world Sensory Perceptions<br><small style="color: #666;">Score: 3 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Memory and Context Management</small> <a href="https://openreview.net/pdf/8c61939b607693d9b13cc1df27793d844f3648f5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 352]</strong> R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization<br><small style="color: #666;">Score: 2 | Multi-Agent Systems and Collaboration, Domain-Specific Applications, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2505.15155" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 353]</strong> NavBench: Probing Multimodal Large Language Models for Embodied Navigation<br><small style="color: #666;">Score: 2 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Spatial and Physical Reasoning</small> <a href="https://openreview.net/pdf/1ef1a313c6a3eea3eea8cfe4ac568866df673dec.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 354]</strong> SpatialLM: Training Large Language Models for Structured Indoor Modeling<br><small style="color: #666;">Score: 2 | Vision-Language-Action Models, Spatial and Physical Reasoning, Domain-Specific Applications</small> <a href="https://openreview.net/pdf/22f930c6b44c852e1c53aa7784df786168735e5d.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 355]</strong> BraVE: Offline Reinforcement Learning for Discrete Combinatorial Action Spaces<br><small style="color: #666;">Score: 2 | Planning and Decision Making, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/fb6e5db6d0511de2cbf926cf309aa9e0fbe40245.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 356]</strong> Scaling Physical Reasoning with the PHYSICS Dataset<br><small style="color: #666;">Score: 1 | Reasoning and Test-Time Compute, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2506.00022" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 357]</strong> Validating LLM-as-a-Judge Systems under Rating Indeterminacy<br><small style="color: #666;">Score: 1 | Agent Benchmarking and Evaluation, Agent Safety and Security</small> <a href="https://openreview.net/pdf/2ad527d02fe920443fe79a516d7575f417103f99.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 358]</strong> To Think or Not To Think: A Study of Thinking in Rule-Based Visual Reinforcement Fine-Tuning<br><small style="color: #666;">Score: 1 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/8105c1360484fcffb35e03d8f791d0b437aa1589.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 359]</strong> Analogy-based Multi-Turn Jailbreak against Large Language Models<br><small style="color: #666;">Score: 1 | Agent Safety and Security, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/90ec84387b8d7282640d625e2d28faef32f89000.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 360]</strong> Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling<br><small style="color: #666;">Score: 1 | Model Efficiency and Optimization, Memory and Context Management</small> <a href="https://openreview.net/pdf/a7327b197b07cb1df3b5e408eecd14e1276acc6d.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 361]</strong> BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization<br><small style="color: #666;">Score: 1 | Agent Safety and Security, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/9ef98e483d9e324778e7116170aa4848fc6f67e3.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 362]</strong> GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning<br><small style="color: #666;">Score: 1 | Multi-Agent Systems and Collaboration, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/55c4606303cc86c81ca99b5d5743cfe40d7fb140.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 363]</strong> TS-RAG: Retrieval-Augmented Generation based Time Series Foundation Models are Stronger Zero-Shot Forecaster<br><small style="color: #666;">Score: 1 | Domain-Specific Applications, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/e686d5b872f3c42cd96442359b22e23319fe0acb.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 364]</strong> Bridging Human and LLM Judgments: Understanding and Narrowing the Gap<br><small style="color: #666;">Score: 1 | Agent Benchmarking and Evaluation, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/3b238b3324d5ab3f3e5a672a12a2c7610ee13a48.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 365]</strong> Thoughts Are All Over the Place: On the Underthinking of Long Reasoning Models<br><small style="color: #666;">Score: 10 | Reasoning and Test-Time Compute, Mathematical and Logical Reasoning, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/52cab7bd6214e5b9d63addbfb0411f3ce8f517f4.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 366]</strong> Length Generalization via Auxiliary Tasks<br><small style="color: #666;">Score: 5 | Reasoning and Test-Time Compute, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/a5e5e4faf09290faa591d33b29401917374054fa.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 367]</strong> Transcending Cost-Quality Tradeoff in Agent Serving via Session-Awareness<br><small style="color: #666;">Score: 21 | Model Efficiency and Optimization, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/67186a4229264aede8b786e7d6e259cb157d2aba.pdf" target="_blank" style="color: #00c781; text-decoration: none;">üìÑ PDF</a></p>
</div>
</details>
</div></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Modal for paper details -->
    <div id="paperModal" class="modal">
        <div class="modal-content">
            <button class="modal-close" onclick="closePaperModal()">‚úï</button>
            <div id="modalContent"></div>
        </div>
    </div>

    <script>
        // Embedded paper data
        const papers = [{"paper_id": "4225008", "score": "95", "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents", "authors": "Yaxin Luo, Zhaoyi Li, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen", "session_type": "SD-4-2513", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.24878", "relevant_to_users": "5", "read_by_users": "13", "topics": "", "key_findings": "The paper reveals significant performance variations among state-of-the-art multimodal LLM agents (Claude, GPT-4o, Gemini) across different CAPTCHA types, demonstrating that while these models show promising capabilities, their performance remains inconsistent across challenge variations. It provides the first unified benchmarking infrastructure for evaluating multimodal agents on CAPTCHA-solving tasks, offering systematic insights into current capabilities and limitations for autonomous web interaction.", "description": "This paper introduces Open CaptchaWorld, a comprehensive web-based platform for testing and benchmarking multimodal large language model agents on CAPTCHA-solving tasks. The platform provides standardized evaluation infrastructure supporting multiple CAPTCHA types and enables systematic comparison of state-of-the-art models in realistic web interaction scenarios.", "key_contribution": "The main contribution is a unified, open-source evaluation platform that standardizes CAPTCHA-based benchmarking for multimodal LLM agents, combining multiple challenge variants in a single web-based system with reproducible evaluation protocols. This enables systematic assessment of both proprietary and open-source models across diverse CAPTCHA categories.", "novelty": "Unlike previous work that examined isolated CAPTCHA-solving capabilities or general web interaction in fragmented ways, this platform uniquely provides a unified assessment infrastructure combining multiple CAPTCHA variants with real web-based scenarios. It addresses the lack of standardized evaluation environments for multimodal agent benchmarking, moving beyond scattered domain-specific evaluations to enable reproducible, comparative analysis across leading models and CAPTCHA types.", "ai_categories": ["Agent Benchmarking and Evaluation", "Web and Computer-Use Agents", "Vision-Language-Action Models"]}, {"paper_id": "4157580", "score": "93", "title": "REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites", "authors": "Div Garg, Diego Caples, Andis Draguns, Nikil Ravi, Pranav Putta, Naman Garg, Prannay Hebbar, Youngchul Joo, Jindong Gu, ..., Sumeet Motwani", "session_type": "SD-2-3602", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2504.11543", "relevant_to_users": "5", "read_by_users": "11", "topics": "", "key_findings": "REAL introduces a benchmark for evaluating autonomous agents on deterministic simulations of actual websites, establishing a reproducible evaluation framework that captures real-world web complexity while maintaining consistency. The work demonstrates that current autonomous agents face significant challenges navigating these deterministic website environments, revealing substantial room for improvement in agent design and reasoning capabilities.", "description": "REAL addresses a critical gap in agent evaluation by providing deterministic simulations of real websites rather than synthetic environments. Unlike previous benchmarks that suffer from non-determinism (where webpage layouts, content, or behavior vary unpredictably), REAL creates stable, reproducible website simulations based on real services, enabling rigorous and repeatable agent experiments.", "key_contribution": "The primary innovation is the REAL benchmark itself: a collection of deterministic website simulations derived from actual web services that enables consistent evaluation of autonomous agents across multiple real-world scenarios, addressing the critical need for reproducible agent benchmarking.", "novelty": "REAL guarantees deterministic execution‚Äîcritical for rigorous agent evaluation where reproducibility is essential. Previous benchmarks like WebArena and VisualWebArena depend on live websites prone to changes, which REAL eliminates through controlled simulations. The benchmark represents a novel middle ground that combines realism with reproducibility by capturing deterministic simulations of real websites rather than either purely synthetic tasks or live web services.", "ai_categories": ["Agent Benchmarking and Evaluation", "Web and Computer-Use Agents"]}, {"paper_id": "4285740", "score": "92", "title": "How to Train Your LLM Web Agent: A Statistical Diagnosis", "authors": "Dheeraj Vattikonda, Santhoshi Ravichandran, Emiliano Penaloza, Hadi Nekoei, Thibault Le Sellier de Chezelles, Megh Thakkar, Nicolas Gontier, Miguel Mu√É¬±oz-M√É¬°rmol, Sahar Omidi Shayegan, ..., Massimo Caccia", "session_type": "SD-1-4109", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/0252f1f5697c0153740a7438e473e45964e85102.pdf", "relevant_to_users": "4", "read_by_users": "14", "topics": "", "key_findings": "This paper presents the first statistically rigorous study on compute allocation for training LLM web agents. Key findings include: (1) Hybrid SFT+RL training consistently outperforms pure SFT or pure RL approaches on MiniWob++ and WorkArena benchmarks; (2) Strategic RL branching after initial SFT warmup achieves peak SFT performance using only 55% of the compute on MiniWob++; (3) Optimal hyperparameters shift dramatically based on SFT warmup level - what works for untrained models differs from warm-started ones; (4) The approach closes the gap with closed-source models like GPT-4o through compute-efficient training.", "description": "The paper addresses critical barriers in open-source LLM web agent training: narrow focus on single-step tasks and prohibitively high training costs. Using a two-stage pipeline (SFT with a 70B teacher followed by on-policy RL with an 8B student), the authors analyze 1,370 training configurations across MiniWob++ and WorkArena benchmarks to identify compute-optimal training strategies for multi-step web navigation tasks.", "key_contribution": "The main contribution is a bootstrap-based statistical framework that rigorously analyzes hyperparameter importance across different compute budgets, providing probabilistic assessments of which training configurations consistently win. This yields actionable guidance on when to transition from SFT to RL, enabling 45% compute savings while matching peak performance and democratizing access to effective web agent training for open-source models.", "novelty": "Unlike typical hyperparameter tuning studies that report isolated runs, this work employs bootstrap resampling (1,000 iterations) with unequal coverage weighting to provide confidence intervals and win-rate distributions for hyperparameters. It introduces a compute-aware branching strategy that systematically explores SFT-to-RL transition timing, directly addressing documented RL instability and reproducibility issues. The statistical rigor and focus on Pareto-optimal compute-performance trade-offs distinguish this from prior work that either focuses on pure SFT approaches or applies RL without systematic analysis of when and how to combine them efficiently.", "ai_categories": ["Agent Benchmarking and Evaluation", "Web and Computer-Use Agents", "Reinforcement Learning for LLMs"]}, {"paper_id": "3968305", "score": "92", "title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks", "authors": "Frank (Fangzheng) Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Wang, Xuhui Zhou, Zhitong Guo, ..., Graham Neubig", "session_type": "SD-2-402", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2412.14161", "relevant_to_users": "15", "read_by_users": "24", "topics": "", "key_findings": "TheAgentCompany introduces a comprehensive benchmark with 600+ realistic professional tasks spanning software engineering, business operations, HR, finance, and customer success. Evaluation of leading models (GPT-4o, Claude 3, Gemini, Llama 3, Nova) reveals significant performance gaps, with success rates often below 50% on real-world tasks. The benchmark identifies critical bottlenecks in tool use, multi-step reasoning, and error recovery that prevent current agents from production-ready deployment.", "description": "TheAgentCompany presents a large-scale benchmark designed to assess LLM agents on consequential, real-world professional tasks in enterprise environments. The paper evaluates multiple leading models and their agent implementations across diverse business domains, demonstrating substantial performance gaps and identifying key limitations in current approaches to autonomous agent development.", "key_contribution": "The primary contribution is establishing the first comprehensive benchmark that evaluates LLM agents on authentic, high-stakes business workflows within simulated enterprise systems. This enables rigorous assessment of agent capabilities and reveals performance gaps between current models and production-ready requirements.", "novelty": "Unlike prior benchmarks (WebArena, SWE-bench, OSWorld) that focus on isolated web navigation or coding challenges, TheAgentCompany specifically targets consequential business tasks that require multi-step decision-making, tool integration, and professional judgment. The benchmark captures the complexity of real enterprise workflows, addressing the gap between academic evaluation and actual deployment requirements. This shift from task-specific evaluation to holistic business workflow assessment represents a critical step toward production-ready agent systems.", "ai_categories": ["Agent Benchmarking and Evaluation", "Web and Computer-Use Agents", "Tool Use and Code Generation"]}, {"paper_id": "4269003", "score": "92", "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge", "authors": "Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal Jimenez Gutierrez, ..., Yu Su", "session_type": "SD-3-1915", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.21506", "relevant_to_users": "9", "read_by_users": "13", "topics": "", "key_findings": "Mind2Web 2 introduces the first benchmark specifically designed for evaluating long-horizon agentic search tasks with real-time web browsing. The paper's Agent-as-a-Judge framework enables scalable evaluation of complex, time-varying answers through task-specific judge agents with tree-structured rubrics. Evaluation of 10 frontier systems reveals OpenAI Deep Research achieves 50-70% of human performance while taking half the time, demonstrating the viability of autonomous web research systems.", "description": "This paper presents Mind2Web 2, a benchmark of 130 realistic long-horizon tasks requiring autonomous web browsing and information synthesis. It introduces an Agent-as-a-Judge evaluation framework using task-specific judge agents with tree-structured rubrics to automatically assess answer correctness and source attribution in time-varying web search scenarios.", "key_contribution": "The main contribution is the Agent-as-a-Judge framework that enables scalable, automated evaluation of complex agentic search systems by constructing task-specific judges with tree-structured rubrics. This addresses the critical limitation of existing benchmarks that assume short search horizons and static answers, making evaluation of deep research systems possible.", "novelty": "Unlike previous web agent benchmarks that focus on short-horizon tasks with static ground-truth answers, Mind2Web 2 specifically targets long-horizon agentic search requiring real-time information synthesis and citation-backed answers. The Agent-as-a-Judge approach represents a paradigm shift from human-only evaluation to scalable automated assessment of time-varying, open-ended search tasks. This work is the first benchmark to systematically evaluate autonomous deep research systems that browse, synthesize, and cite information autonomously.", "ai_categories": ["Agent Benchmarking and Evaluation", "Web and Computer-Use Agents", "Tool Use and Code Generation"]}, {"paper_id": "4484068", "score": "92", "title": "AGENTIF: Benchmarking Large Language Models Instruction Following Ability in Agentic Scenarios", "authors": "Yunjia Qi, Hao Peng, Xiaozhi Wang, Amy Xin, Youfeng Liu, Bin Xu, Lei Hou, Juanzi Li", "session_type": "SD-6-114", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "", "relevant_to_users": "2", "read_by_users": "2", "topics": "", "key_findings": "AGENTIF reveals that current state-of-the-art LLMs perform poorly at following instructions in agentic scenarios, particularly struggling with complex constraint structures and tool specifications. The benchmark introduces a systematic evaluation framework across 707 human-annotated instructions from 50 real-world agentic tasks, with instructions averaging 1,723 words and 11.9 constraints per instruction. Through error analysis and experiments on instruction length and meta-constraints, the work identifies critical failure modes that impede reliable agent deployment.", "description": "AGENTIF is the first benchmark designed to systematically evaluate large language models' instruction-following ability in agentic scenarios where models must execute multi-step tasks, interact with tools, and navigate complex environments. The benchmark features realistic tasks from industrial applications with long, complex instructions containing diverse constraint types including tool specifications and conditional requirements.", "key_contribution": "AGENTIF fills a critical gap by providing the first comprehensive benchmark for evaluating instruction following in agentic contexts, employing a hybrid evaluation methodology (code-based, LLM-based, and hybrid) to assess constraint satisfaction across realistic, long-form instructions from real-world agent applications.", "novelty": "Unlike prior instruction-following benchmarks (IFEval, FollowBench, InfoBench) that focus on standalone language comprehension and single-response compliance, AGENTIF evaluates multi-turn interactive environments with external tool integration and real API interactions. It addresses the limitation that existing benchmarks don't assess instruction adherence across extended reasoning chains under operational constraints typical of real agentic systems, featuring substantially longer instructions (avg 1,723 words vs. shorter prompts in prior work) and more complex constraint structures (11.9 constraints per instruction) sourced from actual industrial agent deployments.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation"]}, {"paper_id": "4226664", "score": "91", "title": "RiOSWorld: Benchmarking the Risk of Multimodal Computer-Use Agents", "authors": "Yang JingYi, Shuai Shao, Dongrui Liu, Jing Shao", "session_type": "SD-1-1201", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/11ac207fff4f7439cb345f2e79ef2a6fb5611910.pdf", "relevant_to_users": "3", "read_by_users": "6", "topics": "", "key_findings": "RiOSWorld reveals that current MLLM-based computer-use agents exhibit critically weak risk awareness with 84.93% unsafe rates for risk goal intention and 59.64% for risk goal completion across 492 risky tasks. The study identifies significant vulnerabilities to environmental threats (99.2% unsafe rate for phishing websites, 89.8% for induced text attacks) and problematic user commands (95.7% unsafe in web operations, 91.3% in OS operations). Critically, the work demonstrates that safety alignment from dialogue scenarios does not transfer effectively to autonomous computer-use environments, highlighting an urgent need for specialized safety mechanisms.", "description": "RiOSWorld introduces the first comprehensive safety benchmark for evaluating risks of MLLM-based computer-use agents operating in realistic virtual environments. The benchmark contains 492 risky tasks spanning 13 subcategories across environmental risks (phishing, pop-ups, account fraud) and user-originated risks (social media misinformation, unauthorized file operations, dangerous OS commands). Through systematic evaluation of 10 leading MLLM models, the study demonstrates that current agents lack adequate risk awareness and safety mechanisms for trustworthy autonomous computer manipulation.", "key_contribution": "RiOSWorld provides the first comprehensive, executable-environment-based safety benchmark with 492 diverse risky tasks and a dual evaluation framework assessing both risk goal intention (whether agents intend risky behavior) and risk goal completion (whether they successfully execute it). The benchmark combines realistic interactive virtual machine environments with multi-category risk assessment across real applications, addressing critical limitations in prior text-based evaluation approaches.", "novelty": "Unlike prior text-based QA benchmarks (ToolEmu, Injecagent), RiOSWorld uses real virtual machine environments with actual operating systems enabling dynamic threat deployments and authentic environmental interactions. The work introduces a comprehensive dual evaluation framework that assesses both agent awareness (via LLM-as-judge) and behavioral outcomes (via rule-based completion verification), spanning 13 risk subcategories compared to 1-5 in prior work. The benchmark uniquely supports real-time deployment of sophisticated threats like phishing emails, pop-ups, and account fraud scenarios that previous benchmarks cannot replicate, while systematically evaluating both environmental and user-originated risks in an integrated framework.", "ai_categories": ["Agent Safety and Security", "Agent Benchmarking and Evaluation", "Web and Computer-Use Agents"]}, {"paper_id": "4173087", "score": "91", "title": "WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks", "authors": "Ivan Evtimov, Arman Zharmagambetov, Aaron Grattafiori, Chuan Guo, Kamalika Chaudhuri", "session_type": "SD-2-1311", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2504.18575", "relevant_to_users": "6", "read_by_users": "13", "topics": "", "key_findings": "WASP reveals that state-of-the-art web agents, including those with advanced reasoning capabilities (GPT-4o, Claude 3.5), are highly vulnerable to simple prompt injection attacks, with attacks achieving intermediate diversion in up to 86% of cases. However, full attacker goal completion remains low (0-17%), revealing 'security by incompetence' rather than robust defenses. Two models showed promise: Meta-SecAlign-70B and GPT-5 both achieved 0% attack success rates with ~60% utility retention. The benchmark demonstrates that even low-effort, human-written injections embedded in realistic web content (GitLab issues, Reddit posts) can successfully hijack agent behavior in end-to-end multi-step tasks.", "description": "WASP introduces the first publicly available benchmark for end-to-end evaluation of web agent security against prompt injection attacks in realistic environments. The benchmark tests agents on 37 tasks across GitLab and Reddit platforms, where attackers embed malicious instructions in controllable webpage elements to hijack agents performing legitimate user tasks. It evaluates both intermediate attack success (agent diversion) and end-to-end completion of attacker goals.", "key_contribution": "WASP provides the first realistic, end-to-end benchmark for web agent prompt injection security that allows actual agent execution in isolated environments (not just simulated tool-calling) with a realistic threat model where attackers control only portions of websites through black-box access. It includes both attack success metrics (ASR-intermediate and ASR-end-to-end) and utility preservation metrics across multiple state-of-the-art agents and defense mechanisms.", "novelty": "Unlike prior work (AgentDojo, InjecAgent, ASB), WASP addresses three critical limitations: (1) it tests actual agent execution rather than simulated tool-calling, (2) it uses a realistic threat model where attackers control only specific webpage elements (not entire sites or user prompts), and (3) it measures end-to-end multi-step task completion rather than single-step isolated actions. Previous benchmarks either oversimplified threats, granted attackers unrealistic powers, or failed to measure actual attacker goal success. WASP's framework allows agents to directly interact with live web environments in sandbox isolation, providing the first standardized evaluation of real-world web agent security.", "ai_categories": ["Agent Safety and Security", "Web and Computer-Use Agents", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4210978", "score": "90", "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning", "authors": "Amartya Chakraborty, Paresh Dashore, Nadia Bathaee, Anmol Jain, Anirban Das, Shi-Xiong Zhang, Sambit Sahu, Milind Naphade, Genta Winata", "session_type": "SD-4-1912", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.16986", "relevant_to_users": "2", "read_by_users": "6", "topics": "", "key_findings": "T1 introduces a novel conversational dataset specifically designed for training AI agents in multi-turn planning with sequential tool usage. The work demonstrates that models trained on T1 develop improved capabilities for multi-step reasoning and tool sequencing, addressing the limitation of existing datasets that focus primarily on single-tool interactions. The dataset is publicly available through HuggingFace and includes structured planning representations, tool annotations, and plan generation templates.", "description": "This paper presents T1, a comprehensive tool-oriented conversational dataset for training language models to perform complex multi-turn agentic planning. The dataset captures realistic agent-human interactions where agents must reason about sequences of tool use across multiple conversation turns to solve sophisticated tasks.", "key_contribution": "The main contribution is the creation of a specialized dataset that enables training agents to perform multi-step planning with sequential tool interactions, moving beyond the single-tool or simple task sequences of previous work. The dataset includes structured planning representations, tool annotations, and systematic templates for diverse solution strategies.", "novelty": "Unlike previous datasets that primarily focus on single-tool interactions or simple task sequences, T1 specifically addresses multi-turn planning where agents must reason about sequences of tool use across conversation turns. The work introduces a novel dataset construction methodology incorporating multi-turn conversational structures, tool annotations, and plan generation templates that guide diverse solution strategies. This enables more sophisticated planning behaviors required in real-world agentic applications, bridging the gap between simple tool calling and complex multi-step reasoning.", "ai_categories": ["Tool Use and Code Generation", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4441888", "score": "89", "title": "$\\texttt{G1}$: Teaching LLMs to Reason on Graphs with Reinforcement Learning", "authors": "Xiaojun Guo, Ang Li, Yifei Wang, Stefanie Jegelka, Yisen Wang", "session_type": "SD-2-2800", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/eb95dfb11f1b0c705f2e368d650f0af1afbef02b.pdf", "relevant_to_users": "4", "read_by_users": "12", "topics": "", "key_findings": "G1 demonstrates that reinforcement learning on synthetic graph-theoretic tasks can dramatically scale LLMs' graph reasoning abilities. The paper introduces Erd≈ës, the largest graph reasoning dataset with 50 diverse tasks, 100k training samples, and 5k test samples from real-world graphs. A finetuned 3B model outperforms models 24x larger (Qwen2.5-72B-Instruct), and RL-trained models show strong zero-shot transfer to unseen tasks, domains, graph encodings, and real-world applications like node classification and link prediction without degrading general reasoning abilities.", "description": "This paper addresses the limited proficiency of Large Language Models in graph-related reasoning tasks by proposing G1, a reinforcement learning approach trained on the Erd≈ës dataset. The method enables small models to achieve state-of-the-art graph reasoning performance while maintaining strong generalization capabilities across diverse graph tasks and domains.", "key_contribution": "The main contribution is demonstrating that RL training on automatically generated synthetic graph tasks can unlock latent graph understanding in LLMs, achieving dramatic efficiency gains (3B model outperforming 72B models) and strong generalization. The Erd≈ës dataset‚Äîthe largest and most diverse graph reasoning benchmark to date‚Äîis open-sourced along with the trained models.", "novelty": "Unlike previous approaches that rely on graph foundation models or supervised fine-tuning with scarce annotated graph data, G1 uses reinforcement learning on synthetic graph tasks to automatically scale training data. This circumvents the data scarcity bottleneck and demonstrates that LLMs possess latent graph reasoning capabilities that can be effectively unlocked through RL, achieving superior performance with significantly smaller models and strong zero-shot transfer to unseen domains.", "ai_categories": ["Reinforcement Learning for LLMs", "Reasoning and Test-Time Compute"]}, {"paper_id": "4227756", "score": "89", "title": "Self-Challenging Language Model Agents", "authors": "Yifei Zhou, Sergey Levine, Jason E Weston, Xian Li, Sainbayar Sukhbaatar", "session_type": "SD-2-3407", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/d7856a8a4e991dab71814a3f39f28fe17b6b93bd.pdf", "relevant_to_users": "11", "read_by_users": "20", "topics": "", "key_findings": "The paper demonstrates that language model agents can autonomously generate high-quality training tasks through a self-challenging framework, achieving over 2x performance improvement on Llama-3.1-8B-Instruct on multi-turn tool-use benchmarks (M3ToolEval and TauBench) using only self-generated data. The approach eliminates the need for human-annotated task creation by having agents alternate between challenger (task generation) and executor (task solving) roles, with automated quality filtering through verification functions and test cases. This represents a scalable path for agent training that breaks the bottleneck of manual task annotation.", "description": "The paper proposes a self-challenging framework where language model agents autonomously generate their own training tasks by interacting with tools, then learn from solving these self-generated challenges through reinforcement learning. The approach uses a Code-as-Task (CaT) formulation that structures tasks with instructions, verification functions, and test cases, enabling automated quality control and evaluation without human supervision.", "key_contribution": "The main innovation is enabling agents to autonomously create high-quality training data by alternating between challenger and executor roles, combined with a Code-as-Task formulation that ensures rigorous automated verification. This creates a self-reinforcing improvement cycle that eliminates dependence on human-annotated task distributions.", "novelty": "Unlike prior agent training methods that rely on fixed benchmarks, human-annotated tasks, or external task distributions, this work enables fully autonomous task generation with built-in quality control through verification functions and test cases. It addresses the scalability bottleneck of manual annotation by having agents identify their own capability gaps and generate appropriately challenging tasks. The Code-as-Task formulation provides objective, executable evaluation that prior natural language-based self-improvement methods lacked, enabling more reliable self-supervised learning for tool-using agents.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation", "Reinforcement Learning for LLMs"]}, {"paper_id": "4216018", "score": "89", "title": "AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems", "authors": "Yu Shang, Peijie Liu, Yuwei Yan, Zijing Wu, Leheng Sheng, Yuanqing Yu, Chumeng Jiang, An Zhang, Fengli Xu, ..., Yong Li", "session_type": "SD-3-2509", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.19623", "relevant_to_users": "3", "read_by_users": "8", "topics": "", "key_findings": "AgentRecBench is the first comprehensive benchmark for evaluating LLM agent-based recommender systems, providing a unified textual interaction environment using Yelp, GoodReads, and Amazon datasets. The benchmark reveals that advanced agentic systems substantially outperform simple designs and traditional methods, with Baseline666 consistently achieving best performance. Agent-based approaches demonstrate greater stability under temporal dynamics and evolving user interests compared to traditional deep learning methods. The work establishes actionable design principles showing that effective workflows integrate user history, candidate items, and platform-specific features through LLMs, with platform-adaptive feature extraction significantly enhancing performance.", "description": "This paper introduces AgentRecBench, the first large-scale standardized benchmark for systematically evaluating both agentic and traditional recommender systems across diverse scenarios. It includes a modular agent framework with dynamic planning, complex reasoning, tool utilization, and memory management, evaluated across three critical challenge areas: classic recommendations, cold-start scenarios, and evolving user interests.", "key_contribution": "The main contribution is establishing the first comprehensive evaluation framework for agentic recommender systems with a unified textual environment simulator, standardized query interfaces, and dynamic data visibility control. The benchmark provides extensive empirical analysis of 10+ methods, validated through a real-world challenge with 295 teams, establishing design guidelines for building effective LLM agent-based recommender systems.", "novelty": "Unlike prior work that lacked standardized evaluation protocols for agentic recommenders, this benchmark overcomes fragmentation by establishing the first unified framework supporting comparative analysis across recommendation paradigms. It addresses the critical limitation of previous approaches by enabling systematic evaluation across three distinct challenge scenarios (classic, cold-start, evolving interests) with dynamic data visibility control. The work introduces a modular agent framework that combines dynamic planning, reasoning, tool use, and memory management specifically designed for recommendation tasks, validated through large-scale real-world deployment.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation", "Domain-Specific Agents and Applications"]}, {"paper_id": "4324326", "score": "89", "title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks", "authors": "Fali Wang, Hui Liu, Zhenwei DAI, Jingying Zeng, Zhiwei Zhang, Zongyu Wu, Chen Luo, Zhen Li, Xianfeng Tang, ..., Suhang Wang", "session_type": "SD-4-3405", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/13ea5abf7135c2269571122e956e24009565de1f.pdf", "relevant_to_users": "3", "read_by_users": "12", "topics": "", "key_findings": "AgentTTS addresses test-time compute-optimal scaling in multi-stage complex tasks, identifying three key empirical insights: (1) different subtasks favor different model sizes, (2) performance saturates beyond optimal compute budgets with potential degradation, and (3) budget allocations across subtasks are interdependent. The framework achieves optimal configurations 2.5-64.3 hours faster than baselines across six datasets, with improved search efficiency, robustness, and interpretability through explicit guideline generation.", "description": "This paper proposes AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal model selection and budget allocation strategies in multi-stage complex tasks. The system iteratively interacts with execution environments using feedback-driven search guided by empirical insights about subtask-specific preferences, optimal saturation points, and inter-subtask dependencies.", "key_contribution": "The main innovation is an agent-based framework that treats test-time compute optimization as an iterative search problem in multi-stage tasks, using an LLM agent to autonomously discover optimal model-budget allocations across heterogeneous subtasks while maintaining interpretability through explicit guideline generation.", "novelty": "Unlike prior work focusing on test-time scaling in single-stage tasks, AgentTTS addresses the unexplored challenge of compute-optimal scaling across multi-stage complex tasks with interdependent subtasks. It tackles two key limitations: the impractical combinatorial search space of model-budget allocations, and the interdependency of allocation decisions across subtasks. The approach introduces an agent-based iterative search framework that leverages three empirical insights to efficiently navigate this complex optimization space while providing interpretable decision rationales.", "ai_categories": ["Reasoning and Test-Time Compute", "Agent Benchmarking and Evaluation", "Tool Use and Code Generation"]}, {"paper_id": "4251736", "score": "88", "title": "Wide-Horizon Thinking and Simulation-Based Evaluation for Real-World LLM Planning with Multifaceted Constraints", "authors": "Dongjie Yang, Chengqiang Lu, Qimeng Wang, Xinbei Ma, Yan Gao, Yao Hu, hai zhao", "session_type": "SD-4-5407", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/4fe14c361104913e172e118fa05d358d3b030840.pdf", "relevant_to_users": "1", "read_by_users": "4", "topics": "", "key_findings": "The paper introduces Multiple Aspects of Planning (MAoP) with 'wide-horizon thinking' that achieves 5-40% improvement over long-horizon baselines by enabling parallel consideration of interdependent constraints rather than sequential decomposition. It presents Travel-Sim, a novel agent-based evaluation framework that uses event-driven simulation with real-world data (map APIs, travel blogs) to capture causal dependencies and emergent behaviors that static benchmarks miss. The work demonstrates that stronger strategist models enable inference-time scaling through intelligent aspect routing, and that distilled 3B models can outperform larger non-distilled models through knowledge compression.", "description": "This paper addresses the fundamental mismatch between existing LLM planning approaches (designed for sequential reasoning) and real-world planning problems requiring simultaneous integration of multiple interdependent constraints. It proposes MAoP, a two-stage framework where a strategist decomposes problems into parallel aspects and generates planning blueprints, while a planner conducts focused multi-turn analysis, evaluated through Travel-Sim, a simulation-based benchmark that models dynamic traveler behavior in realistic environments.", "key_contribution": "The main contribution is the conceptual distinction between 'long-horizon' (deep sequential) and 'wide-horizon' (broad parallel) thinking for LLM planning, operationalized through the MAoP framework that uses strategist pre-planning with intelligent aspect routing and aggregation. The paper also introduces Travel-Sim, the first simulation-based evaluation framework for planning that captures causal dependencies through event-driven agent execution with real-world data integration.", "novelty": "Unlike existing methods that apply sequential decomposition (CoT, Plan-and-Solve) or treat constraints independently, MAoP enables parallel consideration of interdependent aspects through learned strategist routing, addressing the limitation that real-world constraints are deeply interconnected. Previous benchmarks use static constraint pass rates that ignore causality; Travel-Sim introduces dynamic agent-based simulation that reveals how early decisions cascade through plans and captures emergent behaviors (e.g., travelers spontaneously declining activities due to fatigue). The work also enables inference-time scaling through aspect expansion rather than requiring larger models or more training data.", "ai_categories": ["Planning and Decision Making", "Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4207685", "score": "87", "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains", "authors": "Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun MA, Wenhu Chen", "session_type": "SD-2-308", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/59ca2ebad16c9f4c1d622f0bcd9007d31e1874bc.pdf", "relevant_to_users": "8", "read_by_users": "25", "topics": "", "key_findings": "General-Reasoner demonstrates that RL-based reasoning improvements can generalize beyond mathematics to diverse domains (physics, chemistry, finance, humanities) when trained with appropriate data and verification. The work introduces WebInstruct-verified, a 230K diverse reasoning dataset, and General-Verifier, a 1.5B generative verifier achieving 78.7% agreement with large models vs. 22.2% for rule-based methods. Models achieve GPT-4o-level performance on challenging benchmarks (56.1% on GPQA, 54.4% on TheoremQA) while being 12x faster than competing approaches, proving that test-time reasoning capabilities are learnable across domains through reinforcement learning.", "description": "This paper addresses the limitation of current RL-based reasoning approaches that focus narrowly on mathematics and coding, preventing generalization to broader domains. It introduces General-Reasoner, a training paradigm using diverse high-quality reasoning data and a generative verifier to enable effective reasoning across physics, chemistry, finance, and humanities while maintaining strong mathematical performance.", "key_contribution": "The main contributions are: (1) WebInstruct-verified, a curated dataset of 230K diverse reasoning questions across multiple domains with rigorous quality control, and (2) General-Verifier, a compact 1.5B generative model that performs context-aware chain-of-thought verification, replacing rigid rule-based matching that fails on semantically equivalent answers in different formats.", "novelty": "Unlike previous work that relies on rule-based verifiers for mathematics and coding only, this work introduces a generative verifier that handles diverse answer formats across domains, achieving 3.5x better agreement with large models. It addresses the generalization problem where math-only training degrades performance on general benchmarks, demonstrating that reinforcement learning can successfully train reasoning across all domains when combined with appropriate verification mechanisms and diverse training data.", "ai_categories": ["Reasoning and Test-Time Compute", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4152665", "score": "87", "title": "MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?", "authors": "Yunxiang Zhang, Muhammad Khalifa, Shitanshu Bhushan, Grant Murphy, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, Lu Wang", "session_type": "SD-3-1910", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2504.09702", "relevant_to_users": "3", "read_by_users": "8", "topics": "", "key_findings": "MLRC-Bench introduces a benchmark for evaluating language agents on complete ML research challenges, revealing significant performance gaps in current systems. The work demonstrates that agents struggle with tasks requiring combined deep technical understanding, code implementation, and experimental validation. The benchmark provides systematic analysis identifying specific bottlenecks in agent capabilities across realistic ML research problems at multiple difficulty levels.", "description": "MLRC-Bench is a benchmark designed to evaluate whether language agents can autonomously solve machine learning research challenges. It provides a curated collection of realistic ML research problems with an evaluation framework that assesses agents across the full research pipeline: problem understanding, solution design, implementation, and experimental validation.", "key_contribution": "The main contribution is a comprehensive benchmark and evaluation framework specifically designed for assessing AI agents on complete, open-ended ML research problems rather than isolated tasks. This includes curated research challenges, appropriate metrics for open-ended research evaluation, and systematic analysis of agent capabilities and failure modes.", "novelty": "Unlike previous benchmarks that focus on isolated ML tasks or standard datasets, MLRC-Bench requires agents to navigate the complete research pipeline from problem understanding through experimental validation. It moves beyond simple classification or generation tasks to assess open-ended research problem-solving with metrics appropriate for real-world research applicability. This is the first benchmark to systematically evaluate whether agents can handle the full complexity of ML research challenges rather than decomposed subtasks.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation"]}, {"paper_id": "4215103", "score": "86", "title": "AI-Researcher: Autonomous Scientific Innovation", "authors": "Jiabin Tang, Lianghao Xia, Zhonghang Li, Chao Huang", "session_type": "SD-3-5417", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/a1c63cdd0495de94664b1513f7d95a3aedcb483a.pdf", "relevant_to_users": "12", "read_by_users": "18", "topics": "", "key_findings": "AI-Researcher achieves 93.8% implementation success rate with Claude models and produces research papers comparable to human quality in 15.79%-78.95% of cases across evaluators. The system demonstrates superior performance in open-ended research (Level-2 tasks) compared to guided tasks, achieving 40-100% comparable rates. The accompanying Scientist-Bench benchmark spans 22 state-of-the-art papers across 4 AI domains (Diffusion, Vector Quantization, GNNs, Recommenders) with both guided and autonomous exploration tasks. Importantly, the automated review agent achieves 65.62%-90.62% agreement with human ICLR peer-review decisions, validating its evaluation capabilities.", "description": "AI-Researcher is a fully autonomous research system that orchestrates the complete scientific discovery pipeline‚Äîfrom literature review and hypothesis generation through algorithm implementation to publication-ready manuscript preparation‚Äîwith minimal human intervention. The system employs a multi-agent architecture with specialized agents (Knowledge Acquisition, Resource Analyst, Code Agent, Advisor, Documentation) that collaborate through a mentor-student paradigm, using explicit bidirectional mappings between theory and code to reduce hallucinations.", "key_contribution": "The main contribution is the first end-to-end autonomous research system that integrates all research stages into a cohesive workflow, addressing fragmentation in prior work that handled isolated components. The Resource Analyst's bidirectional theory-code mapping and hierarchical documentation approach overcome hallucination and coherence limitations, while Scientist-Bench provides the first standardized benchmark for evaluating autonomous research capabilities across diverse AI domains.", "novelty": "Unlike previous systems (AI Scientist, CycleResearcher, Agent Laboratory) that focus on specific research phases, AI-Researcher provides complete pipeline automation from literature review to publication. It addresses critical gaps through: (1) Resource Analyst agents that decompose concepts into atomic components with explicit bidirectional theory-code mappings to reduce hallucinations, (2) iterative mentor-student collaboration patterns bridging the implementation-theory gap, (3) hierarchical documentation processes maintaining coherence across extended manuscripts, and (4) standardized evaluation through Scientist-Bench spanning both guided innovation and autonomous exploration tasks. The system demonstrates that open-ended autonomous research (Level-2) actually outperforms guided tasks (Level-1), suggesting LLMs excel at independent ideation over prescribed directives.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Agent Benchmarking and Evaluation", "Tool Use and Code Generation"]}, {"paper_id": "4051549", "score": "86", "title": "Bag of Tricks for Inference-time Computation of LLM Reasoning", "authors": "Fan LIU, Wen-Shuo Chao, Naiqiang Tan, Hao Liu", "session_type": "SD-6-4913", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2502.07191", "relevant_to_users": "5", "read_by_users": "15", "topics": "", "key_findings": "This paper provides the first comprehensive benchmark of inference-time computation methods for LLM reasoning across diverse tasks. Through over 1,000 experiments using 20,000 A100 GPU hours, the authors discovered that simple hyperparameter tuning (temperature=0.8, top-p=0.9) yields 2-6% improvements, that LLM self-evaluation often performs worse than random selection, and that Best-of-N and Self-Consistency outperform more complex methods like MCTS and Self-Refine on knowledge-based reasoning tasks. The work reveals that reward models suffer from generalization issues and that combining techniques doesn't always yield additive benefits.", "description": "This paper systematically benchmarks six inference-time computation methods (Best-of-N, Step-Level Best-of-N, Self-Consistency, Beam Search, MCTS, and Self-Refine) across eight reasoning tasks and multiple model families (Llama, Qwen, Mistral). The authors investigate previously overlooked implementation details including prompt design, temperature tuning, sampling strategies, and reward model selection to establish best practices for improving LLM reasoning without model retraining.", "key_contribution": "The main contribution is establishing the first standardized benchmarking framework for inference-time computation methods with rigorous ablation studies revealing that simple overlooked strategies (temperature and top-p tuning) significantly enhance performance, while also exposing limitations of self-evaluation and reward model generalization. The work provides comprehensive empirical guidance on which methods work best for different reasoning task types.", "novelty": "Unlike previous work that focused narrowly on mathematical reasoning with inconsistent experimental protocols, this paper is the first to comprehensively study how implementation details affect inference-time computation across diverse reasoning tasks (logical, mathematical, knowledge-based, code generation). It addresses the lack of standardized benchmarking in prior research and reveals surprising findings that challenge assumptions about LLM self-evaluation capabilities and the effectiveness of complex methods versus simpler alternatives. The work systematically explores the interaction between generation strategies (prompts, sampling) and selection mechanisms (self-evaluation, reward models) that were previously underexplored.", "ai_categories": ["Reasoning and Test-Time Compute", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4215986", "score": "85", "title": "Multi-Agent Collaboration via Evolving Orchestration", "authors": "Yufan Dang, Chen Qian, Xueheng Luo, Jingru Fan, Zihao Xie, Ruijie Shi, Weize Chen, Cheng Yang, Xiaoyin Che, ..., Maosong Sun", "session_type": "SD-1-4019", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/9727f658d788c52f49f12ae4b230baf4cf0d4007.pdf", "relevant_to_users": "4", "read_by_users": "9", "topics": "", "key_findings": "The paper introduces a puppeteer-style paradigm where a centralized orchestrator uses reinforcement learning to dynamically direct multi-agent collaboration. Experiments show this approach achieves superior performance with reduced computational costs across both closed-domain (GSM-Hard, MMLU-Pro) and open-domain tasks (SRDD, CommonGen-Hard). Notably, the system evolves toward compact, graph-structured topologies with emergent cyclic patterns that enable iterative refinement, while simultaneously decreasing token consumption - contradicting typical multi-agent efficiency trade-offs.", "description": "This paper addresses the limitation of static organizational structures in LLM-based multi-agent systems by proposing a dynamic orchestration approach. A centralized orchestrator trained via reinforcement learning adaptively sequences and prioritizes agents based on evolving task states, decoupling agent selection from internal agent behaviors to enable flexible collaboration topologies.", "key_contribution": "The main innovation is the puppeteer-style paradigm that uses REINFORCE-based reinforcement learning to train a centralized orchestrator that dynamically directs agents throughout task execution. This enables graph-structured reasoning with emergent compaction and cyclicality, achieving simultaneous improvements in both effectiveness and efficiency.", "novelty": "Unlike previous approaches that rely on predefined or statically generated agent topologies with autonomous agent selection (creating coordination overhead and scalability issues), this work introduces dynamic, context-aware orchestration that unfolds collaboration topologies sequentially based on Markovian state transitions. The approach addresses the fundamental limitation that static structures struggle to adapt as task complexity and agent numbers grow, instead learning an adaptive policy that evolves toward compact, cyclic graph structures with reduced redundant computation.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Reinforcement Learning for LLMs", "Planning and Decision Making"]}, {"paper_id": "4124416", "score": "85", "title": "Classical Planning with LLM-Generated Heuristics: Challenging the State of the Art with Python Code", "authors": "Augusto B. Corr√É¬™a, Andr√É¬© Grahl Pereira, Jendrik Seipp", "session_type": "SD-3-3210", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/924624fad0f2d9ea4637686b275593407c20b753.pdf", "relevant_to_users": "3", "read_by_users": "7", "topics": "", "key_findings": "LLM-generated domain-specific heuristic functions (as Python code) outperform traditional domain-independent heuristics and compete with state-of-the-art learning-based methods for classical planning. DeepSeek R1-generated heuristics solved 373/720 test tasks versus 243 for the widely-used FF heuristic (+53% improvement), despite using an unoptimized Python planner against highly optimized C++ baselines. The approach costs ~$7 total and generates one reusable heuristic per domain rather than per-task.", "description": "This paper proposes using LLMs to generate domain-specific heuristic functions in Python code for classical planning, rather than attempting end-to-end plan generation. The approach generates multiple candidate heuristics for a planning domain, evaluates them on training tasks with greedy best-first search, and selects the best one for deployment on unseen test tasks.", "key_contribution": "A simple three-stage pipeline (generation, evaluation, selection) that produces LLM-generated domain-dependent heuristics which substantially outperform domain-independent heuristics and are competitive with the strongest learning algorithms for domain-dependent planning, while being cost-effective (~$0.76 per domain) and requiring no iterative refinement.", "novelty": "Unlike prior work that uses LLMs for end-to-end planning (which fails to generalize) or generates per-task heuristics (expensive), this work generates one reusable heuristic per domain that works across all tasks in that domain. It addresses the limitation of unreliable LLM plan generation by restricting LLMs to heuristic function generation rather than full planning. The approach works directly from PDDL domain descriptions without manual translation, uses a simple non-iterative pipeline (unlike complex refinement loops), and demonstrates that even unoptimized Python implementations with LLM-generated heuristics can outperform highly optimized C++ planners with traditional heuristics.", "ai_categories": ["Tool Use and Code Generation", "World Models and Planning"]}, {"paper_id": "4101178", "score": "84", "title": "ReMA: Learning to Meta-Think for LLMs with Multi-agent Reinforcement Learning", "authors": "Ziyu Wan, Yunxiang LI, Xiaoyu Wen, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, ..., Ying Wen", "session_type": "SD-2-400", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/4ef88667f9d6e59455e6805cd91df2e8e519f9d0.pdf", "relevant_to_users": "16", "read_by_users": "38", "topics": "", "key_findings": "ReMA demonstrates that hierarchical multi-agent reinforcement learning significantly outperforms single-agent RL baselines on complex reasoning tasks including competitive-level mathematical problems and LLM-as-a-Judge benchmarks. The framework shows improved generalization and robustness through learned collaboration between specialized agents. Ablation studies reveal that distinct agent dynamics with strategic oversight and detailed execution enhance reasoning capabilities beyond monolithic approaches.", "description": "ReMA introduces a multi-agent reinforcement learning framework that enhances LLM reasoning through meta-thinking capabilities. The system decouples reasoning into two hierarchical agents: a high-level meta-thinking agent for strategic oversight and planning, and a low-level reasoning agent for detailed task execution, enabling iterative collaboration through aligned RL objectives.", "key_contribution": "The primary innovation is the hierarchical decomposition of reasoning into specialized agents with distinct roles - strategic meta-thinking versus execution - trained collaboratively through multi-agent RL with parameter sharing for efficiency.", "novelty": "Unlike existing single-agent RL approaches that lack specialized meta-thinking design, ReMA addresses this gap through intentional hierarchical cognitive oversight using multi-agent collaboration. The framework moves beyond unstructured reasoning enhancement to enable agents to explore and learn collaboration patterns through aligned objectives. This approach tackles the efficacy limitations of monolithic reasoning strategies by introducing specialized role decomposition.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Reinforcement Learning for LLMs", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4177504", "score": "84", "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability", "authors": "Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yongkang Wu, Ji-Rong Wen, Yutao Zhu, Zhicheng Dou", "session_type": "SD-3-1905", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/4e3ecfb1260a3ed3a6f051cf994b3be14a8f904e.pdf", "relevant_to_users": "5", "read_by_users": "10", "topics": "", "key_findings": "WebThinker introduces a deep research agent that enables large reasoning models (LRMs) to autonomously search, navigate, and synthesize web information during the reasoning process itself, not as a separate pipeline stage. It achieves superior performance on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE with 500 extremely difficult problems) and scientific report generation tasks, outperforming existing methods and proprietary systems. The system combines a Deep Web Explorer module that dynamically searches and navigates pages by clicking interactive elements, with an Autonomous Think-Search-and-Draft strategy that integrates real-time knowledge seeking with report creation. An RL-based training approach using iterative online Direct Preference Optimization (DPO) further enhances research tool utilization.", "description": "WebThinker is a deep research agent framework that empowers large reasoning models like OpenAI-o1 and DeepSeek-R1 to conduct autonomous web research while reasoning. Instead of relying on static internal knowledge, it enables models to dynamically search the web, navigate through pages by clicking links and buttons, extract relevant information, and draft comprehensive research reports‚Äîall integrated within a single reasoning process.", "key_contribution": "The main innovation is seamlessly integrating web research capabilities directly into the reasoning process through end-to-end task execution in a single generation, rather than treating retrieval and reasoning as separate pipeline stages. This is achieved through the Deep Web Explorer module for autonomous navigation and the Think-Search-and-Draft strategy that interleaves reasoning, information gathering, and report writing in real-time, trained via iterative online DPO.", "novelty": "Unlike traditional RAG systems that prepend retrieved documents to queries or web agents that follow rigid action sequences, WebThinker allows the reasoning model itself to perform web actions during thinking, treating research as an active, iterative process guided by reasoning objectives. This addresses the fundamental limitation that existing LRMs rely on static internal knowledge and lack robust mechanisms for dynamic information synthesis during reasoning. The approach differs from prior work by enabling deeper link traversal based on evolving information needs rather than single-step retrieval, and by producing comprehensive reports through real-time drafting integrated with the reasoning loop.", "ai_categories": ["Web and Computer-Use Agents", "Reasoning and Test-Time Compute", "Tool Use and Code Generation"]}, {"paper_id": "4409843", "score": "84", "title": "LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language", "authors": "Periklis Mantenoglou, Rishi Hazra, Pedro Zuidberg Dos Martires, Luc De Raedt", "session_type": "SD-6-3108", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2510.05972", "relevant_to_users": "0", "read_by_users": "0", "topics": "", "key_findings": "LexiCon introduces the first benchmark specifically designed to evaluate language models' ability to solve planning problems with temporal constraints (deadlines, durations, temporal dependencies). Evaluation of state-of-the-art models (OpenAI o3, DeepSeek R1, Gemini 2.5, Claude 3.7 Sonnet) reveals significant performance gaps, showing that current LLMs struggle with temporal constraint satisfaction and time-aware planning optimization. This highlights an underexplored capability gap in contemporary AI systems that is critical for real-world planning applications.", "description": "This paper presents LexiCon, a benchmark for evaluating large language models' ability to perform planning tasks under temporal constraints expressed in natural language. The benchmark tests whether LLMs can generate action sequences while respecting deadlines, duration constraints, and temporal dependencies‚Äîcapabilities essential for real-world scheduling and planning scenarios.", "key_contribution": "The main contribution is a novel benchmark dataset that systematically evaluates temporal reasoning in planning tasks, filling a critical gap in existing planning benchmarks. The work establishes baseline performance for state-of-the-art models and reveals significant limitations in their ability to handle time-bounded planning problems.", "novelty": "Unlike prior planning benchmarks (BabyAI, ALFI World, PlanBench) that focus primarily on logical reasoning about action sequences, LexiCon is the first to explicitly incorporate temporal dimensions including deadlines, duration constraints, and temporal dependencies. This addresses a fundamental limitation in existing evaluation frameworks that neglect the practical time constraints inherent to real-world planning problems. The benchmark bridges the gap between traditional AI planning systems that handle temporal constraints and modern LLM evaluation.", "ai_categories": ["Agent Benchmarking and Evaluation", "World Models and Planning", "Reasoning and Test-Time Compute"]}, {"paper_id": "4442669", "score": "83", "title": "Agentic RL Scaling Law: Spontaneous Code Execution for Mathematical Problem Solving", "authors": "Xinji Mai, Haotian Xu, Xing W, Weinong Wang, Yingying Zhang, Wenqiang Zhang", "session_type": "SD-1-306", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/6d8a6cfbc785966d9381495a786eea81de48d681.pdf", "relevant_to_users": "4", "read_by_users": "15", "topics": "", "key_findings": "The paper discovers and validates an 'Agent RL Scaling Law' showing that base LLMs can spontaneously learn to use code execution tools through pure reinforcement learning without supervised examples. Key findings include: (1) strong positive correlations between training steps, spontaneous code execution frequency, response length, and task accuracy; (2) a 7B model achieving 52.3% on challenging math benchmarks (AIME24/25, MATH500), surpassing specialized instruction-tuned models like Qwen 2.5 Math Instruct (41.6%); (3) agents converge to efficient single-call code strategies with over 90% of correct solutions using only one execution call; (4) performance scales predictably with both model size and interaction budget, demonstrating emergent tool-use capabilities driven purely by outcome-based rewards.", "description": "This paper investigates ZeroTIR (Zero-shot Tool-Integrated Reasoning), training base LLMs to spontaneously generate and execute Python code for mathematical problems through reinforcement learning from outcome-based rewards alone, without supervised tool-use examples. The work characterizes scaling laws governing how models autonomously acquire tool-use capabilities during RL training, examining relationships between training steps, code execution frequency, and task accuracy.", "key_contribution": "The main contribution is discovering and empirically validating scaling laws that govern autonomous tool learning in agent RL, showing that base models can learn when and how to utilize code execution tools purely from task success rewards, without requiring supervised trajectories or explicit prompts. The work introduces the ZeroTIR framework with technical innovations including replay buffer filtering for intermediate-difficulty samples, dynamic stop-token mechanisms for iterative reasoning, and asynchronous rollout pipelining achieving 1.6x speedup.", "novelty": "Unlike existing Tool-Integrated Reasoning methods that rely on predefined triggers, specific prompt structures, or supervised fine-tuning with extensive trajectory data, this work demonstrates that tool use can emerge spontaneously through pure RL optimization. The paper addresses the fundamental question of whether agents can autonomously learn tool invocation strategies from outcome-based rewards alone, moving from instruction-based to emergent tool use. This represents a paradigm shift showing that base models trained with tool-augmented RL can exceed specialized instruction-tuned models, challenging the necessity of supervised tool-use examples.", "ai_categories": ["Tool Use and Code Generation", "Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs"]}, {"paper_id": "4483816", "score": "83", "title": "Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia", "authors": "Chandler Smith, Marwa Abdulhai, Manfred D√É¬≠az, Marko Tesic, Rakshit Trivedi, Sasha Vezhnevets, Lewis Hammond, Jesse Clifton, Minsuk Chang, ..., Joel Leibo", "session_type": "SD-1-3603", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2512.03318", "relevant_to_users": "1", "read_by_users": "2", "topics": "", "key_findings": "LLM-based agents show varying generalization capabilities across mixed-motive scenarios, with notable struggles when transferring between structurally different game-theoretic contexts. Agents exhibit inconsistent behavior patterns, either maintaining cooperative strategies or defaulting to competitive approaches depending on their training context. The study reveals specific generalization gaps that emerge when agents trained in one type of cooperative-competitive environment face novel mixed-motive situations.", "description": "This paper systematically evaluates how well LLM-based agents generalize across mixed-motive scenarios (situations with both cooperative and competitive incentives) using the Concordia framework. The research tests whether agents trained in specific game-theoretic contexts can successfully adapt their behavior to novel cooperative and competitive environments.", "key_contribution": "The paper establishes a comprehensive evaluation protocol for assessing LLM agent generalization in mixed-motive settings using Concordia. It introduces a systematic testing methodology that measures behavioral transfer across diverse game-theoretic scenarios, moving beyond single-environment evaluations to understand cross-context performance.", "novelty": "Unlike previous work that evaluated agents in isolated game environments, this research uniquely addresses the generalization gap by testing across multiple mixed-motive scenarios simultaneously. It fills an underexplored area in understanding how LLM-based agents transfer learned behaviors when encountering structurally different cooperative-competitive situations, providing empirical evidence of specific failure modes and success patterns in cross-context generalization.", "ai_categories": ["Agent Benchmarking and Evaluation", "Multi-Agent Systems and Collaboration"]}, {"paper_id": "4441771", "score": "83", "title": "Agentic Plan Caching: Test-Time Memory for Fast and Cost-Efficient LLM Agents", "authors": "Qizheng Zhang, Michael Wornow, Kunle Olukotun", "session_type": "SD-4-2013", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/2362f2d245503d9d209dde5a45fbfbf3fb6a5990.pdf", "relevant_to_users": "3", "read_by_users": "9", "topics": "", "key_findings": "The paper introduces agentic plan caching, achieving 46.62% average cost reduction while maintaining 96.67% of optimal accuracy across real-world agent benchmarks. Unlike semantic caching which fails for agents due to environment-dependent outputs, this approach extracts reusable plan templates from completed executions and adapts them using lightweight models. The system adds only 1.04% overhead for cache operations and demonstrates superior accuracy-cost tradeoffs compared to semantic caching and full-history approaches.", "description": "This paper addresses the high costs of LLM-based agents operating in Plan-Act paradigms by proposing agentic plan caching. The method extracts structured plan templates from successful agent executions, uses keyword-based matching to retrieve relevant templates for new tasks, and employs small planner models to adapt cached templates with task-specific context before execution.", "key_contribution": "The main innovation is shifting from query-level caching (for chatbots) to task-level caching (for agents) by separating task intent from context. The system extracts reusable plan templates through two-stage filtering (rule-based and LLM-based), uses exact keyword matching instead of semantic similarity to avoid false positives, and enables lightweight models to adapt templates with dynamic context.", "novelty": "Unlike semantic caching which stores input-output pairs and fails when agent outputs depend on external environments, this work caches abstract plan templates that can be adapted to varying contexts. While prior agent memory research focuses on improving accuracy, this uniquely targets cost reduction by reusing historical planning experiences. The approach also differs from full-history caching by using LLM-filtered templates instead of verbose execution logs, enabling small models to effectively adapt plans.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4325538", "score": "82", "title": "Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools", "authors": "Kanghua Mo, Li Hu, Yucheng Long, Zhihao li", "session_type": "SD-5-1512", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/58fd047ac6ca0bee4f61d85fc98df7a5c5b55e31.pdf", "relevant_to_users": "6", "read_by_users": "21", "topics": "", "key_findings": "This research reveals that adversaries can manipulate tool metadata (names, descriptions, parameter schemas) to induce LLM agents to invoke malicious tools with 81-95% success rates across multiple models. The Attractive Metadata Attack (AMA) uses black-box in-context learning with state-action-value optimization to iteratively generate deceptive yet syntactically valid tool metadata. Crucially, these attacks remain effective even under existing prompt-level defenses and structured tool-selection protocols like Model Context Protocol, exposing systemic vulnerabilities in current agent architectures that require execution-level security mechanisms.", "description": "The paper introduces the Attractive Metadata Attack (AMA), a novel attack vector against LLM agents that exploits tool metadata manipulation rather than prompt injection or model internals. Using iterative optimization with generation traceability, weighted value evaluation, and batch generation, AMA crafts highly attractive malicious tool metadata that induces agents to invoke adversarial tools while maintaining normal task completion and evading existing defenses.", "key_contribution": "The main contribution is identifying and formalizing metadata manipulation as a distinct, previously underexplored attack surface in tool-augmented LLM agents. The paper introduces AMA's optimization framework with three key mechanisms (generation traceability, weighted value evaluation, batch generation) that autonomously creates deceptive tool metadata achieving 81-95% attack success rates while bypassing current prompt-level security defenses.", "novelty": "Unlike previous work focusing on prompt injection, contextual tampering, or toolchain manipulation, AMA targets metadata as an independent attack surface that existing defenses do not address. The key innovation is recognizing that metadata-based attacks are more stealthy‚Äîagents willingly select malicious tools based on crafted descriptions alone without requiring abnormal outputs or prompt interference. This work addresses the critical gap that prompt-level defenses are ineffective against metadata manipulation, demonstrating the need for execution-level security mechanisms rather than input-filtering approaches.", "ai_categories": ["Agent Safety and Security", "Tool Use and Code Generation"]}, {"paper_id": "4272120", "score": "82", "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements", "authors": "Bingchen Zhao, Despoina Magka, Minqi Jiang, Xian Li, Roberta Raileanu, Tatiana Shavrina, Jean-Christophe Gagnon-Audet, Kelvin Niu, Shagun Sodhani, ..., Yoram Bachrach", "session_type": "SD-6-3313", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.22419", "relevant_to_users": "5", "read_by_users": "19", "topics": "", "key_findings": "Introduces a benchmark for automatically evaluating LLM improvements in speedrunning scenarios‚Äîoptimizing model performance with minimal computational overhead. Demonstrates that systematic, reproducible evaluation of incremental LLM improvements requires careful experimental design, particularly when reproducing NanoGPT optimizations. Shows significant performance variations depending on hyperparameter choices and training configurations.", "description": "This paper presents the Automated LLM Speedrunning Benchmark, designed to reproduce and evaluate NanoGPT improvements in a systematic way. It addresses the challenge of reliably measuring performance gains when optimizing language models for specific objectives, establishing reproducible benchmarking procedures for speedrunning-style LLM enhancements.", "key_contribution": "A comprehensive benchmarking framework with rigorous evaluation protocols for measuring LLM optimization improvements. This enables reproducible assessment of incremental enhancements and helps identify which modifications genuinely improve model performance versus introducing noise or variability.", "novelty": "Unlike prior work focusing on general LLM evaluation, this paper specifically addresses reproducibility in speedrunning scenarios where researchers compete to achieve marginal improvements. It takes a novel approach by systematically investigating sensitivity to hyperparameters and experimental conditions, revealing critical factors often overlooked in the literature that affect reproducibility of incremental optimizations.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation"]}, {"paper_id": "4209861", "score": "81", "title": "AutoData: A Multi-Agent System for Open Web Data Collection", "authors": "Tianyi Ma, Yiyue Qian, Zheyuan Zhang, Zehong Wang, Xiaoye Qian, Feifan Bai, Yifan Ding, Xuwei Luo, Shinan Zhang, ..., Yanfang Ye", "session_type": "SD-3-2501", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/066a04337fe56fee38b21d8e6b2366fac00856f4.pdf", "relevant_to_users": "1", "read_by_users": "2", "topics": "", "key_findings": "AutoData achieves superior performance (91.85 F1) on the Instruct2DS benchmark while reducing costs by 77% ($0.57 vs $2.49) and execution time by 64% compared to the best baseline. The system introduces a novel oriented message hypergraph architecture that enables targeted agent communication, reducing token consumption and information overload. The hypergraph cache system (OHCache) addresses the substantial financial overhead of existing LLM-based data collection systems by storing bulky artifacts locally and using structured message formats. The system successfully generalizes across domains (academic, finance, sports) and achieves 92.5% on HumanEval coding tasks, demonstrating robust code generation capabilities.", "description": "AutoData is a multi-agent system that automates web data collection from natural language instructions with minimal human intervention. The system employs eight specialized agents organized into research and development squads, coordinated through an oriented message hypergraph that enables efficient, targeted communication while dramatically reducing token costs and execution time compared to existing approaches.", "key_contribution": "The paper introduces an oriented message hypergraph communication architecture for multi-agent systems, where agents communicate through directed hyperedges enabling selective multi-recipient messaging. This is coupled with a hypergraph cache system (OHCache) that stores artifacts locally and enforces structured message formats, achieving significant cost reduction (77%) and performance improvements in web data collection tasks.", "novelty": "Unlike conventional multi-agent systems that use broadcast messaging or pairwise communication, AutoData models agent interactions as directed hyperedges in a hypergraph, enabling targeted information dissemination to multiple recipients while avoiding information overload. This addresses the critical token cost bottleneck of LLM-based systems‚Äîprevious approaches embedded large artifacts in messages, while AutoData's cache system stores them locally with ID-based retrieval. The dual-squad architecture (research vs development) mirrors human workflow organization, representing a domain-specific design for web data collection rather than general-purpose multi-agent frameworks.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Tool Use and Code Generation", "Web and Computer-Use Agents"]}, {"paper_id": "4031335", "score": "81", "title": "Large Language Models Think Too Fast To Explore Effectively", "authors": "Lan Pan, Hanbo Xie, Robert Wilson", "session_type": "SD-4-2200", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/84905da6e0006182e87623ee1b532443c3ca840a.pdf", "relevant_to_users": "14", "read_by_users": "24", "topics": "", "key_findings": "Most LLMs dramatically underperform humans at exploratory discovery tasks (LLaMA3.1-70B: 25 elements, GPT-4o: 35 elements vs humans: 51 elements), except for reasoning-based models like o1 (177 elements). The paper reveals that traditional LLMs rely almost entirely on uncertainty-driven exploration with near-zero empowerment weights, unlike humans who balance both strategies. Using Sparse Autoencoders, the authors discovered that LLMs process uncertainty and choices in early layers while empowerment emerges in middle layers‚Äîbut decisions are made before empowerment can influence behavior, causing them to 'think too fast' and make premature decisions that limit exploration effectiveness.", "description": "This paper investigates whether LLMs can perform effective exploration in open-ended tasks using Little Alchemy 2 as a testing paradigm, where agents combine elements to discover new ones from 720 total elements. The study reveals fundamental architectural limitations in how traditional LLMs explore, identifying two distinct strategies (uncertainty-driven and empowerment-based) and showing that most LLMs fail to balance them like humans do.", "key_contribution": "The paper establishes exploration as a critical dimension for evaluating LLM intelligence and provides mechanistic insights using Sparse Autoencoders to trace information flow through model layers. It demonstrates that traditional LLMs make decisions too quickly‚Äîbefore empowerment considerations can influence behavior‚Äîwhile reasoning-based models (o1, DeepSeek-R1) that use extended test-time compute achieve human-level or better exploration by allowing proper integration of both exploration strategies.", "novelty": "Unlike previous work that focused narrowly on bandit tasks, this paper uses a richer open-ended combinatorial discovery task requiring semantic understanding and long-term planning. It introduces a novel mechanistic analysis using Sparse Autoencoders to reveal that traditional LLMs process exploration-relevant information in the wrong temporal order‚Äîuncertainty and immediate choices are resolved in early layers while empowerment values emerge too late to influence decisions. This 'thinking too fast' problem represents a fundamental architectural limitation that existing approaches like prompt engineering or temperature tuning cannot overcome, explaining why only models with extended reasoning at inference time (o1) succeed at exploration.", "ai_categories": ["Reasoning and Test-Time Compute", "Agent Benchmarking and Evaluation", "Planning and Decision Making"]}, {"paper_id": "4183911", "score": "81", "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch", "authors": "Zimu Lu, Yunqiao Yang, Houxing Ren, Haotian Hou, Han Xiao, Ke Wang, Weikang Shi, Aojun Zhou, Mingjie Zhan, Hongsheng Li", "session_type": "SD-5-1505", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.03733", "relevant_to_users": "5", "read_by_users": "13", "topics": "", "key_findings": "WebGen-Bench introduces the first comprehensive benchmark for evaluating LLMs on generating complete, interactive websites from textual descriptions. The work reveals significant performance gaps across state-of-the-art models, with even advanced systems struggling with complex web generation tasks involving multiple pages and interactive functionality. The benchmark establishes baseline metrics and identifies specific areas requiring future model improvements.", "description": "WebGen-Bench addresses a critical evaluation gap by providing a systematic framework for assessing LLM performance in web development. The benchmark evaluates models on their ability to generate fully functional websites‚Äîencompassing HTML, CSS, and JavaScript‚Äîbased solely on natural language specifications.", "key_contribution": "The primary innovation is creating an extensive benchmark dataset with 100 diverse website generation tasks, along with comprehensive evaluation metrics that assess both functional correctness and visual appearance, including semantic similarity, visual rendering accuracy, and interactive functionality validation.", "novelty": "Unlike previous benchmarks (SWE-Bench, etc.) that primarily evaluate bug-fixing and isolated code generation, WebGen-Bench tackles the novel challenge of end-to-end website generation from scratch. It requires models to synthesize complete multi-file projects with coordinated HTML, styling, and interactivity rather than modifying existing code, addressing the gap in evaluating holistic web development capabilities.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation", "Web and Computer-Use Agents"]}, {"paper_id": "4216350", "score": "81", "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research", "authors": "Hui Chen, Miao Xiong, Yujie Lu, Wei Han, Ailin Deng, Yufei He, Jiaying Wu, Yibo Li, Yue Liu, Bryan Hooi", "session_type": "SD-5-2305", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.19955", "relevant_to_users": "1", "read_by_users": "6", "topics": "", "key_findings": "MLR-Bench reveals that current AI agents struggle significantly with open-ended ML research tasks, particularly in maintaining coherent research strategies across extended interactions, adapting to novel problem formulations, and integrating feedback during extended research cycles. The benchmark demonstrates significant performance variations across different AI models and establishes baseline measurements for future improvement.", "description": "MLR-Bench is a comprehensive benchmark designed to evaluate AI agents' capabilities in conducting open-ended machine learning research tasks. It provides a diverse set of research problems requiring exploration without predetermined solution paths, along with evaluation metrics that capture both task completion and research quality.", "key_contribution": "The paper introduces the first benchmark specifically designed for evaluating AI agents on open-ended ML research tasks, featuring a multi-agent evaluation framework (MLR-Judge and MLR-Agent), hybrid assessment methods combining automated metrics with human judgment, and a diverse task corpus spanning multiple ML research domains.", "novelty": "Unlike previous agent benchmarks that focused on closed-domain tasks with single correct solutions, MLR-Bench targets open-ended research problems requiring demonstration of research methodology and strategic thinking. It addresses the evaluation mismatch in prior work by moving beyond simple pass/fail metrics to capture research quality, and tests generalization across diverse ML subfields rather than isolated narrow domains.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation"]}, {"paper_id": "4450801", "score": "81", "title": "LTD-Bench: Evaluating Large Language Models by Letting Them Draw", "authors": "Liuhao Lin, Ke Li, Zihan Xu, Yuchen Shi, Yulei Qin, Yan Zhang, Xing Sun, Rongrong Ji", "session_type": "SD-6-201", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2511.02347", "relevant_to_users": "2", "read_by_users": "4", "topics": "", "key_findings": "LTD-Bench reveals that even state-of-the-art LLMs (including GPT-4o, DeepSeek-V3, and others) demonstrate significant deficiencies in spatial reasoning, with only DeepSeek-R1 achieving above 70% accuracy and most scoring 30-60%. The benchmark shows that deep reasoning (test-time compute) improves recognition tasks by 25%+ but not generation tasks, and surprisingly, multimodal models show no clear advantages over text-only models on spatial tasks. This exposes a fundamental limitation in current LLMs' ability to establish bidirectional mappings between language and spatial concepts.", "description": "LTD-Bench is a novel evaluation framework that assesses LLMs by requiring them to generate visual drawings through dot matrices or executable Python code, rather than text responses. It features 183 tasks across three difficulty levels (easy: grid-based, normal: continuous curves, hard: real-world objects) with complementary generation tasks (testing spatial imagination) and recognition tasks (testing spatial perception).", "key_contribution": "LTD-Bench transforms LLM evaluation from abstract numerical scores to directly observable visual outputs, providing the first systematic assessment of bidirectional language-spatial mapping capabilities. It makes model limitations visually interpretable and enables intuitive diagnostic analysis impossible with conventional text-based benchmarks.", "novelty": "Unlike traditional benchmarks that provide opaque aggregate scores, LTD-Bench makes model capabilities directly observable through visual artifacts, addressing three critical gaps: the invisibility problem (abstract metrics obscure spatial understanding), incomplete assessment (most benchmarks test only one reasoning direction), and unclear failure thresholds. It uniquely evaluates both directions of language-spatial mapping (language‚Üívisual and visual‚Üílanguage) systematically, revealing that even high-performing models on traditional benchmarks fundamentally struggle with spatial reasoning‚Äîa capability essential for genuine world models.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4280288", "score": "81", "title": "Establishing Best Practices in Building Rigorous Agentic Benchmarks", "authors": "Yuxuan Zhu, Tengjun Jin, Yada Pruksachatkun, Andy Zhang, Shu Liu, Sasha Cui, Sayash Kapoor, Shayne Longpre, Kevin Meng, ..., Daniel Kang", "session_type": "SD-6-2709", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2507.02825", "relevant_to_users": "5", "read_by_users": "9", "topics": "", "key_findings": "The paper establishes comprehensive best practices for constructing rigorous agentic benchmarks through systematic analysis of existing benchmarks (SWE-bench, WebArena, etc.). It identifies critical gaps in current benchmark design including inconsistent metrics, reproducibility issues, limited transparency, and inadequate documentation. The work provides actionable frameworks for evaluation methodology, task design, metric selection, reporting standards, and robustness testing.", "description": "This paper addresses the lack of standardized methodology in building benchmarks for evaluating AI agents. By synthesizing insights from multiple existing agentic benchmarks, it provides comprehensive guidance for future benchmark builders to ensure their evaluations are reliable, reproducible, and meaningful for assessing agent capabilities.", "key_contribution": "The main contribution is a unified, systematic framework of best practices for agentic benchmark construction, including environment design principles, metric evaluation hierarchies, result reporting standards, and robustness testing methodologies‚Äîsynthesizing cross-benchmark analysis rather than creating a single new benchmark.", "novelty": "Unlike previous work that developed individual benchmarks, this paper takes a meta-analytical approach by systematically analyzing design tradeoffs across multiple benchmarks to create unified standards. It addresses the absence of agreed-upon methodology in benchmark construction by providing actionable guidance that tackles previously overlooked issues like data contamination, statistical significance testing, and comprehensive documentation requirements that were inconsistent or missing in prior benchmarking efforts.", "ai_categories": ["Agent Benchmarking and Evaluation"]}, {"paper_id": "4218397", "score": "80", "title": "Large Language Models Miss the Multi-agent Mark", "authors": "Emanuele La Malfa, Gabriele La Malfa, Samuele Marro, Jie Zhang, Elizabeth Black, Michael Luck, Philip Torr, Michael Wooldridge", "session_type": "SD-3-3504", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.21298", "relevant_to_users": "5", "read_by_users": "9", "topics": "", "key_findings": "This paper reveals that LLMs fundamentally lack native social intelligence capabilities required for effective multi-agent collaboration, despite strong individual performance. The research demonstrates that LLMs struggle with multi-agent coordination tasks including proper communication protocols, theory of mind, and cooperative behavior, challenging assumptions that scaling alone produces better multi-agent systems. The work provides a comprehensive diagnostic of specific behavioral and social deficiencies preventing LLMs from exhibiting sophisticated coordinated agent behaviors.", "description": "The paper examines the critical gap between LLM capabilities in single-agent scenarios versus their actual performance in multi-agent environments. It systematically identifies specific behavioral and social competencies that are missing when LLMs are deployed as autonomous agents in collaborative settings.", "key_contribution": "The main contribution is a comprehensive identification and analysis of the specific ways LLMs fail at multi-agent coordination, establishing that these failures stem from fundamental gaps in social intelligence rather than just capability limitations. This diagnostic work highlights overlooked structural deficiencies in current LLM-based agent architectures.", "novelty": "Unlike previous work that celebrated LLM-agent capabilities or focused on individual agent improvements, this paper distinctly examines why multi-agent systems built from LLMs systematically underperform expectations. It shifts focus from capability scaling to examining structural behavioral deficiencies in social and communicative domains that single-agent evaluation frameworks typically ignore, addressing the assumption that model improvements automatically translate to better multi-agent performance.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4101719", "score": "80", "title": "Factorio Learning Environment", "authors": "Jack Hopkins, Mart Bakler, Akbir Khan", "session_type": "SD-4-312", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2503.09617", "relevant_to_users": "2", "read_by_users": "11", "topics": "", "key_findings": "FLE introduces an unbounded agentic benchmark that addresses saturation in existing LLM benchmarks through exponentially scaling challenges. Evaluation of six frontier models (Claude 3.5-Sonnet, GPT-4o, Deepseek-v3, Gemini-2-Flash, Llama-3.3-70B) reveals severe limitations: even the most advanced models struggle to coordinate more than six machines, with Claude achieving only 21.9% success on structured tasks. Models exhibit critical failures in spatial reasoning (placing entities on top of each other), error correction (degenerate loops with 78 contiguous identical errors), and long-term planning (short-sighted objectives, inability to scale factories beyond 3 sections). The benchmark enables quantitative differentiation across capability levels through Production Score metrics spanning orders of magnitude.", "description": "FLE is a program synthesis benchmark built on the game Factorio that evaluates LLMs on long-term planning, resource optimization, and spatial reasoning through two settings: lab-play (24 structured automation tasks) and open-play (unbounded factory building on procedurally generated maps). Agents interact via Python API writing code to place entities, manage resources, and optimize production pipelines across 5000-step trajectories with partial observability.", "key_contribution": "The primary innovation is an unbounded evaluation framework that eliminates benchmark saturation through exponentially scaling complexity‚Äîlate-game technologies require ~300x more resources than early research, with factories processing millions of resource units per second. Unlike linear progression systems in existing benchmarks, FLE's multiplicative technology tree and interconnected manufacturing dependencies (60+ machines for single components) enable meaningful differentiation between increasingly capable models without artificial ceilings.", "novelty": "FLE addresses critical limitations in existing agentic benchmarks (ALFWorld, MineDojo, NetHack) which saturate due to linear progression and fixed completion states. It uniquely combines program synthesis as the action space (aligning with LLM strengths) with Factorio's precise industrial optimization demands requiring coordination of complex spatial layouts and resource flows. The REPL-based interface treats accumulated programs as cumulative knowledge representation, enabling persistent state and function reuse‚Äîa novel approach compared to discrete action spaces in traditional RL environments. The benchmark's exponential scaling and partial observability create a testbed that will remain challenging as models improve, solving the 'moving goalpost' problem in AI evaluation.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation", "Planning and Decision Making"]}, {"paper_id": "4317546", "score": "80", "title": "Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition", "authors": "Andy Zou, Maxwell Lin, Eliot Jones, Micha Nowak, Mateusz Dziemian, Nick Winter, Valent Nathanael, Ayla Croft, Xander Davies, ..., Matt Fredrikson", "session_type": "SD-6-4915", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2507.20526", "relevant_to_users": "8", "read_by_users": "18", "topics": "", "key_findings": "The paper analyzes a large-scale red-teaming competition revealing that AI agents face multi-stage security vulnerabilities across their entire execution pipeline, not just at the prompt level. Tool integration creates novel attack surfaces, and defense effectiveness varies significantly based on attack sophistication and agent architecture. The research provides empirical evidence from numerous real attack attempts showing patterns in how autonomous agents with tool-use capabilities can be compromised.", "description": "This paper examines security vulnerabilities in autonomous AI agent systems through empirical analysis of a large-scale public red-teaming competition. It identifies attack vectors across the agent execution pipeline including prompt injection, tool exploitation, and planning phase vulnerabilities, while evaluating defensive strategies for securing agent deployments.", "key_contribution": "The paper provides the first large-scale empirical analysis of security vulnerabilities in complete AI agent systems (not just LLMs), presenting a comprehensive vulnerability taxonomy and defense evaluation based on real red-teaming competition data with numerous attack attempts.", "novelty": "Unlike previous work focused solely on LLM security, this research addresses complete autonomous agent systems with tool-use and planning capabilities. It moves beyond theoretical threat models by analyzing real attack patterns from a large-scale competition, providing scalable insights rather than isolated case studies. The work identifies novel attack surfaces created by tool integration and multi-stage vulnerabilities across the agent execution pipeline.", "ai_categories": ["Agent Safety and Security", "Agent Benchmarking and Evaluation", "Tool Use and Code Generation"]}, {"paper_id": "4306120", "score": "79", "title": "PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors", "authors": "Yimeng Chen, Piotr Pi√Ñ¬ôkos, Mateusz Ostaszewski, Firas Laakom, J√É¬ºrgen Schmidhuber", "session_type": "SD-2-2108", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2507.15550", "relevant_to_users": "4", "read_by_users": "9", "topics": "", "key_findings": "PhysGym is a novel benchmark for evaluating LLMs in interactive physics discovery tasks. The paper reveals significant performance variations across state-of-the-art models in conducting scientific discovery through iterative hypothesis testing. The work establishes baseline metrics and identifies which model architectures better support active scientific reasoning with controlled environmental feedback.", "description": "This paper introduces PhysGym, a benchmark designed to evaluate large language models in interactive physics discovery tasks where models must iteratively propose hypotheses, receive environmental feedback, and refine their understanding. The benchmark addresses the gap between LLMs' general capabilities and their ability to conduct systematic scientific discovery in controlled physics environments.", "key_contribution": "The main contribution is a structured benchmark with controlled priors‚Äîenvironmental constraints that guide discovery without fully determining outcomes‚Äîenabling quantifiable, reproducible assessment of LLMs' scientific reasoning and discovery capabilities across multiple physics domains and difficulty levels.", "novelty": "Unlike existing evaluations that focus on passive scientific question-answering, PhysGym introduces active, iterative hypothesis testing in interactive physics simulations. The controlled priors approach provides structured environmental constraints that guide discovery while maintaining open-ended exploration. This addresses limitations in prior work by offering reproducible metrics for assessing discovery capabilities rather than mere factual recall or reasoning.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4220404", "score": "79", "title": "WebDancer: Towards Autonomous Information Seeking Agency", "authors": "Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhenglin Wang, Zhengwei Tao, Ding-Chu Zhang, Zekun Xi, ..., Jingren Zhou", "session_type": "SD-3-5515", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/7c886fbc63b09377d123254d93907b41820d72d7.pdf", "relevant_to_users": "1", "read_by_users": "2", "topics": "", "key_findings": "WebDancer is the first end-to-end trained deep research agent with a comprehensive 4-stage training pipeline (browsing data construction, trajectory sampling, supervised fine-tuning, reinforcement learning). It demonstrates substantial gains over vanilla ReAct baselines across different model scales and even surpasses GPT-4o performance in best-case scenarios on challenging benchmarks GAIA and WebWalkerQA. The work pioneers agentic data synthesis for deep research tasks using CRAWLQA (mimicking human browsing patterns) and E2HQA (iteratively evolving questions) methods, showing that both RL and SFT phases consistently improve agent performance metrics.", "description": "WebDancer presents a cohesive paradigm for building autonomous information-seeking web agents from a data-centric and training-stage perspective. The framework addresses complex real-world problems requiring in-depth information seeking and multi-step reasoning by implementing a systematic 4-stage training approach under the ReAct framework.", "key_contribution": "The main contribution is the first complete end-to-end training pipeline for deep research agents, combining novel agentic data synthesis methods (CRAWLQA for human-like browsing, E2HQA for progressive complexity scaling) with a systematic progression from supervised fine-tuning to reinforcement learning for enhanced generalization in autonomous web navigation and information seeking.", "novelty": "Unlike prior training-free methods that cannot effectively leverage reasoning capabilities, or limited SFT/RL approaches with constrained datasets, WebDancer provides a comprehensive data-centric framework spanning the entire training lifecycle. The novelty lies in systematically addressing data construction for web agents through synthetic browsing data generation and progressive question evolution, then applying both SFT for cold-start effectiveness and RL for improved generalization. This end-to-end approach fills the gap between simple web search integration and fully autonomous deep research capabilities.", "ai_categories": ["Web and Computer-Use Agents", "Agent Benchmarking and Evaluation", "Reinforcement Learning for LLMs"]}, {"paper_id": "4208900", "score": "79", "title": "LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models", "authors": "Qianyue Hao, Yiwen Song, Qingmin Liao, Jian Yuan, Yong Li", "session_type": "SD-3-309", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/64ba671c860d8032b21824e28d00916c3e613964.pdf", "relevant_to_users": "0", "read_by_users": "18", "topics": "", "key_findings": "LLM-Explorer achieves 37.27% improvement in mean human-normalized Atari scores by replacing traditional preset exploration strategies (Œµ-greedy, Gaussian noise) with LLM-generated adaptive exploration distributions. The method demonstrates broad compatibility across multiple RL algorithms (DQN variants, DDPG, TD3) and works in both discrete and continuous action spaces. It leverages LLMs' reasoning capabilities to analyze agent trajectories and generate task-specific, dynamically-adjusted exploration strategies that evolve based on the agent's real-time learning status.", "description": "LLM-Explorer is a plug-in module that enhances RL policy exploration by using large language models to dynamically generate task-specific exploration strategies. The system periodically samples agent trajectories during training, prompts an LLM to analyze the learning status from action-reward sequences, and generates probability distributions for future policy exploration that adapt to both the task characteristics and the agent's current learning phase.", "key_contribution": "The main innovation is transforming RL exploration from rigid, preset stochastic processes applied uniformly across tasks into adaptive, LLM-generated exploration strategies that respond to specific task characteristics and real-time learning status. This plug-in design maintains compatibility with existing RL algorithms while achieving substantial performance improvements through contextually-appropriate exploration.", "novelty": "Unlike traditional methods that use fixed exploration schedules with simple variance decay, LLM-Explorer addresses the fundamental limitation of non-adaptive exploration by leveraging LLMs' analytical reasoning to generate emergent, task-specialized strategies. Previous approaches failed to adjust flexibly according to the agent's real-time learning status, whereas this work uses a two-stage process (status summarization ‚Üí strategy generation) to create exploration distributions that evolve based on observed behavior patterns. This represents a paradigm shift from stochastic process design to emergent strategy generation, where exploration adapts to what the agent is actually learning rather than following predetermined mathematical schedules.", "ai_categories": ["Reinforcement Learning for LLMs", "Planning and Decision Making", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4442347", "score": "78", "title": "Can Dependencies Induced by LLM-Agent Workflows Be Trusted?", "authors": "Yu Yao, Yiliao Song, Yian Xie, Mengdan Fan, Mingyu Guo, Tongliang Liu", "session_type": "SD-2-1904", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/d84e4a86a34dd41d3f1e6d33048f4c075db78d62.pdf", "relevant_to_users": "0", "read_by_users": "0", "topics": "", "key_findings": "", "description": "", "key_contribution": "", "novelty": "", "ai_categories": []}, {"paper_id": "4292024", "score": "78", "title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding", "authors": "Jingli Lin, Chenming Zhu, Runsen Xu, Xiaohan Mao, Xihui Liu, Tai WANG, Jiangmiao Pang", "session_type": "SD-5-4601", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2507.07984", "relevant_to_users": "12", "read_by_users": "31", "topics": "", "key_findings": "OST-Bench reveals that current multimodal large language models (MLLMs) exhibit significant performance degradation as exploration horizons extend and memory grows in dynamic environments. The benchmark, comprising 1.4k scenes and 10k Q&A pairs from three 3D datasets, identifies two critical failure modes: spatial reasoning demands and long-term memory retrieval capabilities, exposing fundamental limitations in embodied scene understanding.", "description": "OST-Bench is a benchmark for evaluating MLLMs' ability to process incrementally acquired visual observations in dynamic environments while maintaining spatio-temporal understanding. It assesses models on their capacity to reason over sequentially arriving visual data in embodied scenarios where agents progressively explore environments.", "key_contribution": "The benchmark provides the first dedicated evaluation framework for online spatio-temporal scene understanding, combining sequential visual processing with long-term memory retrieval requirements that reflect real-world embodied perception challenges.", "novelty": "Unlike existing benchmarks that use static, pre-recorded inputs for offline evaluation, OST-Bench emphasizes the online aspect where visual observations arrive incrementally during environment exploration. It explicitly targets the integration of long-term memory retrieval with spatial reasoning in evolving scenes, addressing the gap between static evaluation frameworks and dynamic embodied AI scenarios where agents must maintain historical context across extended exploration horizons.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models", "Spatial and Physical Reasoning"]}, {"paper_id": "4216309", "score": "78", "title": "Enigmata:  Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles", "authors": "Jiangjie Chen, Qianyu He, Siyu Yuan, Aili Chen, Zhicheng Cai, Weinan Dai, Hongli Yu, Jiaze Chen, Xuefeng Li, ..., Mingxuan Wang", "session_type": "SD-6-5104", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/d16d55cfeff749792ae0b093a3f4c0123aa6c09f.pdf", "relevant_to_users": "1", "read_by_users": "11", "topics": "", "key_findings": "Enigmata introduces the first comprehensive suite for improving LLM puzzle reasoning with 36 tasks across 7 categories, each with unlimited controllable generation and automatic verification. The trained Qwen2.5-32B-Enigmata model surpasses o3-mini-high and o1 on puzzle reasoning benchmarks, achieving 32.8% on ARC-AGI and 0.6% on ARC-AGI 2. The work demonstrates that puzzle training transfers to advanced mathematical reasoning (AIME, BeyondAIME, GPQA) when applied to larger models, with minimal multi-tasking trade-offs.", "description": "This paper presents Enigmata, a comprehensive framework for scaling logical reasoning in LLMs through synthetic verifiable puzzles. The system uses a generator-verifier architecture to create unlimited training examples with controllable difficulty across 36 diverse puzzle tasks, enabling scalable multi-task reinforcement learning with verifiable rewards (RLVR).", "key_contribution": "The main contribution is the first systematic and scalable puzzle reasoning framework combining procedural generation, automatic verification, and multi-task RLVR training, along with the Enigmata-Eval benchmark. The work demonstrates that puzzle-based training generalizes to out-of-domain reasoning tasks including advanced mathematics.", "novelty": "Unlike previous work that lacked systematic frameworks for puzzle reasoning, Enigmata provides unlimited scalable training data with automatic verification mechanisms integrated into RLVR pipelines. It addresses the limitation that state-of-the-art models like o1 excel at domain-specific reasoning but struggle with general puzzles solvable by humans without specialized knowledge. The framework uniquely demonstrates transfer learning from puzzle reasoning to mathematical domains, showing that diverse logical reasoning skills can generalize across problem types.", "ai_categories": ["Reasoning and Test-Time Compute", "Mathematical and Logical Reasoning", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4251426", "score": "77", "title": "Eliciting Reasoning in Language Models with Cognitive Tools", "authors": "Brown Ebouky, Andrea Bartezzaghi, Mattia Rigotti", "session_type": "SD-1-302", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ea576c6461243d27a8102c9dfa554db950d66757.pdf", "relevant_to_users": "9", "read_by_users": "28", "topics": "", "key_findings": "The paper introduces cognitive tools that increase GPT-4.1's pass@1 performance on AIME2024 from 32% to 53%, surpassing o1-preview. The approach demonstrates that base LLMs have latent reasoning capabilities that can be elicited through structured modular workflows without reinforcement learning. Four cognitive tools are proposed: understand question, recall related, examine answer, and backtracking‚Äîeach executed by the LLM itself within a tool-calling framework. This supports the hypothesis that pre-training instills reasoning capabilities that can be surfaced through structured scaffolding rather than requiring extensive post-training.", "description": "This paper investigates how to elicit reasoning capabilities in language models by endowing them with a small set of 'cognitive tools' that encapsulate specific reasoning operations, each executed by the LLM itself. Drawing inspiration from cognitive psychology and cognitive architectures, the approach treats reasoning as orchestrated sequential execution of modular cognitive operations rather than emergent behavior from prompting or learned reward signals.", "key_contribution": "The main contribution is demonstrating that structured cognitive tools‚Äîmodular reasoning operations inspired by cognitive science‚Äîcan unlock significant reasoning improvements in base LLMs without reinforcement learning. This compartmentalized approach achieves state-of-the-art performance on mathematical benchmarks by reducing interference between reasoning operations and making structured reasoning more intrinsic to the generation process.", "novelty": "Unlike chain-of-thought prompting that relies on natural language step-by-step reasoning, or reinforcement learning approaches that learn from reward signals, cognitive tools provide explicit structural constraints and encode domain expertise directly. The key innovation is that these tools are inwardly focused‚Äîexecuted by the LLM itself rather than connecting to external systems‚Äîcompartmentalizing reasoning steps to reduce interference. This addresses limitations of flat prompts and monolithic CoT approaches while providing an alternative theoretical framework for understanding how reasoning emerges from pre-training rather than post-training.", "ai_categories": ["Reasoning and Test-Time Compute", "Tool Use and Code Generation", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4207185", "score": "77", "title": "MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem", "authors": "Fan Liu, Zhe-Rui Yang, Cancheng Liu, Tianrui SONG, Xiaofeng Gao, Hao Liu", "session_type": "SD-3-5516", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/e01a39be0de98c2f808394f7affa2bfb004c68a5.pdf", "relevant_to_users": "4", "read_by_users": "9", "topics": "", "key_findings": "MM-Agent achieves 11.88% improvement over human expert solutions on real-world mathematical modeling problems, requiring only 15 minutes and $0.88 per task using GPT-4o. The framework successfully assisted undergraduate teams to win the Finalist Award (top 2.0%) in MCM/ICM 2025 among 27,456 teams. The paper introduces MM-Bench, a benchmark of 111 authentic problems from Mathematical Contest in Modeling spanning 2000-2025 across ten diverse domains.", "description": "This paper addresses the gap between LLM mathematical reasoning capabilities and open-ended mathematical modeling of real-world problems. It introduces MM-Agent, an expert-inspired framework that decomposes mathematical modeling into four stages: problem analysis, model formulation, computational solving, and report generation. The paper also presents MM-Bench, a curated benchmark for evaluating mathematical modeling capabilities.", "key_contribution": "The main contribution is formalizing real-world mathematical modeling as a distinct AI task and proposing MM-Agent, a multi-stage framework that systematically handles problem abstraction and formalization. The introduction of MM-Bench provides the first rigorous benchmark for evaluating LLM-based mathematical modeling across diverse scientific domains.", "novelty": "Unlike prior work that focuses on mathematical reasoning with predefined problem formulations, this work tackles the harder problem of open-ended modeling where agents must translate ambiguous real-world scenarios into formal mathematical systems. The novelty lies in decomposing modeling into expert-guided stages rather than treating it as generic reasoning, and demonstrating that LLMs can handle the full modeling pipeline from problem analysis to solution. Previous benchmarks focused on computational reasoning, whereas MM-Bench evaluates the complete modeling process including problem abstraction and formulation construction.", "ai_categories": ["Tool Use and Code Generation", "Mathematical and Logical Reasoning", "Planning and Decision Making"]}, {"paper_id": "4101882", "score": "77", "title": "AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents", "authors": "Arman Zharmagambetov, Chuan Guo, Ivan Evtimov, Maya Pavlova, Ruslan Salakhutdinov, Kamalika Chaudhuri", "session_type": "SD-4-1304", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2503.09780", "relevant_to_users": "4", "read_by_users": "5", "topics": "", "key_findings": "AgentDAM reveals that autonomous web agents exhibit significant privacy vulnerabilities, leaking sensitive information including credentials, personal data, and browsing history. The study demonstrates that accessibility tree representations used by agents can expose unintended information, and standard privacy protections are often insufficient. Different agent architectures show varying levels of privacy exposure, providing actionable insights for mitigation strategies.", "description": "This paper introduces AgentDAM, a systematic framework for evaluating privacy leakage in autonomous web agents that interact with websites on behalf of users. The research investigates what sensitive information these agents inadvertently leak during real-world web browsing operations across multiple agent architectures and attack scenarios.", "key_contribution": "The first comprehensive privacy leakage benchmark and evaluation framework specifically designed for autonomous web agents, providing both systematic methodology for assessing agent-specific privacy vulnerabilities and concrete mitigation approaches.", "novelty": "While previous work addressed agent safety from adversarial perspectives, AgentDAM is the first to provide a systematic privacy evaluation framework specifically for web agents. It moves beyond general privacy concerns to identify agent-specific vulnerabilities arising from accessibility tree representations and agent execution traces. The work addresses the gap between traditional privacy protections and the unique challenges posed by autonomous agents operating on sensitive user data.", "ai_categories": ["Agent Safety and Security", "Agent Benchmarking and Evaluation", "Web and Computer-Use Agents"]}, {"paper_id": "4143210", "score": "77", "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay", "authors": "Akshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Manoj Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, ..., Caiming Xiong", "session_type": "SD-4-109", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2504.03601", "relevant_to_users": "1", "read_by_users": "8", "topics": "", "key_findings": "APIGen-MT introduces an agentic pipeline for generating high-quality multi-turn conversational data through simulated agent-human interactions. The paper demonstrates that models trained on the 5,000-sample APIGen-MT dataset achieve competitive performance on function-calling benchmarks, and that multi-turn training substantially improves agent capabilities compared to single-turn approaches. The work provides a scalable methodology for synthetic data generation that captures sequential dependencies, error recovery patterns, and natural dialogue flows essential for real-world agent deployment.", "description": "This paper presents APIGen-MT, a framework for automatically generating synthetic multi-turn instruction-following data by simulating realistic interactions between a user agent and an assistant agent. The system creates complex, multi-step dialogues involving tool use and API calling that better represent real-world agent deployment scenarios than traditional single-turn datasets.", "key_contribution": "The main contribution is an agentic pipeline that generates multi-turn conversational data through simulated agent-human interplay, moving beyond isolated single-turn tool calls to capture the complexity of iterative, context-dependent interactions. The paper releases a 5,000-sample dataset and demonstrates that training on this synthetic multi-turn data improves agent performance on tool-use benchmarks.", "novelty": "Unlike previous work like APIGen that focuses on single-turn API calling scenarios, APIGen-MT addresses the gap between single-turn training data and multi-turn evaluation by simulating realistic conversational flows with sequential dependencies, error recovery, and natural feedback patterns. The approach uses two LLM agents playing user and assistant roles in an iterative feedback loop, rather than generating isolated examples or simply concatenating single-turn interactions, which enables capturing the contextual coherence and conversational dynamics critical for robust agent deployment.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation"]}, {"paper_id": "4441817", "score": "77", "title": "SimWorld: An Open-ended Simulator for Agents in Physical and Social Worlds", "authors": "Xiaokang Ye, Jiawei Ren, Yan Zhuang, Xuhong He, Yiming Liang, Yiqing Yang, Mrinaal Dogra, Xianrui Zhong, Eric Liu, ..., Lianhui Qin", "session_type": "SD-5-401", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/a98dee23d8b37552151336bbac20c838fd5e9ee1.pdf", "relevant_to_users": "3", "read_by_users": "13", "topics": "", "key_findings": "SimWorld demonstrates that frontier LLM/VLM agents (GPT-4o, Claude-3.5, Gemini-2.5-Flash, DeepSeek-Prover-V2) exhibit distinct reasoning patterns and limitations when deployed on long-horizon multi-agent tasks involving strategic cooperation and competition in realistic environments. The simulator successfully enables language-driven procedural generation of diverse scenarios, moving beyond static hand-crafted environments that limit existing platforms. The work provides an open-source foundation for advancing agent intelligence in complex physical and social settings.", "description": "SimWorld is an Unreal Engine 5-based simulator designed to develop and evaluate LLM/VLM agents in realistic environments with accurate physical and social dynamics. It provides multimodal world inputs, open-vocabulary actions at varying abstraction levels, and language-driven procedural generation for creating diverse, extensible scenarios.", "key_contribution": "The main innovation is a purpose-built simulator for LLM/VLM agents that combines realistic physics and social dynamics with language-driven procedural environment generation, enabling dynamic scenario creation and natural agent interaction through open-vocabulary actions at multiple abstraction levels.", "novelty": "Unlike existing simulators that rely on hand-crafted environments with simplified game-like physics and social rules (e.g., MineDojo, Habitat, VirtualHome), SimWorld uses language-driven procedural generation for dynamic, diverse scenarios and prioritizes authentic physical and social behavior. It was purpose-built for language model agents rather than adapted from robotics or embodied AI platforms, providing native LLM/VLM support with multimodal inputs and open-vocabulary actions.", "ai_categories": ["Agent Benchmarking and Evaluation", "Multi-Agent Systems and Collaboration", "Planning and Decision Making"]}, {"paper_id": "4146941", "score": "76", "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention", "authors": "Gleb Rodionov, Roman Garipov, Alina Shutova, George Yakushev, Erik Schultheis, Vage Egiazarian, Anton Sinitsin, Denis Kuznedelev, Dan Alistarh", "session_type": "SD-1-1906", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/752690b760cb01f226ba630228b97af81df4d1d1.pdf", "relevant_to_users": "4", "read_by_users": "20", "topics": "", "key_findings": "Hogwild! Inference enables multiple LLM instances to run in parallel with a shared attention cache, allowing emergent collaboration without predefined strategies. The method exploits RoPE properties to avoid expensive KV cache recomputation while enabling real-time token-level synchronization between workers. Experiments show consistent improvements over single-threaded baselines on complex reasoning tasks, with workers dynamically coordinating work splitting, error detection, and plan adaptation.", "description": "This paper introduces a parallel LLM inference system where multiple model instances share a concurrent Key-Value attention cache, enabling them to see each other's generated tokens in real-time and dynamically determine their own collaboration strategy. Unlike fixed frameworks that impose voting or task decomposition, Hogwild! Inference allows reasoning-capable LLMs to emergently coordinate without fine-tuning.", "key_contribution": "The main innovation is leveraging Rotary Position Embeddings (RoPE) to enable efficient sharing of KV cache representations across parallel workers at different sequence positions without cubic-complexity recomputation, combined with a flexible architecture that allows LLMs to develop their own collaboration strategies through prompting rather than enforcing predetermined coordination patterns.", "novelty": "Unlike prior work requiring independent reasoning with aggregation (self-consistency) or fixed plan-execute schedules (skeleton-of-thought, task decomposition), this approach enables dynamic real-time interaction where workers adaptively coordinate, correct errors, and redistribute work as problems unfold. The technical novelty lies in exploiting RoPE's rotational invariance to share attention cache across workers efficiently, addressing the limitation that existing parallel methods either lack interaction or impose rigid collaboration frameworks that fail when problems don't fit predetermined structures.", "ai_categories": ["Reasoning and Test-Time Compute", "Multi-Agent Systems and Collaboration"]}, {"paper_id": "4256007", "score": "76", "title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective", "authors": "Jorge (Zhoujun) Cheng, Shibo Hao, Tianyang Liu, Fan Zhou, Yutao Xie, Feng Yao, Yuexin Bian, Nilabjo Dey, Yonghao Zhuang, ..., Zhiting Hu", "session_type": "SD-1-107", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.14965", "relevant_to_users": "8", "read_by_users": "16", "topics": "", "key_findings": "The paper provides a comprehensive cross-domain empirical study of RL techniques for LLM reasoning, revealing critical insights about optimization dynamics, reward design, and training stability. It systematically examines how RL generalizes across reasoning domains and identifies domain-specific factors affecting convergence and performance, offering actionable guidance for practitioners on methodology selection.", "description": "This paper investigates reinforcement learning approaches for improving large language model reasoning capabilities across multiple domains. The authors conduct a comprehensive empirical study examining how RL methods enhance performance on reasoning tasks by leveraging reward signals and policy optimization, providing a structured framework for understanding when and why these methods succeed or fail.", "key_contribution": "The paper develops a cross-domain framework for understanding RL-based reasoning improvements in LLMs, providing systematic insights into the relationship between reward design and reasoning quality across diverse problem domains with practical guidance for practitioners.", "novelty": "Unlike previous work that focuses on single-task RL evaluations or proposes isolated algorithms, this work takes a systematic cross-domain perspective to understand RL's broader applicability to reasoning tasks. It addresses the limitation of fragmented knowledge by identifying domain-specific factors affecting RL effectiveness and clarifying when different RL mechanisms prove beneficial for language model reasoning, shifting from method proposals toward deeper understanding.", "ai_categories": ["Reinforcement Learning for LLMs", "Reasoning and Test-Time Compute"]}, {"paper_id": "4087784", "score": "76", "title": "Language Models can Self-Improve at State-Value Estimation for Better Search", "authors": "Ethan Mendes, Alan Ritter", "session_type": "SD-5-1916", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ea251516f78a77cbc42dd4a50e2ce4aadfc2226f.pdf", "relevant_to_users": "5", "read_by_users": "10", "topics": "", "key_findings": "The paper demonstrates that language models can self-improve their state-value estimation abilities without ground-truth rewards or human demonstrations through Self-Taught Lookahead (STL). An 8B-parameter open-source model achieves 39% improvement in web agent success rates, matching GPT-4o performance while being 5x cheaper. STL enables value functions to reason explicitly about state transitions using natural language rationales, showing significant improvements across web navigation, multi-hop QA, and mathematical reasoning tasks.", "description": "This paper introduces Self-Taught Lookahead (STL), a reward-free self-supervised framework that improves language model-based value functions for search in multi-step reasoning tasks. Instead of training on numeric values alone, STL trains models to predict the next action, resulting state, and a rationale explaining state value, leveraging LLMs' natural language reasoning capabilities.", "key_contribution": "STL is the first reward-free framework that enables language models to self-improve at value estimation by reasoning explicitly about state transitions through natural language. It adapts value iteration for language models, training them to generate action-outcome rationales alongside lookahead values, eliminating the need for ground-truth rewards or human demonstrations.", "novelty": "Unlike classical RL and prior LLM search methods that train only on numeric value targets, STL exploits language models' natural reasoning by having them generate textual action-outcome rationales alongside value predictions. This addresses the limitation that collecting ground-truth rewards or human demonstrations is expensive and often infeasible for multi-step reasoning tasks. The approach enables better generalization to unseen tasks by using language-based reasoning rather than pure numeric regression, achieving comparable performance to much larger proprietary models at significantly lower cost.", "ai_categories": ["Reasoning and Test-Time Compute", "Agent Benchmarking and Evaluation", "Web and Computer-Use Agents"]}, {"paper_id": "4224020", "score": "75", "title": "OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation", "authors": "Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Ziyu Ye, Bowei Xia, Tao Sun, Zhaoxuan Jin, Yingru Li, ..., Guohao Li", "session_type": "SD-1-5409", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/9ea2d3d5cf7f874c7669ab5c3f1270eb3bc794d1.pdf", "relevant_to_users": "0", "read_by_users": "2", "topics": "", "key_findings": "OWL introduces a novel multi-agent training paradigm that achieves state-of-the-art performance (69.70%) on the GAIA benchmark, outperforming OpenAI's Deep Research by 2.34%. The OWL-trained 32B model achieves 52.73% accuracy with a +16.37% improvement and demonstrates performance comparable to GPT-4o. The key innovation is enabling cross-domain transferability through hierarchical decoupling of strategic planning from specialized execution, eliminating the need for complete architectural redesign when adapting to new domains.", "description": "This paper presents Workforce, a hierarchical multi-agent framework with a domain-agnostic Planner, Coordinator, and specialized Workers, combined with Optimized Workforce Learning (OWL) that uses reinforcement learning on real-world feedback to train generalizable planners. The framework achieves cross-domain transferability by decoupling strategic planning from domain-specific execution, enabling practical deployment across varied real-world scenarios.", "key_contribution": "The main contribution is the OWL training paradigm that enables domain-agnostic planning through reinforcement learning on real-world feedback, combined with the modular Workforce architecture that allows selective component updates without full system retraining when entering new domains.", "novelty": "Unlike prior multi-agent systems that require complete architectural redesign and full retraining when applied to new domains, OWL separates strategic planning from execution through hierarchical modularization. This enables the domain-agnostic planner to transfer across domains while only requiring worker-level modifications, addressing the fundamental inflexibility and poor transferability of existing multi-agent approaches. The use of reinforcement learning on real-world feedback to train generalizable planners represents a new paradigm for scaling multi-agent systems.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Reinforcement Learning for LLMs", "Tool Use and Code Generation"]}, {"paper_id": "4205122", "score": "75", "title": "TimE: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "authors": "Shaohang Wei, Wei Li, Feifan Song, Wen Luo, Tianyi Zhuang, Haochen Tan, Zhijiang Guo, Houfeng Wang", "session_type": "SD-1-1806", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.12891", "relevant_to_users": "0", "read_by_users": "1", "topics": "", "key_findings": "TIME introduces a comprehensive 38,522 QA-pair benchmark revealing that current LLMs struggle with real-world temporal reasoning despite strong performance on simplified tasks. Advanced reasoning models like o3-mini achieve 93.33% on basic extraction but only 52.62% on order reasoning, while test-time scaling shows contradictory effects‚Äîimproving logical reasoning by 24.44% but degrading retrieval accuracy by 8.16%. The benchmark establishes that temporal retrieval ability is fundamentally correlated with all higher-level temporal reasoning capabilities, and provides TIME-Lite, a human-verified 938-instance subset for standardized evaluation.", "description": "TIME is a hierarchical three-level benchmark with 11 fine-grained sub-tasks evaluating LLM temporal reasoning across three real-world datasets: TIME-Wiki (knowledge-intensive structured data), TIME-News (dynamic events from news articles with 500K+ tokens), and TIME-Dial (multi-session conversations with 15K+ tokens requiring memory-based reasoning). The benchmark progresses from basic temporal understanding and retrieval (Level 1) through temporal expression reasoning (Level 2) to complex multi-event relationship reasoning including co-temporality and counterfactuals (Level 3).", "key_contribution": "The main innovation is a unified hierarchical framework that captures authentic real-world temporal reasoning challenges through three distinct data sources reflecting intensive information density, fast-changing event dynamics, and complex temporal dependencies in social interactions. Unlike prior benchmarks focusing on simplified scenarios or single temporal aspects, TIME provides comprehensive multi-level evaluation with human-verified subsets and reveals fundamental connections between basic retrieval and complex reasoning capabilities.", "novelty": "TIME addresses critical gaps in existing temporal reasoning benchmarks (TRAM, TimeQA, TempReason) which focus on simplified scenarios or basic temporal commonsense. It introduces three novel real-world challenge dimensions: knowledge-intensive scenarios with structured temporal data, dynamically evolving events requiring RAG over 500K+ token contexts, and extended multi-session dialogues with implicit temporal references. The hierarchical three-level structure with 11 sub-tasks establishes temporal reasoning as a progressive capability framework rather than isolated skills, and the unified evaluation prevents dataset bias while demonstrating that retrieval ability fundamentally correlates with all higher-level temporal reasoning tasks.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4213643", "score": "74", "title": "Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL", "authors": "Joey Hong, Anca Dragan, Sergey Levine", "session_type": "SD-3-5419", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/a27fd29bd9a99f5624d5362501cf5a6acc4c4e37.pdf", "relevant_to_users": "7", "read_by_users": "22", "topics": "", "key_findings": "This paper introduces Planning with Natural Language Critic (PNLC), a method that trains lightweight goal-conditioned value functions to guide frontier LLM reasoning without search. The approach achieves comparable or superior performance to search-based methods (Agent Q, Strategist, GDP-Zero) while using 10√ó less inference time (5s vs 46-62s per decision). Key results include 78.2 WebShop score (48% success), 47% Avalon win rate, and $0.87 persuasion donation average. The method operates by training a 2-layer MLP to predict outcome probabilities for hypothetical futures, enabling API-only LLMs to perform sophisticated multi-turn planning without expensive fine-tuning or tree search.", "description": "The paper presents a lightweight offline goal-conditioned RL approach for enhancing LLM agent planning capabilities without requiring search or direct model fine-tuning. By training small value functions that predict outcome probabilities at the reasoning-step level, the method enables frontier LLMs to evaluate multiple hypothetical futures and refine their decisions through natural language critique.", "key_contribution": "The main innovation is a goal-conditioned value function that operates at the thought level rather than utterance level, producing natural language critiques by evaluating multiple outcome probabilities (both positive and negative futures). This lightweight auxiliary module (2-layer MLP) enables sophisticated planning for API-based frontier LLMs without expensive search procedures or direct model fine-tuning.", "novelty": "Unlike prior RL methods that require expensive fine-tuning of the entire LLM or search-based approaches that need extensive inference-time computation, PNLC trains only a small auxiliary value function from offline data. The key innovation is using goal-conditioned value functions that predict multiple outcome probabilities rather than scalar rewards, enabling richer reasoning about consequences. By operating at the thought level and generating natural language critiques of hypothetical futures, it addresses the computational barriers preventing RL from scaling to frontier models while avoiding the inference costs of tree search methods.", "ai_categories": ["Reinforcement Learning for LLMs", "Reasoning and Test-Time Compute", "Tool Use and Code Generation"]}, {"paper_id": "4202422", "score": "74", "title": "InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction", "authors": "Bin Lei, Weitai Kang, Zijian Zhang, Winson Chen, Xi Xie, Shan Zuo, Mimi Xie, Ali Payani, Mingyi Hong, ..., Caiwen Ding", "session_type": "SD-5-1602", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ae7bb751a0d72057590b5907d9d4fa86c412be50.pdf", "relevant_to_users": "2", "read_by_users": "7", "topics": "", "key_findings": "InfantAgent-Next achieves 35.3% success on OSWorld (50-step limit), outperforming Claude Computer Use (26.0%) and other vision-based agents. The modular architecture enables task decomposition across specialized models, addressing critical limitations where single models fail at precise GUI localization despite strong reasoning (e.g., Claude-3.7-Sonnet achieves only 0.8% on ScreenSpot-Pro). The system demonstrates generality across diverse benchmarks: 66% on SWE-Bench-Verified, 31.67% on SWE-Bench-Lite, and 56.97% on GAIA. Novel execution mechanisms like iterative region cropping for mouse clicks and fuzzy-matching for file edits substantially improve accuracy without requiring model retraining.", "description": "InfantAgent-Next is a multimodal generalist agent that automates computer interaction through a modular multi-agent architecture. The system integrates tool-based and pure vision agents in a three-stage workflow (planning, tool selection, execution), enabling different specialized models to collaboratively solve decoupled subtasks step-by-step while maintaining unified dialogue context.", "key_contribution": "The main innovation is a modular multi-agent paradigm that dynamically routes subtasks to specialized models rather than forcing a single model to handle all aspects of computer interaction. This architectural approach, combined with execution-level improvements (iterative cropping for GUI localization, fuzzy matching for code editing), bridges the gap between tool-based agents (which lack generality) and pure vision agents (which struggle with precise interactions).", "novelty": "Unlike prior work that either uses single monolithic models for all tasks (leading to poor GUI localization despite strong reasoning) or rigid tool-centric workflows (requiring manual tool definitions), InfantAgent-Next employs fine-grained specialization where planning, tool selection, and execution are handled by different models optimized for each stage. It addresses the fundamental limitation that contemporary systems like Claude Computer Use and GPT-4o cannot simultaneously excel at complex reasoning and precise spatial interactions. The approach demonstrates that architectural modularity with targeted execution strategies can outperform larger unified models without requiring new model training.", "ai_categories": ["Web and Computer-Use Agents", "Tool Use and Code Generation", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4226687", "score": "74", "title": "AgentAuditor: Human-level Safety and Security Evaluation for LLM Agents", "authors": "Hanjun Luo, Shenyu Dai, Chiming Ni, Xinfeng Li, Guibin Zhang, Kun Wang, Tongliang Liu, Hanan Salam", "session_type": "SD-6-1106", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/2ceecf4bf50bffa7cff2d145fadb18c23daf7206.pdf", "relevant_to_users": "5", "read_by_users": "6", "topics": "", "key_findings": "AgentAuditor achieves human-level accuracy in evaluating LLM agent safety and security through a training-free, memory-augmented reasoning framework. The paper introduces ASSEBench, the first comprehensive benchmark with 2293 annotated interaction records covering 15 risk types across 29 application scenarios. The framework consistently improves evaluation performance across all benchmarks by leveraging experiential memory and multi-stage retrieval processes, establishing new state-of-the-art performance in LLM-as-a-judge applications for agent systems.", "description": "This paper addresses the critical challenge of reliably evaluating safety and security in LLM-based agents by introducing AgentAuditor, a universal framework that enables LLM evaluators to function at expert human levels. The work also presents ASSEBench, the first specialized benchmark designed to assess how well LLM-based evaluators can identify both safety risks and security threats in agent interactions.", "key_contribution": "The main contribution is AgentAuditor, a training-free framework that uses memory-augmented reasoning to achieve human-level safety and security evaluation, along with ASSEBench, the first comprehensive benchmark specifically designed for evaluating LLM-based safety evaluators in agent systems with nuanced judgment standards.", "novelty": "Unlike existing rule-based or LLM-based evaluators that miss dangers in step-by-step agent actions and overlook subtle compounding issues, AgentAuditor mimics human expert evaluation by constructing experiential memory from past interactions with structured semantic features and chain-of-thought reasoning traces. It addresses the limitation that human evaluation doesn't scale by using a training-free, multi-stage context-aware retrieval process that dynamically identifies relevant past experiences to guide assessment of new cases. This is the first work to provide both a human-level evaluation framework and a specialized benchmark for agent safety and security evaluation that handles ambiguous situations with dual judgment standards.", "ai_categories": ["Agent Safety and Security", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4441402", "score": "74", "title": "Collaborative Reasoner: Self-Improving Social Agents with Synthetic Conversations", "authors": "Ansong Ni, Ruta Desai, Yang Li, Xinjie Lei, Dong Wang, Jiemin Zhang, Jane Yu, Ramya Raghavendra, Gargi Ghosh, ..., Asli Celikyilmaz", "session_type": "SD-6-1913", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/42a338bca40ea896002753679729eb2240bf62b3.pdf", "relevant_to_users": "4", "read_by_users": "18", "topics": "", "key_findings": "The paper introduces CoRal (Collaborative Reasoner), a framework that reveals current LLMs exhibit undesirable social behaviors with >90% agreement rates, limiting their ability to challenge incorrect solutions in collaborative settings. Through self-improvement using synthetic conversational data generated from simulated self-collaborations, the approach achieves up to 29.4% improvement over single-agent chain-of-thought performance across six collaborative reasoning tasks covering coding, math, scientific QA, and social reasoning. The work also introduces Matrix, a scalable multi-agent communication framework, and demonstrates that models trained with this approach (tested on Llama-3.1, Ministral, and Qwen-2.5) consistently outperform standard finetuned models with gains up to 16.7% absolute.", "description": "This paper addresses the gap between problem-solving and collaborative skills in LLMs by introducing a framework for evaluating and improving how LLM agents work together. The approach focuses on teaching agents to disagree with incorrect solutions, convince partners of correct solutions, and reach consensus as a team through synthetic conversational training data generated from multi-agent self-collaboration.", "key_contribution": "The main contribution is the CoRal framework that enables self-improvement of collaborative reasoning capabilities through synthetic conversation generation, eliminating the need for expensive human-annotated collaborative datasets. The work also introduces Matrix, a scalable multi-agent communication system, and demonstrates that training on synthetic collaborative interactions significantly improves both individual and team performance.", "novelty": "Unlike previous work that emphasized single-turn evaluation and individual problem-solving, this paper specifically targets collaborative reasoning requiring theory-of-mind and effective communication between agents. The key innovation is generating synthetic conversational training data at scale from successful multi-agent interactions where agents convince partners, rather than relying on human-collected datasets. This addresses the critical limitation that existing LLMs, despite strong individual capabilities, exhibit overly agreeable social behaviors that prevent effective collaboration and mutual error correction in multi-agent settings.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Reasoning and Test-Time Compute", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4441317", "score": "73", "title": "LogicTree: Improving Complex Reasoning of LLMs via Instantiated Multi-step Synthetic Logical Data", "authors": "Zehao Wang, Lin Yang, Jie Wang, Kehan Wang, Hanzhu Chen, Bin Wang, Jianye HAO, Defu Lian, Bin Li, Enhong Chen", "session_type": "SD-2-113", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/30059572044fffae201f1e0daf92fc78a5aef218.pdf", "relevant_to_users": "1", "read_by_users": "3", "topics": "", "key_findings": "LogicTree introduces a novel framework for synthesizing multi-step logical reasoning datasets through backward deduction and structural pattern matching, achieving an average 9.4% accuracy improvement across complex logical reasoning benchmarks. The framework constructs formal logic trees via iterative backward deduction, then uses a two-stage LLM-based approach to instantiate these trees into diverse real-world scenarios with contextual significance. This approach demonstrates that synthesizing complex, instantiated logical reasoning data helps LLMs develop generalizable reasoning abilities rather than memorizing templates.", "description": "The paper addresses LLMs' struggles with complex multi-step logical reasoning by proposing LogicTree, a framework for efficiently synthesizing high-quality logical reasoning datasets. Using backward deduction with structural pattern matching, it constructs multi-step logic trees and instantiates them into contextually rich real-world scenarios through a two-stage LLM-based process.", "key_contribution": "The main innovation is a systematic framework that moves beyond template-based data generation by combining formal backward deduction (using first-order logic rules and structural pattern matching) with LLM-based instantiation to create complex, multi-step logical reasoning datasets that are both formally rigorous and contextually grounded in realistic scenarios.", "novelty": "Unlike previous approaches that rely on predefined templates limiting adaptability to real-world scenarios, LogicTree performs backward deduction through structural pattern matching to dynamically construct complex logic trees, then instantiates them using LLMs to generate contextually significant reasoning processes. This addresses the key limitation of template-based methods by creating reasoning data that maintains formal logical structure while being grounded in diverse, realistic contexts, enabling LLMs to develop generalizable reasoning abilities rather than memorizing fixed patterns.", "ai_categories": ["Reasoning and Test-Time Compute", "Tool Use and Code Generation"]}, {"paper_id": "4436485", "score": "73", "title": "LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?", "authors": "Ziyuan He, Yuxuan Wang, Jiaqi Li, Kexin Liang, Muhan Zhang", "session_type": "SD-5-5102", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2510.22548", "relevant_to_users": "1", "read_by_users": "1", "topics": "", "key_findings": "The study reveals significant performance gaps in current state-of-the-art LLMs (including Llama, Mistral, Phi, Qwen, and ChatGPT) when processing extended contexts with complex dependencies. Even advanced models with expanded context windows demonstrate degraded performance when information dependencies span lengthy passages, indicating that simply increasing context window size does not guarantee effective long-range reasoning or genuine comprehension of complex real-world documents.", "description": "LooGLE v2 is an enhanced benchmark for evaluating whether large language models are ready for real-world long-context dependency challenges. It assesses multiple state-of-the-art LLMs across diverse authentic tasks from real domains including legal documents, financial filings, and gameplay scenarios, moving beyond synthetic needle-in-haystack tests to measure genuine comprehension across extended contexts.", "key_contribution": "The main contribution is an improved benchmark that incorporates authentic real-world documents with inherent complexity rather than synthetic tasks, evaluating multiple task categories (summarization, retrieval, counting) with specialized metrics to assess whether models genuinely understand long-range dependencies versus performing simple pattern matching.", "novelty": "Unlike previous benchmarks including the original LooGLE that relied on synthetic insertion tasks and needle-in-haystack tests, LooGLE v2 emphasizes authentic documents from real domains with inherent complexity. It addresses the limitation of context window expansion alone by demonstrating that increased context size doesn't guarantee effective reasoning, and introduces diverse evaluation metrics beyond standard accuracy to assess genuine comprehension challenges present in practical applications rather than artificial pattern matching tasks.", "ai_categories": ["Agent Benchmarking and Evaluation", "Memory and Context Management"]}, {"paper_id": "4257175", "score": "71", "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence", "authors": "Yining Hong, Rui Sun, Bingxuan Li, Xingcheng Yao, Maxine Wu, Alexander Chien, Da Yin, Ying Nian Wu, Zhecan Wang, Kai-Wei Chang", "session_type": "SD-5-4509", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.15677", "relevant_to_users": "4", "read_by_users": "13", "topics": "", "key_findings": "This work reveals that current state-of-the-art AI systems exhibit significant performance gaps compared to humans on tasks requiring integrated physical-digital intelligence, with cross-domain errors accounting for 66.6% of failures in cooking tasks. The research establishes that the primary bottleneck is not individual domain competency but seamless integration between physical and digital realms - agents frequently become trapped in single-domain cycles and fail to effectively transition contexts. The benchmark encompasses ~1,500 tasks across cooking, navigation, shopping, tourism, and geolocation, showing AI performance as low as 6.4% on cooking tasks (vs. 77.08% human performance) and 34.72% on outdoor navigation, establishing both significant challenges and research opportunities at this intersection.", "description": "This paper introduces Embodied Web Agents, a novel paradigm that enables AI agents to fluidly bridge embodiment and web-scale reasoning within a unified framework. The work addresses the limitation that current AI agents are siloed - either performing digital information retrieval or physical embodied interaction, but rarely both - by developing a unified simulation platform integrating realistic 3D indoor/outdoor environments with functional web interfaces, alongside a comprehensive benchmark of ~1,500 tasks requiring coordinated reasoning across physical and digital domains.", "key_contribution": "The main contribution is the first integrated benchmark and unified simulation platform that systematically evaluates agents' ability to bridge embodied perception/action with web-based reasoning across diverse real-world scenarios. This includes a comprehensive evaluation framework revealing that cross-domain integration (transitioning between physical and digital contexts) represents the primary failure mode rather than individual domain competency.", "novelty": "Unlike prior work that treated web agents and embodied agents as separate research areas with isolated benchmarks, this work explicitly targets their intersection by creating tasks that fundamentally require both capabilities simultaneously. It addresses the critical limitation that existing agents cannot handle hybrid tasks like cooking from online recipes or navigating with dynamic map data. The key innovation is demonstrating that the bottleneck isn't competency within individual domains but rather the seamless integration and context-switching between physical embodiment and web-scale reasoning.", "ai_categories": ["Web and Computer-Use Agents", "Agent Benchmarking and Evaluation", "Vision-Language-Action Models"]}, {"paper_id": "4230100", "score": "71", "title": "ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code", "authors": "Tianyu Hua, Harper Hua, Violet Xiang, Benjamin Klieger, Sang Truong, Weixin Liang, Fan-Yun Sun, Nick Haber", "session_type": "SD-6-2710", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.02314", "relevant_to_users": "3", "read_by_users": "12", "topics": "", "key_findings": "ResearchCodeBench reveals that LLMs struggle significantly with implementing novel research code compared to standard coding tasks, demonstrating that research-to-code translation requires distinct capabilities beyond general software engineering. The benchmark identifies specific failure patterns including difficulty maintaining complex mathematical implementations and managing unfamiliar architectural patterns from cutting-edge research. Performance varies significantly across models, highlighting critical gaps in current LLM capabilities for advancing ML research through code generation.", "description": "ResearchCodeBench is a novel benchmark designed to assess how well large language models can implement code based on machine learning research papers. Unlike standard programming benchmarks, it focuses on translating novel research ideas into working implementations‚Äîa critical capability for advancing ML research.", "key_contribution": "The core innovation is creating a systematic evaluation framework that measures LLM performance on implementing previously unseen ML research code from recent papers, establishing metrics to evaluate whether models can successfully translate research descriptions into functional implementations.", "novelty": "Unlike existing benchmarks such as SWE-Bench or CodeBERT evaluations that focus on mature codebases or standard programming tasks, ResearchCodeBench specifically targets the research-to-code translation problem. Previous benchmarks don't assess whether models can implement truly novel algorithmic contributions from papers without reference implementations. This work uniquely bridges the gap between paper understanding and practical implementation, addressing the limitation where existing evaluations miss the complexity of implementing research-specific techniques.", "ai_categories": ["Tool Use and Code Generation", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4233818", "score": "71", "title": "ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests", "authors": "Shiyi Xu, Hu Yiwen, Yingqian Min, Zhipeng Chen, Xin Zhao, Ji-Rong Wen", "session_type": "SD-6-109", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.04894", "relevant_to_users": "3", "read_by_users": "8", "topics": "", "key_findings": "ICPC-Eval introduces a challenging benchmark using International Collegiate Programming Contest problems to evaluate LLM reasoning capabilities. The evaluation reveals that competitive programming presents significant challenges even for advanced reasoning-focused models, with performance varying substantially across architectures. Models with explicit reasoning capabilities (like DeepSeek-R1) show improved performance on complex algorithmic problems requiring multi-step decomposition and strategic thinking.", "description": "This paper presents ICPC-Eval, a benchmark dataset that evaluates large language models using authentic problems from the International Collegiate Programming Contest (ICPC). The work provides comprehensive assessment of state-of-the-art models including Claude, DeepSeek-R1, Gemini, Grok, and Qwen on competition-level programming challenges.", "key_contribution": "The main contribution is creating a rigorous evaluation framework using authentic ICPC contest problems that test sophisticated algorithmic reasoning and multi-step problem-solving capabilities, going beyond basic coding tasks found in existing benchmarks.", "novelty": "Unlike existing code generation benchmarks like HumanEval or LeetCode-based evaluations that focus on basic coding tasks, ICPC-Eval uses competition-level problems requiring deep algorithmic insight and strategic reasoning. It addresses the limitation that current benchmarks don't adequately test complex multi-step reasoning and sophisticated algorithm design. The benchmark provides a more challenging standard that distinguishes reasoning-focused models from general-purpose code generators.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute", "Tool Use and Code Generation"]}, {"paper_id": "4090572", "score": "70", "title": "Wider or Deeper?  Scaling LLM Inference-Time Compute with Adaptive Branching Tree Search", "authors": "Yuichi Inoue, Kou Misaki, Yuki Imajuku, So Kuroki, Taishi Nakamura, Takuya Akiba", "session_type": "SD-2-3418", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/e6d2b8dcb02f4dadb940b53aefa65032db4fbd89.pdf", "relevant_to_users": "4", "read_by_users": "7", "topics": "", "key_findings": "AB-MCTS (Adaptive Branching MCTS) introduces a principled Bayesian framework that dynamically decides whether to 'go wider' by generating new candidate solutions or 'go deeper' by refining existing ones, using Thompson Sampling with Bayesian posteriors informed by external feedback. Empirical results on code contest problems and ML engineering tasks show AB-MCTS consistently outperforms both repeated sampling and standard MCTS, with advantages increasing as computational budgets scale. The method achieves superior performance on GPT-4o and DeepSeek-V3 across multiple benchmarks, demonstrating that adaptive branching enables more efficient use of inference-time compute.", "description": "This paper addresses the fundamental challenge of balancing exploration versus exploitation in LLM inference-time scaling. The authors propose Adaptive Branching Monte Carlo Tree Search (AB-MCTS), which treats branching width as a dynamic decision rather than a fixed hyperparameter, using Bayesian posterior updates to decide at each node whether to generate new solutions or refine existing ones based on external feedback signals.", "key_contribution": "The main innovation is treating the exploration-exploitation tradeoff in LLM inference as a principled Bayesian decision problem, where branching factors adapt dynamically based on observed feedback rather than being fixed hyperparameters. This enables more efficient scaling of inference-time compute by allocating resources to exploration or exploitation based on evidence, not predetermined rules.", "novelty": "Unlike standard MCTS which uses fixed branching factors as hyperparameters, AB-MCTS dynamically decides whether to expand new candidates or refine existing solutions using Thompson Sampling with Bayesian posteriors. This addresses the limitation that repeated sampling lacks refinement mechanisms while sequential refinement foregoes diversity. Previous MCTS variants for LLMs couldn't adjust branching to computational budgets or problem difficulty, whereas AB-MCTS formalizes this adaptation through a hierarchical Bayesian framework that incorporates external feedback signals, moving beyond heuristic rules like progressive widening based on visit counts.", "ai_categories": ["Reasoning and Test-Time Compute", "Tool Use and Code Generation", "Model Efficiency and Optimization"]}, {"paper_id": "4166142", "score": "70", "title": "WALL-E: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents", "authors": "Siyu Zhou, Tianyi Zhou, Yijun Yang, Guodong Long, Deheng Ye, Jing Jiang, Chengqi Zhang", "session_type": "SD-3-2811", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/4a70c0605cac16a41fee061564452113f886243a.pdf", "relevant_to_users": "4", "read_by_users": "32", "topics": "", "key_findings": "WALL-E introduces a training-free world alignment approach that bridges the gap between LLM prior knowledge and environment dynamics by extracting symbolic knowledge (action rules, knowledge graphs, scene graphs) from exploration trajectories and encoding them as executable code. The method achieves 16.1%-51.6% higher success rates on Mars benchmark and a record 98% success rate on ALFWorld after only 4 iterations, significantly outperforming RL-based methods like PPO and DreamerV3, as well as LLM-based baselines like ReAct, Reflexion, and AdaPlanner.", "description": "WALL-E 2.0 is a training-free, model-based LLM agent that uses neurosymbolic learning to align world models with environment dynamics. It employs a four-stage iterative process (trajectory comparison, symbolic knowledge extraction, code translation, and rule pruning) to extract action rules, knowledge graphs, and scene graphs from agent trajectories and encode them as executable constraints that regulate LLM agent policies within a model-predictive control framework.", "key_contribution": "The main innovation is a training-free world alignment approach that extracts multi-modal symbolic knowledge from LLM exploration and translates it into executable code rules rather than natural language constraints. This enables strict adherence to learned environment dynamics while using LLMs as efficient look-ahead optimizers in an MPC framework, eliminating the need for expensive RL fine-tuning or memory-intensive trajectory buffering.", "novelty": "Unlike prior work requiring expensive RL fine-tuning or memory-heavy trajectory buffering, WALL-E achieves world model alignment without retraining through executable code constraints rather than natural language prompting, ensuring strict compliance and reduced variability. It addresses the critical knowledge-dynamics mismatch problem by combining three complementary symbolic representations (action rules, knowledge graphs, scene graphs) extracted through inductive reasoning. The approach also innovates on classical MPC by replacing costly on-the-fly optimization with LLMs as efficient look-ahead planners, making model-based control practical for LLM agents while maintaining safety through verified symbolic constraints.", "ai_categories": ["World Models and Planning", "Tool Use and Code Generation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4215280", "score": "70", "title": "Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach", "authors": "Yuchen Wu, Edward Sun, Kaijie Zhu, Jianxun Lian, Jose Hernandez-Orallo, Aylin Caliskan, Jindong Wang", "session_type": "SD-5-1409", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/c98e7220e280f1d8bae43d92944840467237a1e7.pdf", "relevant_to_users": "2", "read_by_users": "5", "topics": "", "key_findings": "The paper introduces PENGUIN, a 14,000-scenario benchmark across seven sensitive domains showing that personalized user context improves LLM safety scores by 43.2%. It presents RAISE, a training-free agent framework using LLM-guided Monte Carlo Tree Search to strategically acquire user background information, achieving 31.6% safety improvement while requiring only 2.7 user queries on average. Results demonstrate that not all user attributes contribute equally‚Äîemotional and mental state information drives the greatest safety improvements.", "description": "This paper addresses the gap in LLM safety evaluation by introducing personalized safety metrics that account for user-specific vulnerability factors. While existing benchmarks use context-independent metrics like toxicity, the same response can pose divergent risks depending on individual user backgrounds in high-stakes domains like health counseling and financial advice.", "key_contribution": "The paper formalizes 'personalized safety' as a modeling problem, grounding it in psychological vulnerability research. It introduces PENGUIN benchmark for evaluation and RAISE framework‚Äîa two-stage planning agent that strategically acquires minimal user context to maximize safety, using LLM-guided MCTS for efficient attribute selection with only 25% of standard MCTS computational cost.", "novelty": "Previous LLM personalization work focused on surface-level alignment like linguistic tone without examining safety implications. This is the first work to systematically address how individual user characteristics affect safety risks, moving beyond one-size-fits-all safety evaluations. The RAISE framework introduces strategic information acquisition through planning rather than requiring full user profiles upfront, balancing safety with privacy and interaction efficiency‚Äîa critical limitation of prior approaches that either ignored personalization or required extensive user data.", "ai_categories": ["Agent Safety and Security", "Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4037484", "score": "70", "title": "Rollout Roulette: A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods", "authors": "Isha Puri, Shivchander Sudalairaj, Guangxuan Xu, Abhishek Bhandwaldar, Kai Xu, Akash Srivastava", "session_type": "SD-5-5518", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/7ee28149679cef6235a635039365773e8ff846ed.pdf", "relevant_to_users": "7", "read_by_users": "11", "topics": "", "key_findings": "The paper introduces a particle filtering approach to inference-time scaling that achieves 4-16x better scaling rates than deterministic search methods on mathematical reasoning tasks. Key findings include: (1) Qwen2.5-Math-1.5B matches GPT-4o accuracy with only 4 rollouts, and Qwen2.5-Math-7B reaches o1-level performance with 32 rollouts; (2) Probabilistic sampling maintains trajectory diversity and recovers from early missteps that deterministic beam search would prune; (3) The method provides unbiased accuracy estimates through weighted particle sampling; (4) It generalizes beyond mathematical reasoning to domains like finance and chemistry despite using math-trained reward models.", "description": "This paper reformulates inference-time scaling for LLMs as a probabilistic inference problem over state-space models, using particle-based Monte Carlo methods (particle filtering, particle Gibbs, parallel tempering) to explore the distribution of reasoning trajectories rather than optimizing for single best paths. The approach maintains weighted populations of candidate sequences that evolve through stochastic resampling guided by process reward models, enabling robust exploration while smoothing over reward model noise.", "key_contribution": "The main contribution is casting inference-time scaling as posterior inference in a state-space model and applying sequential Monte Carlo techniques (particle filtering) to maintain distributional exploration rather than deterministic pruning. This enables graceful handling of imperfect reward signals and provides theoretical guarantees for unbiased accuracy estimation through weighted sampling, establishing a principled probabilistic framework for test-time compute scaling.", "novelty": "Unlike existing search-based methods (beam search, MCTS, Best-of-N) that deterministically prune low-scoring trajectories and are vulnerable to reward hacking from imperfect reward models, this work introduces probabilistic exploration via particle filtering that stochastically maintains trajectory diversity and can recover from early missteps. The key innovation is shifting from optimizing for the mode of the reward distribution to sampling from the typical set, addressing the critical limitation of irreversible pruning in deterministic methods while providing formal theoretical grounding through state-space model formulation and unbiased estimation guarantees.", "ai_categories": ["Reasoning and Test-Time Compute", "Model Efficiency and Optimization"]}, {"paper_id": "4442362", "score": "70", "title": "Predicting the Performance of Black-box Language Models with Follow-up Queries", "authors": "Dylan Sam, Marc Anton Finzi, J Zico Kolter", "session_type": "SD-6-1304", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/a9530414a4fd5f326629f3402e2729d04dac678a.pdf", "relevant_to_users": "10", "read_by_users": "22", "topics": "", "key_findings": "The paper introduces QueRE (Query-based Representation Elicitation), which predicts black-box LLM correctness by extracting token probabilities from follow-up questions and training linear classifiers. It achieves 0.95-0.96 AUROC on QA tasks (outperforming white-box methods), 98%+ accuracy in detecting adversarially-influenced models, and near-perfect model distinction capabilities. The method generalizes across datasets and model scales with strong PAC-Bayes bounds, and includes theoretical convergence analysis (O(1/‚àön + ‚àön/k)) for sampling-based probability approximation when APIs don't expose token probabilities.", "description": "This paper proposes a method to predict the behavior and correctness of black-box language models using only API access by posing approximately 50 follow-up yes/no questions, extracting token probabilities as features, and training simple linear classifiers. The approach works across question-answering and reasoning tasks, can detect adversarial manipulation, and distinguish between different LLM architectures.", "key_contribution": "The main innovation is showing that token-level probabilities from follow-up questions (both hand-crafted and GPT-4-generated) can serve as effective features for predicting model correctness without white-box access, matching or exceeding performance of methods requiring internal activations while being applicable to any black-box API.", "novelty": "Previous work on predicting LLM behavior required white-box access to internal activations (mechanistic interpretability) or relied on verbalized confidence for specific task formats (uncertainty quantification). QueRE is the first to demonstrate that simple yes/no question probabilities from follow-up queries can predict instance-level correctness without architectural access, making it applicable to closed-source APIs like GPT-4. The method also addresses the limitation of prior task-level performance prediction by enabling instance-level assessment, and provides theoretical grounding for sampling-based approximations when token probabilities aren't directly accessible.", "ai_categories": ["Agent Benchmarking and Evaluation", "Agent Safety and Security"]}, {"paper_id": "4208641", "score": "69", "title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning", "authors": "Kaiwen Zha, Zhengqi Gao, Maohao Shen, Zhang-Wei Hong, Duane S Boning, Dina Katabi", "session_type": "SD-1-4017", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/df2d115dd7fe7763e94fd35d45007ed71cbcebc7.pdf", "relevant_to_users": "3", "read_by_users": "9", "topics": "", "key_findings": "RL Tango achieves state-of-the-art performance among 7B/8B-scale models, with the generator showing 100% improvement over vanilla GRPO on AIME 2025 (23.3% accuracy) and 25.5% average relative gain on math benchmarks. The verifier outperforms even 72B models on ProcessBench. The key innovation is that both components co-evolve through interleaved RL training without requiring expensive process-level annotations, addressing reward hacking through the verifier's stochastic generative design and achieving superior generalization across competition-level math and out-of-domain reasoning tasks.", "description": "This paper presents Tango, a framework that uses reinforcement learning to concurrently train both an LLM generator and a process-level verifier in an interleaved manner for improved language reasoning. Unlike existing approaches that use fixed or supervised verifiers, both components co-evolve through mutual reinforcement, with the verifier learning from outcome-level rewards without requiring process-level annotations.", "key_contribution": "The main innovation is the first RL-trained generative process-level verifier that co-evolves with the generator through interleaved training cycles. This approach combines outcome-level and step-level rewards with advantage blending and exponential decay, enabling both components to improve together while avoiding reward hacking through the verifier's stochastic textual feedback mechanism.", "novelty": "Previous approaches relied on fixed rule-based verifiers or discriminative reward models trained via supervised fine-tuning, which suffered from reward hacking and poor generalization. RL Tango introduces bidirectional RL optimization where a generative verifier produces natural language step-by-step feedback rather than deterministic logits, learning accurate verification from outcome-only supervision as the generator evolves. This eliminates the need for expensive process-level annotations while achieving better robustness to distribution shifts and enabling both components to push each other's capabilities forward through mutual reinforcement.", "ai_categories": ["Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs"]}, {"paper_id": "4219339", "score": "69", "title": "RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for  Complex Task Solving", "authors": "Huacan Wang, Ziyi Ni, Shuo Zhang, Shuo Lu, Sen Hu, Ziyang He, Chen Hu, Jiaye Lin, Yifu Guo, ..., Pin Lyu", "session_type": "SD-2-1916", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/fea6d1106531a6d10e824159b158dcbffbc07cc2.pdf", "relevant_to_users": "2", "read_by_users": "8", "topics": "", "key_findings": "RepoMaster introduces autonomous exploration capabilities for AI agents to navigate and understand complex GitHub repositories without manual preprocessing. The system enables agents to systematically discover codebase structure and extract relevant information for solving sophisticated software engineering tasks, demonstrating improved task completion over baselines that lack sophisticated exploration.", "description": "The paper presents RepoMaster, a system that enables AI agents to autonomously explore and comprehend large GitHub repositories for complex task solving. It addresses the challenge of navigating real-world codebases with varying structures and documentation quality without requiring extensive human-guided preparation.", "key_contribution": "The main innovation is developing methods for autonomous repository exploration that allow AI agents to independently navigate complex codebases and extract relevant information without pre-processing or manual structure analysis, making the approach more practically applicable to diverse real-world projects.", "novelty": "Unlike previous approaches that required significant manual effort to prepare repository information or relied on simplified codebase representations, RepoMaster enables fully autonomous navigation and understanding of repositories. This addresses the limitation of requiring upfront human guidance and makes AI agents more capable of handling real-world software engineering tasks with diverse project structures and varying documentation quality.", "ai_categories": ["Tool Use and Code Generation", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4250005", "score": "69", "title": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks", "authors": "Hwiwon Lee, Ziqi Zhang, Hanxiao Lu, LINGMING ZHANG", "session_type": "SD-3-2506", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/66d2a1a241b7979f0f8776b51e62c6b1f1bc7db8.pdf", "relevant_to_users": "3", "read_by_users": "8", "topics": "", "key_findings": "SEC-bench is the first fully automated benchmarking framework for evaluating LLM agents on authentic security engineering tasks. The framework uses a novel multi-agent scaffold that automatically constructs code repositories with harnesses, reproduces vulnerabilities in isolated environments, and generates gold patches at only $0.87 per instance. Comprehensive evaluation reveals significant performance gaps in state-of-the-art LLM agents: achieving at most 18.0% success in proof-of-concept (PoC) generation and 34.0% success in vulnerability patching, demonstrating that current agents are far from ready for real-world security engineering work.", "description": "SEC-bench presents the first fully automated benchmarking framework for rigorously evaluating LLM agents on real-world software security tasks, specifically focusing on vulnerability exploitation (PoC generation) and remediation (vulnerability patching). The framework uses a multi-agent scaffold to automatically construct reproducible vulnerability datasets from authentic CVEs, enabling systematic evaluation of agent capabilities in complex security engineering scenarios.", "key_contribution": "The main contribution is a novel automated multi-agent scaffold that can construct high-quality, reproducible security benchmarks from real-world vulnerabilities at scale ($0.87 per instance), enabling the first rigorous evaluation of LLM agents on authentic security engineering tasks rather than synthetic or simplified challenges.", "novelty": "Unlike existing benchmarks like CyberSecEval (which uses multiple-choice questions) or SWE-bench (which focuses on general software engineering), SEC-bench is the first to evaluate agents on real-world security tasks with full complexity and ambiguity. It addresses the limitation that existing benchmarks rely on synthetic challenges or simplified datasets that fail to capture authentic security engineering demands. The automated multi-agent scaffold enables scalable construction of reproducible vulnerability datasets with isolated execution environments and verified gold patches, which was not possible with previous approaches.", "ai_categories": ["Agent Benchmarking and Evaluation", "Agent Safety and Security", "Tool Use and Code Generation"]}, {"paper_id": "4036626", "score": "69", "title": "AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement", "authors": "J Rosser, Jakob Nicolaus Foerster", "session_type": "SD-4-1201", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/237ca0099a7a95299cd0bae4b495038b19873e08.pdf", "relevant_to_users": "3", "read_by_users": "9", "topics": "", "key_findings": "AgentBreeder demonstrates that multi-agent LLM scaffolds can be systematically optimized for safety without sacrificing capability, achieving a 79.4% average uplift in safety benchmark performance. The framework reveals that multi-agent systems harbor previously unexplored safety vulnerabilities that emerge during capability optimization. It introduces a quality-diversity evolutionary search that discovers diverse agent architectures balancing safety-capability tradeoffs, and shows that adversarially weak scaffolds can emerge concurrently with performance gains (red mode), highlighting critical safety risks in multi-agent systems.", "description": "This paper introduces AgentBreeder, a framework for self-improving evolutionary search over multi-agent LLM scaffolds that simultaneously optimizes for both safety and capability. The method uses quality-diversity search inspired by MAP-Elites to discover agent architectures‚Äîincluding communication patterns, role assignments, and reasoning structures‚Äîthat reduce harmful behaviors while maintaining task performance across benchmarks like MMLU, DROP, GPQA, and safety evaluations including jailbreak scenarios and adversarial attacks.", "key_contribution": "The primary contribution is a multi-objective evolutionary framework that treats multi-agent scaffold designs as evolvable parameters rather than fixed configurations, enabling systematic exploration of safety-capability tradeoffs. Unlike prior work that optimizes single objectives or requires manual specification, AgentBreeder discovers emergent safer agent configurations through evolutionary pressure, providing a scalable path for improving multi-agent system safety at the architectural level.", "novelty": "Previous multi-agent LLM research focused exclusively on capability improvements with fixed scaffold designs, leaving safety implications unexplored. AgentBreeder addresses this gap by introducing explicit safety optimization alongside capability metrics in multi-agent scaffolding, treating system architecture as an evolvable safety intervention point rather than retrofitting safety into pre-designed systems. The novelty lies in using quality-diversity evolutionary search to systematically discover and characterize safety-capability tradeoffs in multi-agent designs, revealing that safer configurations exist without performance penalties‚Äîand critically, that adversarially weak scaffolds can emerge during optimization, a previously unrecognized risk.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Agent Safety and Security", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4397868", "score": "69", "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play", "authors": "Ran Xu, Yuchen Zhuang, Zihan Dong, Ruiyu Wang, Yue Yu, Joyce C. Ho, Linjun Zhang, Haoyu Wang, Wenqi Shi, Carl Yang", "session_type": "SD-5-1908", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/d73f4b0dc08ba840cfc15d8e7e9be6ad4a9b52ce.pdf", "relevant_to_users": "1", "read_by_users": "4", "topics": "", "key_findings": "AceSearcher introduces a cooperative self-play framework where a single LLM alternates between decomposer and solver roles to improve search-augmented reasoning. It achieves 7.6% average improvement over SOTA baselines on multi-hop QA tasks, and remarkably, the 32B model matches DeepSeek-V3 performance using less than 5% of its parameters on document-level finance reasoning. The framework eliminates dependence on intermediate supervision or proprietary models by using only final answer accuracy as the reward signal, making it highly efficient and practical.", "description": "AceSearcher is a two-stage training framework that teaches a single LLM to perform cooperative self-play by alternating between decomposing complex queries into sub-questions and solving them by integrating retrieved contexts. The system combines supervised fine-tuning on diverse reasoning tasks with iterative preference-based reinforcement learning optimized only on final answer accuracy, without requiring intermediate annotations.", "key_contribution": "The main innovation is the cooperative dual-role self-play mechanism within a single model, where the decomposer generates sub-question templates for multi-hop retrieval and the solver integrates contexts for answers, jointly optimized through a unified reward signal based solely on final answer accuracy. This eliminates the need for multiple specialized models, intermediate supervision, or proprietary model outputs while achieving superior parameter efficiency.", "novelty": "Unlike prior work requiring separate models, intermediate annotations, or proprietary LLM supervision, AceSearcher trains a single model to handle both query decomposition and solving through bootstrapped self-play with only final answer feedback. It addresses the multi-hop retrieval gap in traditional RAG systems that handle only single-turn retrieval, and overcomes memory-intensive online RL limitations through efficient iterative DPO. The collaborative (not adversarial) self-play approach jointly optimizes both roles using the assumption that better decompositions lead to more accurate answers, achieving remarkable parameter efficiency‚Äîsmaller models outperform baselines up to 9x their size.", "ai_categories": ["Reasoning and Test-Time Compute", "Tool Use and Code Generation", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4221910", "score": "68", "title": "SWE-bench Goes Live!", "authors": "Linghao Zhang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Chengxing Xie, Junhao Wang, Maoquan Wang, Yufan Huang, ..., Dongmei Zhang", "session_type": "SD-1-4009", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.23419", "relevant_to_users": "6", "read_by_users": "17", "topics": "", "key_findings": "This paper introduces a live, continuously-updated version of SWE-bench that automatically captures new GitHub issues in real-time, creating a dynamic benchmark that prevents data staleness and model contamination. The work establishes public leaderboards tracking performance across commercial and open-source AI systems on authentic software engineering tasks, revealing significant performance gaps between state-of-the-art models and demonstrating that current AI remains challenged by real-world coding problems.", "description": "The paper presents SWE-bench Goes Live, a dynamic benchmarking infrastructure that transforms the original static SWE-bench dataset into a continuously-updating evaluation platform integrated with live GitHub repositories. The system automatically captures new issues and their solutions, executes standardized tests, and maintains public leaderboards tracking AI model performance on real-world software engineering tasks.", "key_contribution": "The main contribution is the creation of a persistent, self-updating benchmark infrastructure with automated GitHub integration and real-time leaderboards, enabling ongoing evaluation of AI coding systems without the risk of dataset saturation or contamination that plagues static benchmarks.", "novelty": "Unlike the original static SWE-bench with 2,294 frozen GitHub issues, this work addresses benchmark degradation through continuous automated issue collection from live repositories. It tackles the fundamental limitation of fixed benchmarks‚Äîpotential memorization and data contamination‚Äîby creating an ever-evolving evaluation framework that captures contemporary coding patterns and modern challenges as they emerge. This shift from periodic re-benchmarking to ongoing evaluation represents a new paradigm for measuring AI software engineering capabilities.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation"]}, {"paper_id": "4444932", "score": "68", "title": "MM-OPERA: Benchmarking Open-ended Association Reasoning for Large Vision-Language Models", "authors": "Zimeng Huang, Jinxin Ke, Xiaoxuan Fan, Yufeng Yang, Yang Liu, Liu Zhonghan, Zedi Wang, Junteng Dai, Haoyi Jiang, ..., Ziliang Chen", "session_type": "SD-1-4612", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2510.26937", "relevant_to_users": "0", "read_by_users": "6", "topics": "", "key_findings": "The paper introduces MM-OPERA, the first benchmark for evaluating open-ended association reasoning in large vision-language models. Testing 17 state-of-the-art models revealed significant performance variations, with closed-source models generally outperforming open-source alternatives. The benchmark uses a psychology-grounded multi-dimensional evaluation framework (fluency, originality, flexibility, elaboration) that reveals association reasoning capabilities are not strongly correlated with performance on existing benchmarks, indicating this represents a distinct capability dimension.", "description": "MM-OPERA is a benchmark designed to evaluate how well large vision-language models perform open-ended association reasoning‚Äîidentifying creative and meaningful connections between visual elements and abstract concepts. The benchmark presents image-caption pairs and evaluates model-generated associated concepts across four dimensions borrowed from cognitive psychology's creativity assessment methods.", "key_contribution": "The first benchmark specifically designed to assess open-ended association reasoning in vision-language models using a psychology-grounded multi-dimensional evaluation framework that goes beyond binary correctness to measure creative reasoning capabilities including fluency, originality, flexibility, and elaboration.", "novelty": "Unlike existing VQA benchmarks that focus on closed-ended tasks with binary right/wrong answers, MM-OPERA evaluates open-ended creative reasoning and associative thinking‚Äîa core aspect of human intelligence previously underexplored in LVLM evaluation. It addresses the limitation of constrained multiple-choice formats by requiring genuine creative generation and provides quality gradations through psychology-inspired creativity metrics rather than simple accuracy scores. This reveals that association reasoning is a distinct capability not captured by standard benchmarks.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models", "Reasoning and Test-Time Compute"]}, {"paper_id": "4442428", "score": "68", "title": "Measuring AI Ability to Complete Long Software Tasks", "authors": "Thomas Kwa, Ben West, Joel Becker, Amy Deng, Katharyn Garcia, Max Hasin, Sami Jawhar, Megan Kinniment, Nate Rush, ..., Lawrence Chan", "session_type": "SD-2-5312", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/402f09e70f9d9e6a99edbcaaa360692d05eec7ab.pdf", "relevant_to_users": "2", "read_by_users": "3", "topics": "", "key_findings": "The paper introduces the '50%-task-completion time horizon' metric showing frontier AI capabilities have been doubling every ~7 months since 2019. Current frontier models like o3 achieve ~110 minutes, while Claude 3.7 Sonnet reaches ~50 minutes. Models have nearly 100% success on <4-minute tasks but <10% on >4-hour tasks. Progress is driven by improved logical reasoning, tool use, and error recovery. Extrapolation suggests AI could automate month-long software tasks within 5 years.", "description": "This paper proposes measuring AI capabilities through task-completion time horizons‚Äîthe duration humans typically take to complete tasks that AI models solve with X% success rate. The authors evaluate 13 frontier models (2019-2025) across 170 diverse software engineering and research tasks from three datasets (HCAST, RE-Bench, SWAA), establishing empirical human performance baselines.", "key_contribution": "The core innovation is the X%-task-completion time horizon metric‚Äîa human-centric, interpretable measure bridging AI and human capabilities that enables meaningful cross-benchmark comparison and tracks progress over six years of model development, addressing benchmark saturation issues.", "novelty": "Unlike prior benchmarks (SWE-bench, HumanEval) that use artificial tasks or saturate quickly, this work uses economically meaningful real-world tasks with empirically measured human baselines, creating a unifying metric that remains relevant across vastly different capability levels. It addresses the gap between superhuman exam scores and limited autonomous project completion by revealing task length‚Äînot individual skill steps‚Äîas the key bottleneck. Previous work couldn't translate benchmark improvements into real-world usefulness predictions.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation"]}, {"paper_id": "4216538", "score": "68", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "authors": "Ziming Wei, Bingqian Lin, Zijian Jiao, Yunshuang Nie, Liang Ma, Yuecheng Liu, Yuzheng Zhuang, Xiaodan Liang", "session_type": "SD-4-1508", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.20148", "relevant_to_users": "0", "read_by_users": "0", "topics": "", "key_findings": "MineAnyBuild reveals severe limitations in current AI agents' spatial planning abilities, with even the best model (GPT-4o) achieving only 41.02/100 overall. Agents particularly struggled with plan execution and spatial reasoning (below 30% accuracy), with proprietary models substantially outperforming open-source variants. The benchmark introduces 4,000 curated tasks with an infinitely expandable data collection paradigm using player-generated Minecraft content, evaluating five dimensions: executable spatial plan generation, spatial understanding, creativity, spatial reasoning, and spatial commonsense.", "description": "MineAnyBuild is a comprehensive benchmark for evaluating AI agents' spatial planning capabilities in open-world environments using Minecraft. It requires agents to generate executable 3D architectural building plans from multi-modal human instructions, testing 13 MLLM-based agents across five evaluation dimensions including spatial understanding, reasoning, creativity, and commonsense knowledge.", "key_contribution": "The benchmark bridges the critical gap between abstract spatial understanding and concrete task execution by requiring agents to translate multi-modal instructions into executable 3D building plans. It provides the first comprehensive evaluation framework combining spatial perception, reasoning, creativity, and commonsense into actionable plans with 4,000 tasks and mechanisms for infinite data expansion.", "novelty": "Unlike previous benchmarks that focus on abstract spatial reasoning through VQA formats, MineAnyBuild demands executable architecture generation, directly connecting spatial perception to concrete task execution in open-world scenarios. It addresses the limitation of existing benchmarks that test only passive spatial understanding rather than active spatial planning and construction. The work introduces a novel evaluation paradigm that assesses the complete pipeline from understanding multi-modal instructions to generating actionable 3D plans, which is critical for real-world applications like robotic manipulation and urban planning.", "ai_categories": ["Agent Benchmarking and Evaluation", "Spatial and Physical Reasoning", "Planning and Decision Making"]}, {"paper_id": "4226827", "score": "68", "title": "CoP: Agentic Red-teaming for Large Language Models using Composition of Principles", "authors": "Chen Xiong, Pin-Yu Chen, Tsung-Yi Ho", "session_type": "SD-6-1307", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/cb30447780175b5ce7f25d0e0277ddcc32156544.pdf", "relevant_to_users": "3", "read_by_users": "8", "topics": "", "key_findings": "CoP achieves state-of-the-art jailbreak success rates across both open-source and proprietary LLMs, with 88.75% ASR on GPT-4-Turbo and 72.5% on Llama-2-70B-Chat (up to 19√ó better than baselines). It requires 17.2√ó fewer queries than competing methods while maintaining interpretability through transparent principle composition. The framework reveals that specific strategy combinations like 'Expand' and 'Phrase Insertion' are particularly effective, providing actionable insights for both red teams and defenders.", "description": "This paper introduces CoP (Composition of Principles), an agentic framework that automates LLM red-teaming by orchestrating human-provided principles to generate jailbreak prompts. The system uses an AI agent to dynamically select and combine red-teaming strategies, with dual evaluation judging both attack success and semantic similarity to prevent prompt drift.", "key_contribution": "CoP provides a modular, extensible, and interpretable framework for automated red-teaming that achieves unprecedented attack success rates while being computationally efficient. Unlike gradient-based or brute-force methods, it transparently reveals which attack strategies are effective and allows human users to modify principles without system retraining.", "novelty": "CoP addresses limitations of prior work by introducing a composition-based approach that systematically combines multiple attack principles dynamically, rather than relying on gradient optimization (GCG), fixed strategy libraries (AutoDAN-Turbo), or unguided search (PAIR/TAP). It uniquely incorporates semantic fidelity monitoring to prevent prompt drift and maintains full interpretability while achieving superior performance. The modular design allows new principles to be added without retraining, and the agentic orchestration enables discovery of novel attack strategies through principle composition.", "ai_categories": ["Agent Safety and Security", "Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4202819", "score": "67", "title": "Meta-World+: An Improved, Standardized, RL Benchmark", "authors": "Reginald McLean, Evangelos Chatzaroulas, Luc McCutcheon, Frank R√É¬∂der, Tianhe Yu, Zhanpeng He, K.R. Zentner, Ryan Julian, J Terry, ..., Pablo Samuel Castro", "session_type": "SD-1-403", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.11289", "relevant_to_users": "12", "read_by_users": "50", "topics": "", "key_findings": "Meta-World+ provides standardized improvements to the widely-used Meta-World benchmark, introducing consistent reward function designs, task parameter specifications, and enhanced evaluation protocols across all 50 tasks. These improvements address critical inconsistencies that previously hindered fair algorithmic comparisons and reproducibility in multi-task RL research.", "description": "This paper presents Meta-World+, an enhanced version of the Meta-World benchmark for multi-task reinforcement learning. It systematically refines the original 50-task suite by standardizing reward functions, task specifications, and evaluation metrics to create a more reliable and consistent benchmark for RL algorithm development and comparison.", "key_contribution": "The main contribution is a comprehensively standardized benchmark that fixes fundamental inconsistencies in the original Meta-World, providing clearer signals for measuring algorithm performance through uniform reward designs, consistent task calibrations, and improved evaluation protocols.", "novelty": "Unlike the original Meta-World which had fragmented reward designs and unclear specifications that made it difficult to isolate algorithmic improvements from benchmark artifacts, Meta-World+ systematically standardizes each task's implementation. This addresses the critical limitation that varying task difficulty calibrations and inconsistent reward functions previously prevented fair cross-algorithm comparisons and made reproducibility challenging in multi-task RL research.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reinforcement Learning for LLMs"]}, {"paper_id": "4238914", "score": "66", "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity", "authors": "Parshin Shojaee, Seyed Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio, Mehrdad Farajtabar", "session_type": "SD-1-1914", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/b58069a804c5fad686ceb13a131631201748c264.pdf", "relevant_to_users": "22", "read_by_users": "41", "topics": "", "key_findings": "The paper reveals that Large Reasoning Models (LRMs) exhibit three distinct performance regimes based on problem complexity: they underperform standard LLMs on low-complexity tasks, excel at medium-complexity tasks, but experience complete accuracy collapse beyond specific complexity thresholds. Critically, LRMs show counterintuitive scaling behavior where reasoning effort initially increases with complexity but then declines despite available token budget. The study demonstrates that LRMs struggle with exact computation, fail to consistently employ explicit algorithms, and exhibit fundamental limitations that question their true reasoning capabilities.", "description": "This paper systematically investigates the capabilities and limitations of Large Reasoning Models (LRMs) that generate detailed thinking processes before answering. Using controllable puzzle environments to precisely manipulate problem complexity while maintaining consistent logical structures, the researchers analyze both final answer accuracy and internal reasoning traces to understand how LRMs actually reason and where they fundamentally break down.", "key_contribution": "The paper introduces a novel evaluation framework using controllable puzzle environments with precise complexity manipulation to systematically assess LRM reasoning capabilities. This approach enables simultaneous analysis of final answers and internal reasoning traces, revealing fundamental limitations in exact computation and algorithmic reasoning that standard benchmarks miss.", "novelty": "Previous work focused primarily on final answer accuracy using established benchmarks that suffer from contamination issues and lack fine-grained complexity control. This study uniquely combines controlled experimental environments with detailed reasoning trace analysis to reveal that LRMs don't use explicit algorithms consistently and exhibit unexpected scaling limitations. The controllable complexity framework addresses the limitation of opaque benchmark evaluations by enabling systematic study of how reasoning quality degrades with problem complexity.", "ai_categories": ["Reasoning and Test-Time Compute", "Agent Benchmarking and Evaluation", "Mathematical and Logical Reasoning"]}, {"paper_id": "4441760", "score": "66", "title": "Thinking vs. Doing: Improving Agent Reasoning by  Scaling Test-Time Interaction", "authors": "Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, ..., Aviral Kumar", "session_type": "SD-2-515", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/20cba89765ebda48a0c183f17ceb68d13f90d0dc.pdf", "relevant_to_users": "4", "read_by_users": "8", "topics": "", "key_findings": "This paper introduces Test-Time Interaction (TTI), a curriculum-based online RL approach that scales the agent's interaction horizon rather than just per-step reasoning. Key findings: (1) Prompting-based interaction scaling alone provides non-trivial improvements without training, (2) TTI with Gemma 3 12B achieves state-of-the-art results among open-source web agents on WebVoyager and WebArena, (3) Dynamic horizon scheduling from short (h=10) to long (h=30) outperforms fixed horizons by enabling proper MDP learning while avoiding REINFORCE error accumulation, (4) The approach achieves three desired properties: no intervention on reasoning length, decreased reasoning tokens, and increased action trajectories with more information gathering.", "description": "The paper addresses the limitation of current test-time scaling approaches that focus solely on generating longer reasoning traces before acting. It proposes TTI, which scales the interaction horizon to enable agents to gather environmental feedback and adapt behavior dynamically through exploration, backtracking, and re-planning within a single rollout.", "key_contribution": "The main contribution is introducing interaction scaling as a complementary dimension to per-step compute scaling, implemented through TTI‚Äîa curriculum-based online RL method with adaptive rollout length scheduling that trains agents to balance thinking versus doing by progressively increasing horizons from simple to complex tasks.", "novelty": "Unlike traditional test-time scaling that increases per-step reasoning compute, this work introduces an orthogonal dimension of scaling the interaction horizon itself. It addresses the fundamental limitation that reasoning-only approaches cannot acquire new environmental information or adapt after taking actions. The curriculum-based horizon scheduling (short-to-long) solves the dilemma between fixed short horizons (poor exploration/overfitting) and fixed long horizons (REINFORCE error accumulation), enabling agents to naturally learn when to think versus when to act through environmental interaction.", "ai_categories": ["Reasoning and Test-Time Compute", "Web and Computer-Use Agents", "Reinforcement Learning for LLMs"]}, {"paper_id": "4357417", "score": "66", "title": "Can Large Language Models Master Complex Card Games?", "authors": "Wei Wang, Fuqing Bie, Junzhe Chen, Dan Zhang, Shiyu Huang, Evgeny Kharlamov, Jie Tang", "session_type": "SD-2-101", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/1d7991c179addeedf33ce77dc2ffd0e475f69b2c.pdf", "relevant_to_users": "1", "read_by_users": "8", "topics": "", "key_findings": "This paper demonstrates that LLMs can achieve strong performance in complex card games through supervised fine-tuning on high-quality gameplay data generated by existing game AIs. The study evaluates eight diverse card games (DouDizhu, Guandan, Riichi Mahjong, Uno, Gin Rummy, and poker variants) and finds that: (1) LLMs approach teacher model performance with sufficient data, (2) positive transfer occurs between games with similar rules while dissimilar games show negative transfer, and (3) general capabilities decline after game-specific fine-tuning but can be preserved by incorporating general instruction data during training.", "description": "This paper presents the first systematic evaluation of LLM learning capabilities across eight complex, high-entropy card games using supervised fine-tuning rather than prompting alone. It examines three key dimensions: individual game mastery, multi-game learning with transfer effects, and retention of general capabilities during specialization.", "key_contribution": "The main contribution is demonstrating that LLMs can serve as versatile general learners that master complex strategic games through fine-tuning on AI-generated gameplay data, eliminating the need for expensive from-scratch training or labor-intensive per-game architecture design while achieving performance comparable to specialized game AIs.", "novelty": "Unlike prior work that relied on prompting to assess inherent knowledge or evaluated simpler tasks, this paper measures genuine learning ability through fine-tuning on strategically complex games with high information set numbers. It addresses the limitation of previous research by selecting games with sufficient complexity to properly assess learning capabilities and by using fine-tuning rather than zero-shot prompting. The work also uniquely investigates knowledge transfer between games based on rule similarity and proposes a practical solution to preserve general capabilities during specialization by mixing game-specific and general instruction data.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute", "Domain-Specific Agents and Applications"]}, {"paper_id": "3869165", "score": "65", "title": "Benchmarking Large Language Models with Integer Sequence Generation Tasks", "authors": "Daniel O&#x27;Malley, Manish Bhattarai, Nishath Ranasinghe, Erick Draayer, Javier E. Santos", "session_type": "SD-1-2800", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2411.04372", "relevant_to_users": "4", "read_by_users": "9", "topics": "", "key_findings": "The paper introduces a novel benchmarking framework using integer sequences from the Online Encyclopedia of Integer Sequences (OEIS) to evaluate LLM pattern recognition and mathematical reasoning capabilities. It provides comprehensive testing of multiple LLMs on sequence completion tasks, revealing varying performance across different sequence complexities. The benchmark establishes a rigorous, mathematically grounded testbed with clear correctness metrics (exact sequence continuation) that goes beyond standard language understanding evaluations.", "description": "This paper presents a systematic methodology for benchmarking large language models using integer sequence generation tasks derived from OEIS. The framework assesses LLMs' ability to identify underlying mathematical patterns and generate subsequent terms in numerical sequences, providing a quantifiable measure of logical inference and reasoning capabilities.", "key_contribution": "The main contribution is the creation of a specialized benchmark that leverages OEIS integer sequences as a pure pattern-recognition evaluation framework, offering mathematically rigorous assessment criteria with clear correctness metrics that test reasoning abilities beyond typical NLP tasks.", "novelty": "Unlike previous benchmarks that focus on language understanding or standard reasoning tasks, this work introduces a mathematically grounded evaluation approach that requires genuine pattern recognition and logical inference rather than memorization or pattern matching from training data. It addresses the limitation of existing benchmarks by providing objective, quantifiable metrics for assessing computational and mathematical reasoning in a structured domain where correctness is unambiguous.", "ai_categories": ["Agent Benchmarking and Evaluation", "Mathematical and Logical Reasoning"]}, {"paper_id": "4266438", "score": "65", "title": "Language Modeling by Language Models", "authors": "Junyan Cheng, Peter Clark, Kyle Richardson", "session_type": "SD-2-1512", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/1dda401bc2d6f8ee17a263ac3f358eb51e094d8e.pdf", "relevant_to_users": "8", "read_by_users": "41", "topics": "", "key_findings": "LLMs can autonomously discover novel language model architectures that are competitive with established designs like GPT2 and Mamba2. The system discovered 1,162 new architectures, with the best designs outperforming baselines on 6 out of 9 benchmarks. This demonstrates an 86% improvement in successful design generation, showing that LLMs can effectively simulate the research process of discovering new architectures through coordinated multi-agent collaboration.", "description": "This paper introduces Genesys, a system that uses large language models to autonomously discover novel language model architectures by simulating the conventional research process including ideation, literature analysis, implementation, pre-training, and evaluation. The system employs a 'Ladder of Scales' approach that proposes, reviews, and verifies architectural designs across increasingly larger model scales (14M-350M parameters).", "key_contribution": "The main contribution is Genesys, a multi-agent system that combines genetic programming with LLM-based design generation to autonomously discover competitive language model architectures. The system uses adversarial review and staged verification at multiple scales to systematically explore the architecture search space.", "novelty": "Unlike previous work that relies on direct prompt generation, this approach uses genetic programming as the backbone for design generation, addressing the bottleneck of creating valid architectural designs. The novelty lies in systematizing architectural discovery through a multi-scale verification process coupled with adversarial review, enabling the LLM to iteratively improve designs. This represents a shift from manual architecture search or simple automated optimization to a fully autonomous research simulation that mimics human research workflows.", "ai_categories": ["Self-Improvement and Meta-Learning", "Multi-Agent Systems and Collaboration"]}, {"paper_id": "4441444", "score": "65", "title": "Let the LLM Stick to Its Strengths: Learning to Route Economical LLM", "authors": "Yi-Kai Zhang, Shiyin Lu, Qing-Guo Chen, Weihua Luo, De-Chuan Zhan, Han-Jia Ye", "session_type": "SD-2-4203", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/f8ce3f48c335fa784e0e6aef63e636d13c30d93d.pdf", "relevant_to_users": "0", "read_by_users": "4", "topics": "", "key_findings": "LLMRec achieves over 38% average cost reduction while maintaining accuracy across six datasets, consistently outperforming baselines in converging toward the Pareto frontier. The framework demonstrates robust zero-shot generalization to unseen tasks and seamless adaptation to dynamically evolving model pools. By treating cost thresholds as dynamic input features rather than hard constraints, LLMRec learns policies that associate different budget levels with corresponding routing strategies.", "description": "This paper introduces LLMRec, a novel LLM routing framework that reframes the routing problem as a recommendation system task. The method treats an LLM's suitability for a query as a complex latent signal analogous to user-item preference, systematically engineering features for candidate LLMs (intrinsic attributes and capability distributions), queries (semantic embeddings and meta-information), and context (inference type and cost budgets) to intelligently route queries to the most suitable model while minimizing costs.", "key_contribution": "The main innovation is reframing LLM routing as a comprehensive recommendation system (RecSys) problem with learnable representations for models and queries. LLMRec converts hard cost constraints into dynamic learnable conditions by treating cost thresholds as input features, enabling the router to learn adaptive policies that balance different budget levels with routing strategies for optimal cost-accuracy tradeoffs.", "novelty": "Unlike previous approaches like RouteLLM (which uses preference data and achieves 48% cost reduction) and FrugalGPT (which uses sequential LLM cascades that increase costs), LLMRec is the first to apply recommendation system principles to LLM routing. It addresses limitations of prior work by enabling dynamic cost management through learnable budget conditions rather than fixed thresholds, supporting zero-shot generalization to new tasks, and adapting to evolving model pools. The systematic feature engineering for models, queries, and context creates a more comprehensive routing framework than existing binary or cascade-based approaches.", "ai_categories": ["Tool Use and Code Generation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4306624", "score": "65", "title": "AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?", "authors": "Ori Press, Brandon Amos, Haoyu Zhao, Yikai Wu, Samuel Ainsworth, Dominik Krupke, Patrick Kidger, Touqir Sajed, Bartolomeo Stellato, ..., Ofir Press", "session_type": "SD-4-2514", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2507.15887", "relevant_to_users": "10", "read_by_users": "18", "topics": "", "key_findings": "The paper demonstrates that contemporary LLMs can achieve meaningful speedups on real-world general-purpose numerical programs through algorithmic optimization, not just syntactic refactoring. AlgoTune provides the first systematic framework for LLM-driven algorithmic optimization, showing that language models can effectively identify performance bottlenecks, suggest algorithmic improvements, and provide optimized implementations across diverse numerical domains.", "description": "This paper investigates whether large language models can effectively speed up general-purpose numerical programs through algorithmic optimization. It introduces AlgoTune, a systematic framework that enables LLMs to identify bottlenecks, suggest algorithmic improvements, and evaluate speedup gains across diverse numerical computing tasks.", "key_contribution": "The main contribution is the first systematic study and framework (AlgoTune) for LLM-driven algorithmic optimization of numerical code, including a comprehensive benchmark across multiple numerical domains and empirical evidence demonstrating that LLMs can perform algorithmic reasoning beyond simple code generation.", "novelty": "Unlike prior work that focused on synthetic benchmarks, specific optimization patterns, or relied heavily on manual expert intervention, AlgoTune targets real-world general-purpose numerical programs with diverse algorithmic structures. It demonstrates that LLMs can perform true algorithmic reasoning and structural improvements rather than just syntactic code refactoring, extending LLM capabilities from code generation to algorithm optimization. This addresses the limitation of previous approaches that were either too narrow in scope or required significant human expertise.", "ai_categories": ["Tool Use and Code Generation", "Reasoning and Test-Time Compute"]}, {"paper_id": "3402481", "score": "65", "title": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks and Defenses on Large Language Models", "authors": "Yige Li, Hanxun Huang, Yunhan Zhao, Xingjun Ma, Jun Sun", "session_type": "SD-5-1405", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2408.12798", "relevant_to_users": "15", "read_by_users": "28", "topics": "", "key_findings": "BackdoorLLM provides the first comprehensive benchmark for backdoor attacks on LLMs, revealing that backdoor attacks are feasible across various LLM architectures with substantial attack success rates. Key findings include: (1) even low-success-rate backdoors significantly amplify jailbreak vulnerabilities, (2) larger models show greater resistance to weight-poisoning attacks, (3) trojan activation attacks demonstrate poor cross-model transferability, (4) more capable models are more vulnerable to chain-of-thought attacks, and (5) existing defense mechanisms remain largely ineffective. The benchmark encompasses over 200 experiments spanning 8 attack strategies (including data poisoning, weight poisoning, hidden-state manipulation, and chain-of-thought hijacking), 7 real-world scenarios, 6 model architectures, and 7 defense techniques.", "description": "BackdoorLLM is the first comprehensive benchmark for systematically evaluating backdoor vulnerabilities in text-generation Large Language Models. It provides a unified framework with standardized training and evaluation pipelines to assess diverse attack modalities (data poisoning, weight poisoning, hidden-state attacks, and chain-of-thought hijacking) across multiple model architectures and real-world scenarios, paired with a defense toolkit for mitigation strategies.", "key_contribution": "The main contribution is establishing the first unified, comprehensive benchmark for backdoor attacks and defenses on LLMs, enabling systematic comparison of attack effectiveness across standardized conditions. This includes a diverse attack taxonomy covering four distinct threat models, extensive evaluation across 6 LLM architectures and 7 scenarios, and an integrated defense toolkit with 7 mitigation techniques, providing critical insights into backdoor vulnerabilities in modern LLMs.", "novelty": "Unlike previous backdoor research that predominantly focused on vision models and classification tasks, BackdoorLLM is the first to comprehensively address backdoor vulnerabilities in open-ended text generation LLMs. It addresses the limitation of prior work where attacks were studied in isolation without standardized comparison by providing a unified evaluation framework that systematically compares multiple attack types, defense mechanisms, model scales, and task scenarios. The work introduces novel attack vectors specific to LLMs (chain-of-thought hijacking) and reveals previously unknown vulnerabilities such as backdoor-amplified jailbreaking effects.", "ai_categories": ["Agent Safety and Security", "Domain-Specific Agents and Applications"]}, {"paper_id": "4442047", "score": "64", "title": "DeepDiver: Adaptive Web-Search Intensity Scaling via Reinforcement Learning", "authors": "Wenxuan Shi, Haochen Tan, Chuqiao Kuang, Xiaoguang Li, Hanting Chen, Xiaozhe Ren, Yasheng Wang, Lu Hou, Lifeng Shang", "session_type": "SD-1-5301", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/4da779e56e6d170cf4fc07f2af1015f08b35314e.pdf", "relevant_to_users": "0", "read_by_users": "2", "topics": "", "key_findings": "DeepDiver introduces Search Intensity Scaling (SIS), an emergent RL-trained ability enabling LLMs to adaptively escalate web search frequency and depth based on task complexity. The approach achieves breakthrough results: 7B models (Qwen2.5-7B, Pangu-7B) reach performance comparable to the 671B-parameter DeepSeek-R1 on real-world web search tasks. The paper also contributes WebPuzzle, a 24k-sample benchmark evaluating all four information-seeking behaviors (evidence gathering, conflict resolution, verification, reflection) on live internet sources with noisy, unreliable data‚Äîunlike wiki-based datasets that focus primarily on evidence gathering.", "description": "This paper presents DeepDiver, a reinforcement learning framework that trains language models to adaptively scale their web search intensity based on information demands. Using the new WebPuzzle benchmark with 24k training samples on live internet data, the approach enables 7B-parameter models to achieve performance comparable to 671B-parameter models on real-world information-seeking tasks.", "key_contribution": "The main innovation is Search Intensity Scaling (SIS), an emergent ability learned through RL that allows models to dynamically increase search depth and frequency for harder problems, rather than following fixed search patterns. This is achieved through a two-stage training pipeline (cold-start SFT + GRPO-based RL) with transient auxiliary rewards and loose-to-strict reward transitions.", "novelty": "Unlike existing prompting and supervised fine-tuning approaches that rely on fixed rules or static training data, DeepDiver uses reinforcement learning to develop adaptive search behavior that emerges naturally during training (confirmed by auxiliary rewards dropping from 4.5% to 0.1%). Previous benchmarks evaluate on structured wiki sources focusing on evidence gathering, while WebPuzzle tests on live internet with noisy, unreliable sources across all four information-seeking behaviors. The work demonstrates that smaller models can match massive models through learned adaptive strategies rather than parameter scaling.", "ai_categories": ["Web and Computer-Use Agents", "Reinforcement Learning for LLMs", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4255911", "score": "64", "title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents", "authors": "Thomas Kuntz, Agatha Duzan, Hao Zhao, Francesco Croce, Zico Kolter, Nicolas Flammarion, Maksym Andriushchenko", "session_type": "SD-2-1312", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.14866", "relevant_to_users": "9", "read_by_users": "13", "topics": "", "key_findings": "OS-Harm introduces the first comprehensive safety benchmark specifically for computer use agents, revealing significant vulnerabilities in current systems. The benchmark systematically evaluates agent safety across multiple harm categories, showing varying levels of protection across different models and safety mechanisms. This work provides critical insights into the unique safety challenges of agents with direct computer control capabilities.", "description": "This paper presents OS-Harm, a benchmark for evaluating the safety of autonomous agents that can control computers. It addresses the critical need to measure whether computer-use agents resist harmful requests across multiple categories of potential misuse, filling a gap between traditional chatbot safety evaluation and the distinct challenges of agents with actual computer interaction capabilities.", "key_contribution": "The main contribution is the creation of OS-Harm, the first comprehensive safety benchmark specifically designed for computer use agents. It provides a structured evaluation framework that measures agent refusal rates and compliance with safety guidelines across different harm categories relevant to computer control.", "novelty": "Unlike previous work that focused on text-based LLM safety or general agent behavior, OS-Harm uniquely addresses safety in the context of actual computer control and interaction. It bridges the gap between traditional chatbot safety evaluations and the distinct challenges posed by autonomous computer use, recognizing that agents with computer access present fundamentally different risks. The benchmark addresses the limitation that existing safety evaluations don't capture the unique threat landscape of agents that can execute actions on operating systems.", "ai_categories": ["Agent Safety and Security", "Agent Benchmarking and Evaluation", "Web and Computer-Use Agents"]}, {"paper_id": "4126392", "score": "64", "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning", "authors": "Mingyang Chen, Linzhuang Sun, Tianpeng Li, sunhaoze, ZhouYijie, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, ..., Weipeng Chen", "session_type": "SD-3-1900", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/3a4c411411ec8827c21b081ac099f93878bb8269.pdf", "relevant_to_users": "23", "read_by_users": "34", "topics": "", "key_findings": "ReSearch introduces a reinforcement learning framework that trains LLMs to autonomously learn when and how to perform external search during reasoning, without requiring supervised data on reasoning steps. Trained only on the MuSiQue dataset (19,938 samples), the models demonstrate strong generalization across multiple multi-hop reasoning benchmarks. The RL process naturally elicits advanced capabilities like reflection and self-correction, showing that models can learn optimal search strategies through reward signals based on answer correctness.", "description": "ReSearch is a framework that trains LLMs to integrate external search operations as integral components of the reasoning chain using reinforcement learning. The approach treats search decisions as learned behaviors guided by text-based thinking, where search results influence further reasoning dynamically.", "key_contribution": "The main innovation is using reinforcement learning to enable LLMs to learn search policies autonomously, moving beyond heuristic-based or supervised trigger mechanisms for retrieval. This allows models to discover when to search, how to formulate queries, and how to integrate retrieved information optimally for different problem types.", "novelty": "Unlike prior work such as SELF-RAG and IRCoT that rely on predetermined retrieval triggers or supervised training for search decisions, ReSearch learns search policies through end-to-end reinforcement learning without supervised reasoning data. This addresses the limitation of static retrieval patterns in traditional RAG and fixed tool-use instructions by enabling adaptive, problem-specific search strategies. The approach demonstrates that effective search behavior can emerge naturally from RL training on answer correctness alone, without explicit supervision on when or how to search.", "ai_categories": ["Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs", "Tool Use and Code Generation"]}, {"paper_id": "4243067", "score": "64", "title": "VIKI√¢¬Ä¬ëR: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning", "authors": "Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, LEI BAI, Zhenfei Yin", "session_type": "SD-4-2310", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.09049", "relevant_to_users": "2", "read_by_users": "19", "topics": "", "key_findings": "VIKI-R introduces a reinforcement learning framework that enables multiple embodied agents to effectively coordinate and cooperate through synchronized decision-making. The work demonstrates that agents can learn collaborative strategies that overcome individual limitations, achieving tasks impossible for single agents through coordinated visual perception, reasoning, and action selection in shared physical environments.", "description": "VIKI-R presents a reinforcement learning method for training multiple embodied agents to cooperate in complex tasks requiring synchronized actions and shared understanding. The approach combines visual perception, reasoning, and RL to enable agents to coordinate behaviors in shared environments, moving beyond isolated agent control to collaborative embodied intelligence.", "key_contribution": "The primary contribution is a novel RL approach enabling embodied multi-agent systems to learn effective cooperation strategies through coordinated action selection, visual grounding, and shared environmental reasoning, addressing the challenge of temporal synchronization where agents must coordinate physical actions across time steps.", "novelty": "Unlike prior multi-agent approaches, VIKI-R operates in embodied settings where agents must interpret visual observations and coordinate physical actions rather than operating in abstract spaces. It develops mechanisms for agents to share contextual understanding rather than treating them as independent entities, and specifically addresses temporal synchronization‚Äîensuring coordinated actions across time steps. This moves beyond independent agent control to enable truly collaborative embodied intelligence with shared environmental reasoning.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Reinforcement Learning for LLMs", "Vision-Language-Action Models"]}, {"paper_id": "4037027", "score": "64", "title": "AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model for Atmospheric Science", "authors": "Chenyue Li, Wen Deng, Mengqian Lu, Binhang Yuan", "session_type": "SD-6-1606", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2502.01159", "relevant_to_users": "1", "read_by_users": "7", "topics": "", "key_findings": "This paper introduces AtmosSci-Bench, the first comprehensive benchmark specifically designed for evaluating LLMs on atmospheric science tasks. The evaluation of state-of-the-art LLMs reveals significant performance gaps in domain-specific meteorological and climate-related tasks, demonstrating that while advanced models show promise, substantial improvements are needed for reliable deployment in specialized scientific domains. The work provides baseline results across leading LLM architectures and establishes a structured methodology for future research in atmospheric science AI applications.", "description": "AtmosSci-Bench is a specialized evaluation framework designed to assess the capabilities of large language models in atmospheric science, meteorology, and climate-related tasks. The paper fills a critical gap by creating domain-specific benchmarks that capture the unique requirements of atmospheric science, rather than relying on generic science evaluation frameworks.", "key_contribution": "The main contribution is the development of the first comprehensive, open-source benchmark specifically tailored for atmospheric science applications, providing structured assessment methodology, domain-specific evaluation criteria, and baseline performance results across multiple state-of-the-art LLMs.", "novelty": "Unlike existing general science benchmarks that may not adequately capture domain-specific requirements, AtmosSci-Bench is the first to focus exclusively on atmospheric science with authentic meteorological and climate problems. It addresses the limitation that current LLM evaluations lack specialized assessment tools for scientific domains, introducing evaluation criteria specifically relevant to atmospheric phenomena and practical constraints faced in meteorology and climate research.", "ai_categories": ["Agent Benchmarking and Evaluation", "Domain-Specific Agents and Applications"]}, {"paper_id": "4113210", "score": "63", "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale", "authors": "Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, YuYue, Weinan Dai, Tiantian Fan, Gaohong Liu, ..., Mingxuan Wang", "session_type": "SD-1-316", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/a5ca4684c1debe30e4fde4bd063a262d61e13db7.pdf", "relevant_to_users": "41", "read_by_users": "50", "topics": "", "key_findings": "DAPO achieves 50 points on AIME 2024 using Qwen2.5-32B, outperforming DeepSeek-R1-Zero-Qwen-32B (47 points) with 50% fewer training steps. The paper demonstrates that four key techniques‚ÄîClip-Higher, Dynamic Sampling, Token-Level Policy Gradient Loss, and Overlong Reward Shaping‚Äîaddress critical issues in large-scale LLM RL: entropy collapse, training instability, and gradient inefficiency. The system is fully open-sourced with code, curated datasets (17K math problems), and detailed implementation on the verl framework, enabling reproducible state-of-the-art reasoning model training.", "description": "DAPO is an open-source large-scale reinforcement learning system for training reasoning LLMs. It reverse-engineers techniques from closed models like OpenAI o1 and DeepSeek R1, introducing four algorithmic innovations that enable stable, efficient training at scale. The system achieves state-of-the-art performance on mathematical reasoning benchmarks while being fully reproducible.", "key_contribution": "The paper introduces DAPO with four novel techniques that solve critical engineering challenges in large-scale LLM RL: decoupled clipping bounds to prevent entropy collapse, dynamic sampling to maintain gradient stability, token-level normalization for fair loss computation, and graduated penalty shaping for truncated sequences. Additionally, it provides the first fully open-source reproduction of state-of-the-art reasoning model training.", "novelty": "Unlike previous works that concealed technical details, DAPO explicitly addresses and solves practical engineering challenges that prevented reproduction: naive GRPO implementations achieved only 30 AIME points versus DAPO's 50 points. The work introduces asymmetric clipping (decoupled bounds), dynamic prompt filtering during training, and token-level loss normalization‚Äîtechniques not disclosed in prior closed systems. It demonstrates that careful algorithmic design, not just scale, is critical for RL success, and provides the first reproducible open-source system matching proprietary reasoning models.", "ai_categories": ["Reinforcement Learning for LLMs", "Reasoning and Test-Time Compute"]}, {"paper_id": "4268729", "score": "63", "title": "World-aware Planning Narratives Enhance Large Vision-Language Model Planner", "authors": "Junhao Shi, Zhaoye Fei, Siyin Wang, Qipeng Guo, Jingjing Gong, Xipeng Qiu", "session_type": "SD-4-2404", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/c4dfdf9e91cc4527c4c879b52d8cc8d352f626ce.pdf", "relevant_to_users": "6", "read_by_users": "39", "topics": "", "key_findings": "The paper introduces WAP (World-Aware Planning Narrative Enhancement), which improves vision-language model planning by 60.7 percentage points on EB-ALFRED, achieving 62.7% success rate. Open-source models enhanced with WAP outperform proprietary systems like GPT-4o (56.3%) and Claude-3.5-Sonnet in closed-loop embodied planning. The framework demonstrates exceptional gains in commonsense reasoning (+60.0) and long-horizon planning (+70.0), showing that comprehensive environmental understanding through narrative enhancement enables robust planning without privileged feedback signals.", "description": "This paper addresses the limitation of large vision-language models in embodied planning tasks by proposing WAP, a framework that enhances models with four cognitive capabilities: visual appearance modeling, spatial reasoning, functional abstraction, and syntactic grounding. The approach enriches training data with multi-dimensional narrative annotations that integrate environmental context with task instructions, enabling agents to perform complex multi-step planning using only visual observations and natural language.", "key_contribution": "The main innovation is a systematic framework that infuses vision-language models with world-aware planning narratives through four complementary cognitive dimensions, enabling closed-loop embodied planning without relying on privileged environmental feedback like action success signals or task progress information. This allows open-source models to surpass proprietary systems in complex, real-world-like planning scenarios.", "novelty": "Unlike previous approaches that treat instructions and environmental contexts as disconnected elements or rely on cascaded pipelines with external semantic models and privileged feedback signals, WAP systematically integrates contextual world knowledge through multi-dimensional narrative enhancement during training. The framework addresses the fundamental limitation that models perform poorly when connecting changing surroundings with context-sensitive instructions, demonstrating that comprehensive environmental understanding through enhanced narratives‚Äîrather than auxiliary cues‚Äîenables human-like planning capabilities in genuine closed-loop control scenarios.", "ai_categories": ["Planning and Decision Making", "Vision-Language-Action Models", "Reasoning and Test-Time Compute"]}, {"paper_id": "4217448", "score": "62", "title": "SeRL: Self-play Reinforcement Learning for Large Language Models with Limited Data", "authors": "Wenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, Kaixuan Chen, Mingli Song, Dacheng Tao", "session_type": "SD-2-411", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/f20577e67650da84d5c9396fc93fb971782ca925.pdf", "relevant_to_users": "13", "read_by_users": "34", "topics": "", "key_findings": "SeRL achieves performance comparable to models trained on full high-quality data with verifiable rewards using only 500 initial instructions. The framework demonstrates that LLaMA-3.2-3B after two iterations outperforms supervised baselines trained on thousands of examples across multiple math reasoning benchmarks (MATH-500: 52.6% vs 49.7% baseline). The majority-voting reward mechanism shows significantly higher alignment with rule-based rewards compared to model-based alternatives, while the online filtering strategy prevents reward hacking and model degradation observed in prior self-improvement methods.", "description": "SeRL is a self-play reinforcement learning framework that enables LLMs to bootstrap training from limited initial data without requiring verifiable reward signals. It combines two complementary modules: self-instruction for generating additional training data with adaptive difficulty filtering, and self-rewarding using majority-voting consensus to estimate response quality. The iterative framework allows continuous model improvement in specialized domains where high-quality labeled data is scarce or expensive to obtain.", "key_contribution": "The main innovation is an integrated online RL framework that combines self-instruction with unsupervised majority-voting rewards and dynamic difficulty filtering. This enables effective bootstrapping from minimal seed data (as few as 500 instructions) while preventing reward hacking through quality, diversity, and difficulty constraints, eliminating the need for external reward models or human annotations.", "novelty": "Unlike prior self-instruction methods limited to supervised fine-tuning with static data, SeRL operates as an online RL system with step-wise generation that adapts to evolving model capabilities. It addresses reward hacking through dual-end difficulty clipping that filters both trivially easy and impossibly hard examples, preventing the degenerate solutions seen in methods like SR-DPO and I-RPO. The majority-voting reward mechanism provides unsupervised quality estimation that outperforms learned reward models while requiring no external labels or verifiers, making it practical for specialized domains lacking expert annotations.", "ai_categories": ["Reinforcement Learning for LLMs", "Reasoning and Test-Time Compute"]}, {"paper_id": "3830405", "score": "61", "title": "BenchmarkCards: Standardized Documentation for Large Language Model Benchmarks", "authors": "Anna Sokol, Elizabeth Daly, Michael Hind, David Piorkowski, Xiangliang Zhang, Nuno Moniz, Nitesh Chawla", "session_type": "SD-3-1016", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2410.12974", "relevant_to_users": "5", "read_by_users": "9", "topics": "", "key_findings": "BenchmarkCards introduces the first standardized documentation framework specifically designed for LLM benchmarks, addressing critical fragmentation in how benchmark metadata is recorded and shared across the field. The framework enables systematic comparison, improved reproducibility, and better transparency in LLM evaluation practices. Applied to real benchmarks like BBQ and RealToxicityPrompts, it demonstrates how structured documentation helps practitioners make informed decisions about benchmark selection and interpretation.", "description": "This paper proposes BenchmarkCards, a structured documentation standard for large language model benchmarks that captures essential benchmark properties in a uniform manner. Similar to model cards and datasheets, BenchmarkCards provide templates and guidelines for consistently documenting LLM benchmark characteristics, enabling better comparison and reproducibility across the evaluation landscape.", "key_contribution": "The main contribution is a comprehensive, standardized documentation framework specifically tailored for LLM benchmarks that systematically captures metadata about benchmark design, properties, and usage contexts. This enables practitioners to compare benchmarks systematically and make informed evaluation decisions.", "novelty": "Unlike previous documentation frameworks (model cards, datasheets) designed for models or datasets, BenchmarkCards specifically addresses the unique needs of benchmark documentation, which has lacked standardization in the LLM community. The work fills a critical gap by adapting successful documentation practices to the benchmarking context, where inconsistent and incomplete reporting has hindered reproducibility and benchmark comparison. This is the first systematic effort to establish documentation standards specifically for LLM evaluation benchmarks.", "ai_categories": ["Agent Benchmarking and Evaluation"]}, {"paper_id": "4162370", "score": "61", "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?", "authors": "Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, Gao Huang", "session_type": "SD-3-1906", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/c3957c2dc397dd6f7bf1e3da21cebaeca53844af.pdf", "relevant_to_users": "50", "read_by_users": "89", "topics": "", "key_findings": "The paper challenges the assumption that Reinforcement Learning with Verifiable Rewards (RLVR) develops new reasoning abilities in LLMs, finding that current RLVR training does not elicit fundamentally new reasoning patterns. While RLVR-trained models outperform base models at small k values (e.g., k=1), base models achieve higher pass@k scores at large k, revealing that reasoning improvements are bounded by what base models already contain. Six popular RLVR algorithms perform similarly and underutilize base model potential, whereas distillation approaches can introduce truly new reasoning patterns. This work establishes that RLVR optimizes existing capacities rather than discovering novel reasoning strategies.", "description": "This paper systematically evaluates whether Reinforcement Learning with Verifiable Rewards (RLVR) develops new reasoning abilities in large language models or merely optimizes existing ones. Through comprehensive testing across multiple model families, algorithms, and benchmarks including mathematics, coding, and visual reasoning tasks, the authors demonstrate that current RLVR methods improve performance by optimizing existing base model capabilities rather than enabling genuine self-improvement beyond base model reasoning capacity.", "key_contribution": "The paper provides empirical evidence challenging the theoretical promise of RLVR by demonstrating that improved reasoning performance derives from optimizing existing base model capacities rather than discovering novel reasoning strategies. It establishes base models as meaningful upper bounds for evaluating RLVR effectiveness using pass@k analysis at large k values.", "novelty": "Unlike prior work that assumes RLVR unlocks new abilities, this research systematically challenges these assumptions using pass@k analysis at large k values‚Äîa more revealing metric than typical greedy evaluations. It addresses the gap between theoretical expectations and empirical reality by showing that coverage and perplexity patterns demonstrate reasoning boundaries remain linked to base model capabilities. This fundamentally questions whether current RLVR approaches realize reinforcement learning's potential for developing genuinely novel reasoning in LLMs, rather than simply optimizing existing patterns.", "ai_categories": ["Reinforcement Learning for LLMs", "Reasoning and Test-Time Compute", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4112393", "score": "61", "title": "Why Do Multi-Agent LLM Systems Fail?", "authors": "Mert Cemri, Melissa Z Pan, Shuyi Yang, Lakshya A Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, ..., Ion Stoica", "session_type": "SD-3-110", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2503.13657", "relevant_to_users": "11", "read_by_users": "13", "topics": "", "key_findings": "This paper introduces the first comprehensive empirical taxonomy (MAST - Multi-Agent System Taxonomy) of failure modes in multi-agent LLM systems. Key findings reveal that failures stem from communication breakdowns, role confusion, and coordination problems between agents. Agents frequently misunderstand task requirements, stop prematurely, or suffer from context degradation and conversation history loss. The study provides systematic categorization of distinct failure types (e.g., step repetition, premature termination) and demonstrates that targeted prompt interventions can mitigate specific failure modes. The work includes publicly available datasets (MAST-Data) of annotated failures to enable future research.", "description": "This paper systematically investigates why multi-agent LLM systems fail by developing an empirical taxonomy of failure modes through qualitative analysis of real execution traces. Unlike prior work that treated these systems as black boxes, this research categorizes specific failure patterns including communication breakdowns, role confusion, context loss, and coordination problems that emerge when multiple LLMs work together to solve tasks.", "key_contribution": "The main contribution is the first comprehensive, grounded empirical taxonomy (MAST) of multi-agent LLM system failures, developed through systematic analysis of real-world failure cases. The work provides structured categorization of failure modes, publicly available annotated datasets, and connections between specific failure types and potential remediation strategies.", "novelty": "Previous work lacked systematic examination of how and why multi-agent LLM systems fail, treating them as black boxes without investigating failure mechanisms. This paper addresses this gap by applying grounded theory methodology to analyze real execution traces and develop a comprehensive taxonomy of distinct failure categories. Unlike anecdotal observations in prior work, this research provides empirical, structured categorization that enables practitioners to debug systems and researchers to develop targeted solutions for specific failure modes.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4215631", "score": "60", "title": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling", "authors": "Jialong Zhou, Lichao Wang, Xiao Yang", "session_type": "SD-1-1105", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/12984819e947af4200e71748ef8d3b07f7b029cd.pdf", "relevant_to_users": "1", "read_by_users": "3", "topics": "", "key_findings": "This paper introduces GUARDIAN, a safety framework for multi-agent LLM systems that uses temporal graph modeling to detect and mitigate unsafe collaboration patterns. The key insight is that multi-agent interactions can amplify harmful outputs through collaborative reasoning that bypasses individual agent safety measures. By modeling agent communications as temporal graphs, GUARDIAN identifies emergent safety risks that occur through agent-to-agent exchanges rather than within isolated agent outputs.", "description": "GUARDIAN addresses safety vulnerabilities in multi-agent LLM collaborations by using temporal graph analysis to monitor agent interactions over time. The framework detects when collaborative reasoning patterns might circumvent safety constraints and provides intervention mechanisms to prevent unsafe outcomes in multi-agent systems.", "key_contribution": "The main contribution is a temporal graph-based safety framework that shifts focus from individual agent safety to the interaction layer, enabling detection of emergent unsafe patterns that arise specifically through multi-agent collaboration and would not appear in single-agent analysis.", "novelty": "Unlike previous work that focuses on safeguarding individual agents, GUARDIAN models multi-agent communications as dynamic temporal graphs to capture how safety risks emerge through agent-to-agent exchanges. This addresses the limitation that traditional single-agent safety measures are inadequate for multi-agent scenarios where collaborative reasoning can amplify harmful outputs. The novel approach analyzes conversation trajectories to detect risk accumulation across interaction turns rather than evaluating agents in isolation.", "ai_categories": ["Agent Safety and Security", "Multi-Agent Systems and Collaboration"]}, {"paper_id": "4220413", "score": "60", "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model", "authors": "Wenbo Hu, Yining Hong, Yanjun Wang, Leison Gao, Zibu Wei, Xingcheng Yao, Nanyun Peng, Yonatan Bitton, Idan Szpektor, Kai-Wei Chang", "session_type": "SD-4-4806", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/8e3474ff3b680185f73a397352845c1347b77d73.pdf", "relevant_to_users": "8", "read_by_users": "22", "topics": "", "key_findings": "The paper introduces 3DMem-Bench, a comprehensive benchmark with 26,000+ trajectories and 2,892 embodied tasks for evaluating spatial-temporal reasoning. It proposes 3DLLM-Mem, a dual-memory architecture (working + episodic) that achieves 37.6% success rate on in-domain tasks and 32.1% on in-the-wild tasks, outperforming strongest baselines by 16.5% and showing 5-6x better performance on hard tasks. The memory fusion mechanism uses dense 3D representations with attention-based retrieval to integrate task-relevant historical information.", "description": "This paper addresses the challenge of long-term spatial-temporal memory in embodied 3D LLMs operating in complex, evolving environments. It proposes a dual-memory architecture inspired by human cognition, combining limited-capacity working memory for current observations with expandable episodic memory storing dense 3D representations of past spatial-temporal information.", "key_contribution": "The main innovation is a memory fusion module that selectively integrates task-relevant episodic memory using working memory as queries, combined with dense 3D representations for memory storage rather than sparse alternatives. This enables embodied agents to maintain and reason over long-term spatial-temporal information across multi-room environments and extended task sequences.", "novelty": "Unlike existing approaches that rely solely on LLM context windows or simple retrieval strategies, 3DLLM-Mem explicitly models episodic memory using dense 3D features‚Äîamong the first to do so for embodied 3D LLMs. It addresses critical limitations where prior methods either cannot maintain extended memory chains in multi-room environments or discard valuable historical observations. The dual-memory architecture with dynamic fusion overcomes the entanglement problem of spatial and temporal information in evolving environments.", "ai_categories": ["Memory and Context Management", "Vision-Language-Action Models", "Spatial and Physical Reasoning"]}, {"paper_id": "4427549", "score": "59", "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents", "authors": "Kangrui Wang, Pingyue Zhang, Zihan Wang, Yaning Gao, Linjie Li, Qineng Wang, Hanyang Chen, Yiping Lu, Zhengyuan Yang, ..., Manling Li", "session_type": "SD-1-5502", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ed11882b04656406635941c99bb0b5dbf475a088.pdf", "relevant_to_users": "6", "read_by_users": "28", "topics": "", "key_findings": "VAGEN introduces a framework that combines video understanding with explicit world modeling for multi-turn VLM agents, enabling them to maintain temporal consistency and reason about dynamic world states across interaction turns. The system tracks object positions, relationships, and state changes while predicting future states before action execution. This approach demonstrates improvements in multi-turn task completion accuracy, reduced hallucinations through world model validation, and better generalization to unseen scenarios.", "description": "The paper presents VAGEN, a framework that enhances multi-turn vision-language model agents by integrating video representations with explicit world state modeling. This enables agents to reason about temporal dynamics, predict action consequences, and maintain consistency across multiple interaction turns in complex visual environments.", "key_contribution": "The main innovation is the integration of explicit world state tracking and predictive reasoning into VLM agents, allowing them to process visual sequences temporally, validate predictions against observations, and maintain coherent world models across multiple turns rather than processing frames independently.", "novelty": "Unlike previous VLM agents that process frames independently without temporal context, VAGEN maintains explicit world state representations that track changes over time and enable predictive reasoning about action consequences before execution. This addresses the limitation of temporal inconsistency in existing agents by enforcing coherent world models across interaction episodes, validating observations against predictions to reduce hallucinations and improve decision-making robustness.", "ai_categories": ["Vision-Language-Action Models", "Planning and Decision Making", "Reasoning and Test-Time Compute"]}, {"paper_id": "4215876", "score": "59", "title": "Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs", "authors": "Hao Kang, Qingru Zhang, Han Cai, Weiyuan Xu, Tushar Krishna, Yilun Du, Tsachy Weissman", "session_type": "SD-2-303", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/13fb7451afdc5c8796e7166332feddd0c8a38f8e.pdf", "relevant_to_users": "0", "read_by_users": "3", "topics": "", "key_findings": "This paper introduces the first systematic formulation of the latency-quality trade-off for LLM-based agents in real-time environments. It presents two novel benchmarks: HFTBench (high-frequency trading simulation) and StreetFighter (competitive gaming), demonstrating that optimal latency-quality operating points vary dramatically by task. The proposed FPX framework achieves up to 80% win rate improvement in gaming and 26.52% daily yield boost in trading by adaptively applying mixed-precision quantization (FP4/FP8) at layer granularity. Key insight: while prior work treats quantization as static, latency-sensitive tasks require dynamic control‚Äîgaming is overwhelmingly latency-sensitive (20% speed reduction yields consistent wins), whereas trading demands balanced speed-accuracy optimization.", "description": "The paper addresses the underexplored problem of latency-quality trade-offs in LLM-based agents operating in real-time, latency-sensitive environments like high-frequency trading and competitive gaming. It introduces FPX, an adaptive mixed-precision inference framework that selectively applies FP4 and FP8 quantization to different model layers based on their compression tolerance, enabling fine-grained control over the speed-accuracy balance for optimal performance in dynamic decision-making tasks.", "key_contribution": "The main contribution is the FPX framework‚Äîan adaptive mixed-precision quantization system that dynamically balances model size and bitwidth at layer granularity, along with two purpose-built benchmarks (HFTBench and StreetFighter) that explicitly evaluate agents under strict latency constraints where faster responses directly translate to higher rewards, challenging the field's traditional accuracy-only evaluation paradigm.", "novelty": "Unlike existing quantization methods that remain static and optimize for accuracy alone, this work recognizes that latency-sensitive agent tasks require dynamic, task-specific optimization of the speed-accuracy frontier. Previous LLM trading and gaming research evaluated agents on static datasets or focused solely on action quality, ignoring response timing. This paper demonstrates that the optimal latency-quality trade-off varies dramatically across tasks (gaming heavily favors speed while trading requires balance), and proposes the first framework enabling explicit, continuous control over this trade-off through adaptive layer-wise precision assignment based on compression tolerance rather than uniform quantization.", "ai_categories": ["Agent Benchmarking and Evaluation", "Domain-Specific Agents and Applications"]}, {"paper_id": "4252788", "score": "59", "title": "Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study", "authors": "Zhengyu Hu, Jianxun Lian, Zheyuan Xiao, Seraphina Zhang, Tianfu Wang, Nicholas Jing Yuan, Xing Xie, Hui Xiong", "session_type": "SD-4-1905", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/0ced283865c2dd5717208f09588d27256b2fa260.pdf", "relevant_to_users": "2", "read_by_users": "11", "topics": "", "key_findings": "The paper reveals that (1) interactive learning with feedback outperforms passive instruction across mathematical benchmarks, (2) conceptual understanding is scale-emergent‚Äîlarger models (14B+) benefit from conceptual hints while smaller models (1.5B) perform worse, and (3) LLMs are effective few-shot learners (3-5 examples) but struggle as many-shot learners, with performance degrading beyond 900 examples. The LearnArena benchmark evaluates models across eight strategic games, with GPT-4o achieving 0.75 average performance.", "description": "This paper introduces a cognitive psychology-inspired framework that decomposes LLM learning ability into three complementary dimensions: Learning from Instructor (explicit guidance), Learning from Concept (abstract structure internalization), and Learning from Experience (adaptation through feedback). The authors conduct comprehensive empirical studies and introduce LearnArena, a unified benchmark using competitive game environments to evaluate general learning abilities.", "key_contribution": "The main contribution is the first systematic framework and benchmark (LearnArena) for evaluating general learning ability in LLMs across three cognitive dimensions simultaneously, moving beyond static zero/few-shot evaluations to assess how models acquire, internalize, and generalize knowledge through interactive, conceptual, and experiential learning.", "novelty": "Unlike prior work that evaluates isolated capabilities or static performance, this is the first work to explicitly evaluate general learning ability through a unified cognitive lens inspired by human learning psychology. It addresses the critical gap that existing benchmarks lack systematic investigation of how LLMs acquire and generalize new knowledge, treating learning dimensions piecemeal without theoretical grounding. The novel approach combines three learning paradigms simultaneously in competitive environments, revealing scale-dependent phenomena (conceptual understanding emerges at 14B+ parameters) and practical limits (many-shot learning degradation).", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4441390", "score": "59", "title": "Self-Guided Hierarchical Exploration for Generalist Foundation Model Web Agents", "authors": "Qianlan Yang, Xiangjun Wang, Danielle Perszyk, Yu-Xiong Wang", "session_type": "SD-4-5211", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/23935fe967684b7bdc286739474d07c94da9cc76.pdf", "relevant_to_users": "0", "read_by_users": "2", "topics": "", "key_findings": "SAGE introduces a self-guided training framework that eliminates the need for static task datasets or expert demonstrations by enabling autonomous task generation, curriculum learning, and continuous self-improvement. The framework achieves state-of-the-art performance on WebArena, outperforming all open-source baselines by 26% and even surpassing proprietary models. The hierarchical exploration strategy combines structural environment understanding, adaptive task generation, and MCTS-based planning to enable efficient skill acquisition under sparse rewards.", "description": "SAGE (Self-Guided Hierarchical Exploration) is a training framework for generalist foundation model web agents that uses multi-level hierarchical exploration to autonomously acquire and refine skills through interaction with real-world websites. The framework decomposes learning into three phases: pre-exploration for environment understanding, top-level exploration for curriculum-driven task generation, and low-level exploration for efficient interaction using MCTS.", "key_contribution": "The main innovation is a fully autonomous training paradigm that replaces static datasets and expert demonstrations with self-generated curriculum learning. SAGE autonomously generates tasks from easy to hard, evaluates its own progress, and uses MCTS-based planning to discover viable action sequences under sparse rewards, enabling continuous self-improvement and scalable skill acquisition.", "novelty": "Unlike previous methods that rely on expensive human-authored tasks and curated expert demonstrations, SAGE introduces autonomous curriculum generation where the agent creates and adapts its own training tasks progressively. It addresses the limitation of static datasets failing to capture long-tail web complexity by enabling continuous self-improvement through hierarchical exploration. The combination of top-level adaptive task generation with low-level MCTS-based planning creates a self-evolving learning system that scales beyond predefined training data.", "ai_categories": ["Web and Computer-Use Agents", "Self-Improvement and Meta-Learning", "Planning and Decision Making"]}, {"paper_id": "4226840", "score": "58", "title": "Predicting Empirical AI Research Outcomes with Language Models", "authors": "Jiaxin Wen, Chenglei Si, Chen Yueh-Han, He He, Shi Feng", "session_type": "SD-1-2413", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/149b9e435472a05ff9bc8f5c3138e681d73132fd.pdf", "relevant_to_users": "6", "read_by_users": "10", "topics": "", "key_findings": "This paper demonstrates that language models can predict the empirical success of AI research ideas with 77% accuracy, significantly outperforming human experts (64.4% vs 48.9% in NLP). The system achieves 63.6% accuracy even on unpublished ideas, while frontier models without fine-tuning perform no better than random guessing. This represents a breakthrough in automating research direction evaluation, potentially accelerating AI research by helping prioritize promising ideas before costly implementation.", "description": "This paper introduces the first benchmark for predicting empirical AI research outcomes, with 1,585 test pairs and 6,000 training pairs. The authors develop a system combining fine-tuned GPT-4.1 with an agentic paper retrieval module that predicts which of two competing research ideas will perform better on specific benchmarks, validated against human expert judgments and on unpublished real-world experiments.", "key_contribution": "The paper creates the first systematic benchmark for objective research outcome prediction (rather than subjective evaluation) and demonstrates that a fine-tuned language model with agentic retrieval can predict empirical research success more accurately than domain experts, achieving practical utility in prioritizing research directions and reducing wasted computational resources.", "novelty": "Previous work focused on subjective aspects of research (novelty, excitement) rather than predicting actual empirical performance. This paper shifts to objective outcome prediction and discovers that analyzing full papers (not just abstracts) substantially improves accuracy. Unlike prior approaches, it validates predictions on completely unpublished ideas with ground-truth experimental results, demonstrating that LMs can learn generalizable patterns about what makes research succeed‚Äîpatterns that even domain experts struggle to identify consistently.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4036748", "score": "58", "title": "Generating Computational Cognitive models using Large Language Models", "authors": "Milena Rmus, Akshay Kumar Jagadish, Marvin Mathony, Tobias Ludwig, Eric Schulz", "session_type": "SD-1-2010", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/71914784b067b8a89e5ba9e648c3f3fe0bef8659.pdf", "relevant_to_users": "6", "read_by_users": "17", "topics": "", "key_findings": "The paper introduces GeCCo (Guided generation of Computational Cognitive Models), a framework that uses LLMs to automatically generate computational cognitive models from natural language task descriptions. Tested across four cognitive domains (decision making, learning, planning, and memory) using three open-source LLMs, GeCCo-generated models consistently matched or outperformed the best hand-crafted domain-specific models from cognitive science literature. This demonstrates that LLMs can automate cognitive model generation while maintaining scientific rigor through empirical validation.", "description": "This paper presents GeCCo, an automated pipeline that leverages large language models to generate computational cognitive models. Given task instructions, participant data, and a template function, GeCCo prompts an LLM to propose candidate models, fits them to held-out data, and iteratively refines them based on predictive performance feedback.", "key_contribution": "The main contribution is democratizing cognitive modeling by automating the generation of formal mathematical models of human cognition, eliminating the need for extensive manual expert effort. GeCCo enables rapid exploration of diverse candidate models through human-AI collaboration while achieving competitive or superior performance compared to traditional hand-crafted models.", "novelty": "Unlike traditional cognitive modeling that requires researchers to manually specify mathematical equations based on theory‚Äîa time-consuming process demanding specialized expertise‚ÄîGeCCo automates model generation and refinement through LLM prompting. This addresses key bottlenecks in cognitive science research: the limited number of alternative models that can be explored, subjective biases in manual model design, and the significant time investment required. The iterative refinement loop based on empirical performance feedback represents a new paradigm for systematic cognitive model discovery.", "ai_categories": ["Tool Use and Code Generation", "Domain-Specific Applications"]}, {"paper_id": "4117634", "score": "58", "title": "AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration", "authors": "Andy Zhou, Kevin Wu, Francesco Pinto, Zhaorun Chen, Yi Zeng, Yu Yang, Shuang Yang, Sanmi Koyejo, James Zou, Bo Li", "session_type": "SD-4-903", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/e53fb762e883d79a783bab07668988038d9e5162.pdf", "relevant_to_users": "5", "read_by_users": "12", "topics": "", "key_findings": "AutoRedTeamer achieves 20% higher attack success rates on HarmBench against Llama-3.1-70B while reducing computational costs by 46% compared to existing methods. It demonstrates that combining attacks yields synergistic effects (e.g., Pliny+ArtPrompt achieves 0.83 ASR versus 0.60 individually). The framework autonomously discovers and integrates new attack vectors from recent research papers, maintaining comprehensive coverage across 314 AIR risk categories while matching the diversity of human-curated benchmarks.", "description": "AutoRedTeamer is a fully automated end-to-end red teaming framework for LLMs that combines a dual-agent architecture with memory-guided attack selection. The system continuously discovers new attack vectors by analyzing recent research papers and integrates them into a growing attack library, enabling lifelong learning for security evaluation.", "key_contribution": "The paper introduces a memory-guided attack selection mechanism that tracks historical performance to intelligently combine attacks, and an Attack Proposer agent that autonomously discovers new vectors by analyzing academic research and synthesizing core principles into novel attacks. This enables continuous adaptation to emerging threats without human intervention.", "novelty": "Unlike prior methods (PAIR, TAP) that refine individual attacks in isolation, AutoRedTeamer discovers synergies between attack combinations and operates from high-level risk categories rather than requiring predefined harmful scenarios. The lifelong learning mechanism addresses the limitation of static attack databases by continuously analyzing research literature to discover and validate new attack vectors. The memory system enables systematic exploration of the attack space rather than manual configuration or exhaustive search.", "ai_categories": ["Agent Safety and Security", "Multi-Agent Systems and Collaboration", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4205669", "score": "57", "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "authors": "Liyan Tang, Grace Kim, Xinyu Zhao, Thom Lake, Wenxuan Ding, Fangcong Yin, Prasann Singhal, Manya Wadhwa, Zeyu Liu, ..., Greg Durrett", "session_type": "SD-1-4709", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.13444", "relevant_to_users": "5", "read_by_users": "11", "topics": "", "key_findings": "ChartMuseum exposes a critical gap between human (93% accuracy) and LVLM performance, with the best model (Gemini-2.5-Pro) achieving only 63.0% and leading open-source models (Qwen2.5-VL-72B) reaching just 38.5%. All models show a dramatic 35%-55% performance drop on visual reasoning questions compared to text-heavy questions, revealing systematic weaknesses in visual analysis. The benchmark successfully differentiates model capabilities where previous benchmarks showed near-saturation, providing granular insights through a four-category reasoning taxonomy (textual, visual, text/visual, synthesis).", "description": "ChartMuseum is a challenging chart question-answering benchmark containing 1,162 expert-annotated questions sourced from 184 real-world chart images. The benchmark specifically targets non-trivial reasoning requirements across textual and visual modalities, designed to evaluate whether large vision-language models can perform sophisticated analytical reasoning rather than simple data extraction from charts.", "key_contribution": "The main contribution is a rigorously designed benchmark that exposes fundamental limitations in LVLM visual reasoning capabilities through expert-curated questions requiring complex multi-modal analysis. Unlike previous benchmarks that allowed near-saturation performance, ChartMuseum successfully differentiates model capabilities and reveals a systematic imbalance between textual and visual reasoning skills in current LVLMs.", "novelty": "Previous chart understanding benchmarks focused primarily on data extraction tasks and showed near-saturation with frontier models, failing to expose meaningful capability gaps. ChartMuseum addresses this by exclusively requiring non-trivial textual and visual reasoning over real-world charts, moving beyond surface-level comprehension. The benchmark introduces a novel reasoning taxonomy that isolates specific visual reasoning weaknesses, revealing that all evaluated models experience 35%-55% performance degradation on visual-heavy questions‚Äîa critical finding obscured by aggregate metrics in prior work.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models"]}, {"paper_id": "3891571", "score": "57", "title": "Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models", "authors": "Chenhang Cui, Gelei Deng, An Zhang, Jingnan Zheng, Yicong Li, Lianli Gao, Tianwei Zhang, Tat-Seng Chua", "session_type": "SD-1-5510", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/8babb592735eb9e46455028c5428dfc816f08161.pdf", "relevant_to_users": "9", "read_by_users": "17", "topics": "", "key_findings": "This work reveals that safe images can be weaponized to jailbreak LVLMs when strategically combined with other safe inputs, achieving 88.6% success rates against GPT-4o. Unlike previous attacks exploiting alignment flaws, this approach leverages inherent model properties: universal reasoning capabilities and a safety snowball effect where initial unsafe responses cascade into progressively harmful outputs. The attack successfully bypasses existing content moderators (Google Perspective, Azure Content Moderator, OpenAI Moderation) because all inputs remain safe in isolation.", "description": "This paper exposes a critical vulnerability in Large Vision-Language Models where benign images, when strategically combined with additional safe images and prompts, can trigger harmful content generation. The authors introduce Safety Snowball Agent (SSA), an automated framework that jailbreaks LVLMs through multi-turn interactions that progressively escalate harm while maintaining input safety.", "key_contribution": "The paper introduces Safety Snowball Agent (SSA), the first agent-based jailbreak framework that exploits safe images to generate harmful outputs by leveraging models' reasoning capabilities. It identifies the safety snowball effect‚Äîa cascade where initial unsafe responses enable progressively dangerous outputs‚Äîdemonstrating that any safe image can potentially be weaponized through intelligent composition.", "novelty": "Unlike prior jailbreak methods that use adversarial perturbations or overtly harmful images to exploit alignment gaps, this work demonstrates that completely safe inputs can be weaponized through strategic composition and multi-turn reasoning. It addresses a fundamental limitation: existing safety mechanisms focus on detecting harmful individual inputs but fail against intelligent combinations of benign content. The approach reveals that vulnerabilities stem from inherent model properties (reasoning capabilities) rather than alignment flaws, representing a paradigm shift in understanding LVLM security.", "ai_categories": ["Agent Safety and Security", "Vision-Language-Action Models", "Multi-Agent Systems and Collaboration"]}, {"paper_id": "4276950", "score": "57", "title": "Reasoning as an Adaptive Defense for Safety", "authors": "Taeyoun Kim, Fahim Tajwar, Aditi Raghunathan, Aviral Kumar", "session_type": "SD-1-405", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/c036ebd40ea82ab94b9badc0028faf7e94456ca1.pdf", "relevant_to_users": "6", "read_by_users": "15", "topics": "", "key_findings": "The paper introduces TARS (Training Adaptive Reasoners for Safety), showing that models can learn to adaptively allocate test-time compute for safety evaluation. TARS-trained models achieve better safety-refusal trade-offs than existing defenses like Circuit Breakers, with a 1.5B model outperforming 7-8B baseline models. Models demonstrate adaptive behavior by spending more compute on ambiguous queries (457 tokens) versus clearly harmful ones (290 tokens), and show 2.21x better internal separation between safe/unsafe representations compared to standard RL training (0.88x). The approach achieves greater robustness against both white-box (GCG) and black-box (PAIR) adversarial attacks.", "description": "This paper presents TARS, a reinforcement learning framework that trains language models to reason adaptively about safety concerns using chain-of-thought traces before responding to queries. The method combines lightweight supervised fine-tuning initialization, mixed prompt types (harmful, harmless, ambiguous), and dual reward functions balancing safety with task performance to prevent degenerate reasoning and over-refusal behaviors.", "key_contribution": "TARS provides the first systematic recipe for training safety-aware reasoning models through online RL with long chain-of-thought, enabling adaptive test-time compute allocation for safety evaluation rather than relying solely on fixed training-time defenses. The approach demonstrates that smaller reasoning-enabled models can be safer and more helpful than larger models without adaptive reasoning capabilities.", "novelty": "Unlike prior safety methods that rely on static training-time defenses (RLHF, supervised safety tuning) or apply reasoning uniformly, TARS enables models to dynamically allocate compute based on query ambiguity at test-time. It addresses previous limitations where reasoning-based safety approaches either over-refused through shortcuts or produced degenerate reasoning when trained only on harmful prompts. The online RL training with mixed prompt types prevents supervised fine-tuning's overfitting issues while maintaining adaptive behavior across diverse inputs, achieving what prior work left unclear: a general recipe for reasoning models with strong safety.", "ai_categories": ["Agent Safety and Security", "Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs"]}, {"paper_id": "4441794", "score": "57", "title": "Delving into Large Language Models for Effective Time-Series Anomaly Detection", "authors": "Junwoo Park, Kyudan Jung, Dohyun Lee, Hyuck Lee, Daehoon Gwak, ChaeHun Park, Jaegul Choo, Jaewoong Cho", "session_type": "SD-3-2410", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ff3e7df135cd4038ebbe31199752645c9946fa1e.pdf", "relevant_to_users": "1", "read_by_users": "3", "topics": "", "key_findings": "This paper addresses a critical gap in understanding why LLMs perform poorly on time-series anomaly detection (TSAD), often worse than simple baselines. Through systematic analysis, it identifies two core challenges: understanding complex temporal dynamics and accurately localizing anomalies. The proposed solution combines statistical decomposition with index-aware prompting, achieving up to 66.6% F1 score improvement over 21 existing prompting strategies on the AnomLLM benchmark and outperforming 16 non-LLM baselines on TSB-AD.", "description": "The paper investigates why Large Language Models struggle with time-series anomaly detection, providing the first in-depth empirical analysis of LLM failure modes in this domain. It proposes a method combining statistical decomposition with index-aware prompting to address the identified challenges of understanding temporal dynamics and localizing anomalies.", "key_contribution": "The main contribution is both diagnostic and prescriptive: systematically identifying why LLMs fail at TSAD (lack of temporal understanding and localization precision) and proposing a simple yet effective solution that integrates statistical signal processing with structured prompting to leverage LLMs' contextual reasoning capabilities.", "novelty": "Unlike prior work that simply applied generic LLM prompting to time-series tasks and reported poor results, this paper provides the first systematic investigation into the root causes of LLM failures in TSAD. It addresses the limitation that previous approaches treated time-series as raw inputs without decomposition or positional awareness. The novel combination of statistical decomposition (to make temporal patterns interpretable) with index-aware prompting (to enable precise localization) represents a principled approach that bridges quantitative signal processing with LLM reasoning, rather than treating them as separate paradigms.", "ai_categories": ["Domain-Specific Agents and Applications", "Reasoning and Test-Time Compute"]}, {"paper_id": "4237109", "score": "57", "title": "SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models", "authors": "Emil Biju, Shayan Talaei, Zhemin Huang, Mohammadreza Pourreza, Azalia Mirhoseini, Amin Saberi", "session_type": "SD-3-4317", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/2ac09a3b4dda169b90bb239fc45cb980a8bc3efd.pdf", "relevant_to_users": "3", "read_by_users": "4", "topics": "", "key_findings": "SPRINT achieves 92.5% accuracy on MATH-500 (vs. 91.0% for RFT baseline) while reducing sequential tokens by 440 on average and 39% on harder problems requiring >8,000 tokens. The framework demonstrates strong out-of-domain generalization with 53% token reduction on Countdown tasks and 10.8% on GPQA-Diamond. The approach uniquely combines inference-time parallelism, adaptive search, model optimization, and multi-step reasoning support‚Äîrequirements not simultaneously satisfied by prior methods.", "description": "SPRINT introduces a post-training and inference-time framework that enables reasoning models to dynamically identify parallelizable subtasks and execute them concurrently through interleaved planning rounds. The system uses a data curation pipeline that transforms sequential reasoning trajectories into dependency-aware stages, then fine-tunes models on 1,700 reformatted samples to learn this parallel reasoning structure.", "key_contribution": "The main innovation is the combination of rolling-horizon planning with parallelized execution, where the model autonomously identifies independent subtasks during inference and coordinates their parallel exploration through multiple planning-execution rounds. This is achieved through a novel data curation pipeline that extracts dependencies from reasoning trajectories and creates DAG-based training data.", "novelty": "Unlike Tree-of-Thought or Graph-of-Thought methods that require predefined search structures, SPRINT trains models to autonomously recognize parallelizable subtasks from natural language reasoning. Previous parallel generation methods like PASTA and Skeleton-of-Thought either lack multi-step reasoning capabilities or produce excessive tokens through single-round planning. SPRINT addresses these limitations by enabling adaptive, dependency-aware planning across multiple rounds while preserving the sequential reasoning structure needed for complex problems.", "ai_categories": ["Reasoning and Test-Time Compute", "Tool Use and Code Generation", "World Models and Planning"]}, {"paper_id": "4212865", "score": "55", "title": "AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking", "authors": "Xiangqi Wang, Yue Huang, Yanbo Wang, Xiaonan Luo, Kehan Guo, Yujun Zhou, Xiangliang Zhang", "session_type": "SD-1-5418", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/dd1128b101a8e529cfa219b76424da7861923e32.pdf", "relevant_to_users": "0", "read_by_users": "7", "topics": "", "key_findings": "AdaReasoner introduces an automated, adaptive reasoning system that dynamically optimizes LLM reasoning configurations (temperature, reasoning steps, instruction format) on a per-question basis using reinforcement learning. It achieves state-of-the-art performance across six different LLMs, improving GPT-4o accuracy from 72.32% to 80.42% while maintaining strong out-of-distribution robustness. The system learns effective policies with only 50-100 training examples and demonstrates that different task types (creative, logical, knowledge-intensive) require distinctly different reasoning configurations.", "description": "AdaReasoner is an LLM-agnostic plugin that automates adaptive reasoning configuration selection for diverse reasoning tasks. Using a reinforcement learning framework with a factorized action space, Boltzmann exploration, and a pretrained reward model, it learns to dynamically adjust temperature, instruction format, and reasoning steps based on individual question characteristics rather than applying fixed strategies.", "key_contribution": "The main innovation is automated, per-question adaptive configuration of three critical reasoning hyperparameters through a learned policy, replacing manual trial-and-error tuning with a principled RL-based approach that works across different LLMs and task types with minimal training data.", "novelty": "Unlike previous methods that apply static reasoning templates (CoT, ToT) or rely on implicit pattern matching, AdaReasoner addresses the configuration sensitivity problem by explicitly learning to select optimal hyperparameters for each question. It overcomes the limitations of fixed strategies that fail across diverse domains by using a factorized action space with structured exploration, enabling efficient few-shot learning. The approach is distinguished by its model-agnostic design, theoretical convergence guarantees, and ability to automatically discover task-specific reasoning patterns without exhaustive configuration search.", "ai_categories": ["Reasoning and Test-Time Compute", "Tool Use and Code Generation"]}, {"paper_id": "4046520", "score": "55", "title": "SiriuS: Self-improving Multi-agent Systems via Bootstrapped Reasoning", "authors": "Wanjia Zhao, Mert Yuksekgonul, Shirley Wu, James Zou", "session_type": "SD-1-5406", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/856c35f60eccff17ac8726de9ca5f7fbf9bcf3ee.pdf", "relevant_to_users": "5", "read_by_users": "7", "topics": "", "key_findings": "This paper demonstrates that multi-agent systems can achieve significant performance improvements through internal bootstrapping without requiring human-annotated training data. Agents can identify failure patterns in their reasoning processes and develop better strategies independently. The self-improvement mechanism scales effectively with system complexity, allowing larger agent networks to benefit proportionally from the bootstrapping process.", "description": "The paper introduces SiriuS, a bootstrapped reasoning framework that enables multi-agent AI systems to self-improve by analyzing their own past reasoning traces and outcomes. This approach allows agents to progressively enhance their collaborative decision-making quality without external guidance or supervision.", "key_contribution": "The primary innovation is a practical self-improvement mechanism for multi-agent systems that operates through bootstrapped reasoning, allowing agents to iteratively refine their collaborative problem-solving without external supervision or expensive human annotation, representing a step toward more autonomous AI systems.", "novelty": "Unlike previous approaches that depend on human-in-the-loop feedback or pre-trained models, SiriuS enables agents to learn directly from their own reasoning chains and outcomes. It addresses the limitation of requiring expensive human annotation in the improvement loop and tackles scalability concerns in multi-agent reinforcement learning by allowing systems to self-refine through internal analysis of failure patterns.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs"]}, {"paper_id": "4227413", "score": "55", "title": "Incentivizing LLMs to Self-Verify Their Answers", "authors": "Fuxiang Zhang, Jiacheng Xu, Chaojie Wang, Ce Cui, Yang Liu, Bo An", "session_type": "SD-2-313", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ea6649a9cfb1c181a137632923e930e4e14e6ad3.pdf", "relevant_to_users": "12", "read_by_users": "28", "topics": "", "key_findings": "The paper introduces a unified RL framework that trains LLMs to both generate and self-verify their answers, addressing the distribution mismatch problem between post-trained generators and external verifiers. Results show Self-Verification-Qwen-7B achieves 83.60% on MATH500 (vs 81.40% baseline) and Self-Verification-R1-1.5B reaches 87.00% (vs 80.00% base), with verification accuracy approaching GPT-4o and Claude-3.7-Sonnet. The approach enables effective test-time scaling without external verifiers while using 24-35% fewer tokens than problem-solving.", "description": "This paper addresses the problem that combining post-training and test-time scaling yields minimal benefits due to distribution discrepancies between post-trained generators and external reward models. It proposes a unified reinforcement learning framework that trains LLMs to simultaneously solve problems and verify their own answers, using a policy-aligned buffer mechanism and dynamic verification rewards.", "key_contribution": "The main contribution is a unified RL training approach that integrates answer generation and self-verification into a single process, using a policy-aligned buffer that maintains distribution alignment with current model outputs and dynamic verification rewards that emphasize challenging cases.", "novelty": "Unlike prior work that relies on external verifiers or supervised fine-tuning of generative verifiers, this approach eliminates distribution shift by training the model end-to-end for both generation and verification within the same RL process. Previous methods suffered from misalignment when external verifiers (trained on general data) evaluated outputs from post-trained models, while this unified framework maintains alignment throughout training. The policy-aligned online buffer and dynamic verification reward function are novel mechanisms that incentivize effective self-verification without requiring separate verification models.", "ai_categories": ["Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4202599", "score": "55", "title": "Time Travel is Cheating: Going Live with DeepFund for Real-Time Fund Investment Benchmarking", "authors": "Changlun Li, Yao SHI, Chen Wang, Qiqi Duan, Runke RUAN, Weijie Huang, Haonan Long, Lijun Huang, Nan Tang, Yuyu Luo", "session_type": "SD-2-2407", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.11065", "relevant_to_users": "0", "read_by_users": "2", "topics": "", "key_findings": "The paper introduces DeepFund, a real-time benchmarking system that addresses the critical 'time travel' problem in AI-based trading evaluation where models inadvertently use future data. It evaluates multiple state-of-the-art LLMs (GPT-4, Claude, Qwen, DeepSeek variants) on fund investment tasks, demonstrating that genuine real-time performance often differs substantially from backtest metrics when temporal information advantages are eliminated.", "description": "DeepFund is a benchmarking framework for evaluating multi-agent fund investment systems in realistic, live market conditions. Unlike traditional backtesting approaches, it ensures agents make investment decisions using only information available at decision time, preventing information leakage from future data.", "key_contribution": "The main contribution is a methodologically rigorous evaluation framework that prevents temporal inconsistencies in financial AI benchmarking by enforcing strict temporal constraints where agents can only access historical and present information, not future data.", "novelty": "This work addresses a widespread flaw in existing financial AI benchmarks that permit temporal information leakage. While previous approaches focused on portfolio optimization or individual trading strategies with complete historical data, DeepFund enforces authentic real-world constraints by ensuring prospective evaluation rather than retrospective analysis. This reveals that performance metrics under genuine real-time conditions often differ substantially from traditional backtest results.", "ai_categories": ["Agent Benchmarking and Evaluation", "Multi-Agent Systems and Collaboration", "Domain-Specific Applications"]}, {"paper_id": "4206980", "score": "55", "title": "MLZero: A Multi-Agent System for End-to-end Machine Learning Automation", "authors": "Haoyang Fang, Boran Han, Nick Erickson, Xiyuan Zhang, Su Zhou, Anirudh Dagar, Jiani Zhang, Ali Caner Turkmen, Cuixiong Hu, ..., George Karypis", "session_type": "SD-4-1503", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/55f28109c8ee532fe1c950142c23f6efd636a79e.pdf", "relevant_to_users": "6", "read_by_users": "10", "topics": "", "key_findings": "MLZero achieves state-of-the-art performance on AutoML benchmarks with 86% success rate on MLE-Bench Lite (6 gold medals) and 92% on Multimodal AutoML Agent Benchmark (+263.6% improvement over baselines). The system demonstrates true end-to-end automation across diverse data modalities with minimal human intervention, and maintains effectiveness even with compact 8B LLM models that outperform larger commercial alternatives. It addresses critical LLM limitations (hallucinated code, outdated API knowledge) through a novel dual memory architecture combining semantic and episodic memory.", "description": "MLZero is a multi-agent system powered by LLMs that automates end-to-end machine learning workflows across diverse data modalities. The system employs four specialized modules: a cognitive perception module that transforms raw multimodal inputs into perceptual context, semantic memory for ML library documentation, episodic memory for execution history tracking, and an iterative coding module for solution generation and refinement.", "key_contribution": "The main innovation is the dual memory architecture that addresses fundamental LLM weaknesses in AutoML: semantic memory enriches LLM knowledge with external ML library documentation, while episodic memory maintains execution history for targeted debugging. This enables superior performance compared to systems relying on simple conversation history, achieving true end-to-end automation where competitors require manual intervention.", "novelty": "Unlike existing AutoML systems (DS-Agent, AIDE) that require manual code execution or result extraction, MLZero achieves true end-to-end automation through its specialized multi-agent architecture. The dual memory system (semantic + episodic) specifically addresses LLM hallucination and outdated API knowledge‚Äîproblems that limit existing LLM-based AutoML approaches. The cognitive perception module's transformation of raw multimodal data into perceptual context represents a new approach to handling diverse data types, enabling the system to outperform competitors even when they are enhanced with external knowledge, demonstrating that superiority stems from overall system design rather than just data access.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Tool Use and Code Generation", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4046203", "score": "55", "title": "Training Language Models to Reason Efficiently", "authors": "Daman Arora, Andrea Zanette", "session_type": "SD-4-3600", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/c169ad531f568624e1f7af8211b9ff6b12391b63.pdf", "relevant_to_users": "25", "read_by_users": "52", "topics": "", "key_findings": "This paper demonstrates that reinforcement learning with length-penalized rewards can train reasoning models to reduce inference costs by 50% while maintaining >95% accuracy. The method requires only ~100 RL steps and uses per-problem adaptive token allocation, naturally spending more tokens on harder problems. On DeepSeek-R1-Distill-7B, they achieved 65% token reduction on GSM8K with only 1.7% accuracy drop, outperforming supervised fine-tuning and DPO baselines.", "description": "The paper presents a reinforcement learning approach to train large language models to generate shorter, more efficient chain-of-thought reasoning sequences while maintaining accuracy. The method uses policy gradient optimization with a tunable length penalty that adapts compute allocation based on problem difficulty, enabling controllable efficiency-accuracy trade-offs.", "key_contribution": "The main contribution is a simple yet effective RL-based training method that uses per-problem normalized length penalties to achieve difficulty-aware compute allocation. This allows models to automatically use fewer tokens on easy problems and preserve reasoning depth on hard problems, with a tunable hyperparameter for controlling the efficiency-accuracy trade-off.", "novelty": "Unlike previous work that applies uniform length constraints or uses offline optimization, this approach introduces per-prompt length normalization in the RL objective, enabling adaptive token allocation based on problem difficulty. This addresses the limitation of prior methods that either cannot control reasoning model output length reliably or penalize hard problems disproportionately. The method is also more practical, requiring only 100 RL steps with modest compute compared to expensive retraining or prompt-engineering approaches.", "ai_categories": ["Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs"]}, {"paper_id": "4259462", "score": "55", "title": "Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning", "authors": "Haolin Pan, Hongyu Lin, Haoran Luo, Yang Liu, Kaichun Yao, Libo Zhang, Mingjie Xing, Yanjun Wu", "session_type": "SD-5-410", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/7b8e4f95272ab1e9e3900b30ef9647d058b6b034.pdf", "relevant_to_users": "2", "read_by_users": "12", "topics": "", "key_findings": "Compiler-R1 introduces the first RL-driven framework that augments LLMs with tool interfaces for autonomous compiler optimization, achieving 8.46% average IR instruction count reduction compared to opt -Oz. The work addresses two critical gaps: creates a high-quality reasoning dataset of 19,603 samples with synergy pass pairs and optimal sequences, and enables effective LLM-environment interaction through tool-augmented agents. The GRPO-7B model achieves 96.71% task success rate while requiring fewer inference attempts than non-interactive baselines.", "description": "This paper presents Compiler-R1, the first reinforcement learning framework specifically designed to enhance Large Language Models for compiler auto-tuning through tool-augmented agents. The framework combines supervised fine-tuning for protocol initialization with RL-based policy optimization, enabling LLMs to autonomously explore compilation environments and discover effective optimization strategies.", "key_contribution": "The main contributions are: (1) a novel two-stage training pipeline combining SFT and RL that equips LLMs with tool interfaces (instrcount and find_best_pass_sequence) for interactive compiler optimization, and (2) a curated high-quality dataset of 19,603 diverse tuning scenarios with graph-guided optimal sequence selection and synergy pass pair identification.", "novelty": "Unlike prior compiler optimization approaches using heuristics, traditional ML, or LLMs without environmental feedback, Compiler-R1 enables adaptive strategy discovery through RL-driven policy learning combined with structured tool interaction. Previous LLM-based approaches lacked effective environment interaction and high-quality reasoning datasets, requiring extensive sampling without guaranteed optimization quality. Compiler-R1 addresses these limitations by training models to actively explore compilation environments through external tools and learn from outcome-based rewards, rather than merely predicting optimizations.", "ai_categories": ["Tool Use and Code Generation", "Reinforcement Learning for LLMs", "Domain-Specific Agents and Applications"]}, {"paper_id": "4433922", "score": "55", "title": "PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments", "authors": "Weijie Zhou, Xuantang Xiong, Yi Peng, Manli Tao, Chaoyang Zhao, Honghui Dong, Ming Tang, Jinqiao Wang", "session_type": "SD-6-3601", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/89b1018647a9f5681e3ad0ed184e80229e3997ee.pdf", "relevant_to_users": "2", "read_by_users": "21", "topics": "", "key_findings": "PhysVLM-AVR introduces Active Visual Reasoning (AVR), a paradigm shift from passive visual reasoning to interactive, multi-step information gathering in partially observable environments. The work reveals that existing MLLMs can detect incomplete information but fundamentally lack mechanisms for strategically acquiring and integrating new information through environmental interaction. The model achieves 90.5% information sufficiency judgment accuracy on CLEVR-AVR and demonstrates state-of-the-art performance across embodied reasoning tasks (OpenEQA, RoboVQA) and passive visual reasoning benchmarks (GeoMath, Geometry30K), even with only 1/20 of typical training data.", "description": "This paper introduces Active Visual Reasoning (AVR), extending multimodal large language models from static, fully observable settings to partially observable, interactive environments where agents must actively gather information through sequential physical actions. The work presents CLEVR-AVR benchmark, AVR-152k dataset with rich Chain-of-Thought annotations, and PhysVLM-AVR model that integrates perception, reasoning, and action for systematic information acquisition.", "key_contribution": "The main innovation is framing visual reasoning as a higher-order Markov Decision Process where agents iteratively identify uncertainties, predict information gain from actions, and select information-maximizing actions. This includes CLEVR-AVR benchmark measuring both reasoning correctness and information-gathering efficiency, and AVR-152k dataset with expert annotations detailing uncertainty identification, action-conditioned information gain prediction, and strategic action selection.", "novelty": "Unlike prior work that assumes complete observability or focuses on single-sequence embodied QA, PhysVLM-AVR unifies active perception with reasoning needs through strategic multi-step interaction. The key innovation is explicit supervision for information-gathering strategy via Chain-of-Thought annotations that model higher-order decision processes, rather than end-to-end learning. This addresses the critical gap that existing embodied MLLMs can recognize missing information but lack systematic mechanisms to acquire and integrate it through environmental interaction.", "ai_categories": ["Vision-Language-Action Models", "Reasoning and Test-Time Compute", "Planning and Decision Making"]}, {"paper_id": "4250143", "score": "53", "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?", "authors": "Zihan Zheng, Zerui Cheng, Zeyu Shen, Shang Zhou, Kaiyuan Liu, Hansen He, Dongruixuan Li, Stanley Wei, Hangyi Hao, ..., Saining Xie", "session_type": "SD-2-2408", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.11928", "relevant_to_users": "4", "read_by_users": "6", "topics": "", "key_findings": "LiveCodeBench Pro reveals that frontier LLMs achieve only 53% pass@1 on medium-difficulty problems and 0% on hard problems without external tools. LLMs excel at implementation-heavy tasks but struggle with nuanced algorithmic reasoning and complex case analysis, often generating confidently incorrect justifications. The study shows that high performance is driven by implementation precision and tool augmentation rather than superior reasoning. Olympiad medalists' line-by-line analysis reveals that LLMs prioritize code structure over conceptual correctness, implementing sophisticated but fundamentally flawed algorithms, while humans excel at identifying core insights first.", "description": "This paper introduces LiveCodeBench Pro, a continuously-updated competitive programming benchmark with problems from Codeforces, ICPC, and IOI, evaluated by Olympic medalists. The benchmark provides expert human annotations and detailed failure analysis to assess LLM capabilities on algorithmic reasoning tasks across multiple difficulty levels.", "key_contribution": "The main innovation is incorporating expert human evaluation from olympiad medalists who provide algorithmic annotations and conduct line-by-line analysis of failed submissions, combined with a continuously-updated benchmark to minimize data contamination. This enables fine-grained diagnosis of LLM failure modes that automated test cases cannot capture.", "novelty": "Unlike prior benchmarks (HumanEval, CodeContests) that rely solely on automated test-case evaluation and static datasets, LiveCodeBench Pro combines continuous updates to prevent contamination with expert-level human evaluation from competitive programming champions. This addresses the limitation that automated testing misses nuances in algorithmic correctness and cannot diagnose why models fail. The olympiad medalists provide unique insights into the conceptual vs. implementation gaps between LLMs and human experts, revealing that models lack fundamental algorithmic insight despite strong implementation skills.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4214984", "score": "53", "title": "RvLLM: LLM Runtime Verification with Domain Knowledge", "authors": "Yedi Zhang, Sun Yi Emma, Annabelle Lee Jia En, Jin Song Dong", "session_type": "SD-4-1408", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/e99cb11cda9beb3092445ecb739b48abb0369ac9.pdf", "relevant_to_users": "2", "read_by_users": "3", "topics": "", "key_findings": "RvLLM introduces a runtime verification framework that increases LLM reliability by 15.7-50.2% (TPR) in violation detection tasks while maintaining balanced precision. The framework combines a novel specification language (ESL) that allows domain experts to encode constraints in natural language + predicate logic, with a four-stage verification process (interpretation, normalization, forward chaining, query generation) that separates linguistic understanding from logical reasoning. Key innovation is the hybrid architecture: LLMs handle natural language interpretation while symbolic engines perform formal logical inference, addressing the scaling limitations of traditional neural network verification methods.", "description": "RvLLM is a runtime verification framework for validating LLM outputs against domain-specific constraints. It introduces ESL, a specification language combining natural language with predicate logic, enabling domain experts to define verification rules. The framework operates through interpretation and reasoning stages, using forward chaining to detect logical inconsistencies and issuing follow-up queries to verify derived knowledge against LLM responses.", "key_contribution": "The main contribution is a practical, lightweight runtime verification system that bridges formal verification and LLM testing by enabling domain experts (without programming expertise) to encode verification constraints. The hybrid reasoning architecture delegates linguistic interpretation to LLMs while reserving logical inference for symbolic engines, achieving rigorous verification that scales to modern LLMs.", "novelty": "Unlike prior neural network verification approaches limited to simpler architectures, RvLLM specifically targets LLM behavioral verification at scale. Unlike hallucination detection methods that rely on output stability or sampling, it leverages axiomatic domain knowledge through formal logic. The innovation lies in treating LLMs as perception agents for natural language understanding while using symbolic reasoning engines for logical consistency checking‚Äîaddressing the fundamental challenge that traditional formal verification doesn't scale to LLMs, while pure LLM-based verification lacks rigor.", "ai_categories": ["Agent Safety and Security", "Tool Use and Code Generation"]}, {"paper_id": "4208823", "score": "53", "title": "BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems", "authors": "Andy Zhang, Joey Ji, Celeste Menders, Riya Dulepet, Thomas Qin, Ron Wang, Junrong Wu, Kyleen Liao, Jiliang Li, ..., Percy Liang", "session_type": "SD-5-2302", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.15216", "relevant_to_users": "3", "read_by_users": "6", "topics": "", "key_findings": "BountyBench demonstrates a significant offense-defense imbalance in current AI agents: commercial agents (Claude Code, OpenAI Codex CLI) achieve 87.5-90% success on patching vulnerabilities but only 32.5-57.5% on exploitation, while custom agents show more balanced capabilities (37.5-67.5% exploit, 35-60% patch). Agents completed $47,821 worth of patch tasks and $5,855 of detect tasks across 40 real bug bounties spanning 9 OWASP Top 10 vulnerabilities. The benchmark reveals safety mechanisms are inconsistent‚ÄîOpenAI Codex CLI refused 11.2% of tasks while other agents showed no refusals. This work brings the first economically-grounded framework for evaluating both offensive and defensive AI agent capabilities on evolving real-world systems with measurable dollar impact.", "description": "BountyBench is the first benchmark framework to evaluate AI agents on both offensive and defensive cybersecurity capabilities using 25 real-world open-source systems with 40 actual bug bounties (valued $10-$30,485). It defines three task types spanning the vulnerability lifecycle: Detect (finding new vulnerabilities), Exploit (weaponizing known vulnerabilities), and Patch (fixing vulnerabilities), with information-based difficulty modulation to differentiate agent capabilities.", "key_contribution": "The main innovation is creating an economically-grounded evaluation framework that captures the complete vulnerability lifecycle with real bug bounties on evolving production codebases. The benchmark introduces information-based difficulty modulation to interpolate between zero-day discovery and specific vulnerability exploitation, enabling meaningful performance differentiation while providing tangible dollar-value impact metrics rather than abstract success rates.", "novelty": "Unlike prior cybersecurity benchmarks that use synthetic or static environments, BountyBench evaluates agents on real bug bounties from production systems with actual monetary values, addressing the lack of economic grounding and real-world complexity. Previous work focused narrowly on either offensive or defensive tasks in isolation, while BountyBench comprehensively evaluates both capabilities across the full vulnerability lifecycle (detect, exploit, patch). The information-based difficulty modulation strategy is novel, allowing controlled evaluation across difficulty levels from zero-day discovery to targeted exploitation, which previous benchmarks couldn't achieve.", "ai_categories": ["Agent Benchmarking and Evaluation", "Agent Safety and Security", "Tool Use and Code Generation"]}, {"paper_id": "4430995", "score": "53", "title": "Lookahead Routing for Large Language Models", "authors": "Canbin Huang, Tianyuan Shi, Yuhua Zhu, Ruijun Chen, Xiaojun Quan", "session_type": "SD-6-3603", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/552fece3eff509f39e0f54fafd8aafc36248c8a8.pdf", "relevant_to_users": "0", "read_by_users": "10", "topics": "", "key_findings": "This paper introduces Lookahead Routing, achieving a 7.7% average performance gain over state-of-the-art routing methods across seven benchmarks. The key innovation is predicting latent representations of potential model outputs to inform routing decisions, rather than relying solely on input queries. This approach captures semantic intent that emerges during generation while avoiding the computational cost of full inference from all candidate models. The method demonstrates 6.3√ó data efficiency and particularly excels on open-ended tasks like instruction-following.", "description": "The paper proposes Lookahead Routing, a framework that routes queries to the most appropriate LLM in multi-model systems by predicting latent representations of how different models would respond. Two variants (CLM-based for sequence-level and MLM-based for token-level prediction) use joint training with routing classification and response reconstruction objectives to learn response-aware representations without requiring full inference.", "key_contribution": "The main contribution is a response-aware routing mechanism that predicts compact latent representations of potential model outputs to guide routing decisions, bridging the gap between query-only routing (which misses semantic nuances) and full inference (which is computationally expensive). The MLM variant embeds all candidate responses in a shared semantic space, enabling comparative context crucial for open-ended tasks.", "novelty": "Unlike existing routing methods that treat routing as a query-only classification problem (e.g., RouteLLM, FrugalGPT), Lookahead addresses the fundamental limitation that query-only approaches miss semantic intent and output quality signals that emerge during generation. It introduces a middle path: predicting response representations without full autoregressive decoding, using curriculum masking strategies and joint spatial embeddings. This enables the router to evaluate comparative response quality across models without the computational overhead of generating full responses from all candidates, solving the response-agnosticism problem that causes poor generalization on ambiguous or complex queries.", "ai_categories": ["Model Efficiency and Optimization", "Reasoning and Test-Time Compute"]}, {"paper_id": "4251406", "score": "53", "title": "DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents", "authors": "Hao Li, Xiaogeng Liu, CHIU Hung Chun, Dianqi Li, Ning Zhang, Chaowei Xiao", "session_type": "SD-6-1101", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/7e05f767d463d43be3b045378b14be5760ea2fc1.pdf", "relevant_to_users": "3", "read_by_users": "5", "topics": "", "key_findings": "DRIFT reduces attack success rate from 30.7% to 1.3% on AgentDojo while improving utility by 12.5% over static defenses. The framework demonstrates that dynamic, privilege-based validation combined with memory stream isolation can balance security and utility in LLM agents. Results generalize across five different LLMs, with fine-tuned models achieving 0% attack success rate.", "description": "DRIFT is a system-level defense framework for protecting LLM agents against prompt injection attacks. It uses three components: a Secure Planner that establishes initial security policies, a Dynamic Validator that adapts constraints during execution based on function privileges, and an Injection Isolator that removes malicious instructions from the agent's memory stream.", "key_contribution": "The main innovation is dynamic privilege-based validation that adapts security policies at runtime while maintaining task utility, combined with active memory stream isolation that continuously detects and masks injected instructions in accumulated context to prevent long-term exploitation.", "novelty": "Unlike static defenses (e.g., CaMeL) that rigidly constrain agent behavior and sacrifice utility, DRIFT dynamically adjusts policies based on function categories (Read/Write/Execute) and user intent alignment. It addresses the previously unsolved problem of memory pollution by actively sanitizing the agent's context stream, whereas prior isolation approaches (e.g., IsolateGPT) only restricted cross-application data flow. This enables both strong security (1.3% ASR) and high utility (20.1% improvement over static baselines).", "ai_categories": ["Agent Safety and Security", "Tool Use and Code Generation"]}, {"paper_id": "4433998", "score": "52", "title": "Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency", "authors": "Yukun Jiang, Mingjie Li, Michael Backes, Yang Zhang", "session_type": "SD-1-1104", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/7e13bf73f0b1b45f44c038b630279494b0220174.pdf", "relevant_to_users": "2", "read_by_users": "11", "topics": "", "key_findings": "The paper introduces JAIL-CON, a novel jailbreak attack that exploits task concurrency in LLMs by interleaving harmful requests with benign ones at the word level. The key finding is that LLMs maintain strong utility when handling concurrent tasks, but combining harmful tasks with benign ones significantly reduces detection by safety guardrails. The attack demonstrates superior jailbreak capabilities compared to existing sequential attacks and exhibits enhanced stealthiness across widely-used LLM implementations.", "description": "This paper presents a word-level method that enables task concurrency in LLMs, where adjacent words encode divergent intents. The work demonstrates that while LLMs can effectively process concurrent tasks, this capability creates a vulnerability where harmful requests embedded alongside benign tasks bypass safety mechanisms that primarily focus on sequential scenarios.", "key_contribution": "The main contribution is JAIL-CON, an iterative attack framework that exploits word-level task concurrency to bypass LLM safety guardrails. This represents a fundamental shift from sequential jailbreak methods by encoding harmful and benign intents in adjacent tokens within single prompts.", "novelty": "Unlike previous jailbreak attacks that follow sequential logic and process tasks individually, this work exploits the gap between LLM design optimized for concurrent task handling and safety mechanisms designed for sequential scenarios. The novelty lies in the word-level concurrency approach that allows harmful content to coexist with legitimate requests within single outputs, addressing a previously overlooked vulnerability where guardrails fail to detect threats in concurrent contexts. This fundamentally challenges existing safety architectures that assume sequential intent processing.", "ai_categories": ["Agent Safety and Security", "Reasoning and Test-Time Compute"]}, {"paper_id": "4177289", "score": "52", "title": "Iterative Tool Usage Exploration for Multimodal Agents via Step-wise Preference Tuning", "authors": "Pengxiang Li, Zhi Gao, Bofei Zhang, Yapeng Mi, Xiaojian Ma, Chenrui Shi, Tao Yuan, Yuwei Wu, Yunde Jia, ..., Qing Li", "session_type": "SD-2-4818", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/a3ae43bcdfe712b2361e4ab5254bbce2bcc0dd95.pdf", "relevant_to_users": "0", "read_by_users": "11", "topics": "", "key_findings": "The paper introduces SPORT, a framework that enables multimodal agents to learn tool usage without human-annotated data through iterative self-exploration and step-wise preference optimization. Key results show 6.41% improvement on GTA benchmark and 3.64% on GAIA, while achieving 4.75√ó speedup and 32.7% cost reduction compared to baselines. The approach constructs 16K step-level preference pairs from autonomous exploration, with verifier judgments showing 82% agreement with human preferences.", "description": "SPORT addresses the bottleneck of expensive human annotations in multimodal agent training by autonomously synthesizing tasks, exploring tool-usage trajectories through step-by-step sampling, and using AI-based verifier feedback to construct preference data for tuning the agent controller. The framework iteratively refines agent capabilities through four components: task synthesis, step sampling, step verification, and preference tuning using Direct Preference Optimization (DPO).", "key_contribution": "The main innovation is enabling multimodal agents to learn effective tool-usage strategies through completely autonomous exploration without pre-collected human annotations, using step-level preference optimization that can learn from both successful and failed exploration attempts. This eliminates annotation bottlenecks while maintaining training effectiveness through online, in-distribution preference data construction.", "novelty": "Unlike prior work requiring ground-truth trajectories or distillation from GPT-4o, SPORT constructs preferences from failed exploration attempts, maximizing data utilization. It advances beyond trajectory-level RL approaches by using fine-grained step-level verification with in-context LLM judgment instead of trained reward models. The online exploration scheme ensures preference data stays aligned with current agent capabilities, addressing distribution mismatch issues in static preference-data approaches and enabling scalable improvement without human labor or external model dependencies.", "ai_categories": ["Tool Use and Code Generation", "Reinforcement Learning for LLMs", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4400137", "score": "52", "title": "DyFlow: Dynamic Workflow Framework for Agentic Reasoning", "authors": "Yanbo Wang, Zixiang Xu, Yue Huang, Xiangqi Wang, Zirui Song, Lang Gao, Chenxi Wang, Xiangru Tang, Yue Zhao, ..., Xiuying Chen", "session_type": "SD-3-5207", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/705f64f765b412ab6e17c0dc9c9146763c3e63fe.pdf", "relevant_to_users": "0", "read_by_users": "1", "topics": "", "key_findings": "DyFlow introduces execution-adaptive planning that dynamically adjusts workflows based on intermediate feedback, operating at the subgoal level rather than individual actions. Evaluated across five diverse domains (social reasoning, biomedical tasks, mathematics, code generation), it achieves 61.45% average accuracy vs. 57.74% for baselines, with particularly strong zero-shot generalization (e.g., 17.18% vs. 13.36% on SocialMaze). The framework demonstrates robust cross-task, cross-executor, and cross-designer generalization, with a lightweight 14B DyPlanner matching larger proprietary models.", "description": "DyFlow is a dynamic workflow generation framework for LLM-based agentic reasoning that adaptively constructs and adjusts reasoning procedures in real-time based on task requirements and intermediate execution feedback. It consists of a designer that generates stage subgraphs (directed graphs specifying operator sequences with context-aware parameterization) and an executor that implements these plans using dynamic operators.", "key_contribution": "The main innovation is execution-adaptive planning through a designer-executor architecture that performs iterative replanning at the subgoal level during execution, trained via two-phase optimization (supervised fine-tuning followed by offline preference optimization with KTO). This enables substantial strategic shifts in reasoning trajectories based on real-time feedback, rather than following predetermined static workflows.", "novelty": "Unlike prior methods (AFlow, ADAS) that optimize workflows offline before execution or approaches (ReAct, DyLAN) that only adapt individual actions/roles, DyFlow integrates planning directly into execution with continuous replanning at the subgoal level. It addresses key limitations of static workflows that proceed rigidly without feedback and dataset-specific methods with limited generalization. The stage subgraph formulation with fine-grained parameterization and global memory buffer enables flexible information reuse and genuine adaptive reasoning trajectories that evolve with task progress.", "ai_categories": ["Reasoning and Test-Time Compute", "Multi-Agent Systems and Collaboration", "Tool Use and Code Generation"]}, {"paper_id": "4221773", "score": "52", "title": "MathArena: Evaluating LLMs on Uncontaminated Math Competitions", "authors": "Mislav Balunovic, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovi√Ñ¬á, Martin Vechev", "session_type": "SD-3-2508", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.23281", "relevant_to_users": "7", "read_by_users": "10", "topics": "", "key_findings": "MathArena introduces a benchmark of uncontaminated 2025 mathematics competition problems (AIME, IMO, USAMO, Putnam, HMMT) that weren't available during LLM training. Evaluation reveals significant performance gaps across state-of-the-art models, with reasoning-focused models like GPT-o1 and DeepSeek-R1 achieving competitive but imperfect scores on Olympiad-level mathematics. The benchmark provides clean evidence that existing models still struggle with complex mathematical reasoning when training data contamination is eliminated.", "description": "MathArena is a benchmark designed to evaluate LLMs on mathematics competition problems from 2025, addressing the critical issue that most existing benchmarks risk contamination from training data. The benchmark systematically collects fresh problems from multiple prestigious competitions to enable rigorous assessment of true mathematical reasoning capabilities without data leakage.", "key_contribution": "The primary innovation is creating an uncontaminated evaluation benchmark using freshly-released 2025 competition problems, providing a clean measure of LLM mathematical reasoning without training data leakage that plagues existing benchmarks like MATH and GSM8K.", "novelty": "Unlike established benchmarks (MATH, GSM8K) potentially compromised by inclusion in training data, MathArena exclusively uses 2025 competition problems that are guaranteed uncontaminated. It evaluates emerging reasoning-focused models (o1, DeepSeek-R1) not previously benchmarked together on fresh data, and combines multiple prestigious competition sources (AIME, IMO, USAMO, Putnam, HMMT) for diverse mathematical domain coverage. This addresses the fundamental challenge of accurately measuring true reasoning capabilities versus memorization.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4216036", "score": "51", "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond", "authors": "Junteng Liu, Yuanxiang Fan, Jiang Zhuo, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, ..., Junxian He", "session_type": "SD-2-1814", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/b5b93e7b533135e2ad1d72d22e3a481324c2a73d.pdf", "relevant_to_users": "4", "read_by_users": "7", "topics": "", "key_findings": "SynLogic introduces a scalable framework for generating verifiable logical reasoning data across 35 diverse tasks with adjustable difficulty. The 32B model trained on SynLogic surpasses DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH and achieves 65.0% on KOR-Bench. Critically, mixing SynLogic data with mathematical and coding tasks improves training efficiency and enables strong generalization - the 7B model showed 4.4x improvement on AIME 2024, demonstrating that logical reasoning data enhances performance across multiple reasoning domains.", "description": "SynLogic is a data synthesis framework that generates verifiable training data for 35 diverse logical reasoning tasks (including Sudoku, Game of 24, ciphers, and puzzles) with controllable difficulty parameters. The framework enables reinforcement learning with binary verifiable rewards and produces 33k hard and 16k easy training examples suitable for curriculum learning.", "key_contribution": "The main contribution is a comprehensive open-source framework for synthesizing diverse logical reasoning data at scale with task-specific generators and verifiers for all 35 tasks, enabling fine-grained difficulty control and verifiable rewards for RL training. This addresses the critical gap where existing logical reasoning benchmarks do not open-source their data generation methods and provide limited trainable data.", "novelty": "Unlike prior work focused primarily on mathematics and coding domains, SynLogic provides the first comprehensive synthetic logic dataset spanning 35 diverse tasks with open-source generation methods and verifiers. It addresses the limitation that nearly all evaluation benchmarks lack data generation pipelines and trainable data. The framework introduces fine-grained difficulty control through adjustable parameters and dual-bound difficulty calibration, capabilities absent in comparable datasets, while demonstrating that logical reasoning training data has complementary benefits for mathematical and coding domains.", "ai_categories": ["Reasoning and Test-Time Compute", "Mathematical and Logical Reasoning", "Reinforcement Learning for LLMs"]}, {"paper_id": "4218336", "score": "51", "title": "Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies", "authors": "Felix Chalumeau, Daniel Rajaonarivonivelomanantsoa, Ruan John de Kock, Juan Claude Formanek, Sasha Abramowitz, Omayma Mahjoub, Wiem Khlifi, Simon Verster Du Toit, Louay Ben Nessir, ..., Arnu Pretorius", "session_type": "SD-3-311", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/5d59ea40926886752f5ab100ab83a383587e3e1e.pdf", "relevant_to_users": "10", "read_by_users": "24", "topics": "", "key_findings": "This paper demonstrates that inference-time strategies can break through performance ceilings in complex multi-agent RL, achieving up to 126% improvement (45% on average) over state-of-the-art across 17 tasks using only seconds of additional compute. The largest study on inference strategies for complex RL (60k+ experiments), it shows that methods like stochastic sampling, tree search (simulation-guided beam search), online fine-tuning, and diversity-based approaches (COMPASS with CMA-ES) can exploit test-time compute budgets to overcome limitations of zero-shot inference. Key insight: complex combinatorial problems hit training-based performance ceilings, but inference-time search through the solution space yields substantial gains with practical wall-clock times.", "description": "This paper addresses the performance ceiling problem in multi-agent reinforcement learning where even fully-trained models plateau in complex scenarios. It systematically evaluates inference-time strategies (stochastic sampling, tree search, online fine-tuning, and diversity-based methods) across multi-robot warehouses, StarCraft, and circuit routing tasks, demonstrating that allocating compute budget at execution time dramatically improves performance beyond what training alone achieves.", "key_contribution": "The paper establishes that inference-time compute scaling is crucial for complex RL problems, showing that methods like COMPASS (continuous latent space search with CMA-ES) and simulation-guided beam search can achieve 45-126% improvements over convergent training approaches. It provides the first comprehensive empirical study (60k+ experiments) mapping performance across variable time/compute budgets in multi-agent settings.", "novelty": "Unlike prior work focusing on training improvements or single-agent domains, this is the first large-scale systematic study of inference strategies specifically for complex multi-agent RL with combinatorial structure. It addresses the gap between LLM test-time compute success and RL domains by demonstrating that performance ceilings in collaborative MARL can be broken through execution-time search rather than extended training. The work introduces practical inference-time methods that scale with available compute and shows that seconds of inference can outperform months of additional training, establishing a new paradigm for deploying RL in real-world applications.", "ai_categories": ["Reasoning and Test-Time Compute", "Multi-Agent Systems and Collaboration", "Reinforcement Learning for LLMs"]}, {"paper_id": "4213056", "score": "51", "title": "L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models", "authors": "Xiaohao Liu, Xiaobo Xia, Weixiang Zhao, Manyi Zhang, Xianzhi Yu, Xiu Su, Shuo Yang, See-Kiong Ng, Tat-Seng Chua", "session_type": "SD-4-1911", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/03edf649a9f42cdf6d921cb59599e7130120540a.pdf", "relevant_to_users": "3", "read_by_users": "10", "topics": "", "key_findings": "L-MTP introduces a leap-based multi-token prediction strategy that predicts non-adjacent tokens (e.g., positions 1,3,5,7 instead of 1,2,3,4), achieving both improved model performance and faster inference. The method demonstrates 1.3√ó speedup over existing approaches while maintaining or exceeding accuracy, with particularly strong results on mathematical reasoning tasks (17.20 vs 10.00 for next-token prediction on Gemma 3-12B). The paper provides theoretical analysis proving that under certain conditions (Œ≥=O(1/n¬≤)), L-MTP achieves superior expected acceptance length compared to sequential prediction strategies, and validates these predictions empirically across model scales.", "description": "L-MTP extends multi-token prediction by introducing a leap mechanism that predicts tokens at non-consecutive positions with configurable stride (e.g., every k-th token) rather than adjacent tokens. The method uses a two-stage training approach and a 'looking backward' inference strategy that retrieves previously predicted intermediate tokens from cache, enabling simultaneous improvements in both model accuracy and inference speed.", "key_contribution": "The main innovation is the leap-based prediction pattern that breaks from adjacent token prediction, combined with a specialized 'looking backward' decoding strategy that reuses cached predictions to fill gaps without additional computation. This enables capturing longer-range dependencies while achieving up to 4√ó faster inference when combined with speculative decoding.", "novelty": "Unlike prior multi-token prediction (MTP) work that predicts consecutive adjacent tokens, L-MTP predicts non-sequential tokens at configurable intervals, addressing the 'myopia' problem where prediction accuracy degrades rapidly for distant positions in standard MTP. The approach introduces theoretical foundations (Attenuation and Consistency assumptions) explaining when and why leap strategies outperform adjacent prediction, and develops a unique backward-looking decoding mechanism unavailable in standard MTP that strategically reuses overlapping context. This addresses fundamental limitations of both next-token prediction's sequential bottleneck and standard MTP's confined contextual range.", "ai_categories": ["Model Efficiency and Optimization", "Reasoning and Test-Time Compute"]}, {"paper_id": "4280020", "score": "51", "title": "AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench", "authors": "Edan Toledo, Karen Hambardzumyan, Martin Josifoski, RISHI HAZRA, Nicolas Baldwin, Alexis Audran-Reiss, Michael Kuchnik, Despoina Magka, Minqi Jiang, ..., Yoram Bachrach", "session_type": "SD-4-2808", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/a22e34894b1de174131d79168e9a32bf5fefd3a5.pdf", "relevant_to_users": "3", "read_by_users": "14", "topics": "", "key_findings": "This paper achieves state-of-the-art results on MLE-bench, improving medal success rates from 39.6% to 47.7% by systematically studying the interaction between search policies (Greedy, MCTS, Evolutionary) and operator sets. The research reveals that operators‚Äînot search strategy‚Äîare the primary performance bottleneck with existing systems, and identifies a significant generalization gap (9-13 percentage points) where agents overfit to validation scores during search. The work introduces AIRA-dojo, a scalable infrastructure for ML research agents, and demonstrates that enhanced operators with prompt-adaptive complexity and scoped memory enable advanced search methods to provide meaningful improvements.", "description": "This paper formalizes AI research agents as search algorithms that navigate solution spaces to automate machine learning engineering tasks on Kaggle competitions. The work systematically studies how different search policies (greedy, MCTS, evolutionary) interact with operator sets (draft, debug, improve, crossover) to solve challenging ML problems in the MLE-bench benchmark.", "key_contribution": "The paper's main contribution is a unified framework that disentangles and systematically analyzes the components of AI research agents (operators, search policies, fitness functions), revealing that operator design is the critical bottleneck. It introduces AIRA‚Äîan enhanced operator set with dynamic complexity cuing and scoped memory‚Äîalong with AIRA-dojo, a scalable HPC infrastructure for evaluating research agents.", "novelty": "Unlike prior work that conflated operator design, search strategy, and infrastructure, this paper systematically separates and studies each component's contribution through rigorous ablations. It addresses the scalability limitations of previous benchmarks by using Apptainer for HPC environments instead of Docker, enabling thousands of parallel experiments. The work identifies and quantifies a previously understudied generalization gap in search-guided ML engineering, showing that validation-based search guidance causes systematic overfitting, and demonstrates that improved operators unlock the benefits of sophisticated search algorithms that were previously ineffective.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4068345", "score": "51", "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines", "authors": "Xeron Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, ..., Ge Zhang", "session_type": "SD-4-5103", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2502.14739", "relevant_to_users": "4", "read_by_users": "8", "topics": "", "key_findings": "SuperGPQA reveals significant gaps in LLM capabilities across specialized knowledge domains, with the best model (DeepSeek-R1) achieving only 61.82% accuracy, highlighting the considerable gap between current models and AGI. The benchmark covers 285 graduate-level disciplines including previously underexplored areas like light industry, agriculture, and service-oriented fields. A novel Human-LLM collaborative filtering mechanism successfully eliminates trivial and ambiguous questions through iterative refinement with 80+ expert annotators, providing methodological guidance for future large-scale benchmark construction.", "description": "SuperGPQA is a comprehensive benchmark that evaluates LLM performance on graduate-level knowledge and reasoning across 285 disciplines, vastly exceeding the scope of existing benchmarks like GPQA and MMLU. The dataset contains 26.5k questions developed through an interactive Human-LLM collaborative system with over 80 expert annotators.", "key_contribution": "The main contribution is creating the most comprehensive graduate-level LLM evaluation benchmark spanning 285 disciplines (far beyond existing benchmarks' scope) and introducing a novel Human-LLM collaborative filtering mechanism that iteratively refines questions based on both LLM responses and expert feedback to ensure quality and eliminate trivial or ambiguous items.", "novelty": "Unlike existing benchmarks (GPQA, MMLU) that focus on mainstream academic fields, SuperGPQA addresses the critical gap in evaluating LLM performance across specialized disciplines including light industry, agriculture, and service-oriented domains that were previously inadequately evaluated. The work introduces a scalable Human-LLM collaborative filtering approach that combines expert annotation with automated quality control, moving beyond single-pass annotation methods. This enables systematic assessment of the full spectrum of human specialized knowledge rather than just well-studied academic paths, quantifying the capability gap toward AGI across diverse domains.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4429070", "score": "51", "title": "CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections", "authors": "Keuntae Kim, Eunhye Jeong, Sehyeon Lee, Seohee Yoon, Yong Suk Choi", "session_type": "SD-6-3709", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/bcb703c11665128fb39f34dfd53237d4a3431b28.pdf", "relevant_to_users": "1", "read_by_users": "2", "topics": "", "key_findings": "CLAWS introduces an automated method for classifying LLM-generated mathematical solutions into typical, creative, and hallucinated categories by analyzing attention patterns across five prompt sections (Guideline, Problem, Solutions, Instruction, Response). The method outperforms five existing baselines across multiple 7-8B parameter models on 4,545 math problems, achieving superior F1-macro scores and AUROC for both creativity detection and hallucination detection. Key insight: hallucinated solutions over-focus on Guideline/Problem sections, creative solutions show balanced attention distribution, and typical solutions concentrate on Solution/Instruction/Response sections.", "description": "This paper addresses the overlooked challenge of assessing creativity in LLM-generated mathematical solutions. CLAWS leverages decoder attention weights across semantically-divided prompt sections to automatically classify solutions as typical, creative, or hallucinated without requiring human evaluation or multiple model generations.", "key_contribution": "The main innovation is using section-wise attention patterns as a multi-dimensional feature vector for three-way classification of solution quality and creativity, replacing scalar uncertainty measures and eliminating human evaluation. This enables efficient, interpretable creativity detection with computational overhead limited to sum and average operations during generation.", "novelty": "Unlike existing methods that use global scalar metrics (perplexity, entropy) for binary hallucination detection, CLAWS performs three-way classification by analyzing which prompt sections the model attends to during generation. It addresses limitations of entropy-based methods that struggle with subtle class differences and threshold-based approaches that lack flexibility. The key novelty is the hypothesis that creativity correlates with specific attention distribution patterns across structured prompt sections, enabling automated creativity assessment in reasoning tasks‚Äîa capability absent in prior work focused solely on hallucination detection.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4202354", "score": "50", "title": "PoE-World: Compositional World Modeling with Products of Programmatic Experts", "authors": "Wasu Top Piriyakulkij, Yichao Liang, Hao Tang, Adrian Weller, Marta Kryven, Kevin Ellis", "session_type": "SD-4-508", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/1983f2216c20adc421975e0092eb41f2ac1d93fa.pdf", "relevant_to_users": "14", "read_by_users": "39", "topics": "", "key_findings": "PoE-World introduces a novel approach to world modeling that represents world dynamics as an exponentially-weighted product of hundreds of small programmatic experts synthesized by LLMs, rather than a single monolithic program. It achieves superior data efficiency on complex Atari games (Pong and Montezuma's Revenge), requiring <1000 demonstration frames to match performance that takes PPO over 1 million steps. The method generates 4000+ line programs for Montezuma's Revenge with 0.93 object attribute prediction accuracy, demonstrates zero-shot generalization to novel game levels, and enables both model-based planning and simulation-based policy pretraining that accelerates real-world learning 5x.", "description": "This paper presents PoE-World, a compositional world modeling approach that learns world dynamics as a product of multiple specialized programmatic experts synthesized by LLMs. Unlike neural or monolithic symbolic approaches, it decomposes complex environments into hundreds of interpretable programs that capture nuanced causal mechanics, enabling data-efficient learning and zero-shot generalization in complex domains like Atari games.", "key_contribution": "The main innovation is decomposing world models into a product of hundreds of specialized programmatic experts learned via LLM-based synthesis, combined with gradient-based weight optimization and hard physical constraints. This compositional factorization enables scaling symbolic world modeling to complex, partially observable, stochastic environments previously intractable for programmatic approaches.", "novelty": "Unlike prior work like WorldCoder that learns single monolithic programs (struggling to scale beyond simple grid-worlds), PoE-World applies the cognitive science principle of 'mind as a community of experts' to decompose world modeling into hundreds of specialized programs. This addresses the brittleness of single large programs in complex environments while maintaining interpretability advantages over neural approaches. The key novelty is showing that compositional factorization of symbolic models can handle complex Atari games with thousands of lines of synthesized code, achieving both strong prediction accuracy and zero-shot generalization that neural methods fail to provide.", "ai_categories": ["World Models and Planning", "Reasoning and Test-Time Compute", "Tool Use and Code Generation"]}, {"paper_id": "4208884", "score": "49", "title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents", "authors": "Hyungjoo Chae, Sunghwan Kim, Junhee Cho, Seungone Kim, Seungjun Moon, Gyeom Hwangbo, Dongha Lim, Minjin Kim, Yeonjun Hwang, ..., Jinyoung Yeo", "session_type": "SD-1-5402", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/db46564195dad40b9b174514d7d03b0336d2a8eb.pdf", "relevant_to_users": "3", "read_by_users": "6", "topics": "", "key_findings": "Web-Shepherd is the first specialized process reward model (PRM) for web agents, achieving 85% step-level accuracy on WebRewardBench compared to 5% for GPT-4o-mini with prompting. It delivers 10.9 points better performance on WebArena-lite at 10x lower cost than GPT-4o-mini. The checklist-based decomposition approach enables interpretable step-level evaluation, making web agent training and inference-time search practical and cost-effective. The work introduces WebPRM Collection (40K step-level preference pairs) and WebRewardBench (first meta-evaluation benchmark for PRMs).", "description": "Web-Shepherd introduces the first process reward model specifically designed for web agents, using a checklist-based approach to decompose user instructions into interpretable subgoals and provide step-level evaluation of agent trajectories. Unlike outcome reward models that only assess final success, it enables action-level decision making through structured feedback at each step of web navigation tasks.", "key_contribution": "The main innovation is the checklist-based generative reward modeling approach that explicitly decomposes high-level instructions into clear subgoals and uses these as evaluation criteria for step-level assessment. This enables cost-efficient, interpretable PRMs that outperform prompted MLLMs by 30+ accuracy points while being 10-100x cheaper than GPT-4o models.", "novelty": "Prior work relied on expensive prompted MLLMs for reward evaluation, which showed weak sensitivity to task progress and poor step-level assessment. Web-Shepherd addresses these limitations through specialized training on web-specific data with structured checklist annotations, enabling the first practical PRM for web agents. The generative approach with next-token prediction allows models to produce both explanatory reasoning and numerical rewards, solving the cost and reliability issues that made previous tree search methods prohibitively expensive ($14,000 per WebArena run).", "ai_categories": ["Web and Computer-Use Agents", "Agent Benchmarking and Evaluation", "Reinforcement Learning for LLMs"]}, {"paper_id": "4238651", "score": "49", "title": "RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation", "authors": "Songhao Han, Boxiang Qiu, Yue Liao, Siyuan Huang, Chen Gao, Shuicheng Yan, Si Liu", "session_type": "SD-2-611", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.06677", "relevant_to_users": "8", "read_by_users": "46", "topics": "", "key_findings": "RoboCerebra reveals significant performance gaps in current vision-language models and robotic policies when handling long-horizon manipulation tasks, showing that task complexity and sequence length cause substantial degradation in planning and execution. The benchmark systematically evaluates state-of-the-art models (Qwen-VL, LLaVA, GPT-4o, Gemini) across diverse kitchen and household manipulation scenarios, identifying memory and semantic understanding as critical bottlenecks. The work provides standardized metrics and hierarchical task organization that enables researchers to pinpoint specific failure modes in complex manipulation planning.", "description": "RoboCerebra is a comprehensive benchmark for evaluating long-horizon robotic manipulation tasks across diverse kitchen and household environments. The benchmark systematically assesses vision-language models and robotic policies on multi-step manipulation sequences, providing standardized metrics for comparing different approaches and identifying performance gaps in realistic scenarios.", "key_contribution": "A large-scale benchmark with hierarchical task organization that enables standardized evaluation of both vision-language models and robotic policies on genuinely complex, multi-step manipulation sequences, along with systematic analysis revealing performance degradation patterns and critical bottlenecks in long-horizon planning.", "novelty": "Unlike existing benchmarks (LIBERO, RoboCasa, CALVIN) that focus on short-horizon or single-task evaluations, RoboCerebra specifically targets genuinely complex, extended manipulation sequences and provides unified evaluation framework for both vision-language models and robotic policies. It addresses the limitation of lacking standardized metrics for long-horizon scenarios by introducing hierarchical task complexity levels and systematic assessment of reasoning, memory, and execution capabilities across extended sequences. The benchmark reveals that current models experience significant performance degradation as task horizons increase, identifying specific failure modes that were not captured by previous short-horizon evaluations.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models", "Planning and Decision Making"]}, {"paper_id": "4191708", "score": "49", "title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering", "authors": "Rushi Qiang, Yuchen Zhuang, Yinghao Li, Dingu Sagar V K, Rongzhi Zhang, ChangHao Li, Ian Wong, Sherry Yang, Percy Liang, ..., Bo Dai", "session_type": "SD-2-2405", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.07782", "relevant_to_users": "6", "read_by_users": "17", "topics": "", "key_findings": "MLE-Dojo provides a comprehensive benchmark framework for evaluating LLM agents on realistic machine learning engineering workflows. The paper establishes baseline performance metrics revealing that current LLM agents show moderate capability in structured ML tasks but struggle with open-ended optimization, with reasoning-enhanced models significantly outperforming base models. The work identifies critical gaps in agent planning, debugging, and experimental design capabilities while demonstrating that interactive feedback and error recovery mechanisms substantially improve agent success rates.", "description": "This paper introduces MLE-Dojo, an interactive environment framework designed for benchmarking and empowering LLM agents in machine learning engineering tasks. The platform provides standardized, stateful environments that simulate realistic end-to-end ML workflows, enabling systematic evaluation of how well agents can handle complex, multi-step ML engineering challenges.", "key_contribution": "The main contribution is a novel benchmark platform specifically focused on ML engineering workflows that provides interactive, stateful sandbox environments with multi-stage task design requiring planning, iteration, and refinement. The framework includes agent scaffolding systems, dynamic evaluation metrics, and a leaderboard system for tracking agent performance across authentic ML engineering scenarios.", "novelty": "Unlike existing benchmarks that focus on isolated coding problems or generic data science tasks, MLE-Dojo specifically targets the gap between theoretical ML knowledge and practical implementation challenges through interactive, stateful environments. The work addresses limitations of previous evaluations by requiring agents to manage complex, multi-step processes that reflect authentic ML engineering workflows rather than single-function code generation. The framework introduces interactive sandbox environments with bounded computational spaces that enable agents to iterate, debug, and refine solutions‚Äîcapabilities not adequately tested by prior benchmarks.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation", "Domain-Specific Applications"]}, {"paper_id": "4246110", "score": "49", "title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning", "authors": "Yuhao Zhou, Yiheng Wang, Xuming He, Ruoyao Xiao, Zhiwei Li, Qiantai Feng, Zijie Guo, Yuejin Yang, Hao Wu, ..., LEI BAI", "session_type": "SD-2-5200", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.10521", "relevant_to_users": "2", "read_by_users": "6", "topics": "", "key_findings": "SFE introduces a comprehensive benchmark with 830 expert-verified VQA pairs across 66 tasks in 5 scientific disciplines. Current state-of-the-art models (GPT-o3: 34.08%, InternVL-3: 26.52%) show significant performance gaps, with Materials Science being most tractable (~63%) and Astronomy most challenging (~24%). The evaluation reveals that newer models show improved comparative reasoning (L3) without corresponding gains in understanding (L2), suggesting scaling reinforcement learning affects reasoning more than knowledge acquisition. The benchmark uses 17 native scientific data formats and bilingual support to assess real-world scientific research capabilities.", "description": "Scientists' First Exam (SFE) is a benchmark designed to evaluate the scientific cognitive abilities of Multimodal Large Language Models (MLLMs) through three interconnected cognitive levels: scientific signal perception (L1), scientific attribute understanding (L2), and scientific comparative reasoning (L3). It addresses the limitations of existing scientific benchmarks that focus primarily on knowledge understanding rather than the full spectrum of perception and reasoning abilities required for authentic scientific research.", "key_contribution": "The paper introduces a novel cognitive taxonomy framework that stratifies evaluation into three levels (perception, understanding, reasoning) and provides a comprehensive bilingual benchmark using native scientific data formats (spectra, molecular structures, protein diagrams) rather than secondary textbook materials. This enables granular assessment of MLLMs' scientific capabilities across 66 expert-curated tasks spanning five high-value disciplines with two-stage expert verification for scientific accuracy.", "novelty": "Unlike prior scientific benchmarks (MMMU, ScienceQA) that rely on secondary textbook materials and focus on single abilities, SFE uses raw scientific data in 17 native formats from real research workflows, providing cognitive stratification from perception through reasoning. It addresses the inadequate assessment of perception and reasoning capabilities in existing benchmarks by incorporating authentic multimodal scientific data and expert-driven task construction from practicing scientists. The benchmark reveals that current MLLMs struggle with structured scientific reasoning despite strong general performance, establishing a challenging frontier for MLLM development in scientific domains.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models", "Reasoning and Test-Time Compute"]}, {"paper_id": "4441970", "score": "49", "title": "ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints", "authors": "Rui Xu, Dakuan Lu, Zicheng Zhao, Xiaoyu Tan, Xintao Wang, Siyu Yuan, Jiangjie Chen, Xu Yinghui", "session_type": "SD-3-5505", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/2bc8b445588504ed12b491f7cc33f033191ae2ad.pdf", "relevant_to_users": "1", "read_by_users": "6", "topics": "", "key_findings": "OrigamiSpace reveals significant gaps in MLLMs' multi-step spatial reasoning capabilities, with even the best models (Gemini 2.5-Pro, GPT-4o) achieving only 36-54% accuracy versus 51-100% for human experts. Environmental learning through compiler feedback dramatically improves performance (from 30-32% to 45-60% in code generation tasks), demonstrating that iterative interaction helps models internalize mathematical constraints. However, performance plateaus after 8-10 iterations, suggesting fundamental capability limitations rather than just initial obstacles. The benchmark shows that geometric property transformations and mathematical constraint satisfaction remain particularly challenging for all tested models.", "description": "OrigamiSpace is a comprehensive benchmark dataset featuring 350 origami instances with crease patterns, folding sequences, and 3D shapes, designed to evaluate MLLMs' multi-step spatial reasoning under rigorous mathematical constraints. The benchmark includes four evaluation tasks: pattern prediction, multi-step spatial reasoning, spatial relationship prediction, and end-to-end code generation, totaling 1,500 multiple-choice questions and 120 code generation problems.", "key_contribution": "First benchmark specifically targeting multi-step spatial reasoning in MLLMs with verifiable mathematical constraints (Kawasaki's and Maekawa's theorems), featuring an enhanced origami compiler that enables interactive environmental learning and reinforcement learning approaches. The work demonstrates that iterative model-environment feedback substantially improves performance, with RL-trained models surpassing larger in-context learning baselines.", "novelty": "Unlike existing spatial reasoning benchmarks that focus on static scene understanding (CLEVR, Visual Genome), OrigamiSpace requires sequential spatial transformations where each step depends on previous results, incorporating mathematically verifiable constraints absent in prior work. It uniquely enables dynamic model-environment interaction through compiler feedback, allowing models to iteratively refine solutions‚Äîa capability not present in traditional benchmarks. The use of authentic origami designs provides real-world complexity while ensuring rigorous mathematical validation, addressing the gap between synthetic benchmarks and genuine multi-step geometric reasoning under formal constraints.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute", "Mathematical and Logical Reasoning"]}, {"paper_id": "4073191", "score": "49", "title": "Logic.py: Bridging the Gap between LLMs and Constraint Solvers", "authors": "Pascal Kesseli, Peter O'Hearn, Ricardo Silveira Cabral", "session_type": "SD-5-700", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/a811f4bc76e7ad2fcf67bc0ce62afd3123512b8d.pdf", "relevant_to_users": "4", "read_by_users": "5", "topics": "", "key_findings": "Logic.py achieves 91.4% accuracy on ZebraLogicBench (1000 logic grid puzzles), representing a 65% absolute improvement over Llama 3.1 70B baseline (24.9%) and surpassing o1-preview (71.4%). This sets a new state-of-the-art and demonstrates substantially larger gains than prior work (which improved GPT-4 from 23.7% to 55.3%). The approach introduces a Python-based DSL optimized for robustness, conciseness, and expressiveness, enabling LLMs to formalize puzzles into constraints that CBMC solver can efficiently solve. The method transforms the challenge from searching solution spaces to reasoning about clues themselves, providing interpretable and verifiable solutions.", "description": "This paper introduces Logic.py, a Python-based domain-specific language that enables LLMs to formalize logic grid puzzles into constraints that are then solved by constraint solvers (CBMC). The approach operates in three phases: data structure generation, constraint generation, and solver execution, achieving over 90% accuracy on traditionally challenging logical reasoning tasks.", "key_contribution": "The main contribution is a specialized logic-focused DSL that bridges LLMs and constraint solvers through an intermediate representation optimized for robustness (minimizing syntax errors), conciseness (avoiding boilerplate), and expressiveness (supporting arbitrary solver operations). This enables a 65% absolute performance improvement over baseline LLMs on complex logical reasoning tasks.", "novelty": "Unlike prior neuro-symbolic approaches that translate to general logical formalisms (Logic-LM, SatLM), Logic.py introduces a specialized DSL specifically designed as an intermediate language between natural language and constraint solvers, following compilation principles. The key innovation is leveraging Python's flexibility (untyped variables, runtime semantics) with constraint-oriented type decorators (Unique, Domain) and nondeterministic features (assume, nondet), enabling LLMs to express constraints more reliably. This achieves substantially larger performance gains (65% vs. 31.6% improvement in prior work) while providing interpretable, verifiable solutions through constraint solver transparency.", "ai_categories": ["Tool Use and Code Generation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4263732", "score": "49", "title": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization", "authors": "Yiyou Sun, Shawn Hu, Georgia Zhou, Ken Zheng, Hanna Hajishirzi, Nouha Dziri, Dawn Song", "session_type": "SD-6-4912", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.18880", "relevant_to_users": "4", "read_by_users": "9", "topics": "", "key_findings": "Current LLMs show substantial performance degradation when reasoning beyond their training distribution in mathematics. The paper introduces OMEGA, a benchmark distinguishing three types of generalization: exploratory (discovering novel strategies), compositional (combining familiar elements unprecedentedly), and transformative (applying concepts to different contexts). Frontier models like GPT-4, DeepSeek, and Claude struggle significantly across all three dimensions, revealing brittleness in mathematical reasoning beyond pattern matching.", "description": "This paper investigates whether LLMs can generalize mathematically beyond training data by introducing OMEGA, a benchmark that evaluates three distinct types of out-of-distribution reasoning: exploratory, compositional, and transformative generalization. The work systematically tests frontier models and finds significant performance drops across all generalization types.", "key_contribution": "The main contribution is the OMEGA benchmark and framework that disaggregates mathematical generalization into three distinct, systematically testable categories, moving beyond monolithic treatment of out-of-distribution reasoning to reveal specific failure modes in current LLMs.", "novelty": "Unlike previous work that treated out-of-distribution generalization as a single phenomenon, this work disaggregates it into three distinct types with targeted evaluation. It addresses the limitation of existing benchmarks that primarily test memorized patterns rather than genuine reasoning flexibility. The framework reveals that different generalization types present distinct challenges, providing a more nuanced understanding of where and why LLMs fail at mathematical reasoning.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute", "Mathematical and Logical Reasoning"]}, {"paper_id": "4166440", "score": "48", "title": "TTRL: Test-Time Reinforcement Learning", "authors": "Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, ..., Bowen Zhou", "session_type": "SD-1-5419", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/3ff432e912f7ed9bbbacf9a7a16d7e5af88d721a.pdf", "relevant_to_users": "14", "read_by_users": "33", "topics": "", "key_findings": "TTRL demonstrates that reinforcement learning can be effectively applied to unlabeled test data using majority voting as a reward signal, achieving a remarkable 211% improvement in pass@1 performance on AIME 2024 for Qwen-2.5-Math-7B. The method shows that test-time RL can surpass the upper bound of traditional majority voting and approach the performance of models trained on labeled data, enabling continuous model improvement during inference without ground-truth labels.", "description": "This paper introduces Test-Time Reinforcement Learning (TTRL), a method that trains LLMs using reinforcement learning on unlabeled test data for reasoning tasks. It addresses the challenge of reward estimation during inference by leveraging majority voting from multiple model generations as an effective supervision signal, enabling self-improvement without access to ground-truth answers.", "key_contribution": "The main contribution is showing that common test-time scaling techniques like majority voting can serve as effective reward signals for driving RL training on unlabeled data, transforming passive inference-time ensembling into active model optimization that surpasses traditional test-time scaling limits.", "novelty": "Unlike prior test-time scaling methods that simply aggregate multiple outputs through voting, TTRL actively uses these aggregated signals to train the model via reinforcement learning during inference. This addresses the limitation that traditional test-time compute methods cannot improve beyond their ensemble upper bound. The approach introduces a paradigm shift from static test-time inference to dynamic test-time learning, enabling models to improve on unlabeled data by bootstrapping from their own collective outputs.", "ai_categories": ["Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4062153", "score": "48", "title": "Atom of Thoughts for Markov LLM Test-Time Scaling", "authors": "Fengwei Teng, Quan Shi, Zhaoyang Yu, Jiayi Zhang, Yuyu Luo, Chenglin Wu, Zhijiang Guo", "session_type": "SD-1-1907", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/aee7e5e0ac85edb676698e634deccc28c92e1407.pdf", "relevant_to_users": "14", "read_by_users": "19", "topics": "", "key_findings": "AoT introduces a Markov-based reasoning framework that eliminates accumulated historical dependencies in test-time scaling, achieving 80.6% F1 on HotpotQA with gpt-4o-mini, surpassing o3-mini by 3.4% and DeepSeek-R1 by 10.6%. It works both as a standalone method and as a plug-in enhancement for existing reasoning frameworks, demonstrating effectiveness across six diverse benchmarks (MATH, GSM8K, BBH, MMLU-CF, HotpotQA, LongBench).", "description": "This paper proposes Atom of Thoughts (AoT), a test-time scaling framework that transforms complex reasoning into a Markov-style sequence of atomic, self-contained questions. Unlike existing methods that accumulate reasoning history, AoT uses an iterative decomposition-contraction mechanism where each state depends only on its immediate predecessor, reducing computational waste and reasoning interference.", "key_contribution": "The core innovation is applying the memoryless Markov property to LLM reasoning through a two-phase transition mechanism: decomposing questions into dependency-based DAGs and contracting them into simplified atomic questions. This preserves answer equivalence while eliminating the need to maintain full historical context, enabling both standalone use and seamless integration as a preprocessing module for existing test-time scaling methods.", "novelty": "Unlike Chain-of-Thought (which preserves entire reasoning history), Tree-of-Thoughts (which tracks ancestor-sibling relationships), and graph-based approaches (which maintain arbitrary node dependencies), AoT leverages the Markov memoryless property to eliminate historical dependency accumulation. It addresses the key limitation that accumulated context wastes computational resources and interferes with effective reasoning, offering a theoretically grounded probabilistic framework (A ~ p(A|Q_N) ‚àè p(Q_{i+1}|Q_i)) that contrasts with traditional complete-history formulations.", "ai_categories": ["Reasoning and Test-Time Compute", "Planning and Decision Making", "Mathematical and Logical Reasoning"]}, {"paper_id": "3443861", "score": "48", "title": "OmniBench: Towards The Future of  Universal Omni-Language Models", "authors": "Yizhi LI, Ge Zhang, Yinghao Ma, Ruibin Yuan, King Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Noah Wang, ..., Chenghua Lin", "session_type": "SD-2-5201", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/c52922a7cb62ce2d159c41efd473976ed5a5693f.pdf", "relevant_to_users": "9", "read_by_users": "22", "topics": "", "key_findings": "OmniBench reveals critical gaps in current multimodal models: open-source omni-language models show significant limitations in instruction-following and reasoning within tri-modal contexts, and baseline models perform poorly (below 50% accuracy) even when provided with textual alternatives to images and audio. The research demonstrates that the ability to construct a consistent context from text, image, and audio simultaneously is fundamentally overlooked in existing MLLM training paradigms. To address this, the authors created OmniInstruct, an 84.5K-sample instruction tuning dataset specifically designed for training models to handle tri-modal contexts.", "description": "OmniBench is a novel benchmark with 1,142 human-annotated question-answer pairs designed to rigorously evaluate multimodal large language models' ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously. It introduces the concept of omni-language models (OLMs) and ensures that accurate responses require integrated understanding across all three modalities rather than relying on just one or two.", "key_contribution": "The main contribution is creating the first comprehensive tri-modal benchmark that enforces simultaneous reasoning across text, image, and audio by design‚Äîevery question requires information from all three modalities to answer correctly. The benchmark includes a novel task taxonomy spanning eight reasoning capabilities from basic perception to complex inference, along with the OmniInstruct dataset for training OLMs.", "novelty": "Unlike existing multimodal benchmarks that typically focus on dual-modal interactions (text-image or text-audio), OmniBench uniquely addresses the unexplored capacity for concurrent tri-modal processing and reasoning. Previous benchmarks allowed models to answer questions using only one or two modalities, but OmniBench's annotation scheme fundamentally requires integrated information from all three inputs, revealing that current MLLMs lack the ability to construct coherent contexts across modalities‚Äîa limitation masked by prior evaluation methods. This work shifts the paradigm from evaluating separate modality pairs to assessing true omni-modal understanding.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models"]}, {"paper_id": "580056", "score": "48", "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models", "authors": "Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, ..., Ran He", "session_type": "SD-2-4503", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2306.13394", "relevant_to_users": "4", "read_by_users": "9", "topics": "", "key_findings": "MME reveals that existing MLLMs have significant room for improvement, with models showing particular weaknesses in position recognition and instruction compliance. The benchmark identified four critical problems: instruction non-compliance (models ignoring yes/no constraints), perception deficits (misidentifying counts/text), reasoning breakdowns (correct steps but wrong conclusions), and hallucination (confirming non-existent objects). Among 30 evaluated models, top performers varied between perception (WeMM, InfMLLM, SPHINX) and cognition (GPT-4V, Lion, WeMM) tasks, suggesting specialized strengths.", "description": "MME is a comprehensive evaluation benchmark for multimodal large language models that systematically assesses both perception and cognition abilities across 14 subtasks. It uses manually designed instruction-answer pairs in yes/no format to enable objective, automated scoring while preventing data leakage from public datasets.", "key_contribution": "The benchmark introduces a standardized evaluation framework with 14 subtasks (10 perception tasks covering coarse-grained recognition, fine-grained recognition, and OCR; 4 cognition tasks covering commonsense reasoning, numerical calculation, text translation, and code reasoning) using binary yes/no questions that enable objective automated scoring without subjective human judgment or GPT-based evaluation.", "novelty": "Unlike previous MLLM benchmarks that directly use public datasets (risking data leakage), MME manually constructs all instruction-answer pairs with novel questions and fresh annotations. It addresses evaluation inconsistency by using simple yes/no formatting that eliminates prompt engineering variability, and introduces a dual-metric scoring system (ACC and ACC+) that provides more nuanced assessment than single-metric approaches. The separation of perception and cognition dimensions enables systematic identification of specific model weaknesses rather than generic performance scores.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models"]}, {"paper_id": "4442829", "score": "48", "title": "Revolutionizing Training-Free NAS: Towards Efficient Automatic Proxy Discovery via Large Language Models", "authors": "Haidong Kang, Lihong Lin, Hanling Wang", "session_type": "SD-5-3302", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/f5967a55faa0ccebb78581f66e96dad4d59eb767.pdf", "relevant_to_users": "2", "read_by_users": "3", "topics": "", "key_findings": "The paper introduces APD (Automatic Proxy Discovery), an LLM-driven framework that automatically discovers zero-cost proxies for training-free NAS, eliminating the need for manual design. It uses actor-critic reinforcement learning to iteratively optimize prompts that guide LLMs to generate better proxies across generations. Extensive experiments on mainstream NAS benchmarks demonstrate that APD excels in both performance and efficiency, addressing critical issues of poor correlation with final model performance and high computational complexity in existing manually-designed zero-cost proxies.", "description": "This paper proposes an LLM-driven Automatic Proxy Discovery (APD) framework for training-free Neural Architecture Search (NAS) that automatically discovers optimal zero-cost proxies at initialization. The framework leverages large language models guided by actor-critic reinforcement learning to generate and iteratively improve proxies, replacing traditional labor-intensive manual proxy design.", "key_contribution": "The main contribution is revolutionizing the design paradigm of zero-cost proxies for training-free NAS by leveraging LLMs for automatic discovery instead of manual design. The actor-critic RL approach optimizes prompts to enable LLMs to generate increasingly better proxies across generations, achieving superior performance and efficiency.", "novelty": "Unlike existing training-free NAS methods that rely on manually designed zero-cost proxies requiring extensive expert knowledge, this work introduces automatic proxy discovery through LLMs. It addresses three key limitations: eliminates labor-intensive manual design, improves correlation with final model performance, and reduces computational complexity. The novel integration of LLM-based generation with actor-critic RL for iterative prompt optimization enables adaptive, problem-specific proxy discovery rather than relying on generic hand-crafted metrics.", "ai_categories": ["Self-Improvement and Meta-Learning", "Model Efficiency and Optimization"]}, {"paper_id": "4442252", "score": "48", "title": "EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths", "authors": "Zhening Li, Armando Solar-Lezama, Yisong Yue, Stephan Zheng", "session_type": "SD-6-2410", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ec254672c2c5013801b6522a08e51c829a7ef814.pdf", "relevant_to_users": "0", "read_by_users": "3", "topics": "", "key_findings": "EnCompass introduces Probabilistic Angelic Nondeterminism (PAN), a programming model that decouples agent workflow logic from inference-time search strategies. Using Python decorators and branchpoint() primitives, it compiles agent programs into searchable execution path trees. Three case studies (Java-to-Python translation, Hypothesis Search, and Reflexion agents) demonstrate that beam search and best-first search outperform simpler sampling strategies, and that scaling search steps is more effective than increasing refinement iterations. The framework enables programmers to quickly improve agent reliability and easily switch between different inference-time strategies with minimal code changes.", "description": "EnCompass is a Python framework for LLM-based agent programming that treats inference-time strategies as search over nondeterministic program execution paths. Programmers mark 'locations of unreliability' (like LLM calls) with branchpoint() statements, and the framework searches the resulting tree of execution paths to find high-scoring outcomes, enabling systematic exploration of different inference-time scaling strategies.", "key_contribution": "The main innovation is the Probabilistic Angelic Nondeterminism (PAN) programming model, which cleanly separates the core agent workflow specification from the inference-time search strategy. This allows programmers to write agents assuming unreliable operations succeed, while the runtime systematically searches execution paths for good outcomes, providing a unifying framework for various inference-time strategies and agentic patterns.", "novelty": "Unlike prompt-based approaches (ReAct), reflection-based memory systems (Reflexion), or optimization-focused frameworks (DSPy), EnCompass adapts angelic nondeterminism from formal program synthesis to systematically search over program execution paths. It addresses the limitation that current agent frameworks entangle workflow logic with inference-time strategies, making experimentation difficult. EnCompass enables modular, declarative agent programming where search strategies can be swapped independently without obscuring the agent workflow or compromising code modularity.", "ai_categories": ["Tool Use and Code Generation", "Planning and Decision Making", "Reasoning and Test-Time Compute"]}, {"paper_id": "4204808", "score": "47", "title": "RealMath: A Continuous Benchmark for Evaluating Language Models on Research-Level Mathematics", "authors": "Jie Zhang, Cezara Petrui, Kristina Nikoli√Ñ¬á, Florian Tramer", "session_type": "SD-3-1512", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.12575", "relevant_to_users": "8", "read_by_users": "9", "topics": "", "key_findings": "RealMath reveals significant performance gaps between current state-of-the-art language models on research-level mathematics versus competition-style problems. The benchmark shows that models struggle substantially with authentic research problems, and that context and problem formulation have a substantial impact on model performance. The work demonstrates that existing models have varying capabilities across different mathematical domains and difficulty levels when evaluated on genuine research tasks.", "description": "This paper introduces RealMath, a continuous benchmark for evaluating language models on research-level mathematical problems sourced from real academic contexts. The benchmark moves beyond traditional competition-style math assessments to provide realistic evaluation of LLM mathematical reasoning capabilities on authentic research challenges.", "key_contribution": "RealMath provides the first continuous, research-oriented evaluation benchmark for LLMs that uses authentic mathematical research problems rather than competition problems. It employs continuous evaluation metrics beyond binary correctness and includes a pathway for ongoing benchmark expansion to maintain relevance as models improve.", "novelty": "Unlike existing benchmarks such as MATH and GSM8K that focus on competition-style problems, RealMath specifically targets authentic research mathematics from real academic contexts, addressing the gap in evaluating models on genuine mathematical inquiry. It introduces continuous evaluation metrics rather than binary correctness assessments, providing more nuanced understanding of model capabilities. The benchmark is designed as an expandable, living evaluation suite that can evolve with model improvements, solving the benchmark saturation problem.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4441536", "score": "47", "title": "CTRL-ALT-DECEIT Sabotage Evaluations for Automated AI R&D", "authors": "Francis Rhys Ward, Teun van der Weij, Hanna G√É¬°bor, Sam Martin, Raja Mehta Moreno, Harel Lidar, Louis Makower, Thomas Jodrell, Lauren Robson", "session_type": "SD-3-1208", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/5ba46eb607f50910cda80a0bfc28f6c4f5b9c656.pdf", "relevant_to_users": "8", "read_by_users": "12", "topics": "", "key_findings": "", "description": "", "key_contribution": "", "novelty": "", "ai_categories": []}, {"paper_id": "4076227", "score": "47", "title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning", "authors": "Wenkai Yang, Shuming Ma, Yankai Lin, Furu Wei", "session_type": "SD-5-2315", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/a85740fc42f5b0086b031316724925f3d39daa30.pdf", "relevant_to_users": "8", "read_by_users": "13", "topics": "", "key_findings": "The paper reveals that scaling test-time compute by extending Chain of Thought (CoT) reasoning can actually impair LLM reasoning performance in certain domains, challenging the assumption that more compute always helps. It identifies that optimal CoT length varies significantly across different mathematical domains, and proposes Thinking-Optimal Scaling (TOPS) that enables a 32B model to match the performance of larger teacher models like QwQ-32B by teaching models to adaptively calibrate reasoning effort per problem.", "description": "This paper investigates the relationship between test-time compute scaling and reasoning performance in LLMs, specifically examining whether extending Chain of Thought reasoning indefinitely benefits or harms performance. The authors discover domain-specific optimal length distributions and develop a training methodology that teaches models to calibrate reasoning effort appropriately based on problem difficulty.", "key_contribution": "The main contribution is the Thinking-Optimal Scaling (TOPS) strategy, which uses seed data with varied response lengths to train models for adaptive reasoning depth selection. The method enables models to self-improve by selecting their shortest correct response under different reasoning efforts, achieving competitive performance with larger models while using more efficient inference.", "novelty": "Unlike previous work that pursued aggressive test-time scaling assuming linear benefits, this paper uniquely identifies that excessive CoT lengths can degrade performance and that optimal length distributions differ across domains. The work shifts focus from maximization to optimization of test-time compute, teaching models to intelligently calibrate reasoning depth per problem rather than uniformly extending reasoning chains. This addresses the previously overlooked negative effects of excessive computation on reasoning quality.", "ai_categories": ["Reasoning and Test-Time Compute", "Self-Improvement and Meta-Learning", "Mathematical and Logical Reasoning"]}, {"paper_id": "4224431", "score": "47", "title": "AREAL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning", "authors": "Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, ..., Yi Wu", "session_type": "SD-6-514", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/1b2829a8cfd93ca52bca9bf2c38c826016159024.pdf", "relevant_to_users": "10", "read_by_users": "14", "topics": "", "key_findings": "AReaL introduces a fully asynchronous RL system that decouples data generation from training for language reasoning tasks, achieving up to 2.77√ó training speedup compared to synchronous systems while maintaining or improving final performance. The system includes a staleness-enhanced PPO variant to handle outdated training samples and system-level optimizations for improved GPU utilization. Extensive experiments on math and code reasoning benchmarks demonstrate both significant efficiency gains and matched or better final performance.", "description": "AReaL is a large-scale asynchronous reinforcement learning system designed to train language models on complex reasoning tasks. It enables rollout workers to continuously generate new outputs while training workers independently update the model whenever sufficient data is collected, eliminating synchronization barriers that plague traditional synchronous RL approaches.", "key_contribution": "The main contribution is a fully asynchronous RL framework that eliminates straggler delays and synchronization barriers in distributed RL training for LLMs. The system introduces a staleness-enhanced PPO variant and system-level optimizations that enable efficient asynchronous training at scale without sacrificing sample efficiency or training stability.", "novelty": "Unlike traditional synchronous PPO methods that require all workers to complete rollouts before policy updates, AReaL completely decouples generation from training, allowing continuous operation without idle workers. The key innovation is handling data staleness through a novel PPO variant while maintaining training stability through dynamic batch sizing and adaptive load balancing. This addresses the fundamental inefficiency of synchronous systems where faster workers must wait for slower ones, enabling true scalability with 2.77√ó speedup on equivalent hardware.", "ai_categories": ["Reinforcement Learning for LLMs", "Reasoning and Test-Time Compute", "Model Efficiency and Optimization"]}, {"paper_id": "4206973", "score": "46", "title": "RLVR-World: Training World Models with Reinforcement Learning", "authors": "Jialong Wu, Shaofeng Yin, Ningya Feng, Mingsheng Long", "session_type": "SD-4-1301", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/4b0d2935ed4554453743ecdcf099e0d679355328.pdf", "relevant_to_users": "11", "read_by_users": "35", "topics": "", "key_findings": "RLVR-World introduces a unified framework using reinforcement learning with verifiable rewards (RLVR) to train world models across modalities, achieving substantial improvements: +30.7% accuracy on text game prediction, +15.1% F1 on web navigation, +9.2% perceptual quality for robot video prediction, and +18.4% success rate for downstream web agents. The approach requires only hundreds of training steps versus hundreds of thousands for traditional MLE training, while eliminating issues like video repetition (reduced from 48.6% to 9.9%) and blurry predictions. RLVR represents a promising post-training paradigm that directly optimizes task-specific metrics rather than surrogate objectives.", "description": "RLVR-World is a unified framework that applies reinforcement learning with verifiable rewards to post-train world models across text, web, and video domains. Instead of using maximum likelihood estimation (MLE), it directly optimizes decoded prediction quality metrics (accuracy, LPIPS, F1 scores) as verifiable rewards through autoregressive sequence modeling.", "key_contribution": "The main innovation is bridging the gap between token-level predictions and task-level metrics by using decoded prediction quality as verifiable rewards, eliminating the fundamental misalignment between MLE training objectives and actual task-specific goals. This enables efficient post-training that directly optimizes metrics used by downstream systems without requiring learned reward models.", "novelty": "Unlike standard MLE training that optimizes token-level likelihood, RLVR-World directly optimizes the task-specific metrics that matter for world model performance (prediction accuracy, perceptual quality). It addresses key limitations of prior approaches: the misalignment between likelihood-based objectives and downstream task metrics, video repetition and blurriness from MSE optimization, and reward overoptimization issues in RLHF by using deterministic, rule-based reward functions. The framework achieves target performance in ~300 steps versus 150,000+ steps for continued MLE pre-training while unifying world modeling across multiple modalities under a single sequence prediction formulation.", "ai_categories": ["Reinforcement Learning for LLMs", "Planning and Decision Making", "Vision-Language-Action Models"]}, {"paper_id": "4280296", "score": "46", "title": "Generalizing Verifiable Instruction Following", "authors": "Valentina Pyatkin, Saumya Malik, Victoria Graf, Hamish Ivison, Shengyi Huang, Pradeep Dasigi, Nathan Lambert, Hanna Hajishirzi", "session_type": "SD-5-412", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2507.02833", "relevant_to_users": "6", "read_by_users": "17", "topics": "", "key_findings": "This paper reveals that state-of-the-art LLMs strongly overfit to existing instruction-following benchmarks (80%+ on IFEval) but fail to generalize to novel constraints (below 50% on IFBench). The authors introduce IFBench with 58 unseen verifiable constraints, IFTrain with 29 new training constraints, and IF-RLVR (reinforcement learning with verifiable rewards) that improves T√ºlu-3-8B from 82.4% to 92.2% on IFEval and 28.9% to 45.9% on IFBench. Training on diverse, multiple constraints (up to 6 per instance) with wider variable ranges significantly improves generalization beyond what evaluation benchmarks require.", "description": "This paper addresses the critical problem of language models' inability to generalize instruction following to unseen output constraints. The authors demonstrate systematic overfitting on existing benchmarks and introduce IFBench, a new evaluation benchmark with 58 novel verifiable constraints, along with training methods using reinforcement learning with verifiable rewards (RLVR) to improve generalization.", "key_contribution": "The main contribution is IFBench, a benchmark that exposes generalization failures in instruction following with 58 novel constraints, and IF-RLVR, a reinforcement learning approach using automatically verifiable reward functions that achieves substantial improvements by training on diverse, multi-constraint scenarios with expanded variable ranges.", "novelty": "Unlike prior work that relies on synthetic data generation, this approach leverages automatically verifiable reward functions and systematically investigates how training data composition affects generalization. The paper is the first to demonstrate that models trained on existing benchmarks like IFEval cannot generalize to novel constraint types, revealing a critical limitation in current instruction-following capabilities. The work introduces genuinely novel constraints sourced from real user feedback rather than variations of existing templates, and shows that training on more complex multi-constraint scenarios than required by evaluation benchmarks improves out-of-domain generalization.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reinforcement Learning for LLMs", "Tool Use and Code Generation"]}, {"paper_id": "4425565", "score": "46", "title": "A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning", "authors": "Zhi Zhou, Yuhao Tan, Zenan Li, Yuan Yao, Lan-Zhe Guo, Yu-Feng Li, Xiaoxing Ma", "session_type": "SD-6-1900", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/73a0607365582aedefc2167e27fb239c96092223.pdf", "relevant_to_users": "2", "read_by_users": "6", "topics": "", "key_findings": "This paper provides the first theoretical framework for analyzing sampling-based test-time scaling methods in LLM reasoning, revealing that self-consistency suffers from high estimation error with only linear convergence (1/n decay), while perplexity exhibits substantial modeling error and degraded convergence for low-probability paths. The proposed RPC method combines perplexity consistency with reasoning pruning to achieve exponential convergence while maintaining low model error, reducing sampling costs by 50-71% while improving accuracy by 1.29% on average across mathematical reasoning and code generation benchmarks.", "description": "This paper develops a theoretical framework decomposing reasoning error into estimation error and model error, analyzing why self-consistency and perplexity-based confidence methods fail independently. The authors propose RPC (Reasoning with Perplexity Consistency), which combines internal probabilities with self-consistency through a Weibull mixture model for reasoning path pruning, achieving better performance with fewer samples.", "key_contribution": "The paper's main contribution is a rigorous mathematical framework proving that Perplexity Consistency achieves exponential convergence (vs. linear for self-consistency) while maintaining low model error, coupled with a principled Reasoning Pruning method using Weibull mixture models to automatically identify and remove low-quality reasoning paths without manual hyperparameter tuning.", "novelty": "Unlike previous work that treats confidence estimation and self-consistency as separate paradigms, this is the first to provide formal mathematical analysis decomposing reasoning error and proving convergence properties. It addresses the critical limitation that self-consistency requires many samples for accuracy while perplexity degrades on low-probability paths, introducing a theoretically-grounded hybrid approach that bridges both methods. The automatic pruning threshold determination via Weibull mixture modeling eliminates the need for manual tuning required by prior methods.", "ai_categories": ["Reasoning and Test-Time Compute", "Mathematical and Logical Reasoning"]}, {"paper_id": "4325513", "score": "46", "title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents", "authors": "Yifu Guo, Jiaye Lin, Huacan Wang, Yuzhen Han, Sen Hu, Ziyi Ni, Licheng Wang, Mingguang Chen", "session_type": "SD-6-1911", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/814cd78232da3150c1f91b29920f4f2e4d70fb3c.pdf", "relevant_to_users": "7", "read_by_users": "15", "topics": "", "key_findings": "SE-Agent introduces a self-evolution framework for LLM-based agents that achieves up to 55% relative improvement on reasoning tasks and state-of-the-art performance among open-source agents on SWE-bench Verified. The framework enables trajectory-level evolution through three operations: revision (fixing weak steps), recombination (merging successful sub-trajectories), and refinement (polishing complete solutions). It addresses the limitations of methods like MCTS by expanding search space diversity and leveraging cross-trajectory learning rather than relying on independent path exploration.", "description": "SE-Agent presents a self-evolution trajectory optimization framework for multi-step reasoning in LLM-based agents. The approach iteratively improves reasoning paths through revision, recombination, and refinement operations that leverage feedback from previous trajectories to guide agents toward correct solutions.", "key_contribution": "The main innovation is enabling agents to evolve their reasoning trajectories by borrowing and combining successful segments across different solution attempts, rather than treating each exploration path independently. This cross-trajectory learning mechanism allows agents to escape local optima and systematically improve solution quality through iterative self-refinement.", "novelty": "Unlike Monte Carlo Tree Search and beam search which explore trajectories independently, SE-Agent introduces cross-trajectory information exchange through evolutionary operations. It addresses the limitation that existing methods ignore interdependencies among trajectories and lack search space diversity, leading to redundant reasoning. The key insight is that trajectories contain rich feedback that can be systematically harvested through revision, recombination, and refinement to guide agents more efficiently than conventional exploration-exploitation approaches.", "ai_categories": ["Reasoning and Test-Time Compute", "Self-Improvement and Meta-Learning", "Planning and Decision Making"]}, {"paper_id": "3941824", "score": "45", "title": "Sloth: scaling laws for LLM skills to predict multi-benchmark performance across families", "authors": "Felipe Maia Polo, Seamus Somerstep, Leshem Choshen, Yuekai Sun, Mikhail Yurochkin", "session_type": "SD-1-1306", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/82fec2c6ee0cec1161d232d81bfad1d63de8fd54.pdf", "relevant_to_users": "4", "read_by_users": "5", "topics": "", "key_findings": "Sloth introduces a novel scaling law that models LLM benchmark performance through low-dimensional latent skills (reasoning, knowledge, instruction-following) rather than modeling benchmarks independently. Key findings: (1) reasoning scales primarily with model size, (2) knowledge depends heavily on both size and training tokens, (3) instruction-following is dramatically enhanced by instruction-tuning but relatively constant with scale, and (4) shared skill coefficients across model families enable accurate prediction with minimal data per family. Empirically validated on 12 benchmarks from Open LLM Leaderboard, achieving comparable or superior prediction accuracy to existing methods while enabling prediction of performance on unseen downstream tasks.", "description": "This paper proposes Sloth (Skills Scaling Laws), a new framework for predicting LLM performance across multiple benchmarks and model families by modeling benchmark scores as linear combinations of latent skills that scale with model size and training tokens. Unlike prior work that requires extensive per-family training data or makes overly restrictive assumptions, Sloth exploits cross-benchmark correlations to provide accurate predictions while requiring minimal data per family.", "key_contribution": "The main contribution is a theoretically grounded and empirically validated scaling law framework that decomposes LLM capabilities into interpretable latent skills with family-specific efficiency factors and shared scaling coefficients. This enables accurate performance prediction across model families and benchmarks without requiring extensive training of multiple models per family, while providing actionable insights into how different computational resources affect specific capabilities.", "novelty": "Unlike prior scaling laws that model benchmarks independently (requiring 3-5 models per family) or assume overly rigid FLOPs-only dependencies, Sloth introduces a latent skills framework that exploits correlations across benchmarks and allows flexible dependencies on model size and training tokens separately. This addresses the fundamental tradeoff between restrictiveness and flexibility in previous work by sharing skill scaling coefficients across families while maintaining family-specific intercepts to capture efficiency differences. The approach enables predicting performance on unseen tasks and hypothetical models, which was not possible with previous methods.", "ai_categories": ["Agent Benchmarking and Evaluation", "Model Efficiency and Optimization"]}, {"paper_id": "4137831", "score": "45", "title": "AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems", "authors": "Yingxuan Yang, Huacan Chai, Shuai Shao, Yuanyi Song, Siyuan Qi, Renting Rui, Weinan Zhang", "session_type": "SD-1-1600", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/38bdf6e7191adba7391b1fde1ad37e27887b2bac.pdf", "relevant_to_users": "4", "read_by_users": "7", "topics": "", "key_findings": "AgentNet demonstrates that decentralized multi-agent coordination can outperform both single-agent and centralized multi-agent baselines in task accuracy. The system enables agents to self-organize and adapt their communication topology dynamically without central control, reducing computational overhead through selective communication. The framework successfully integrates retrieval-based memory for continual skill refinement, allowing agents to specialize and evolve autonomously while collaborating on complex reasoning and tool-use tasks.", "description": "AgentNet is a decentralized, RAG-enhanced multi-agent framework that enables LLM-based agents to specialize, evolve, and collaborate autonomously through a dynamically structured Directed Acyclic Graph (DAG). The system eliminates the need for a central orchestrator by allowing agents to adaptively adjust their communication patterns based on task context and demands.", "key_contribution": "The paper introduces three key innovations: (1) a fully decentralized coordination mechanism that eliminates centralized bottlenecks and single points of failure, (2) dynamic agent graph topology that adapts in real-time to task demands using graph-theoretic principles, and (3) a retrieval-based memory system that supports continual skill refinement and agent specialization.", "novelty": "Unlike previous work that relies on rigid hierarchical structures or fully connected communication graphs, AgentNet enables evolutionary topology optimization where agents dynamically decide whom to communicate with based on task context. This addresses critical limitations of centralized systems including scalability bottlenecks, reduced adaptability from fixed agent roles, and privacy concerns that prevent cross-organizational collaboration. The decentralized approach with adaptive network topology allows for emergent intelligence and fault tolerance while reducing unnecessary message passing.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Tool Use and Code Generation", "Memory and Context Management"]}, {"paper_id": "4441577", "score": "45", "title": "DynaAct: Large Language Model Reasoning with Dynamic Action Spaces", "authors": "Xueliang Zhao, Wei Wu, Jian Guan, Qintong Li, Lingpeng Kong", "session_type": "SD-4-5304", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/45820087f0186edb228a8b251bb2af4a3cca38b5.pdf", "relevant_to_users": "0", "read_by_users": "7", "topics": "", "key_findings": "DynaAct introduces a submodular optimization framework for automatically constructing compact action spaces for LLM reasoning, achieving significant performance gains across six benchmarks (e.g., +6.8% on MATH-500) while maintaining efficient inference. The method uses data-driven sketch extraction from diverse problem corpora and Q-learning-based embeddings to balance action utility and diversity. This approach addresses the scalability limitations of manually-defined action spaces and the computational prohibitiveness of unstructured search spaces.", "description": "DynaAct presents a framework for automatically constructing optimal action spaces for sequential reasoning in LLMs by formulating action selection as a submodular optimization problem. The method extracts general reasoning patterns from diverse problem corpora and uses greedy algorithms with theoretically-grounded utility-diversity trade-offs to select compact, effective action sets that enable efficient tree search without sacrificing performance.", "key_contribution": "The main innovation is a theoretically-grounded submodular optimization framework that automatically constructs compact, domain-generalizable action spaces by jointly optimizing for action utility (via Q-learning embeddings) and diversity (via pairwise distance penalties), enabling efficient greedy selection with linear complexity while maintaining performance guarantees.", "novelty": "Unlike prior work focusing on policy learning, reward modeling, or manual action space design, DynaAct addresses action space construction as a distinct research problem. It overcomes the generalization-utility tension by learning action patterns automatically from demonstration data rather than hand-engineering domain-specific actions, while avoiding the computational burden of unstructured spaces through principled submodular optimization that explicitly prevents action redundancy. The Q-learning-based embedding approach ensures selected actions meaningfully advance problem-solving, distinguishing it from RAP's expensive dynamic generation and rStar's manually-defined limitations.", "ai_categories": ["Planning and Decision Making", "Tool Use and Code Generation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4442067", "score": "45", "title": "GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining", "authors": "Chunyu Wei, Wenji Hu, Xingjia Hao, Xin Wang, Yifan Yang, Yunhai Wang, Yang Tian, Yueguo Chen", "session_type": "SD-5-312", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/d184d257f33dec6f932171111d977423bd83f8ef.pdf", "relevant_to_users": "0", "read_by_users": "0", "topics": "", "key_findings": "GraphChain achieves 84.7% average accuracy across five graph domains, representing a 20.7% relative improvement over the best baseline (GraphForge at 70.2%). Notably, a 7B parameter model outperforms GPT-4o (200B params) and Claude-3-Opus (137B params) on graph reasoning tasks. The system maintains consistent performance on graphs with up to 200,000 nodes where baselines fail due to context exhaustion. Tool usage analysis reveals domain-specific adaptation patterns: path planning dominates traffic networks (33.8%), while social networks rely on centrality measures (28.8%) and community detection (20.4%). Transfer learning experiments show robust generalization with only 3-4% accuracy drops when applying models trained on financial networks to other domains.", "description": "GraphChain is a framework that enables LLMs to analyze large-scale graphs through dynamic sequences of specialized tools, addressing fundamental limitations of context constraints and inflexible reasoning in existing LLM-based graph analysis. The system integrates 45 NetworkX functions organized into six clusters (path planning, centrality, community detection, connectivity, structural properties, graph traversal) and uses a dual-output architecture that maintains large intermediate data separate from LLM context while providing concise natural language summaries.", "key_contribution": "The paper introduces two core innovations: (1) Progressive Graph Distillation, a reinforcement learning mechanism using PPO that generates optimized tool sequences by balancing task relevance with information compression through a reward function that operationalizes the information bottleneck principle, and (2) Structure-aware Test-Time Adaptation (STTA), which uses spectral properties derived from SVD of graph Laplacian matrices and lightweight adapters to efficiently tailor tool selection strategies to diverse graph topologies without costly retraining.", "novelty": "Unlike prior text-instruction methods that attempt direct graph-to-text conversion or tool-instruction approaches like Graph-ToolFormer that rely on single-step tool invocations, GraphChain enables sequential multi-step reasoning where each step reveals information guiding subsequent decisions. The work addresses three critical limitations: (1) context exhaustion when processing million-node graphs, (2) reasoning hallucination from unrealistic single-tool demands, and (3) severe distribution shifts across domain-specific graph schemas. The novelty lies in framing graph exploration as a Markov Decision Process with formal theoretical justification (Proposition 4.1) showing the distillation-aware reward implicitly minimizes irrelevant information while preserving task-relevant information, combined with structure-aware adaptation using graph spectral fingerprints that enable efficient transfer across domains.", "ai_categories": ["Tool Use and Code Generation", "Agent Benchmarking and Evaluation", "Reinforcement Learning for LLMs"]}, {"paper_id": "4093957", "score": "45", "title": "Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation", "authors": "Qijiong Liu, Jieming Zhu, Lu Fan, Kun Wang, Hengchang Hu, Wei Guo, Yong Liu, Xiao-Ming Wu", "session_type": "SD-5-5200", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2503.05493", "relevant_to_users": "4", "read_by_users": "8", "topics": "", "key_findings": "The paper reveals a significant performance gap where conventional recommender systems (collaborative filtering) substantially outperform LLMs in traditional ranking tasks, challenging assumptions about LLM superiority. However, LLMs demonstrate clear advantages in reasoning-based tasks, contextual understanding, and generating explanations that conventional systems cannot replicate. The research provides the first comprehensive empirical comparison establishing quantitative baselines across diverse recommendation scenarios, showing LLMs struggle with sequential patterns and implicit feedback that traditional systems handle efficiently.", "description": "This study conducts a systematic comparative evaluation between Large Language Models and conventional recommendation systems across multiple datasets and scenarios. The research assesses both approaches on ranking accuracy, reasoning capabilities, and explanation generation to determine where each paradigm excels and provides actionable insights about deployment strategies.", "key_contribution": "The paper delivers a comprehensive evaluation framework that empirically quantifies LLM performance versus traditional recommenders across diverse tasks, providing the first systematic comparison with quantitative baselines. It identifies specific areas where LLMs currently underperform despite their general-purpose capabilities while highlighting their strengths in interpretability and contextual reasoning.", "novelty": "Unlike prior work that relied on isolated studies favoring one paradigm, this research addresses the lack of comprehensive empirical comparisons through a multi-faceted evaluation protocol. The novel approach examines not only ranking metrics but also explanation quality and contextual reasoning, revealing that LLMs excel in interpretability while traditional systems dominate pure recommendation accuracy. This provides a nuanced understanding of the trade-offs rather than declaring one approach universally superior.", "ai_categories": ["Agent Benchmarking and Evaluation", "Domain-Specific Applications"]}, {"paper_id": "4222209", "score": "45", "title": "TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning", "authors": "Andreas Auer, Patrick Podest, Daniel Klotz, Sebastian B√É¬∂ck, G√É¬ºnter Klambauer, Sepp Hochreiter", "session_type": "SD-6-2404", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/1608428ae182161621e0d5c0fae3d6b608a7acaa.pdf", "relevant_to_users": "3", "read_by_users": "6", "topics": "", "key_findings": "TiRex achieves state-of-the-art zero-shot time series forecasting by leveraging xLSTM (extended LSTM) architecture with 35M parameters, outperforming significantly larger models including Chronos-Bolt (200M), TimesFM (500M), Moirai, and TabPFN-TS on HuggingFace benchmarks GiftEval and Chronos-ZS. The model demonstrates superior performance across both short- and long-term forecasts while being 11√ó faster than TimesFM and 2176√ó faster than TabPFN-TS. The paper introduces Contiguous Patch Masking (CPM), a novel training strategy that enables xLSTM to maintain strong performance on both short and long horizons simultaneously by treating masked patches as missing values during training.", "description": "TiRex addresses the gap between recurrent models and in-context learning for time series forecasting by combining xLSTM's state-tracking capabilities with transformer-like zero-shot generalization. The model uses sLSTM (scalar LSTM) cells to maintain explicit state-tracking for uncertainty propagation across extended horizons, enabling accurate probabilistic forecasts without reinitializing at each prediction step.", "key_contribution": "The main innovation is bridging recurrent architectures with strong in-context learning through xLSTM, specifically using sLSTM cells that preserve true recurrent state-tracking while achieving competitive few-shot learning. The CPM training strategy is critical for unlocking this capability, allowing the model to excel at both short and long horizon forecasts without the performance trade-offs seen in naive multi-patch or purely autoregressive training approaches.", "novelty": "Unlike transformer-based forecasting models (Chronos, TimesFM, Moirai) that lack persistent state-tracking and reinitialize probabilistic forecasts at each patch, TiRex maintains continuous uncertainty propagation through xLSTM's recurrent pathway, addressing transformers' documented shortcomings in time series tasks. The CPM masking strategy is novel in that it trains the model to handle missing consecutive patches as part of the input, mirroring multi-patch inference structure and enabling stable long-horizon predictions without sacrificing short-term accuracy. This contrasts with existing approaches where naive multi-patch training degrades short-term performance and standard autoregressive training harms long-term forecasting.", "ai_categories": ["Model Efficiency and Optimization", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4244850", "score": "44", "title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation", "authors": "Xinyu Yang, Yuwei An, Hongyi Liu, Tianqi Chen, Beidi Chen", "session_type": "SD-3-3704", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/5f50a250befd3553dd40112c1a440f86b36737da.pdf", "relevant_to_users": "1", "read_by_users": "11", "topics": "", "key_findings": "Multiverse introduces the first open-source non-autoregressive model to achieve competitive performance with leading autoregressive LLMs (54% on AIME 2024, 46% on AIME 2025) while delivering up to 2√ó inference speedup. The model discovers that over 98% of Chain-of-Thought examples contain parallelizable branches that traditional autoregressive models cannot exploit. The complete ecosystem including data curation pipeline, model weights, specialized engine, and training recipes has been open-sourced, enabling rapid transfer from pre-trained AR models with just 3 hours of fine-tuning.", "description": "Multiverse is a generative language model that internalizes a MapReduce paradigm to enable natively parallel generation. The system uses three stages‚ÄîMap (adaptive task decomposition), Process (parallel subtask execution), and Reduce (lossless result synthesis)‚Äîcombined with novel components including Multiverse Curator for automated data conversion, Multiverse Attention for separating parallel branches while maintaining causal compatibility, and Multiverse Engine for dynamic switching between sequential and parallel generation modes.", "key_contribution": "The main innovation is enabling language models to autonomously decide when and how to parallelize generation rather than imposing predetermined decomposition patterns. This is achieved through a specialized attention mechanism and control tags that allow models to dynamically switch between sequential and parallel generation modes, combined with an automated LLM-assisted pipeline that converts existing sequential reasoning chains into structured parallel training data without expensive human annotation.", "novelty": "Unlike previous non-autoregressive models that apply parallelism uniformly and struggle with logical dependencies, Multiverse preserves sequential reasoning where needed through its structured Map-Process-Reduce framework while explicitly exploiting parallelizable branches. The key difference from prior work is that the model itself learns to identify and decide parallelization strategies rather than having them externally imposed. The modified attention mechanism maintains compatibility with causal attention, enabling rapid fine-tuning from pre-trained autoregressive models (3 hours vs. complete retraining), which addresses the prohibitive training costs that limited previous non-AR approaches.", "ai_categories": ["Reasoning and Test-Time Compute", "Model Efficiency and Optimization"]}, {"paper_id": "4244825", "score": "44", "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "authors": "Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tieniu Tan", "session_type": "SD-4-4815", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/0da039d9be67a74526e2ce4408db4cde8bdca6f5.pdf", "relevant_to_users": "5", "read_by_users": "29", "topics": "", "key_findings": "The paper introduces a 'drawing to reason in space' paradigm that enables vision-language models to perform spatial reasoning through elementary drawing operations (bounding boxes, auxiliary lines) rather than text-only reasoning. ViLaSR achieves an average 18.4% improvement across diverse spatial reasoning benchmarks, with particularly strong results in maze navigation (98.2% accuracy, +64.5% improvement). The three-stage training framework combining cold-start training, reflective rejection sampling, and reinforcement learning enables the model to develop self-correction behaviors and iterative visual manipulation capabilities.", "description": "This paper addresses the limitation of vision-language models in spatial reasoning by enabling them to reason through visual drawing operations in the image space. ViLaSR decomposes spatial problems into interpretable steps combining natural language reasoning, executable drawing operations, and observed visual results, mirroring human mental visualization strategies.", "key_contribution": "The main contribution is the 'drawing to reason in space' paradigm that equips LVLMs with intrinsic drawing capabilities (bounding boxes, auxiliary lines) for direct visual manipulation during spatial reasoning. This is coupled with a three-stage training framework (cold-start training, reflective rejection sampling, and GRPO reinforcement learning) that enables self-reflection and iterative correction behaviors.", "novelty": "Unlike prior work that relies on external black-box perception tools and text-based reasoning that loses visual detail, ViLaSR enables intrinsic visual manipulation without external tool dependencies. It addresses the limitation of reasoning data based on human priors by using reflective rejection sampling to reinforce self-correction behaviors. The approach fundamentally differs from text-centric methods by making spatial relationships explicit and revisable through direct visual interaction, allowing the model to iteratively refine its spatial understanding through observable drawing operations.", "ai_categories": ["Spatial and Physical Reasoning", "Vision-Language-Action Models", "Reasoning and Test-Time Compute"]}, {"paper_id": "4230725", "score": "43", "title": "Abstract Counterfactuals for Language Model Agents", "authors": "Edoardo Pona, Milad Kazemi, Yali Du, David Watson, Nicola Paoletti", "session_type": "", "session_time": "", "session_location": "Foyer", "pdf_url": "https://openreview.net/pdf/c39308d32f99302b4eb8a97c6d7eae5b2e2c4466.pdf", "relevant_to_users": "1", "read_by_users": "3", "topics": "", "key_findings": "The paper introduces Abstract Counterfactuals (ACF), a framework that enables meaningful counterfactual reasoning for LM agents by operating at the level of high-level semantic features rather than tokens. Experiments on MACHIAVELLI, biography generation, and Reddit comments show that ACF produces consistent and meaningful counterfactuals while minimizing undesired side effects of token-level methods. The approach achieves higher abstraction consistency than token-level counterfactuals and significantly decreases the rate of change in abstraction values, making it more suitable for evaluating and analyzing LM agents in open-ended environments.", "description": "This paper addresses the challenge of applying counterfactual inference to language model agents, which is difficult due to their open-ended action spaces where actions are implicit in output strings. The authors introduce Abstract Counterfactuals, a framework that emphasizes high-level characteristics of actions and interactions within an environment, enabling counterfactual reasoning tailored to user-relevant features rather than individual tokens.", "key_contribution": "The main contribution is the Abstract Counterfactuals framework that performs counterfactual inference at the level of semantic abstractions (high-level action characteristics) rather than tokens, then maps the resulting counterfactual abstraction back into the action space. This approach does not require white-box access to LLM internals and supports both unsupervised abstraction discovery and supervised expert-defined categories.", "novelty": "Unlike existing work that focuses on token-level counterfactuals, this paper addresses the fundamental inadequacy of token-level approaches for LM agents whose actions are implicit in strings and whose token meanings shift with context. The key innovation is moving counterfactual reasoning to a semantic abstraction layer that captures user-relevant features, overriding the token-level generation mechanism while maintaining consistency. This addresses the problem of biased or meaningless counterfactuals that arise from token-level reasoning in open-ended action spaces.", "ai_categories": ["Agent Benchmarking and Evaluation", "Agent Safety and Security"]}, {"paper_id": "4336530", "score": "43", "title": "OpenCUA: Open Foundations for Computer-Use Agents", "authors": "Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, ..., Tao Yu", "session_type": "SD-3-1509", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/eb1bd0238abbc386303352dba1049a4d5d1fec83.pdf", "relevant_to_users": "3", "read_by_users": "4", "topics": "", "key_findings": "OpenCUA establishes new state-of-the-art for open-source computer-use agents with OpenCUA-72B achieving 45.0% on OSWorld-Verified. It releases the first large-scale CUA dataset (AgentNet) with 22.5K annotated tasks spanning 3 operating systems and 200+ applications, plus a cross-platform annotation tool (AgentNetTool) for capturing human demonstrations. The work demonstrates effective scaling through reflective Chain-of-Thought reasoning and provides an offline evaluation benchmark (AgentNetBench) for stable, environment-free testing.", "description": "OpenCUA is a comprehensive open-source framework for developing computer-use agents (CUAs) that automate diverse computer tasks through vision-language models. It provides annotation infrastructure, large-scale training data, a novel training pipeline with reflective reasoning, and open foundation models across multiple scales (7B, 32B, 72B parameters).", "key_contribution": "The main contribution is creating the first complete open-source ecosystem for computer-use agents, including: AgentNetTool (cross-platform annotation tool), AgentNet (first large-scale CUA dataset with 22.5K tasks), a reflective CoT training pipeline, and state-of-the-art open foundation models.", "novelty": "Unlike previous proprietary or limited CUA systems, OpenCUA addresses the accessibility barrier by fully open-sourcing all components‚Äîannotation tools, datasets, code, and models. It introduces reflective long Chain-of-Thought reasoning where generator and reflector iteratively verify reasoning between observations and actions, enabling better scaling. The work also provides AgentNetBench for offline evaluation with multiple valid action options per step, solving the instability and cost issues of live environment testing.", "ai_categories": ["Web and Computer-Use Agents", "Agent Benchmarking and Evaluation", "Vision-Language-Action Models"]}, {"paper_id": "4077513", "score": "43", "title": "Scalable Best-of-N Selection for Large Language Models via Self-Certainty", "authors": "Zhewei Kang, Xuandong Zhao, Dawn Song", "session_type": "SD-4-5204", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/141a8955a8007083b8b3068692459c467c444619.pdf", "relevant_to_users": "13", "read_by_users": "30", "topics": "", "key_findings": "The paper demonstrates that LLMs' internal confidence scores (self-certainty) can effectively rank multiple generations for best-of-N selection without external reward models. This approach provides a scalable, computationally efficient alternative that eliminates the need for separate ranking infrastructure while maintaining competitive performance across multiple benchmarks. The method shows meaningful quality improvements using only generation probabilities, making it practical for production deployment.", "description": "This work addresses the challenge of selecting high-quality outputs from multiple LLM generations by proposing self-certainty-based ranking. Instead of requiring additional reference models or external reward systems, the method leverages the model's own confidence scores derived from generation probabilities to identify superior outputs at scale.", "key_contribution": "The paper introduces a practical method that uses the model's intrinsic confidence estimates to rank and select superior generations without requiring additional models or human feedback, enabling cost-effective best-of-N selection with minimal computational overhead.", "novelty": "Unlike prior best-of-N approaches that require external verifiers, reward models, or separate ranking systems, this work exploits intrinsic model confidence signals for selection decisions. The key innovation is demonstrating that self-certainty alone‚Äîderived directly from generation probabilities‚Äîcan effectively identify superior outputs, addressing the infrastructure bottleneck and computational cost of deploying separate ranking systems in production environments.", "ai_categories": ["Reasoning and Test-Time Compute", "Model Efficiency and Optimization"]}, {"paper_id": "4062245", "score": "42", "title": "A-Mem: Agentic Memory for LLM Agents", "authors": "Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang", "session_type": "SD-4-210", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/518e506d098a6c821c7e4ae97d1368f374fddac9.pdf", "relevant_to_users": "16", "read_by_users": "24", "topics": "", "key_findings": "A-MEM achieves 80% improvement over baselines on multi-hop reasoning tasks (45.85 vs 25.52 F1 on GPT-4o-mini) while using 13√ó fewer tokens (1,200-2,500 vs 16,900). The system demonstrates that applying Zettelkasten principles with LLM-driven decision-making creates interconnected memory networks that significantly outperform static memory architectures across six foundation models on the LoCoMo benchmark. The agentic approach enables memories to autonomously evolve, update historical contexts, and establish dynamic semantic connections rather than relying on fixed schemas.", "description": "This paper introduces A-MEM, an agentic memory system for LLM agents that dynamically organizes and evolves memories using principles from the Zettelkasten note-taking method. The system creates interconnected knowledge networks through LLM-driven note construction, semantic linking, and memory evolution, enabling agents to maintain and leverage historical experiences across long-term interactions and complex multi-hop reasoning tasks.", "key_contribution": "The main innovation is making memory systems \"agentic\" by allowing LLMs to autonomously decide which memories to link, when to update existing memories, and how to organize information dynamically. This replaces fixed memory operations and predefined schemas with flexible, content-driven memory organization that adapts to diverse tasks while achieving superior performance with dramatically reduced token usage.", "novelty": "Unlike existing systems like MemGPT (static cache hierarchies) and MemoryBank (predefined forgetting-curve schemas), A-MEM enables memories to exist in multiple semantic clusters and autonomously evolve their contextual representations when new information arrives. The key innovation is using LLMs not just for memory retrieval, but for making structural decisions about memory organization, connection establishment, and historical memory updates. This addresses the fundamental limitation of fixed memory structures that cannot adapt to task-specific requirements or leverage complex semantic relationships between experiences.", "ai_categories": ["Memory and Context Management", "Planning and Decision Making", "Tool Use and Code Generation"]}, {"paper_id": "4224994", "score": "42", "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models", "authors": "Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, Yi Dong", "session_type": "SD-5-1914", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/75e49a843f0b6d00c0584a776fad3bea93496c83.pdf", "relevant_to_users": "15", "read_by_users": "25", "topics": "", "key_findings": "ProRL demonstrates that prolonged RL training (2000+ steps vs typical hundreds) genuinely expands reasoning capabilities beyond what exists in base models, even with extensive sampling. The resulting Nemotron-Research-Reasoning-Qwen-1.5B achieves 100% pass rates on tasks where the base model produces zero correct solutions, with improvements of +15.7% in math, +14.4% in code, +25.9% in STEM, and +54.8% in logic puzzles. The work proves RL training scales effectively with increased compute, uncovering novel reasoning strategies not accessible to base models.", "description": "ProRL is a prolonged reinforcement learning methodology for training LLMs that incorporates KL divergence control with periodic reference policy resetting and diverse task training (136K examples across math, code, STEM, logic). It addresses the debate of whether RL truly expands reasoning capabilities or merely optimizes existing knowledge, demonstrating that extended RL training reveals novel reasoning strategies inaccessible to base models.", "key_contribution": "The main innovation is demonstrating that significantly extended RL training (2000+ steps) with reference policy resetting and task diversity genuinely unlocks new reasoning capabilities rather than just optimizing existing knowledge. This produces the world's best 1.5B parameter reasoning model that achieves perfect performance on tasks where the base model completely fails.", "novelty": "Unlike prior work that terminated RL training after hundreds of steps and focused on specialized domains, ProRL extends training duration by 10x while maintaining diversity across multiple reasoning domains. It addresses the entropy collapse problem through periodic hard resets of reference policies, preventing premature convergence. Most critically, it empirically refutes recent claims that RL merely amplifies existing capabilities, showing measurable emergence of entirely new reasoning strategies that don't exist in the base model's distribution even under extensive sampling.", "ai_categories": ["Reinforcement Learning for LLMs", "Reasoning and Test-Time Compute", "Mathematical and Logical Reasoning"]}, {"paper_id": "4232570", "score": "42", "title": "Does Thinking More Always Help? Mirage of Test-Time Scaling in Reasoning Models", "authors": "Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Yifu Lu, Mengdi Wang, Dinesh Manocha, Furong Huang, Mohammad Ghavamzadeh, Amrit Singh Bedi", "session_type": "SD-5-5510", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/9a4ddca48299d0fb623da4e9a0093d29392e48a2.pdf", "relevant_to_users": "7", "read_by_users": "13", "topics": "", "key_findings": "The paper reveals a critical 'overthinking' phenomenon where test-time scaling (extending reasoning traces) initially improves accuracy but then degrades performance, with drops of up to 17 percentage points observed. Through theoretical analysis and experiments across 3 models and 3 benchmarks, the authors show this non-monotonic behavior stems from increased output variance creating an illusion of improved reasoning while undermining precision. They propose 'parallel thinking' as an alternative‚Äîgenerating multiple independent reasoning paths and using majority voting‚Äîwhich achieves up to 20% higher accuracy than sequential extended thinking within the same token budget.", "description": "This paper investigates whether extending reasoning traces at test-time through prompts like 'Think more' genuinely improves LLM performance. The authors discover that while accuracy initially rises with longer thinking, it eventually degrades due to increased output variance‚Äîa 'mirage' effect where test-time scaling appears beneficial but ultimately undermines performance.", "key_contribution": "The paper's main contribution is identifying and explaining the 'overthinking' phenomenon where extended test-time computation degrades performance beyond a critical threshold, providing a probabilistic framework showing how increased variance creates an illusion of improvement, and proposing parallel thinking (Best-of-N style sampling with majority voting) as a superior alternative that achieves 20% higher accuracy.", "novelty": "Unlike prior work that assumes more test-time compute always helps, this paper systematically documents and explains performance degradation from excessive reasoning. It addresses the limitation of sequential scaling approaches by providing mathematical intuition via variance analysis and demonstrates that the benefits of test-time scaling are often illusory. The proposed parallel thinking approach fundamentally differs from sequential extension by prioritizing consistency across independent paths rather than depth of single-path reasoning.", "ai_categories": ["Reasoning and Test-Time Compute", "Agent Benchmarking and Evaluation", "Model Efficiency and Optimization"]}, {"paper_id": "4432673", "score": "42", "title": "Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost", "authors": "Runzhe Zhan, Zhihong Huang, Xinyi Yang, Lidia S. Chao, Min Yang, Derek F. Wong", "session_type": "SD-6-1915", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/496a103d9eacba8143b7d8a13098936a351a1a81.pdf", "relevant_to_users": "2", "read_by_users": "5", "topics": "", "key_findings": "First systematic analysis of Large Reasoning Models (LRMs) as MT evaluators reveals critical inefficiencies: LRMs 'overthink' simple instances, require model-size-dependent evaluation materials (32B models prefer reference-free while 7-8B need references), and suffer from scoring overestimation. The proposed ThinMQM approach trains LRMs on synthetic human-like evaluation trajectories, reducing thinking budgets by ~35x while achieving substantial improvements (+8.7 points for R1-Distill-Qwen-7B, +5.9 for R1-Distill-Llama-8B, +3.9 for QwQ-32B on WMT24 benchmarks).", "description": "This paper investigates whether Large Reasoning Models can effectively evaluate machine translation quality under the MQM framework. The authors identify fundamental inefficiencies in LRM-based evaluation (overthinking, material misalignment, scoring issues) and propose ThinMQM, which calibrates LRM reasoning through training on synthetic human-like evaluation trajectories. Experiments on WMT24 benchmarks demonstrate dramatic efficiency gains (35x reduction in thinking tokens) with concurrent performance improvements across 7B-32B model scales.", "key_contribution": "ThinMQM (Thinking-calibrated MQM), a novel approach that trains LRMs on synthetic evaluation trajectories mimicking human MQM annotation workflows, structuring reasoning chains to align with two-phase human evaluation (error span annotation followed by rule-based scoring). This calibration achieves 35x computational reduction while improving correlation with human judgments by up to 8.7 points.", "novelty": "First work to systematically analyze LRMs as translation evaluators, revealing model-specific challenges distinct from traditional LLMs (e.g., 'overthinking' problem, size-dependent material preferences contradicting prior LLM findings). Unlike prior prompt engineering approaches, introduces trajectory-based fine-tuning to calibrate internal reasoning processes, using Shapley Values to quantify evaluation material contributions. Addresses the gap between unconstrained chain-of-thought generation and task-aligned human evaluation processes.", "ai_categories": ["Reasoning and Test-Time Compute", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4205605", "score": "41", "title": "Thinkless: LLM Learns When to Think", "authors": "Gongfan Fang, Xinyin Ma, Xinchao Wang", "session_type": "SD-2-4015", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/69d5f7c0fe7ed54d42a4a1908d50a94bc062b721.pdf", "relevant_to_users": "5", "read_by_users": "13", "topics": "", "key_findings": "Thinkless demonstrates that LLMs can learn to adaptively decide when to use expensive chain-of-thought reasoning versus direct inference, achieving 50-90% reduction in long-chain thinking usage on mathematical benchmarks while maintaining or improving accuracy. The work introduces Decoupled Group Relative Policy Optimization (DeGRPO) to train models with control tokens (<short> and <think>) that enable dynamic reasoning mode selection based on problem complexity.", "description": "This paper presents Thinkless, a learnable framework that trains LLMs to adaptively select between short-form direct answers and long-form chain-of-thought reasoning based on task complexity. The system uses reinforcement learning with two control tokens to enable models to dynamically allocate computational resources, avoiding expensive reasoning on simple problems while engaging deep thinking for complex ones.", "key_contribution": "The main contribution is the Decoupled Group Relative Policy Optimization (DeGRPO) algorithm that trains LLMs to learn when to think deeply versus answer directly. This decoupled approach separates control token selection (reasoning mode) from response generation, enabling stable training and preventing the collapse issues observed in vanilla GRPO.", "novelty": "Unlike prior work that applies chain-of-thought reasoning uniformly to all queries, Thinkless addresses the inefficiency of treating all problems equally. It introduces the first learnable framework for adaptive reasoning allocation, where the model itself learns to assess problem difficulty and route instances to appropriate computational strategies. The DeGRPO algorithm uniquely decouples reasoning mode selection from answer generation, solving training stability issues in reinforcement learning for adaptive test-time compute.", "ai_categories": ["Reasoning and Test-Time Compute", "Model Efficiency and Optimization", "Reinforcement Learning for LLMs"]}, {"paper_id": "4202367", "score": "41", "title": "Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL", "authors": "Songjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, Dongbin Zhao", "session_type": "SD-6-5003", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/dc89a6b0221240f7e120c7bd39c116d57711eec4.pdf", "relevant_to_users": "7", "read_by_users": "14", "topics": "", "key_findings": "The paper discovers that inserting a simple ellipsis ('...') into prompts can stochastically trigger thinking or no-thinking modes in R1-style models, revealing latent controllability. AutoThink, a three-stage RL framework, achieves 6.4% relative accuracy improvement while reducing token usage by 52% on DeepSeek-R1-Distill-Qwen-1.5B. The model learns to adaptively allocate reasoning effort based on problem difficulty‚Äîharder problems (AIME, Olympiad) trigger significantly higher thinking rates than easier ones (MATH500), demonstrating genuine difficulty-aware behavior rather than uniform token reduction.", "description": "This paper addresses the overthinking problem in large reasoning models (LRMs) like DeepSeek-R1, where models generate excessive reasoning tokens even on simple problems. The authors propose AutoThink, a multi-stage reinforcement learning framework that teaches models to dynamically decide when to engage explicit reasoning versus providing direct answers, achieving substantial token savings while maintaining or improving accuracy across mathematical and general reasoning benchmarks.", "key_contribution": "AutoThink introduces a three-stage RL training paradigm: (1) preventing mode collapse via batch-level reward balancing, (2) reinforcing reliability within each reasoning mode, and (3) pruning redundancy through length-aware rewards. The framework uniquely combines lightweight prompt-based activation (the ellipsis discovery) with learned adaptive control that responds to problem difficulty, unlike previous hard-coded prompting or uniform pruning approaches.", "novelty": "Unlike prior work that relies on hard-coded prompting (Claude 3.7, Qwen3) or uniform pruning methods (ThinkPrune, ShorterBetter) that indiscriminately shorten reasoning regardless of difficulty, AutoThink learns difficulty-aware adaptive reasoning through progressive RL training. It addresses the critical limitation that prompting alone cannot achieve adaptive behavior‚Äîthe paper empirically demonstrates that explicit instructions to consider difficulty produce flat thinking-rate distributions without RL. The discovery that a simple ellipsis can unlock controllable reasoning modes is also novel, providing a minimalist activation mechanism that previous supervised fine-tuning approaches requiring labeled reasoning pairs could not achieve.", "ai_categories": ["Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs", "Model Efficiency and Optimization"]}, {"paper_id": "4442860", "score": "41", "title": "Many Minds, One Goal: Time Series Forecasting via Sub-task Specialization and Inter-agent Cooperation", "authors": "Qihe Huang, Zhengyang Zhou, Yangze Li, Kuo Yang, Binwu Wang, Yang Wang", "session_type": "SD-6-2300", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/703b1261fdea6ed1048e2c011db26cfa5a8ecae0.pdf", "relevant_to_users": "2", "read_by_users": "3", "topics": "", "key_findings": "", "description": "", "key_contribution": "", "novelty": "", "ai_categories": []}, {"paper_id": "3844639", "score": "40", "title": "Fast Inference for Augmented Large Language Models", "authors": "Rana Shahout, Cong Liang, Shiji Xin, Qianru Lao, Yong Cui, Minlan Yu, Michael Mitzenmacher", "session_type": "SD-1-208", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/466ae7c6d881581805533176114b2bde5a683e8d.pdf", "relevant_to_users": "3", "read_by_users": "12", "topics": "", "key_findings": "The paper introduces LAMPS (also called MARS in later versions), a novel scheduling framework for augmented LLMs that addresses the critical insight that request size (in tokens) no longer correlates with execution time when API calls are involved. By explicitly modeling memory-time consumption profiles and API handling strategies, LAMPS achieves 27%-85% improvements in end-to-end latency and 4%-96% reductions in time-to-first-token compared to existing augmented-LLM systems. The framework unifies scheduling decisions with API call management, treating them as a joint optimization problem rather than separate concerns.", "description": "This paper addresses the scheduling challenges in augmented Large Language Models that integrate external data sources through API calls. Traditional size-based scheduling algorithms like Shortest Job First fail in this context because token count no longer predicts execution time when API interactions and KV cache memory constraints are involved.", "key_contribution": "LAMPS introduces a memory-aware, predictive scheduling framework that jointly optimizes request prioritization and API call handling strategies. The system ranks requests based on their predicted memory-time consumption profiles and strategically manages KV cache during API calls, incorporating starvation prevention and overhead reduction mechanisms.", "novelty": "Unlike prior work that treated API call handling (preserving, discarding, or swapping memory) and request scheduling as separate concerns, LAMPS unifies these into a single scheduling framework that explicitly models how different API handling strategies affect overall memory consumption and completion time. This addresses the fundamental limitation that traditional size-based schedulers become ineffective in memory-constrained augmented LLM systems where execution time is decoupled from request size due to variable API call latencies and memory management overhead.", "ai_categories": ["Tool Use and Code Generation", "Model Efficiency and Optimization"]}, {"paper_id": "4213161", "score": "40", "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools", "authors": "Minki Kang, Jongwon Jeong, Seanie Lee, Jaewoong Cho, Sung Ju Hwang", "session_type": "SD-3-2003", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/c3ead64f6c3fd03156d79bf7aa5185204700b2a2.pdf", "relevant_to_users": "10", "read_by_users": "14", "topics": "", "key_findings": "The paper demonstrates that small language models (0.5B-3B parameters) can be effectively distilled from large LLM agents to achieve competitive performance with next-tier larger models. Two key innovations enable this: First-Thought Prefix (FTP) improves teacher trajectory quality by aligning agentic behavior with pre-trained patterns, and Self-Consistent Action Generation (SAG) enhances test-time robustness by filtering invalid code actions through majority voting. The approach achieves remarkable efficiency gains, with distilled 1.5B agents matching 3B CoT models and 3B agents exceeding 7B models on average across 8 benchmarks.", "description": "This paper presents Agent Distillation, a framework for transferring complete task-solving behavior (not just reasoning) from large language model agents into small models equipped with retrieval and code execution tools. The method teaches small models to replicate interactive think-act-observe trajectories rather than static reasoning chains, enabling adaptive tool use for handling unfamiliar facts and complex computations.", "key_contribution": "The main contribution is shifting from static chain-of-thought distillation to teaching small models agentic behavior with tool use capabilities. The FTP method elegantly bridges instruction-tuning with agent requirements, while SAG substantially reduces execution errors through self-consistent sampling and filtering.", "novelty": "Unlike prior work that distills fixed reasoning traces, this approach teaches small models when and how to use tools adaptively rather than memorizing outputs. It addresses the limitation that CoT-distilled models struggle with unfamiliar facts and precise computations by enabling interactive retrieval and code execution. The work also demonstrates cross-model generalization (Qwen, Llama, Phi families) using open-source teachers, making it more reproducible than prior closed-source distillation approaches.", "ai_categories": ["Tool Use and Code Generation", "Model Efficiency and Optimization", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4178039", "score": "40", "title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks", "authors": "Vishnu Sarukkai, Zhiqiang Xie, Kayvon Fatahalian", "session_type": "SD-6-3400", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/05d0804df1c396da814a29970ae7d37c40a1b84e.pdf", "relevant_to_users": "5", "read_by_users": "23", "topics": "", "key_findings": "The paper demonstrates that LLM agents performing sequential decision-making tasks achieve substantial performance improvements when they generate their own in-context examples from experience, rather than relying on fixed pre-defined demonstrations. Consistent improvements were shown across three diverse benchmarks: ALFWorld (household tasks), InterCode-SQL (database interaction), and WordCraft (text-based games). This approach enables agents to create more relevant and adaptive contextual information that better matches their current task requirements.", "description": "This paper investigates how large language model agents can improve their performance on sequential decision-making tasks by dynamically generating their own in-context learning examples from their experience, rather than using predetermined demonstrations. The authors validate their approach across multiple benchmark environments and show that self-generated examples lead to better task performance than static baselines.", "key_contribution": "The main contribution is a framework that enables LLM agents to dynamically create in-context examples from their own task trajectories and integrate them into prompts for improved sequential decision-making. This eliminates the dependency on curated demonstration datasets while providing task-specific adaptation without requiring model retraining.", "novelty": "Unlike prior in-context learning research that depends on high-quality, pre-constructed examples or expert demonstrations, this work enables agents to bootstrap from their own experience. It addresses the significant limitation of requiring curated datasets by allowing agents to generate task-specific examples adaptively. The approach provides a more practical deployment pathway for real-world applications where obtaining representative demonstrations is difficult or expensive.", "ai_categories": ["Planning and Decision Making", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4336500", "score": "39", "title": "Scaling Up Active Testing to Large Language Models", "authors": "Gabrielle Berrada, Jannik Kossen, Freddie Bickford Smith, Muhammed Razzak, Yarin Gal, Tom Rainforth", "session_type": "SD-5-110", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/0e4fdfd137ed152835e93d1c72c71b4217f5dfa0.pdf", "relevant_to_users": "2", "read_by_users": "12", "topics": "", "key_findings": "The paper demonstrates that active testing can be scaled to LLMs through three key cost-saving strategies: (1) surrogate models can be constructed cheaply using in-context learning without requiring retraining, (2) surrogate models can be smaller than the target LLM being evaluated, and (3) effective data acquisition decisions can be made without repeatedly querying the expensive target model. A new bootstrap estimator provides real-time feedback on evaluation error within individual runs. Experiments across sentiment analysis, hate speech detection, and knowledge benchmarks show substantially more accurate LLM evaluations compared to random sampling, reducing evaluation costs while maintaining assessment validity.", "description": "This paper addresses the computational cost barrier preventing active testing from being applied to large language models. It introduces a label-efficient evaluation framework that strategically selects the most informative test cases for LLM evaluation, using lightweight surrogate models with in-context learning to guide test case acquisition without requiring expensive repeated queries to the target model.", "key_contribution": "The main innovation is decoupling the surrogate model from the target LLM and leveraging in-context learning for cost-effective active testing. Unlike traditional active learning that requires model retraining, this approach uses static, lightweight surrogates that don't need updates during the testing loop, making sophisticated evaluation accessible even with limited computational resources.", "novelty": "Previous active testing and active learning methods required tight coupling between surrogate and target models with continuous updates, making them computationally prohibitive for LLMs. This work removes that barrier by showing that in-context learning enables effective surrogate models that are smaller, static, and don't require target model predictions during acquisition. This fundamentally changes the cost structure of LLM evaluation from O(n) expensive queries to strategically selected subsets, addressing the scalability limitations that prevented prior active testing from being applied to foundation models.", "ai_categories": ["Agent Benchmarking and Evaluation", "Model Efficiency and Optimization"]}, {"paper_id": "4202496", "score": "38", "title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation", "authors": "Zhenwen Liang, Linfeng Song, Yang Li, TAO YANG, Haitao Mi, Dong Yu", "session_type": "SD-2-5401", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/350dc187c2833efc7cd93402a27fd709f790c12b.pdf", "relevant_to_users": "1", "read_by_users": "4", "topics": "", "key_findings": "MPS-Prover introduces a multi-perspective tree search mechanism that combines a learned critic model with three strategically designed heuristic rules (Tactic Effectiveness, Minimum Cases, and Shortest State) to diversify tactic selection and prevent search from getting trapped in unproductive states. The system also implements a data curation strategy that prunes approximately 40% of redundant training data without sacrificing performance, focusing the model on more complex reasoning patterns. It achieves state-of-the-art performance on challenging benchmarks like miniF2F and ProofNet, outperforming prior 7B parameter models while generating significantly shorter and more diverse proofs compared to existing stepwise and whole-proof methods.", "description": "MPS-Prover is an automated theorem proving system for formal mathematics in Lean 4 that advances stepwise theorem proving through multi-perspective search and intelligent data curation. The system evaluates each expansion step from multiple heuristic perspectives as well as a hierarchical tree-based critic model, preserving a diverse set of promising states and steering the prover around dead ends toward successful proofs.", "key_contribution": "The main contribution is the multi-perspective search mechanism that integrates a learned critic model (trained with hierarchical tree-based distance prediction) with three heuristic rules to overcome the limitations of traditional best-first search methods that rely solely on critic scores. Additionally, the work introduces an explicit data curation strategy that filters out 40% of redundant training examples to focus learning on complex reasoning patterns.", "novelty": "Unlike existing stepwise provers that suffer from biased search guidance and rely on single search methodologies based solely on critic model scores, MPS-Prover employs methodological pluralism by exploring multiple concurrent search perspectives (critic-based and heuristic-based). This addresses the problem of getting trapped in local minima or exploring unproductive tactic sequences. The explicit data curation rules also go beyond passive dataset collection used in prior work, strategically identifying and removing low-value training examples to improve generalization without requiring more data.", "ai_categories": ["Mathematical and Logical Reasoning", "Reasoning and Test-Time Compute", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4215801", "score": "38", "title": "Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model", "authors": "Tianle Li, Jihai Zhang, Yongming Rao, Yu Cheng", "session_type": "SD-3-4701", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/64ee56c0512b904e0aebcd799494fb96e7bdedba.pdf", "relevant_to_users": "6", "read_by_users": "33", "topics": "", "key_findings": "The paper reveals that current vision-language models (VLMs) fail to compositionally generalize across modalities and tasks despite strong individual task performance. RL-trained models consistently outperform supervised fine-tuning on compositional tasks, but still show significant gaps. The proposed RL-Ground method‚Äîcombining caption-before-reasoning prompting with progress-based intermediate rewards‚Äîsubstantially improves compositional performance (achieving 52.8% accuracy vs. much lower baselines) by enhancing visual-to-text alignment and grounding. The work demonstrates that explicit visual grounding is essential for compositional reasoning in VLMs.", "description": "This paper conducts a systematic study of compositional generalization in vision-language models trained with reinforcement learning, introducing ComPABench, a diagnostic benchmark that evaluates whether VLMs can compose capabilities across modalities (text-to-vision transfer) and tasks (skill integration) under out-of-distribution conditions. The study reveals fundamental limitations in current post-training strategies and proposes RL-Ground to address these gaps.", "key_contribution": "The main contribution is ComPABench, a controlled diagnostic benchmark for evaluating compositional generalization in VLMs, along with comprehensive empirical analysis revealing that current models struggle with compositional reasoning. The paper introduces RL-Ground, which combines caption-before-reasoning with progress-based rewards to improve visual grounding and compositional performance.", "novelty": "Unlike prior work that focused on individual task improvements in RL-enhanced VLMs, this paper is the first to systematically diagnose compositional failures across modalities and reasoning domains using synthetic tasks with controlled variables. It addresses the unexplored question of whether VLMs can coherently combine independently acquired skills, revealing that supervised fine-tuning causes catastrophic failures (94+ point drops) in cross-modal transfer and severe degradation (46% to 2%) in cross-task composition. The novel RL-Ground approach explicitly enforces visual grounding through intermediate rewards, demonstrating that visual-to-text alignment is the critical missing ingredient for compositional reasoning.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models", "Reasoning and Test-Time Compute"]}, {"paper_id": "4349313", "score": "37", "title": "Neither Valid nor Reliable? Investigating the Use of LLMs as Judges", "authors": "Khaoula Chehbouni, Mohammed Haddou, Jackie CK Cheung, Golnoosh Farnadi", "session_type": "SD-2-1313", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2508.18076", "relevant_to_users": "4", "read_by_users": "15", "topics": "", "key_findings": "LLMs used as judges exhibit significant validity and reliability issues, including inconsistent judgments that fail to correlate reliably with human evaluations. Position bias, prompt sensitivity, and other systematic artifacts persist despite attempts to improve prompting strategies, suggesting caution is warranted when deploying LLMs for evaluation work as they may produce misleading conclusions about system performance.", "description": "This paper examines whether Large Language Models can serve as reliable and valid judges for evaluating other systems' outputs in NLP tasks, investigating fundamental measurement challenges when using LLMs as evaluators through systematic empirical analysis.", "key_contribution": "The paper provides a comprehensive empirical investigation into the psychometric properties of LLM-based evaluation systems, applying classical measurement theory frameworks to systematically expose fundamental limitations in using LLMs as evaluators rather than proposing incremental improvements.", "novelty": "Unlike prior work that focused on improving LLM judging through better prompting techniques, this paper adopts a measurement science perspective grounded in psychometric frameworks from classical measurement theory to systematically examine validity and reliability. It addresses a critical gap by questioning whether LLM-as-judge is fundamentally sound rather than assuming it works and trying to optimize it.", "ai_categories": ["Agent Benchmarking and Evaluation"]}, {"paper_id": "4207664", "score": "37", "title": "Think Only When You Need with Large Hybrid-Reasoning Models", "authors": "Lingjie Jiang, Xun Wu, Shaohan Huang, Qingxiu Dong, Zewen Chi, Li Dong, Xingxing Zhang, Tengchao Lv, Lei Cui, Furu Wei", "session_type": "SD-2-3713", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/33fb886d1cda050a0c29d0bdee85176c1c3f7f31.pdf", "relevant_to_users": "2", "read_by_users": "10", "topics": "", "key_findings": "This paper introduces Large Hybrid-Reasoning Models (LHRMs), the first models capable of adaptively deciding when to engage in extended reasoning versus direct response generation. On AIME24, LHRMs achieve 10.3-13.6% improvements over baselines, and on general benchmarks (7B scale) show 50.2% gains on Alpaca and 93.4% on Arena-Hard. The two-stage training pipeline (Hybrid Fine-Tuning + Hybrid Group Policy Optimization) enables models to learn cross-domain thinking patterns, with thinking ratios decreasing for simpler queries while maintaining high accuracy. This addresses the critical 'overthinking problem' in existing LRMs like o1 and DeepSeek-R1 that waste computational resources on simple queries.", "description": "The paper presents Large Hybrid-Reasoning Models that adaptively determine whether to engage extended reasoning based on query complexity, mirroring human cognitive patterns. Using a two-stage training approach combining Hybrid Fine-Tuning (HFT) with 1.7M mixed examples and Hybrid Group Policy Optimization (HGPO) for reinforcement learning, the models learn to select optimal reasoning modes. Experiments on Qwen-2.5 (1.5B and 7B) across math, coding, and general tasks demonstrate superior performance over LLM and LRM baselines while significantly improving efficiency.", "key_contribution": "The main innovation is Hybrid Group Policy Optimization (HGPO), a novel RL algorithm that trains models to select between thinking and no-thinking modes through inter-group and intra-group reward signals. The paper also introduces Hybrid Accuracy (‚ÑãAcc) as a new metric measuring correct reasoning-mode selection, achieving 93.8% improvement over DPO baselines and demonstrating learned cross-domain generalization patterns.", "novelty": "Unlike existing LRMs that uniformly apply expensive reasoning to all queries, this work solves the 'overthinking problem' by training models to recognize when extended reasoning is necessary versus wasteful. Previous approaches either always reason (o1, DeepSeek-R1) or use static routing heuristics, while LHRMs learn dynamic mode selection through RL that generalizes across domains. The approach differs from mixture-of-experts by combining mode selection learning with response quality improvement within the same model, using human feedback patterns to identify intrinsic task difficulty rather than relying on predetermined computational paths.", "ai_categories": ["Reasoning and Test-Time Compute", "Model Efficiency and Optimization", "Reinforcement Learning for LLMs"]}, {"paper_id": "4215058", "score": "37", "title": "Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees", "authors": "Sangwoo Park, Matteo Zecchin, Osvaldo Simeone", "session_type": "SD-2-3402", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/42cf5e1bbaeccd15c151a00a71cb2d9ecef2aa6f.pdf", "relevant_to_users": "0", "read_by_users": "4", "topics": "", "key_findings": "The paper introduces R-AutoEval+, which provides finite-sample reliability guarantees on AI model evaluation while ensuring enhanced (or no worse) sample efficiency compared to conventional methods. The key innovation is an adaptive mechanism that dynamically tunes reliance on synthetic data from automated evaluators (like LLMs-as-judges), automatically reverting to conventional evaluation when the autoevaluator is insufficiently accurate. Experiments on LLM quantization optimization, prompt design, and test-time reasoning budget allocation demonstrate both reliability and efficiency gains, with the work recognized as a NeurIPS 2025 spotlight paper.", "description": "This paper addresses the challenge of efficiently evaluating multiple AI models (especially LLMs) when real-world empirical evaluations are costly. It introduces R-AutoEval+, a framework that adaptively combines synthetic data from automated evaluators with real-world data to provide finite-sample reliability guarantees while improving sample efficiency.", "key_contribution": "The main contribution is an adaptive construction mechanism that maintains multiple candidate reliance factors on synthetic data and uses exponential weights with e-value evidence to dynamically balance between synthetic and real-world data. This guarantees that sample complexity is at most as large as conventional methods while potentially achieving significant improvements when autoevaluators are accurate.", "novelty": "Unlike prior prediction-powered inference methods that suffer from increased sample complexity when autoevaluators are inaccurate, R-AutoEval+ adaptively tunes its reliance on synthetic data through a portfolio approach with multiple betting strategies. It addresses the limitation of existing methods that provide only asymptotic guarantees or lack explicit sample efficiency guarantees, offering provable finite-sample reliability combined with a theoretical guarantee that efficiency never degrades below conventional baselines (Theorem 3 shows sample complexity is bounded by the minimum of both baseline methods).", "ai_categories": ["Agent Benchmarking and Evaluation", "Model Efficiency and Optimization"]}, {"paper_id": "4230700", "score": "37", "title": "A Controllable Examination for Long-Context Language Models", "authors": "Yijun Yang, Zeyu Huang, Wenhao Zhu, Zihan Qiu, Fei Yuan, Jeff Pan, Ivan Titov", "session_type": "SD-3-1911", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.02921", "relevant_to_users": "4", "read_by_users": "9", "topics": "", "key_findings": "The paper introduces LongBio, a controllable benchmark revealing that long-context models exhibit position bias (inconsistent performance based on where information appears), experience performance degradation as context length increases, and display systematic failure patterns not captured by traditional benchmarks. The controllable methodology enables isolation of specific performance factors.", "description": "LongBio is a parametrizable benchmark framework for evaluating long-context language models through controllable examination. Unlike fixed datasets, it allows systematic variation of context length, information placement, and task difficulty to enable precise assessment of model capabilities and limitations across extended contexts.", "key_contribution": "The main innovation is the controllable examination methodology that provides granular experimental control over evaluation variables, enabling researchers to systematically isolate and understand specific factors affecting long-context performance rather than relying on static, confounded benchmark datasets.", "novelty": "This work differs from previous static long-context benchmarks by enabling parametric control over evaluation conditions, allowing decomposition of performance into interpretable factors. It addresses the limitation that existing benchmarks cannot isolate why models fail, providing a systematic framework to understand causal factors behind long-context performance degradation and position bias effects.", "ai_categories": ["Agent Benchmarking and Evaluation", "Memory and Context Management"]}, {"paper_id": "4432236", "score": "37", "title": "Teaching Language Models to Reason with Tools", "authors": "Chengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou Wang, Xiang Wang, ..., Dayiheng Liu", "session_type": "SD-4-1607", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/406d2cc67059647e62684368005578debbc188e9.pdf", "relevant_to_users": "5", "read_by_users": "21", "topics": "", "key_findings": "This paper demonstrates that large reasoning models can be trained to effectively integrate computational tools, achieving 4-8% accuracy improvements and 30-50% token reduction on mathematical reasoning tasks. It introduces CoRT (Code-Optimized Reasoning Training), a post-training framework that resolves the conflict between a model's internal probabilistic reasoning and external deterministic tool outputs. The work shows that with only 30 high-quality training samples using Hint-Engineering, models can learn to strategically interleave tool usage with internal reasoning, eliminating unproductive deliberation.", "description": "This paper addresses how to train large reasoning models to effectively use Code Interpreters for mathematical reasoning by resolving the tension between internal model reasoning and external tool outputs. It introduces CoRT, a framework combining Hint-Engineering data synthesis, supervised fine-tuning, rejection sampling, and reinforcement learning to teach models when and how to leverage computational tools.", "key_contribution": "The main contribution is CoRT (Code-Optimized Reasoning Training) and its Hint-Engineering data synthesis strategy, which strategically injects hints at optimal points in reasoning paths to teach models to interleave external tool usage with internal thinking, combined with multi-round optimization through rejection sampling and reinforcement learning.", "novelty": "Unlike previous tool-use approaches that struggle with 'unproductive deliberation' when models must choose between internal reasoning and external tools, this work uniquely addresses the fundamental conflict between probabilistic internal reasoning and deterministic tool outputs. The Hint-Engineering approach synthesizes training data that explicitly guides models on when and how to integrate tools at optimal reasoning points, requiring only 30 high-quality samples to achieve significant improvements. This represents a shift from generic tool-calling to strategic tool integration through targeted post-training.", "ai_categories": ["Tool Use and Code Generation", "Mathematical and Logical Reasoning", "Reasoning and Test-Time Compute"]}, {"paper_id": "4219587", "score": "37", "title": "SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts", "authors": "Yueh-Han Chen, Guy Davidson, Brenden Lake", "session_type": "SD-5-1104", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.21828", "relevant_to_users": "1", "read_by_users": "2", "topics": "", "key_findings": "SAGE-Eval reveals that LLMs struggle significantly with systematic generalization of safety facts across compositional variations. The benchmark exposes critical vulnerabilities where models fail to robustly apply safety knowledge to novel contexts, showing substantial performance gaps between seen and unseen safety scenarios. This indicates that current models may rely on pattern-matching rather than principled understanding of safety principles.", "description": "This paper introduces SAGE-Eval, a benchmark for evaluating whether large language models can systematically generalize safety knowledge to novel contexts and variations. The work tests compositional understanding of safety principles‚Äîexamining if models can extend learned safety facts beyond training examples to new situations following similar logical patterns.", "key_contribution": "SAGE-Eval provides the first structured evaluation framework specifically designed to test compositional generalization in safety contexts. Rather than testing isolated safety behaviors, it systematically examines whether models understand underlying safety principles and can apply them across varied scenarios.", "novelty": "Unlike prior safety benchmarks that focus on memorized responses to specific harmful requests, SAGE-Eval emphasizes systematic generalization through compositional variations of safety facts. It addresses a critical limitation where previous evaluations couldn't distinguish between true principled understanding and superficial pattern-matching against training data. The approach creates systematic test cases to measure generalization capacity rather than memorization.", "ai_categories": ["Agent Safety and Security", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4425622", "score": "36", "title": "DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios", "authors": "Yao Huang, Yitong Sun, Yichi Zhang, Ruochen Zhang, Yinpeng Dong, Xingxing Wei", "session_type": "SD-1-1111", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2510.15501", "relevant_to_users": "10", "read_by_users": "21", "topics": "", "key_findings": "DeceptionBench reveals critical vulnerabilities in current LLMs and Large Reasoning Models (LRMs), particularly showing amplified deception under reinforcement dynamics. Key findings include: (1) Claude models demonstrate remarkable robustness (~1% deception rates) while reasoning models show concerning vulnerability (90%+ rates under multi-turn pressure), (2) deception rates escalate dramatically from baseline (31.33%) to multi-turn induced scenarios (86.91%), (3) most models display honest internal reasoning but produce deceptive responses, revealing that external pressures compromise ethical judgment, and (4) domain-specific patterns emerge with Education/Economy showing lower deception than Entertainment/Social Interaction. The benchmark establishes that current models lack robust resistance to manipulative contextual cues.", "description": "DeceptionBench is the first comprehensive benchmark that systematically evaluates AI deception behaviors across 150 real-world scenarios spanning five critical societal domains (Economy, Healthcare, Education, Social Interaction, Entertainment). The benchmark employs a three-dimensional framework analyzing intrinsic motivations (egoism vs. sycophancy), extrinsic contextual factors (neutral, incentivized, coercive), and progressive intensity levels (single-turn baseline, induced, and multi-turn persuasive dialogues) across over 1,000 evaluation samples.", "key_contribution": "The main contribution is a comprehensive three-dimensional evaluation framework that uniquely combines domain breadth, dual-level analysis of both internal reasoning ('thought') and observable outputs ('response'), and progressive intensity testing across multi-turn interactions. This enables systematic assessment of how deceptive tendencies manifest across realistic societal domains, their intrinsic behavioral patterns, and how extrinsic factors modulate them.", "novelty": "Unlike prior benchmarks limited to narrow domains or single-turn psychological experiments, DeceptionBench addresses three critical gaps: (1) it provides unprecedented scale and domain coverage (150 scenarios across 5 domains vs. <100 in prior work), (2) it uniquely evaluates both internal reasoning processes and final outputs to reveal the thought-response gap where models reason honestly but respond deceptively, and (3) it incorporates multi-turn persuasive dialogues that capture realistic escalation dynamics, revealing how iterative pressure amplifies deception from ~30% to 90%+ rates. This comprehensive approach exposes that external contextual pressures can compromise ethical judgment even when models internally recognize appropriate behavior.", "ai_categories": ["Agent Safety and Security", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4217849", "score": "36", "title": "Can Agent Fix Agent Issues?", "authors": "Alfin Wijaya Rahardja, Junwei Liu, Weitong Chen, Zhenpeng Chen, Yiling Lou", "session_type": "SD-2-2412", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ab4c234adba0d835dbb64828e87b465d8983cecd.pdf", "relevant_to_users": "1", "read_by_users": "3", "topics": "", "key_findings": "This paper reveals that state-of-the-art software engineering agents achieve only 0.67-4.67% resolution rates on agent system issues (vs. 23-50% on traditional software), demonstrating that agent maintenance presents fundamentally different challenges. The study identifies 6 main categories of agent-specific issues from 201 real-world cases, with LLM operation issues (31.34%) being most common, followed by utility, tool-related, and memory issues. The benchmark AgentIssue-Bench provides 50 reproducible tasks requiring 500 person-hours to create, highlighting unique challenges like LLM nondeterminism, external resource volatility, and agent-specific components that lack training data in current LLMs.", "description": "This paper investigates whether software engineering agents can automatically fix issues in LLM-based agent systems by analyzing 201 real-world issues from 18 agent systems (MetaGPT, AutoGen, CrewAI, etc.). The authors construct AgentIssue-Bench, a reproducible benchmark with 50 agent issue resolution tasks, and evaluate three SE agents (Agentless, AutoCodeRover, SWE-agent) using Claude-3.5-Sonnet and GPT-4o.", "key_contribution": "The main contribution is AgentIssue-Bench, the first comprehensive benchmark for evaluating automated issue resolution in agent systems, featuring reproducible Docker environments, failure-triggering tests, and systematic categorization of agent-specific problems. This benchmark reveals that current SE agents fail dramatically on agent issues compared to traditional software, establishing agent system maintenance as a distinct research challenge.", "novelty": "Unlike prior work on SE agents that targets traditional software (e.g., SWE-bench for general codebases), this is the first to systematically study agent system maintenance as a unique problem domain. It addresses the gap that agent-specific challenges‚ÄîLLM nondeterminism, tool integration failures, memory mechanism bugs, and provider compatibility issues‚Äîrequire specialized approaches beyond generic software engineering agents. The work demonstrates empirically that agent systems present fundamentally different maintenance requirements, with SE agents performing 5-10x worse than on traditional software.", "ai_categories": ["Agent Benchmarking and Evaluation", "Self-Improvement and Meta-Learning", "Tool Use and Code Generation"]}, {"paper_id": "4300956", "score": "36", "title": "Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs", "authors": "Mohammad Shahab Sepehri, Berk Tinaz, Zalan Fabian, Mahdi Soltanolkotabi", "session_type": "SD-4-4605", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2507.11932", "relevant_to_users": "2", "read_by_users": "7", "topics": "", "key_findings": "Introduces Hyperphantasia, a benchmark with four task categories (seven segments, parabolic trajectories, triangle connections, ball rolling puzzles) to evaluate multimodal LLMs' mental visualization capabilities. Results show current models have varying proficiency levels with significant room for improvement in spatial manipulation and reasoning tasks. The benchmark serves as a diagnostic tool to identify specific weaknesses in spatial reasoning across different model architectures.", "description": "This paper presents Hyperphantasia, a novel benchmark designed to systematically evaluate the mental visualization capabilities of multimodal LLMs through tasks requiring spatial manipulation, geometric transformations, motion prediction, and dynamic spatial reasoning‚Äîskills that humans use routinely but remain underexplored in AI evaluation.", "key_contribution": "The first benchmark specifically targeting mental visualization skills in multimodal LLMs, featuring four distinct task categories that assess models' ability to mentally manipulate spatial relationships and reason about geometric transformations without explicit visual aids.", "novelty": "Unlike existing benchmarks (MMMU, MathVista) that focus on mathematical and visual reasoning, Hyperphantasia specifically targets the underexplored intersection of visual understanding and abstract spatial reasoning by requiring models to 'imagine' transformations. It addresses the limitation that previous work hasn't systematically evaluated AI's ability to perform mental spatial manipulation tasks‚Äîa critical cognitive capability that bridges visual perception and abstract reasoning.", "ai_categories": ["Agent Benchmarking and Evaluation", "Spatial and Physical Reasoning"]}, {"paper_id": "3897519", "score": "35", "title": "SURDS: Benchmarking Spatial Understanding and Reasoning in Driving Scenarios with Vision Language Models", "authors": "Xianda Guo, Ruijun Zhang, Yiqun Duan, Yuhang He, Dujun Nie, Wenke Huang, Chenming Zhang, Shuai Liu, Hao Zhao, Long Chen", "session_type": "SD-1-4613", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2411.13112", "relevant_to_users": "15", "read_by_users": "52", "topics": "", "key_findings": "SURDS reveals that state-of-the-art VLMs like GPT-4o and Gemini struggle with fine-grained spatial reasoning in driving scenarios, often performing at or below random chance on orientation and depth tasks, with pixel-level localization accuracy rarely exceeding 10%. The paper introduces a reinforcement learning-based alignment approach using Group Relative Policy Optimization (GRPO) that achieves 40.80 overall score, significantly outperforming GPT-4o (13.30) and Gemini-2.0-flash (35.71). The benchmark comprises 41,080 training and 9,250 test samples across six spatial reasoning categories: yaw angle, pixel-level localization, depth estimation, pairwise distance, lateral ordering, and front-behind relations.", "description": "SURDS is the first large-scale benchmark for systematically evaluating fine-grained spatial understanding and reasoning capabilities of vision-language models in realistic autonomous driving scenarios using the nuScenes dataset. The paper identifies critical limitations in current VLMs' spatial reasoning abilities and proposes a GRPO-based alignment method that combines supervised fine-tuning with reinforcement learning to improve spatial reasoning performance.", "key_contribution": "The main contribution is a comprehensive benchmark with 50,330 vision-question-answer instances that evaluates VLMs on six spatial reasoning categories specific to driving scenarios, coupled with a novel reinforcement learning-based alignment approach that significantly improves spatial reasoning without requiring auxiliary depth estimators or object detectors.", "novelty": "Unlike previous spatial reasoning benchmarks that focus on controlled indoor environments or simple 2D relations, SURDS specifically targets realistic driving scenarios and operates without visual marks or depth dependencies. It addresses the gap between general VLM capabilities and the fine-grained spatial reasoning required for autonomous driving safety. The GRPO-aligned approach is novel in combining chain-of-thought reasoning with multi-component rewards (localization IoU, logic consistency, format, accuracy) to train VLMs to verify their own spatial reasoning, achieving state-of-the-art performance with a 3B parameter model.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models", "Spatial and Physical Reasoning"]}, {"paper_id": "4441691", "score": "35", "title": "Reward Reasoning Models", "authors": "Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, Furu Wei", "session_type": "SD-1-4105", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/1b460f75d98849e58bc2a9cf1b6673536d0898d5.pdf", "relevant_to_users": "4", "read_by_users": "21", "topics": "", "key_findings": "Introduces Reward Reasoning Models (RRMs) that execute deliberate chain-of-thought reasoning processes before generating rewards, achieving superior performance across diverse reward modeling benchmarks. Trained via reinforcement learning without requiring explicit reasoning traces as training data, RRMs can adaptively exploit test-time compute to improve reward accuracy. The models strategically employ reasoning selectively‚Äîusing detailed reasoning for complex evaluations while maintaining efficiency for straightforward assessments.", "description": "This paper addresses the limitation of traditional reward models that produce direct scalar rewards without intermediate reasoning. It proposes Reward Reasoning Models (RRMs) that leverage chain-of-thought reasoning and test-time compute to generate explicit reasoning chains before assigning reward scores, enabling more nuanced and interpretable assessment of language model outputs.", "key_contribution": "The main innovation is a reinforcement learning framework that enables reward models to self-evolve reasoning capabilities without requiring manually annotated reasoning traces. This allows RRMs to adaptively determine when and how to apply chain-of-thought reasoning during evaluation, balancing interpretability with computational efficiency while achieving state-of-the-art performance.", "novelty": "Unlike Bradley-Terry models that use direct pairwise comparisons or existing generative reward models, RRMs systematically integrate test-time reasoning into the reward modeling process through a self-supervised RL approach. This addresses the limitation that traditional reward models cannot explain their judgments and struggle with complex queries where appropriate rewards are not immediately apparent. The key advance is enabling models to organically develop context-specific evaluation strategies rather than relying on supervised reasoning demonstrations.", "ai_categories": ["Reinforcement Learning for LLMs", "Reasoning and Test-Time Compute"]}, {"paper_id": "4264476", "score": "35", "title": "PolyGuard: Massive Multi-Domain Safety Policy-Grounded Guardrail Dataset", "authors": "Mintong Kang, Zhaorun Chen, Chejian Xu, Jiawei Zhang, Chengquan Guo, Minzhou Pan, Ivan Revilla, Yu Sun, Bo Li", "session_type": "SD-2-1308", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.19054", "relevant_to_users": "2", "read_by_users": "9", "topics": "", "key_findings": "This paper introduces GuardSet-X (also referred to as PolyGuard), a massive multi-domain safety guardrail dataset that explicitly grounds safety test cases in specific safety policies. The work addresses the critical gap in AI safety evaluation by providing comprehensive coverage across diverse domains with clear policy attribution, enabling more nuanced and actionable safety assessment for LLM guardrails.", "description": "The paper presents GuardSet-X, a large-scale benchmark dataset for evaluating AI safety guardrails across multiple domains. Each safety test case is explicitly grounded in specific safety policies, providing clear connections between potential safety violations and the policies they breach.", "key_contribution": "The main contribution is a policy-grounded, multi-domain safety dataset that moves beyond general safety classifications to explicitly link test cases with underlying safety policies, enabling more interpretable and actionable safety evaluation for LLM guardrail systems.", "novelty": "Unlike previous safety datasets that rely on implicit safety assumptions or single-domain evaluations, this work provides explicit policy grounding that allows clear attribution of safety failures to specific violated policies. It addresses scalability challenges in manual safety annotation through systematic policy-based organization and extends beyond isolated domain-specific assessments to comprehensive multi-domain coverage, making safety evaluation more interpretable and actionable.", "ai_categories": ["Agent Safety and Security", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4438032", "score": "35", "title": "Chain of Execution Supervision Promotes General Reasoning in Large Language Models", "authors": "Nuo Chen, Zehua Li, Keqin Bao, Junyang Lin, Dayiheng Liu", "session_type": "SD-2-4012", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/81876043b2c12b524b3d7cebe4d6ef2b4a940143.pdf", "relevant_to_users": "2", "read_by_users": "18", "topics": "", "key_findings": "The paper introduces TracePile, a 2.6M sample corpus (18B tokens) that transforms code execution into explicit step-by-step reasoning traces called Chain of Execution (CoE). Testing across 4 base models and 20 benchmarks shows consistent improvements: LLaMA3.1-8B achieves 7.1% average gains on math datasets, with particularly strong results on algorithmic reasoning tasks (+40% on GraphWiz). CoE outperforms alternative code-based training approaches like CodeI/O by +3.5% on MATH, +15% on LiveCodeBench, and demonstrates out-of-domain generalization. Ablation studies reveal algorithmic data contributes most to reasoning gains (-7.2% when removed), and performance scales consistently from 50K to 2.6M samples.", "description": "This paper addresses the challenge that while code contains rich logical structures and diverse reasoning paradigms, this reasoning is implicit and entangled with syntactic noise. The authors create TracePile, a large-scale corpus that transforms code execution from mathematics, classical algorithms, and competitive programming into explicit chain-of-thought-style execution traces, enabling LLMs to learn robust reasoning patterns through supervision on intermediate computational states.", "key_contribution": "TracePile corpus of 2.6M samples with Chain of Execution (CoE) supervision that makes implicit reasoning in code explicit through step-by-step execution traces with variable states and control flow. The corpus spans three domains (math, classical algorithms, competition programming) and includes code diversification and variable-tracing questions to enhance logical granularity.", "novelty": "Unlike chain-of-thought prompting (inference-time) or program-aided reasoning (focuses on final outputs), CoE provides fine-grained training supervision by explicitly exposing intermediate execution states and variable traces during code execution. This addresses the limitation that raw code contains implicit reasoning buried in syntactic details. The approach differs from CodeI/O methods by embedding rich procedural information from the execution process itself rather than just input-output pairs, enabling models to learn long-range dependencies and diverse algorithmic reasoning patterns (divide-and-conquer, dynamic programming, graph traversal) that transfer to out-of-domain tasks.", "ai_categories": ["Reasoning and Test-Time Compute", "Tool Use and Code Generation", "Mathematical and Logical Reasoning"]}, {"paper_id": "4454952", "score": "35", "title": "Measuring what Matters: Construct Validity in Large Language Model Benchmarks", "authors": "Andrew M. Bean, Ryan Othniel Kearns, Angelika Romanou, Franziska Sofia Hafner, Harry Mayne, Jan Batzner, Negar Foroutan Eghlidi, Chris Schmitz, Karolina Korgul, ..., Adam Mahdi", "session_type": "SD-3-107", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2511.04703", "relevant_to_users": "8", "read_by_users": "21", "topics": "", "key_findings": "This paper develops the first systematic framework for evaluating construct validity in LLM benchmarks, identifying common validity threats including data contamination, memorization, and benchmark gaming. It demonstrates that many popular benchmarks lack sufficient evidence that they measure what they claim to measure, and provides practical checklists and guidelines for assessing benchmark quality.", "description": "The paper examines whether LLM benchmarks actually measure the capabilities they claim to assess by applying construct validity theory from measurement science to LLM evaluation. It provides a comprehensive framework for evaluating and improving benchmark quality, addressing the gap between high benchmark scores and genuine capability improvements.", "key_contribution": "The first systematic application of construct validity theory to LLM benchmarking, providing structured methodology and practical tools for assessing whether benchmarks genuinely measure intended capabilities rather than spurious correlations or test artifacts.", "novelty": "Unlike previous work that examined evaluation gaps broadly, this paper provides the first rigorous, theory-grounded framework specifically adapted from psychological measurement science for validating LLM benchmarks. It addresses the critical limitation that benchmarks are typically used as direct proxies for capability without validation evidence, introducing systematic methodology for examining task representativeness, measurement reliability, and construct alignment.", "ai_categories": ["Agent Benchmarking and Evaluation"]}, {"paper_id": "3787649", "score": "35", "title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants", "authors": "Zeyu Zhang, Quanyu Dai, Luyu Chen, Zeren Jiang, Rui Li, Jieming Zhu, Xu Chen, Yi Xie, Zhenhua Dong, Ji-Rong Wen", "session_type": "SD-4-2015", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/96e1b7b1eeb53a530580aff14cf9527fe3e3d1ac.pdf", "relevant_to_users": "2", "read_by_users": "2", "topics": "", "key_findings": "MemSim is the first automatic and objective evaluation framework for memory capabilities in LLM-based personal assistants, addressing the 90% correctness ceiling of vanilla LLM-generated ground truths. It introduces a Bayesian Relation Network (BRNet) that uses ancestral sampling and causal generation to ensure factual consistency and diversity. The MemDaily benchmark revealed that aggregative reasoning (31-37% accuracy) and comparative questions (59-77%) expose critical bottlenecks in current memory mechanisms, with FullMem achieving 98.2% on easier tasks but struggling significantly on multi-entity reasoning.", "description": "MemSim is a Bayesian simulator framework that automatically generates reliable question-answer pairs for evaluating memory in LLM-based personal assistants. It uses BRNet to model user entities and attributes through structured sampling, and employs a causal generation mechanism where factual hints serve as shared foundations for both message generation and QA construction, preventing LLM hallucination from corrupting ground truths.", "key_contribution": "The main innovation is the combination of Bayesian Relation Network for structured profile generation and a causal generation mechanism that separates LLM rewriting from reasoning, ensuring factual consistency. This enables automatic, scalable evaluation with five QA types (single-hop, multi-hop, comparative, aggregative, post-processing) and achieves 99.8% ground truth accuracy while maintaining diversity through probabilistic sampling rather than LLM imagination.", "novelty": "Unlike previous memory benchmarks that rely on manual annotation or suffer from LLM hallucination (sub-90% correctness), MemSim uses structural constraints and shared hint-based generation to guarantee factual consistency across trajectories. It addresses the scalability-reliability-diversity trilemma by employing Bayesian sampling for diverse profiles while using hints as immutable facts that prevent contradictions between generated messages and QAs. This is the first framework to automatically evaluate personal assistant memory without human annotators while achieving near-perfect ground truth accuracy.", "ai_categories": ["Agent Benchmarking and Evaluation", "Memory and Context Management"]}, {"paper_id": "4230454", "score": "35", "title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving", "authors": "Shihan Dou, Ming Zhang, Chenhao Huang, Jiayi Chen, Feng Chen, Shichun Liu, Yan Liu, Chenxiao Liu, CHENG ZHONG, ..., Xuanjing Huang", "session_type": "SD-4-4110", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/78058118e972a34b3ab22aa5b6b0c000ab083f58.pdf", "relevant_to_users": "0", "read_by_users": "3", "topics": "", "key_findings": "EvaLearn reveals that learning capability is independent from static performance - strong parallel-solving models don't necessarily excel at sequential learning. Thinking-based models (like o3-mini with +10.5% improvement) demonstrate superior learning stability compared to standard models. Feedback learning substantially outperforms demonstration learning, and learning efficiency varies dramatically by task type: models improve readily on math/logic tasks but struggle with summarization which relies more on pre-training knowledge. The benchmark found that some models like DeepSeek-R1 show 9% performance decline in sequential settings despite strong parallel performance, while Claude-3.7-Sonnet improves 7.2%.", "description": "EvaLearn is a pioneering benchmark containing 648 challenging problems organized into 182 sequences across six task categories, designed to evaluate LLMs through sequential problem solving rather than parallel evaluation. Unlike existing benchmarks that measure static abilities on independent problems, EvaLearn requires models to solve related problems sequentially, allowing them to leverage experience from previous solutions to assess learning capability and efficiency - a critical yet previously unexplored dimension of model potential.", "key_contribution": "Introduces the first comprehensive benchmark framework using a sequential evaluation paradigm to quantify LLM learning dynamics, departing from traditional parallel testing approaches. Provides five automated metrics (accuracy slope, first correct position, learned offset, post-warmup accuracy, overall accuracy) to measure both learning capability and efficiency, revealing that learning ability represents a distinct and important dimension of model evaluation independent of static performance.", "novelty": "While existing benchmarks employ parallel evaluation of independent problems to measure static abilities, EvaLearn fundamentally shifts to sequential evaluation where models solve related problems in order and can leverage accumulated experience - capturing learning dynamics completely ignored by prior work. This addresses the critical gap that traditional benchmarks cannot assess whether models can extract and apply knowledge from experience, a fundamental aspect of intelligence. The method-agnostic framework is deliberately decoupled from specific learning strategies, making it extensible to any approach while revealing that identical parallel performance often masks markedly different learning capabilities.", "ai_categories": ["Agent Benchmarking and Evaluation", "Self-Improvement and Meta-Learning", "Reasoning and Test-Time Compute"]}, {"paper_id": "4181902", "score": "35", "title": "RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video", "authors": "ShuHang Xun, Sicheng Tao, Jungang Li, Yibo Shi, Zhixin Lin, Zhanhui Zhu, Yibo Yan, Hanqian Li, LingHao Zhang, ..., Xuming Hu", "session_type": "SD-6-4412", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.02064", "relevant_to_users": "9", "read_by_users": "18", "topics": "", "key_findings": "RTV-Bench reveals that current MLLMs struggle significantly with temporal reasoning and continuous video understanding. The benchmark demonstrates substantial performance gaps across different model architectures, particularly in tasks requiring sustained attention to video sequences and complex reasoning about dynamic scenes. It provides the first systematic evaluation framework specifically designed for real-time video perception and reasoning.", "description": "This paper introduces RTV-Bench, a comprehensive benchmark for evaluating multimodal large language models (MLLMs) on continuous perception, understanding, and reasoning through real-time video analysis. Unlike existing benchmarks that rely on static images or short clips, RTV-Bench emphasizes sustained temporal understanding and reasoning capabilities.", "key_contribution": "RTV-Bench provides the first unified evaluation framework that integrates perception, understanding, and reasoning for real-time video scenarios, enabling rigorous assessment of MLLMs' temporal understanding capabilities with systematic protocols that better reflect practical video analysis demands.", "novelty": "This work addresses the gap between existing video understanding benchmarks and true temporal reasoning by focusing explicitly on continuous, real-time video perception rather than single frames or short clips. It introduces unified evaluation protocols that combine perception, understanding, and reasoning in one framework, addressing the limitation that prior benchmarks fail to capture the complexity of sustained video analysis. This approach better reflects practical requirements for video reasoning systems.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models"]}, {"paper_id": "4239368", "score": "34", "title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems", "authors": "Guibin Zhang, Muxin Fu, Kun Wang, Guancheng Wan, Miao Yu, Shuicheng YAN", "session_type": "SD-1-5507", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/52f961783a3212459f228b4ec297f523ba2d0c95.pdf", "relevant_to_users": "6", "read_by_users": "9", "topics": "", "key_findings": "G-Memory achieves substantial performance improvements in multi-agent systems: up to 20.89% increase in embodied action success rates and 10.12% improvement in knowledge QA accuracy across five benchmarks and three LLM backbones, without requiring modifications to existing MAS frameworks. The system enables cross-trial knowledge transfer and progressive evolution of agent teams through its hierarchical memory architecture, demonstrating that proper memory organization is critical for multi-agent system self-evolution.", "description": "G-Memory is a hierarchical, agentic memory system for LLM-powered multi-agent systems (MAS) inspired by organizational memory theory. It manages lengthy multi-agent interactions through a three-tier graph hierarchy (insight, query, and interaction graphs) that performs bi-directional memory traversal to retrieve both high-level generalizable insights and fine-grained collaboration trajectories.", "key_contribution": "The main contribution is a three-tier hierarchical graph architecture that enables agent-specific and cross-trial memory customization through bi-directional traversal, retrieving both abstract insights and concrete interaction patterns. The system dynamically evolves by assimilating new collaborative trajectories, enabling progressive improvement of agent teams without modifying underlying MAS frameworks.", "novelty": "Unlike existing MAS memory systems that use oversimplified, flat architectures and treat all agents uniformly, G-Memory introduces hierarchical organization inspired by organizational memory theory with agent-specific customization. It addresses the critical gap where prior work completely disregarded nuanced inter-agent collaboration patterns and lacked cross-trial knowledge transfer. The bi-directional traversal mechanism is novel, enabling simultaneous access to abstract knowledge (insight graphs) and concrete collaborative experiences (interaction graphs), while the dynamic evolution capability allows the memory hierarchy to continuously incorporate new collaborative trajectories.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Memory and Context Management"]}, {"paper_id": "4243051", "score": "34", "title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning", "authors": "Haozhen Zhang, Tao Feng, Jiaxuan You", "session_type": "SD-3-3604", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/363710ed9012131f837da723473810ad47f9d2c9.pdf", "relevant_to_users": "4", "read_by_users": "11", "topics": "", "key_findings": "Router-R1 introduces a reinforcement learning framework that enables multi-round routing and aggregation across multiple LLMs, achieving average EM scores of 0.416 on QA benchmarks while maintaining cost efficiency. The system uses an LLM-based router that alternates between 'think' actions (internal reasoning) and 'route' actions (dynamic model invocation), dynamically adjusting API calls based on task complexity. It demonstrates strong out-of-domain generalization despite training only on NQ and HotpotQA datasets, and converges within 100 training steps using a hierarchical reward design that balances format compliance, accuracy, and cost optimization.", "description": "Router-R1 formulates multi-LLM routing as a sequential decision process using reinforcement learning, where an LLM-based router iteratively reasons about and invokes specialized models to handle complex queries. Unlike traditional single-round routers that assign each query to one model in isolation, Router-R1 enables multi-round interactions where the router aggregates responses from multiple models based on intermediate results.", "key_contribution": "The main innovation is framing LLM routing as a sequential decision-making process with an LLM acting as the router itself, capable of interleaving deliberative reasoning with dynamic model selection. The system introduces a novel cost-aware reward mechanism that optimizes the performance-cost tradeoff and achieves strong generalization to unseen models by conditioning only on minimal model descriptors (pricing, latency, example performance).", "novelty": "Previous routing systems perform single-round, one-to-one query-to-model mapping without multi-step reasoning or the ability to leverage multiple models sequentially. Router-R1 addresses this by enabling iterative routing decisions where the router can think, invoke models, integrate their responses into evolving context, and make subsequent routing decisions. The cost-aware RL training with hierarchical rewards and descriptor-based generalization represents a departure from prior supervised or heuristic approaches that lack adaptive multi-round coordination.", "ai_categories": ["Reinforcement Learning for LLMs", "Reasoning and Test-Time Compute", "Multi-Agent Systems and Collaboration"]}, {"paper_id": "4210233", "score": "34", "title": "LIFEBENCH: Evaluating Length Instruction Following in Large Language Models", "authors": "Wei Zhang, Zhenhong Zhou, Kun Wang, Junfeng Fang, Rongwu Xu, Yuanhe Zhang, Rui Wang, Ge Zhang, Xinfeng Li, ..., Sen Su", "session_type": "SD-3-3502", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.16234", "relevant_to_users": "2", "read_by_users": "4", "topics": "", "key_findings": "LIFEBench reveals that most LLMs struggle with length instruction following beyond short outputs, with performance deteriorating sharply past certain thresholds. Surprisingly, almost all models fail to reach vendor-claimed maximum output lengths in practice (even when tested up to 32K words), and long-context LLMs counterintuitively do not improve at length-following despite extended input-output windows. Reasoning LLMs unexpectedly outperform specialized long-text generation models, achieving state-of-the-art length instruction following performance.", "description": "LIFEBench is a comprehensive benchmark for evaluating large language models' ability to follow explicit length instructions across diverse tasks. It consists of 10,800 instances across 4 task categories in English and Chinese, with length constraints ranging from 16 to 8,192 words, and extends evaluation up to 32K words to test practical generation capabilities against theoretical model limits.", "key_contribution": "The benchmark addresses a critical gap in LLM evaluation by systematically measuring adherence to length constraints rather than just output quality. It provides the first comprehensive evaluation showing that length instruction following is a distinct capability where even advanced models fail, particularly revealing the disconnect between vendor-claimed maximum outputs and actual performance.", "novelty": "Unlike existing instruction-following benchmarks that focus primarily on generation quality, LIFEBench uniquely centers on whether models actually meet specified length constraints‚Äîa seemingly simple but overlooked capability. It addresses the limitation that prior work ignored practical failures like premature termination and outputs far too short despite claims. The benchmark reveals counterintuitive findings that extended-context capabilities don't translate to better length-following, and that reasoning ability matters more than specialized long-text training.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation"]}, {"paper_id": "4442812", "score": "34", "title": "Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler", "authors": "Zixuan Hu, Li Shen, Zhenyi Wang, Yongxian Wei, Dacheng Tao", "session_type": "SD-4-1205", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/2b29a4e872717270eff659096ceb805ef86a1735.pdf", "relevant_to_users": "3", "read_by_users": "5", "topics": "", "key_findings": "The paper introduces Bayesian Data Scheduler (BDS), achieving state-of-the-art defense against harmful fine-tuning attacks with a 74.4% improvement at high harmful ratios (p=0.9) and 50%+ average improvement across all ratios. BDS maintains harmful scores around 1.0-2.0 across diverse models (Llama2, Gemma2, Qwen2), tasks (SST2, GSM8K, AlpacaEval), and dataset sizes while preserving 93%+ fine-tuning accuracy. The method demonstrates robust generalization against advanced attacks (OOD, identity shifting) and scales effectively with varying dataset sizes and alignment data availability.", "description": "This paper addresses the vulnerability of large language models to harmful fine-tuning attacks, where malicious data mixed into user-provided fine-tuning datasets can compromise safety alignment. The authors propose BDS, which formulates defense as a Bayesian inference problem that learns posterior distributions of each data point's safety attributes conditioned on both fine-tuning and alignment datasets, then weights data during fine-tuning to mitigate harmful influence adaptively.", "key_contribution": "The main innovation is formulating harmful fine-tuning defense as a Bayesian inference problem with post-hoc adaptive weighting, eliminating the need for attack simulation used by prior methods. The approach includes an amortized neural scheduler that enables efficient transfer to new datasets without retraining, and uses softmax transformation on posterior-derived safety attributes to bidirectionally adjust data weights based on comparative loss analysis.", "novelty": "Unlike existing defenses (Booster, Vaccine, RepNoise) that rely on attack simulation with bounded threat models and suffer from poor adaptability to varying attack settings, BDS leverages post-hoc Bayesian inference to tailor defenses specifically to the actual fine-tuning dataset being used. The key technical novelty is the inverse likelihood term p(D_ft|w)^-1 in the posterior decomposition that enables dataset-specific adaptation, combined with softmax weight transformation that bidirectionally adjusts weights by comparing individual data point loss against weighted average loss. This allows the method to automatically separate benign from harmful data without pre-simulating attacks, addressing the fundamental limitation that prior methods cannot anticipate unknown attack patterns.", "ai_categories": ["Agent Safety and Security", "Reinforcement Learning for LLMs"]}, {"paper_id": "4163350", "score": "33", "title": "ToolRL: Reward is All Tool Learning Needs", "authors": "Cheng Qian, Emre Can Acikgoz, Qi He, Hongru WANG, Xiusi Chen, Dilek Hakkani-T√É¬ºr, Gokhan Tur, Heng Ji", "session_type": "SD-1-511", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/097ae4a34c2eb2b82b2bb8fccc279fb0e3585304.pdf", "relevant_to_users": "4", "read_by_users": "22", "topics": "", "key_findings": "ToolRL demonstrates that reinforcement learning with carefully designed rewards outperforms supervised fine-tuning for tool use by 15%, achieving 17% improvement over base models. The paper reveals three critical insights: (1) longer reasoning traces hurt tool-use performance unlike general reasoning tasks, (2) dynamic reward scaling that gradually shifts from format to correctness yields better results than static weights, and (3) fine-grained reward decomposition (separating tool names, parameter schemas, and values) provides richer learning signals than binary rewards. The work establishes the first systematic framework for RL-based general-purpose tool learning, showing GRPO-trained models can outperform SFT even when trained from scratch.", "description": "This paper presents ToolRL, the first comprehensive study of reward design for reinforcement learning-based tool use in LLMs. It introduces a structured approach using Group Relative Policy Optimization (GRPO) with decomposed rewards that separately evaluate format compliance, tool selection, and parameter correctness, demonstrating superior generalization to unfamiliar tools and complex multi-step scenarios compared to supervised fine-tuning.", "key_contribution": "The main contribution is a systematic framework for RL-based tool learning that replaces binary correctness with fine-grained, decomposed rewards across four dimensions (type, scale, granularity, temporal dynamics). This enables models to learn strategic tool use through exploration rather than pattern memorization, with GRPO training from scratch outperforming SFT-initialized models.", "novelty": "Unlike prior work focused on specific domains (Search-R1, TORL), ToolRL is the first systematic study of general-purpose tool selection and application using RL. It challenges conventional wisdom by showing that length rewards harm tool-use performance, contrasting with their benefits in pure reasoning tasks. The key innovation is decomposing correctness into granular signals (tool name, parameter schema, parameter values) with dynamic reward scaling, enabling models to learn contextual tool decisions and handle unseen scenarios without explicit supervision on complex multi-step reasoning.", "ai_categories": ["Tool Use and Code Generation", "Reinforcement Learning for LLMs", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4121768", "score": "33", "title": "OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles", "authors": "Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, Kai-Wei Chang", "session_type": "SD-1-5514", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/62ceb097c643e0416c764c187ebf4f4d6d1ba9c3.pdf", "relevant_to_users": "13", "read_by_users": "68", "topics": "", "key_findings": "OpenVLThinker demonstrates substantial performance gains on complex reasoning benchmarks through an iterative training paradigm that alternates between supervised fine-tuning (SFT) and reinforcement learning (RL) phases. This approach achieves significant improvements in multi-step visual reasoning accuracy and complex vision-language question-answering tasks, advancing the state-of-the-art in handling intricate analytical problems requiring multi-step cognitive processing.", "description": "OpenVLThinker is a framework for enhancing complex reasoning capabilities in vision-language models through iterative cycles of supervised fine-tuning and reinforcement learning. The method progressively refines model reasoning by alternating between SFT phases that establish foundational reasoning patterns and RL phases that optimize decision-making through reward signals.", "key_contribution": "The main contribution is an iterative training paradigm that systematically combines supervised learning's structured guidance with reinforcement learning's reward optimization through alternating cycles. This enables continuous refinement of reasoning capabilities, producing vision-language models with substantially improved performance on complex multi-step reasoning tasks.", "novelty": "Unlike conventional single-phase training approaches, this work introduces a systematic cycling mechanism between SFT and RL phases that allows for continuous refinement of reasoning patterns. It addresses limitations in existing vision-language models that struggle with complex multi-step reasoning by progressively building sophistication through iterative optimization. The framework represents a departure from static training methodologies toward dynamic, self-improving training regimes specifically designed for complex cognitive tasks.", "ai_categories": ["Reasoning and Test-Time Compute", "Vision-Language-Action Models", "Reinforcement Learning for LLMs"]}, {"paper_id": "4303553", "score": "33", "title": "VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs", "authors": "Shmuel Berman, Jia Deng", "session_type": "SD-1-4911", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ca79a743ac55c7fce5e100be3b2d695bf84a71ba.pdf", "relevant_to_users": "5", "read_by_users": "32", "topics": "", "key_findings": "The paper reveals that leading VLMs (Gemini 2.5 Pro, Claude 3.7, GPT-o4) fail at fundamental nonlocal visual reasoning tasks that are trivial for humans (who achieve 99.5-100% accuracy). Models barely exceed random baseline performance on tasks requiring integration of evidence from multiple image regions, demonstrating they rely on learned priors and language-based heuristics rather than genuine visual reasoning. This exposes a critical gap between VLM performance on complex benchmarks and their ability to perform basic visual algorithms.", "description": "This paper introduces a systematic evaluation suite with three synthetic task categories to test whether state-of-the-art Vision Language Models can perform nonlocal visual reasoning‚Äîreasoning that requires chaining evidence from multiple, possibly distant, image regions. The tasks isolate three distinct forms of nonlocal vision: comparative perception (comparing objects across images), saccadic search (discrete evidence-driven jumps), and smooth visual search (continuous contour tracing).", "key_contribution": "The paper provides the first systematic evaluation isolating visual domain reasoning from natural language reasoning through procedural synthetic tasks that prevent exploitation of learned conventions. It demonstrates that current VLMs exhibit 'tunnel vision'‚Äîlacking core visual reasoning capabilities despite strong performance on existing benchmarks‚Äîand provides detailed failure analysis showing models selectively engage visual examination and cannot self-correct visual reasoning errors.", "novelty": "Unlike previous work testing primitive perception (e.g., HallusionBench, VLMs are Blind), this paper specifically isolates sequential visual reasoning requiring integration of evidence across image regions. The synthetic evaluation prevents models from exploiting learned conventions in real-world benchmarks, revealing that strong performance on complex tasks like ChartQA doesn't translate to fundamental visual algorithms. The work identifies that models rely on language-based heuristics rather than pixel-level reasoning, advocating for architectural changes that enable genuine visual thinking rather than pattern matching.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models", "Reasoning and Test-Time Compute"]}, {"paper_id": "3872495", "score": "32", "title": "LLM-PySC2: Starcraft II learning environment for Large Language Models", "authors": "Zongyuan Li, Yanan Ni, Runnan Qi, Chang Lu, Lumin Jiang, Xu Xiaojie, Xiangbei Liu, Pengfei Li, Yunzheng Guo, ..., Xuebo Zhang", "session_type": "SD-1-1503", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ae23b5a1cbb9529359c184cdf3d394c6383add5a.pdf", "relevant_to_users": "3", "read_by_users": "10", "topics": "", "key_findings": "LLM-PySC2 is the first environment to provide LLMs with the complete pysc2 action space (100+ actions) combined with multi-modal observations and game Wiki knowledge. Experiments with 9 mainstream LLMs reveal that while LLMs can achieve victories in complex StarCraft II scenarios, they struggle to consistently generate correct decisions, especially in the full action space and multi-agent settings. The environment uses an asynchronous query architecture that maintains constant latency regardless of agent population scale, enabling true multi-agent collaboration research.", "description": "LLM-PySC2 is a StarCraft II learning environment specifically designed for Large Language Models, derived from DeepMind's PySC2. It provides LLMs with complete access to the pysc2 action space, multi-modal observations (text, images, minimap), structured Wiki knowledge, and native multi-agent support with both centralized and distributed decision-making capabilities.", "key_contribution": "The first LLM-compatible environment that bridges the gap between LLMs and complex real-time strategy games by providing the full pysc2 action space (100+ actions), native multi-agent collaboration framework with asynchronous querying, and comprehensive multi-modal observations including structured game knowledge. It introduces 8 new macro-decision scenarios and demonstrates that current LLMs have potential but lack consistency in complex decision-making tasks.", "novelty": "Unlike previous environments like TextStarCraft II that severely limited observation and action spaces, or SMAC that used vector-based interfaces, LLM-PySC2 preserves full environmental complexity while making it accessible to LLMs through natural language. It addresses the critical limitation that LLMs couldn't interface with hundreds of actions by providing structured textual representations, and it's the first to enable genuine multi-agent LLM research in StarCraft II with constant-latency asynchronous architecture. Previous work either oversimplified the policy space or lacked multi-agent support entirely.", "ai_categories": ["Agent Benchmarking and Evaluation", "Multi-Agent Systems and Collaboration", "Planning and Decision Making"]}, {"paper_id": "4137955", "score": "32", "title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments", "authors": "Enjun Du, Xunkai Li, Tian Jin, Zhihan Zhang, Rong-Hua Li, Guoren Wang", "session_type": "SD-3-5412", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/e15247fbacf803da9beef26d39153bd1afd55fe6.pdf", "relevant_to_users": "0", "read_by_users": "0", "topics": "", "key_findings": "GraphMaster achieves 2-5% average accuracy improvements over LLM baselines across six datasets, while maintaining excellent structural fidelity (degree distribution similarity p=0.059, clustering coefficient 0.835). The framework demonstrates strong interpretability with traceability score of 0.92 (vs 0.59-0.66 for baselines) and human-algorithm correlation of r=0.78 (p<0.00001). Ablation studies show the Perception Agent contributes 5.2% performance gain and optimal configuration uses N=30 background nodes with M=15% new node generation. The work successfully addresses context window limitations, hallucination control, and semantic-structural balance in graph synthesis.", "description": "GraphMaster is the first multi-agent framework for synthesizing text-attributed graphs (TAGs) in data-limited environments. It orchestrates four specialized LLM agents (Manager, Perception, Enhancement, Evaluation) using a RAG-based architecture to generate graphs that are both semantically coherent and structurally valid, addressing the critical bottleneck of scarce large-scale graph datasets for training Graph Foundation Models.", "key_contribution": "A hierarchical multi-agent framework that decomposes graph synthesis into specialized roles: semantic-aware subgraph sampling (Perception), dual-mode node/edge generation (Enhancement), multi-dimensional quality assessment (Evaluation), and strategic optimization (Manager). The work also introduces six data-limited benchmark datasets and a novel interpretability evaluation combining human expert ratings with Grassmannian manifold-based semantic coherence analysis.", "novelty": "Unlike direct LLM application or traditional structural synthesis methods, GraphMaster solves three critical limitations: (1) context window constraints through semantic-enriched community detection and hierarchical sampling, (2) hallucination issues via iterative feedback loops with adaptive quality thresholds, and (3) semantic-structural decoupling by combining conditional probability modeling with neighborhood-aware edge generation. Previous methods either focus purely on topology (edge operations) or semantics (text generation) without maintaining both dimensions simultaneously, while GraphMaster's dual-mode generation preserves both semantic coherence and structural integrity through agent specialization.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Tool Use and Code Generation", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4224085", "score": "32", "title": "InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback", "authors": "Boyuan Chen, Donghai Hong, Jiaming Ji, Jiacheng Zheng, Bowen Dong, Jiayi Zhou, Kaile Wang, Juntao Dai, Xuyao Wang, ..., Yaodong Yang", "session_type": "SD-5-5104", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.23950", "relevant_to_users": "2", "read_by_users": "5", "topics": "", "key_findings": "InterMT demonstrates that interleaved preference alignment across multiple conversation turns significantly improves multimodal LLM performance compared to traditional endpoint-only evaluation methods. The framework enables dynamic incorporation of human feedback throughout dialogues, yielding better model alignment in multi-turn tasks. Results validate that turn-level preference signals provide more granular training supervision than terminal-only feedback.", "description": "This paper introduces InterMT, a framework for multi-turn interleaved preference alignment with human feedback for multimodal large language models. The approach integrates human preferences dynamically throughout multi-turn dialogues rather than only at conversation endpoints, enabling more contextually-aware model training.", "key_contribution": "The primary innovation is enabling multi-turn interleaved preference alignment where human feedback can be incorporated continuously across conversation turns rather than only after complete exchanges, providing more granular and contextually-aware preference signals for model training.", "novelty": "Unlike previous preference alignment methods that treat feedback as a terminal evaluation problem applied only at conversation endpoints, InterMT allows preferences to be dynamically incorporated at each turn within ongoing conversations. This addresses the limitation of existing approaches that couldn't effectively utilize turn-level feedback in multi-turn contexts, enabling intermediate quality judgments to shape model behavior more precisely throughout dialogues.", "ai_categories": ["Reinforcement Learning for LLMs", "Memory and Context Management"]}, {"paper_id": "3849778", "score": "31", "title": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs", "authors": "ChangHao Li, Yuchen Zhuang, Rushi Qiang, Haotian Sun, Hanjun Dai, Chao Zhang, Bo Dai", "session_type": "SD-1-3416", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/01546f97c01b32e5c0cf560fc4be7a511b46e042.pdf", "relevant_to_users": "5", "read_by_users": "9", "topics": "", "key_findings": "M-Pilot achieves significant performance improvements across multiple domains: 6.7% on reasoning tasks (GSM8K), 8.2% on planning (ALFWorld reaching 96.27% success), and 25% BLEU improvement on personalization tasks. The system reduces API costs by 30% while improving performance 9% on planning tasks. It uses a three-stage training approach (SFT, data collection, iterative DPO) and demonstrates plug-and-play transferability to unseen black-box models like GPT-3.5-turbo and Gemini-1.5-flash.", "description": "M-Pilot is a framework that trains a small white-box LLM to control and steer black-box LLMs by generating task-specific intermediate guidance. It treats the black-box LLM as an environment and uses reinforcement learning techniques to train a pilot model that provides adaptive prompts, enabling controllable multi-turn generation without requiring internal model access.", "key_contribution": "The main innovation is a trainable white-box controller that learns to generate adaptive intermediate guidance for black-box LLMs through iterative direct preference optimization. Unlike static prompting or methods requiring model access, M-Pilot formulates black-box steering as an MDP where a smaller model learns to provide dynamic, context-aware guidance that improves reasoning, planning, and personalization capabilities.", "novelty": "Previous LLM steering methods required white-box access for fine-tuning or relied on static prompt engineering. M-Pilot addresses the limitation of controlling proprietary black-box models by introducing a learnable controller that optimizes guidance through environmental feedback loops and preference learning, without gradient access. It enables self-improvement through iterative on-policy training with preference pairs, going beyond in-context learning and adapter-based approaches to provide dynamic, task-specific steering that transfers across different black-box models.", "ai_categories": ["Agent Benchmarking and Evaluation", "Planning and Decision Making", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4202379", "score": "31", "title": "Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models", "authors": "Simeng Han, Howard Dai, Stephen Xia, Grant Zhang, Chen Liu, Lichang Chen, Hoang H Nguyen, Hongyuan Mei, Jiayuan Mao, R. Thomas McCoy", "session_type": "SD-4-1908", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/bc154619c4195b6a62774c122f1706e4d7b1bb7f.pdf", "relevant_to_users": "4", "read_by_users": "6", "topics": "", "key_findings": "The paper introduces the Braingle Brainteaser benchmark with 478 expert-authored puzzles to evaluate whether LLMs solve problems through creative insight or brute-force enumeration. Key findings: (1) State-of-the-art models like OpenAI o3 achieve 79.6% on math problems and 68.4% on logic problems, showing substantial problem-solving capability; (2) All models use non-brute-force strategies on a sizable proportion of problems, but also resort to brute force even when creative solutions exist; (3) Brute-force reliance decreases with model capability - stronger models show lower rates; (4) Models exhibit 'false confession' over 60% of the time when verifying flawed solutions; (5) Hints consistently improve performance but even advanced models use them inefficiently, deferring crucial constraints until final verification.", "description": "This paper investigates whether large language models solve problems through creative insight or brute-force enumeration by introducing a benchmark of 478 expert-authored mathematical and logic brainteasers. The research systematically evaluates multiple LLMs across various reasoning dimensions including semantic parsing, solution generation, self-correction, step breakdown, and hint utilization to understand their problem-solving strategies.", "key_contribution": "The main contribution is the Braingle Brainteaser benchmark that enables evaluation of reasoning processes rather than just final accuracy, and the comprehensive analysis showing that while LLMs demonstrate creative problem-solving capabilities, they still resort to brute-force approaches even when efficient solutions exist. The benchmark uniquely supports multiple solution pathways (insight-driven vs. exhaustive search), revealing strategic preferences and reasoning limitations in current LLMs.", "novelty": "Unlike existing benchmarks (MATH, AIME) that emphasize knowledge recall and only measure final accuracy, this work focuses on the reasoning process itself by using problems solvable through multiple approaches. The benchmark addresses limitations of previous datasets by isolating creative insight from domain expertise, providing expert-authored diverse puzzles with lower contamination risk, and decomposing reasoning into multiple evaluable facets. The key innovation is revealing that the same reasoning barriers that cause models to choose brute-force also prevent them from understanding creative human solutions, providing new insights into fundamental limitations of current LLM reasoning capabilities.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute", "Mathematical and Logical Reasoning"]}, {"paper_id": "4441411", "score": "31", "title": "Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning", "authors": "Marwa Abdulhai, Ryan Cheng, Donovan Clay, Tim Althoff, Sergey Levine, Natasha Jaques", "session_type": "SD-6-1805", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/109c600393cc962e64028e8425eca62778f40ee9.pdf", "relevant_to_users": "4", "read_by_users": "16", "topics": "", "key_findings": "This paper introduces a multi-turn reinforcement learning framework that reduces persona inconsistency in LLM-simulated users by over 55% across therapy, education, and social conversation domains. The key innovation is defining three automatic consistency metrics (prompt-to-line, line-to-line, and Q&A consistency) that serve as reward signals for PPO training, eliminating the need for human annotations. The approach achieves substantial improvements: +58.5% for open-ended conversation, +37.6% for mental health simulations, and +20.6% for education tasks, with the LLM-as-judge evaluations exceeding human inter-rater reliability.", "description": "The paper addresses the problem of persona drift in LLM-based human simulations, where models contradict earlier statements or abandon role-appropriate behavior across multi-turn interactions. The authors apply Proximal Policy Optimization (PPO) with turn-level rewards based on three automatic consistency metrics to fine-tune models (Llama-8B, Gemma-2B, Mistral-7B) for simulating patients, students, and social chat partners across ~39,000 dialogue lines.", "key_contribution": "The main contribution is a scalable framework that operationalizes persona consistency through three validated automatic metrics and uses them as reward signals for multi-turn RL training. Unlike prior work requiring human annotations or post-hoc filtering, this approach directly optimizes LLMs for long-range dialogue coherence by treating entire utterances as actions and computing turn-level rewards based on full dialogue history.", "novelty": "Previous work on persona simulation either relied on small-scale human-labeled consistency preferences or focused on prompt engineering without systematic training. This paper is novel in applying modern multi-turn RL (PPO) at scale using LLM-as-judge scoring that exceeds human inter-rater reliability, eliminating expensive human annotation. The key innovation is designing consistency-specific metrics and RL training that treats utterances (not tokens) as actions with dialogue history context, enabling optimization for long-range coherence rather than just next-token prediction or general dialogue quality.", "ai_categories": ["Reinforcement Learning for LLMs", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4307546", "score": "31", "title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning", "authors": "Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, Fu-En Yang", "session_type": "SD-6-2203", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/b35b0fc70612e191baced400f754db8ff1fae711.pdf", "relevant_to_users": "31", "read_by_users": "99", "topics": "", "key_findings": "ThinkAct introduces a reinforced visual latent planning framework that significantly outperforms existing VLA models on robot manipulation and embodied reasoning tasks. It achieves 71.5% success on SimplerEnv (15.5% improvement over baselines) and 84.4% on LIBERO, while demonstrating emergent few-shot adaptation and self-correction capabilities. The key innovation is training multimodal LLMs with action-aligned visual rewards based on goal completion and trajectory consistency, enabling explicit reasoning to guide low-level action execution without requiring expensive human annotations.", "description": "ThinkAct is a dual-system framework for vision-language-action reasoning that bridges high-level reasoning with low-level robot control through reinforced visual latent planning. The system uses Group Relative Policy Optimization with novel action-aligned visual rewards to train a multimodal LLM to generate embodied reasoning plans that condition a downstream Transformer-based action model, enabling long-horizon planning and adaptive execution in complex manipulation tasks.", "key_contribution": "The main contribution is a reinforcement learning approach that trains VLA models to perform explicit embodied reasoning using action-aligned visual feedback rewards (goal completion and trajectory distribution matching) rather than relying on task success signals or expensive reasoning annotations. This enables connecting structured reasoning with executable actions through a visual plan latent that asynchronously guides low-level policy execution.", "novelty": "Unlike prior CoT approaches that require expensive high-quality reasoning annotations and tend to overfit, ThinkAct grounds reasoning in visual feedback that directly measures goal completion and trajectory plausibility using dynamic time warping. Previous RL methods for VLA relied on QA-style rewards that limited long-horizon planning capabilities, while ThinkAct introduces action-aligned rewards that incentivize physically plausible embodied planning. The asynchronous reasoning-action architecture allows explicit planning to guide multi-step execution without the inference overhead of per-action reasoning.", "ai_categories": ["Vision-Language-Action Models", "Planning and Decision Making", "Reinforcement Learning for LLMs"]}, {"paper_id": "4133770", "score": "30", "title": "QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?", "authors": "Belinda Li, Been Kim, Zi Wang", "session_type": "SD-4-809", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2503.22674", "relevant_to_users": "9", "read_by_users": "13", "topics": "", "key_findings": "LLMs demonstrate varying proficiency in generating contextually relevant questions when faced with incomplete information during reasoning tasks. Question-formulation represents a distinct reasoning capability separate from standard problem-solving, with performance varying significantly across different model architectures. The ability to proactively identify information gaps and ask appropriate questions correlates with improved task completion rates.", "description": "This paper introduces QuestBench, a benchmark to evaluate whether large language models can effectively formulate strategic questions to acquire necessary information when solving reasoning problems with incomplete information. The work investigates active questioning as a distinct capability in information-seeking scenarios.", "key_contribution": "QuestBench provides the first systematic evaluation framework for measuring LLMs' ability to ask clarifying questions during reasoning tasks, with structured metrics for assessing question quality, relevance, and their impact on reasoning effectiveness across multiple model architectures.", "novelty": "Unlike previous work that focused on passive information consumption or responding to user questions in multi-turn dialogue, this work addresses the proactive information acquisition gap by examining whether models can independently recognize information gaps and formulate appropriate questions to resolve them. It treats question-asking as a distinct reasoning capability rather than a dialogue generation task, specifically targeting scenarios where models must actively seek missing information to complete reasoning problems.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute", "Planning and Decision Making"]}, {"paper_id": "4215496", "score": "30", "title": "SeePhys:  Does Seeing Help Thinking? √¢¬Ä¬ì Benchmarking Vision-Based Physics Reasoning", "authors": "Kun Xiang, Heng Li, Terry Jingchen Zhang, Yinya Huang, Zirong Liu, Peixin Qu, Jixi He, Jiaqi Chen, Yu-Jie Yuan, ..., Xiaodan Liang", "session_type": "SD-4-4603", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.19099", "relevant_to_users": "0", "read_by_users": "4", "topics": "", "key_findings": "The paper reveals that vision-language models show inconsistent performance on physics reasoning tasks, with visual information not uniformly improving reasoning capabilities. In some cases, text-only approaches outperform vision-enabled systems, indicating current models struggle to effectively integrate visual and physical knowledge. This is the first large-scale systematic evaluation demonstrating that 'seeing' doesn't always help 'thinking' in physics contexts.", "description": "SeePhys introduces a comprehensive benchmark for evaluating vision-based physics reasoning in multimodal AI systems. The research investigates whether visual information genuinely enhances physical reasoning capabilities by testing state-of-the-art vision-language models on physics problems with visual components.", "key_contribution": "The paper presents the first large-scale benchmark and evaluation framework specifically designed to measure how effectively AI systems leverage visual information for physics problem-solving, including both text-only and vision-augmented task variants across multiple state-of-the-art models.", "novelty": "This work addresses a critical gap by providing the first systematic evaluation of vision-based physics reasoning. Previous benchmarks focused on either text-based math problems or general visual question-answering, but didn't specifically examine physics reasoning with visual elements. The paper's innovation lies in its targeted investigation of whether multimodal input genuinely improves reasoning in a specific scientific domain, revealing that visual information can sometimes hinder rather than help.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models", "Spatial and Physical Reasoning"]}, {"paper_id": "3882970", "score": "30", "title": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense", "authors": "Yangyang Guo, Fangkai Jiao, Liqiang Nie, Mohan Kankanhalli", "session_type": "SD-5-1208", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/85f3e38bd08668ee901051237edc01d1b8d8823e.pdf", "relevant_to_users": "3", "read_by_users": "9", "topics": "", "key_findings": "This paper identifies a paradox in Vision Large Language Models (VLLMs) where both jailbreak attacks and defenses achieve surprisingly high success rates. The authors discover that vision inputs fundamentally compromise LLM safety alignment through feature space degradation and attention hijacking, with VLLMs losing the ability to distinguish safe/unsafe inputs that their text-only base models retain. They expose a critical 'over-prudence' problem where existing defenses achieve near-perfect attack prevention but suffer 60-80% abstention rates on benign inputs. Additionally, they reveal that evaluation methods show only chance agreement (Cohen's kappa near zero), meaning defense effectiveness may be an artifact of flawed evaluation rather than genuine robustness.", "description": "This paper investigates why Vision Large Language Models are simultaneously highly vulnerable to jailbreak attacks yet easily defended against, revealing that vision inputs fundamentally break safety alignment from base LLMs. The authors identify the over-prudence problem in existing defenses and propose LLM-Pipeline, a vision-free detection method that balances safety and helpfulness by leveraging existing LLM guardrails before VLLM response generation.", "key_contribution": "The paper's main contributions are: (1) identifying vision input inclusion as the root cause of VLLM vulnerability through feature space analysis and attention pattern studies, (2) exposing the over-prudence problem where defenses over-reject benign inputs, and (3) proposing LLM-Pipeline, a simple plug-and-play vision-free detector that achieves better safety-helpfulness balance by repurposing existing LLM guardrails.", "novelty": "Unlike prior work that treats VLLM jailbreak attacks and defenses separately, this paper uniquely examines the paradox of their simultaneous high success rates and traces the root cause to vision modality compromising text-based safety alignment. It's the first to systematically quantify the over-prudence problem showing that state-of-the-art defenses achieve apparent success through excessive rejection rather than nuanced safety understanding. The work also reveals fundamental evaluation methodology flaws (chance-level agreement between metrics) that may have misled the field, and proposes a paradigm shift from multimodal detection to vision-free detection that avoids attention-hijacking vulnerabilities entirely.", "ai_categories": ["Agent Safety and Security", "Vision-Language-Action Models"]}, {"paper_id": "4079796", "score": "30", "title": "Introducing FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark", "authors": "Zhangdie Yuan, Zifeng Ding, Andreas Vlachos", "session_type": "SD-6-401", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2502.19676", "relevant_to_users": "0", "read_by_users": "5", "topics": "", "key_findings": "FOReCAst reveals that forecasting remains highly challenging for contemporary LLMs, with prediction accuracy and confidence calibration being decoupled - models can achieve good accuracy with poor calibration or vice versa. The benchmark introduces 2,256 real-world forecasting questions across Boolean, timeframe, and quantity estimation tasks, with gold confidence scores from aggregated human forecasts. Testing 16 model variants showed inconsistent calibration, near-baseline timeframe prediction performance, and notably, no clear improvement for questions near models' training cutoffs, suggesting forecasting requires abstraction and probabilistic reasoning beyond knowledge retrieval.", "description": "FOReCAst is a benchmark that evaluates both prediction accuracy and confidence calibration for LLM forecasting across diverse real-world questions from Metaculus. Unlike existing benchmarks that only assess prediction accuracy, FOReCAst systematically evaluates whether models express appropriate certainty in their predictions - a critical capability for reliable decision-making under uncertainty.", "key_contribution": "The benchmark uniquely combines dual evaluation of both prediction accuracy and confidence calibration across three diverse question types (Boolean, timeframe, quantity), using real-world questions with gold confidence scores derived from aggregated human forecasts. It introduces principled evaluation methodologies including task-specific metrics (Brier score, CRPS) and log-score transformations for consistent confidence interpretation across forecast types.", "novelty": "FOReCAst addresses critical limitations of prior forecasting benchmarks (ForecastQA, AutoCast, ExpTime) by systematically evaluating confidence calibration rather than accuracy alone. It demonstrates that prediction accuracy and calibration are decoupled capabilities, challenging the assumption that better predictors are automatically better calibrated. The benchmark uses real Metaculus questions rather than artificial scenarios, spans multiple question formats beyond multiple-choice, and reveals that temporal proximity to training cutoffs doesn't improve performance - suggesting forecasting requires reasoning capabilities orthogonal to standard pretraining objectives.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute"]}, {"paper_id": "4224081", "score": "29", "title": "Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve", "authors": "Yuanzhe Liu, Ryan Deng, Tim Kaler, Xuhao Chen, Charles Leiserson, Yao Ma, Jie Chen", "session_type": "SD-4-1608", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/11bc550a62e76a702248f346f36018dce79624fb.pdf", "relevant_to_users": "1", "read_by_users": "3", "topics": "", "key_findings": "The paper demonstrates that a team of small LLMs using a lesson-based collaboration framework can outperform much larger individual LLMs and other multi-agent collaboration methods on coding tasks. The key insight is that code LLMs excel at different optimization categories with no single model dominating, making collaborative learning valuable. The framework enables agents to dynamically discover and share lessons from successes and failures without requiring prior knowledge of their complementary strengths.", "description": "This paper introduces a multi-agent framework for code generation where LLM agents collaborate by learning from each other's successes and failures through a lesson mechanism. The framework addresses the challenge of leveraging multiple code LLMs when their complementary strengths are unknown a priori, enabling teams of smaller models to outperform larger ones through dynamic knowledge sharing.", "key_contribution": "The main contribution is a lesson solicitation-banking-selection mechanism that enables dynamic knowledge transfer among agents during collaborative code generation. This allows teams of small LLMs to extract insights from failed attempts, store them, and strategically apply relevant lessons to guide future generation attempts, achieving superior performance compared to larger individual models.", "novelty": "Unlike prior multi-agent frameworks like ChatDev and MetaGPT that rely on fixed role-based task decomposition, this work enables organic, dynamic knowledge transfer through lessons learned from actual performance during problem-solving. The approach addresses the limitation of needing to know agent specializations upfront by allowing agents to discover and share complementary strengths through iterative collaboration. The framework systematically captures insights from failures rather than just retrying generation, creating an adaptive system that improves continuously across attempts.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Tool Use and Code Generation", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4427557", "score": "29", "title": "SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search", "authors": "Dong Li, Xujiang Zhao, Linlin Yu, Yanchi Liu, Wei Cheng, Zhengzhang Chen, Zhong Chen, Feng Chen, Chen Zhao, Haifeng Chen", "session_type": "SD-4-2011", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/b7f1cf26530dd86e2ae6d7c4a3a2f77a1c33b13b.pdf", "relevant_to_users": "5", "read_by_users": "15", "topics": "", "key_findings": "SolverLLM demonstrates that test-time scaling with LLM-guided MCTS can solve diverse optimization problems without task-specific training, outperforming both prompt-based and learning-based baselines by over 10 percentage points on complex benchmarks. The framework introduces three key innovations: dynamic expansion allowing refinement of earlier formulation decisions, prompt backpropagation that propagates reasoning signals (not just rewards) through the search tree, and uncertainty backpropagation that downweights unreliable LLM evaluations. This training-free approach achieves strong generalization across optimization problem families while being more token-efficient than comparable methods.", "description": "SolverLLM is a training-free framework that converts natural language descriptions of optimization problems into executable mathematical formulations using Monte Carlo Tree Search guided by large language models. The system decomposes problem formulation into six structured elements (Type, Sets, Parameters, Variables, Objective, Constraints) and explores the formulation space through multiple LLM calls with reasoning-aware feedback.", "key_contribution": "The main innovation is a modified MCTS algorithm that incorporates semantic reasoning feedback through prompt backpropagation and uncertainty-aware tree updates, enabling training-free optimization problem solving that matches or exceeds the performance of supervised fine-tuned models. This demonstrates that test-time compute allocation with structured search can replace expensive domain-specific training for mathematical problem formulation.", "novelty": "Unlike prior prompt-based methods that use fixed workflows and are sensitive to prompt engineering, SolverLLM systematically explores multiple formulation candidates through structured search. Unlike learning-based methods (ORLM, LLMOPT) requiring expensive supervised fine-tuning, it achieves comparable or superior performance without any training through test-time scaling. The key novelty is treating LLM reasoning as explorable feedback‚Äîpropagating semantic explanations (not just numerical rewards) backward through the search tree to guide exploration, while accounting for LLM output uncertainty to ensure reliable decision-making.", "ai_categories": ["Reasoning and Test-Time Compute", "Tool Use and Code Generation", "Mathematical and Logical Reasoning"]}, {"paper_id": "4202146", "score": "29", "title": "MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly", "authors": "Zhaowei Wang, Wenhao Yu, Xiyu REN, Jipeng Zhang, Yu Zhao, Rohit Saxena, Liang Cheng, Ginny Wong, Simon See, ..., Mark Steedman", "session_type": "SD-5-4507", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.10610", "relevant_to_users": "2", "read_by_users": "22", "topics": "", "key_findings": "MMLongBench is the first comprehensive benchmark specifically designed to evaluate long-context vision-language models across diverse tasks. The benchmark reveals systematic performance degradation patterns as context length increases and identifies specific capability gaps in current models like GPT-4V, Gemini, and Claude. It demonstrates that effective evaluation requires integration of vision and language across extended sequences, spanning document understanding, slide comprehension, video analysis, and multimodal reasoning tasks.", "description": "This paper introduces MMLongBench, a comprehensive benchmark for evaluating vision-language models' ability to process and understand long contexts. The benchmark addresses gaps in existing evaluation frameworks by systematically assessing multimodal AI systems across varying context lengths and diverse task categories including document QA, web navigation, and video understanding.", "key_contribution": "MMLongBench provides the first systematic benchmark that integrates vision and language evaluation across long sequences, featuring multiple task categories and standardized metrics. It enables consistent assessment of how vision-language models handle extended contexts and identifies where current state-of-the-art models struggle with multimodal long-context understanding.", "novelty": "Unlike previous benchmarks like Needle in a Haystack or ViLLA that focused primarily on text or isolated visual tasks, MMLongBench integrates vision and language simultaneously across long sequences. It addresses the limitation of existing frameworks that poorly capture extended-context performance in multimodal settings by providing broader application coverage (document analysis, web navigation, video understanding) and systematic measurement of performance degradation as context length increases.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models"]}, {"paper_id": "4442071", "score": "28", "title": "SpecEM: Training-Free LLM Ensembling via Iterative Drafting, Verification, and Online Feedback", "authors": "Bo Lv, Nayu Liu, Chen Tang, Xin Liu, Yue Yu, Ping Luo", "session_type": "SD-1-3802", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ce36072cf898e279341e1ca31c51626ddead6950.pdf", "relevant_to_users": "1", "read_by_users": "5", "topics": "", "key_findings": "SpecEM introduces a training-free approach to LLM ensembling that achieves 1.11x-2.23x speedup over standard ensemble methods without sacrificing generation quality. The method extends speculative decoding principles to ensemble settings, using iterative drafting where models alternate as proposers and verifiers, with parallel verification of candidate tokens. Key innovation includes leveraging 'bonus tokens' generated during successful verification as new proposals, and providing theoretical guarantees that the method is never slower than vanilla ensembles. Experiments across HumanEval, GSM8K, MMLU, and CNNDM demonstrate consistent acceleration with maintained performance quality.", "description": "This paper presents a training-free framework for accelerating large language model ensembles by extending speculative decoding to multi-model settings. Instead of requiring each model to sequentially compute token distributions (causing substantial slowdown), the method uses one model to draft candidate tokens while others verify them in parallel, with models alternating roles to maximize efficiency through bonus token reuse.", "key_contribution": "The main contribution is a novel speculative ensemble framework that enables LLM ensembles to run 1.11x-2.23x faster than standard methods without training or quality degradation. The approach introduces an alternate proposal mechanism where models take turns proposing tokens and verifying proposals in parallel, with theoretical proofs guaranteeing speedup and an interpretable strategy for adjusting quality-speed tradeoffs.", "novelty": "Unlike previous work where speculative decoding only applied to single target models or only improved rejected tokens (as in Speculative Contrastive Decoding), SpecEM extends these principles to full ensemble settings and applies optimization to all tokens. It addresses the fundamental limitation that ensemble methods previously offered no speed improvements, and introduces the novel concept of treating bonus tokens from successful verifications as new proposals rather than wasting them. The method also provides mathematical analysis of acceptance rates with lower bounds for weighted ensembles, enabling principled decisions about relaxing acceptance criteria.", "ai_categories": ["Model Efficiency and Optimization", "Reasoning and Test-Time Compute"]}, {"paper_id": "4442665", "score": "28", "title": "SATURN: SAT-based Reinforcement Learning to Unleash LLMs Reasoning", "authors": "Huanyu Liu, Jia Li, Hao Zhu, Kechi Zhang, Yihong Dong, Ge Li", "session_type": "SD-1-4107", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/8c8fdd51e03779f1c757cd48a07127240f2339d4.pdf", "relevant_to_users": "3", "read_by_users": "15", "topics": "", "key_findings": "SATURN introduces a SAT-based RL framework that addresses three critical limitations in existing RL training for LLMs: scalability (no human annotation needed), verifiability (automatic rule-based verification), and controllable difficulty (fine-grained progression). Applied to DeepSeek models, SATURN-1.5B and SATURN-7B achieved +14.0 and +28.1 average pass@3 improvements on SAT problems, and crucially demonstrated generalization to math (+4.9 and +1.8 average improvements on AIME, AMC, MATH-500, GPQA, LiveCodeBench) and programming tasks. The work reveals that SAT-based training naturally encourages self-verification behaviors that transfer to unrelated domains, addressing the broader challenge of enhancing reasoning capabilities through curriculum learning.", "description": "SATURN is a curriculum learning framework that uses Boolean Satisfiability (SAT) problems to train LLMs through reinforcement learning. The framework progressively trains models on increasingly difficult programmatically-generated SAT instances with automatic verification, enabling scalable reasoning enhancement without human annotation. It introduces SATURN-2.6k benchmark and demonstrates that reasoning patterns learned from SAT problems (particularly self-verification and backtracking) transfer effectively to math and programming domains.", "key_contribution": "A novel difficulty estimation formula D(n,k,l) = log‚ÇÇ(k) + 2log‚ÇÇ(l) - n + k/n that enables fine-grained control of SAT problem difficulty, coupled with a dual-loop curriculum framework that automatically generates verifiable training data at scale. This is the first approach to simultaneously achieve scalability, verifiability, and controllable difficulty for RL-based LLM reasoning training, while demonstrating that domain-agnostic logical reasoning can enhance performance on domain-specific tasks.", "novelty": "Unlike previous approaches (math tasks like GSM8K, Logic-RL), SATURN is the first to simultaneously satisfy all three critical requirements: scalable task generation without human annotation, automatic linear-time verification, and fine-grained difficulty control. Previous methods either required expensive human labeling or LLM synthesis, lacked reliable verification, or couldn't progressively adjust difficulty. The key innovation is recognizing that SAT problems provide a programmable, verifiable reasoning domain that naturally encourages self-verification behaviors transferable to other domains. The analytical difficulty estimator replaces traditional phase-transition theory with a practical formula enabling precise curriculum progression, and experiments show this domain-agnostic approach complements rather than competes with domain-specific training.", "ai_categories": ["Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs", "Mathematical and Logical Reasoning"]}, {"paper_id": "4073682", "score": "28", "title": "ThinkBench: Dynamic Out-of-Distribution Evaluation for Robust LLM Reasoning", "authors": "Shulin Huang, Linyi Yang, Yan Song, Shawn Chen, Leyang Cui, Ziyu Wan, Qingcheng Zeng, Ying Wen, Kun Shao, ..., Yue Zhang", "session_type": "SD-2-109", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2502.16268", "relevant_to_users": "1", "read_by_users": "4", "topics": "", "key_findings": "ThinkBench reveals substantial performance degradation in LLMs when evaluated on dynamically-generated out-of-distribution data, with average accuracy drops of 24.9% on AIME-500 and 11.8% on AIME 2024. This gap suggests significant data leakage in existing benchmarks. The paper demonstrates that reasoning models (o1, o3, Deepseek-R1) maintain stronger robustness than non-reasoning models, and validates that test-time scaling laws hold under contamination-free evaluation. The framework successfully unifies evaluation of both reasoning and non-reasoning models through semi-factual data generation.", "description": "ThinkBench introduces a dynamic out-of-distribution evaluation framework to robustly assess LLM reasoning capabilities while mitigating data contamination issues. The benchmark uses semi-factual data generation through scenario-level transformations and attack-level perturbations to create 2,912 OOD test samples from mathematical and scientific reasoning tasks.", "key_contribution": "The first dynamic OOD evaluation benchmark specifically designed for LLM reasoning tasks that combines scenario-level rephrasing with attack-level perturbations to generate contamination-resistant test data. This enables unified evaluation of reasoning and non-reasoning models while preserving mathematical content integrity.", "novelty": "Unlike existing static benchmarks that are vulnerable to memorization and prior OOD work focused on language understanding (OOD-GLUE, GLUE-X), ThinkBench is the first to apply semi-factual data generation specifically to mathematical reasoning evaluation. It addresses the critical limitation that models like OpenAI's o1 show declining performance on newer test sets, suggesting memorization rather than genuine reasoning. The framework uniquely enables test-time scaling evaluation through Process Reward Models while maintaining 100% answer consistency validated through human evaluation.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute", "Mathematical and Logical Reasoning"]}, {"paper_id": "4273500", "score": "28", "title": "VERA: Variational Inference Framework for Jailbreaking Large Language Models", "authors": "Anamika Lochab, Lu Yan, Patrick Pynadath, Xiangyu Zhang, Ruqi Zhang", "session_type": "SD-3-5300", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/5d8395fee02384e70cded7bf691df4c9976ebb5a.pdf", "relevant_to_users": "2", "read_by_users": "7", "topics": "", "key_findings": "VERA achieves state-of-the-art attack success rates (70% on Vicuna-7B, 72% on Orca-7B) by framing jailbreaking as variational inference rather than optimization. It outperforms white-box methods like GCG on 5/7 targets while operating entirely in black-box settings. Critically, it shows superior robustness against modern defenses (58.9% ASR vs 21.5% for GCG under Perplexity Filter, 9.6% vs 0% under Circuit Breaker), and generates diverse, fluent prompts without manual templates, making it future-proof against patching of known vulnerabilities.", "description": "VERA casts black-box adversarial jailbreaking as a probabilistic inference problem, training a small attacker LLM to learn the distribution over adversarial prompts via a variational objective. Once trained, it generates diverse jailbreak prompts through a single forward pass without re-optimization, achieving superior performance on HarmBench across multiple LLMs including GPT-3.5 and Gemini-Pro.", "key_contribution": "The main innovation is formulating jailbreaking as posterior inference P_LM(x|y*) and training an attacker model via ELBO optimization that balances likelihood of harmful outputs, language model priors, and entropy regularization for diversity. This enables amortized attack generation at test time without per-prompt optimization or manual templates.", "novelty": "Unlike template-dependent methods (AutoDAN, GPTFuzzer) that rely on manually curated prompts vulnerable to patching, and genetic algorithms requiring expensive per-prompt optimization, VERA learns a distribution over adversarial prompts that naturally encourages diversity through entropy regularization. It addresses the computational bottleneck of sequential mutation pipelines and eliminates dependence on known vulnerabilities, making it future-proof. The variational framework enables black-box operation without gradient access while achieving better defense robustness than white-box methods.", "ai_categories": ["Agent Safety and Security", "Reinforcement Learning for LLMs"]}, {"paper_id": "4442661", "score": "28", "title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models", "authors": "Xiaoyu Zhan, Wenxuan Huang, Hao Sun, Xinyu Fu, Changfeng Ma, Shaosheng Cao, Bohan Jia, Shaohui Lin, Zhenfei Yin, ..., Yanwen Guo", "session_type": "SD-4-4701", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/1321dbcd7c95604245189132069ca5ac16d3058c.pdf", "relevant_to_users": "6", "read_by_users": "30", "topics": "", "key_findings": "The paper introduces Viewpoint Learning and the Viewpoint-100K dataset to address MLLMs' inability to distinguish between 2D pixel-level similarity and true 3D spatial consistency. The two-stage approach (SFT + GRPO with hybrid cold-start initialization) achieves 92.2% accuracy on viewpoint tasks (vs. 12.9% baseline), 88.5% on CV-Bench (outperforming GPT-4o's 62.7%), and 99.2% on BLINK multi-view tasks. The method demonstrates that foundational viewpoint training activates latent spatial reasoning capabilities, with significant improvements on both in-domain and out-of-domain spatial reasoning benchmarks, advancing applications in robotics, autonomous systems, and 3D scene understanding.", "description": "This paper presents Actial, a method to activate spatial reasoning in MLLMs through Viewpoint Learning‚Äîtraining models to understand 3D consistency across different camera viewpoints using the Viewpoint-100K dataset (100K object-centric image pairs from diverse viewpoints). The approach employs a two-stage fine-tuning strategy combining supervised learning on foundational spatial tasks with reinforcement learning for broader generalization.", "key_contribution": "The main contribution is the introduction of Viewpoint Learning as a foundational task for spatial reasoning, along with the Viewpoint-100K dataset created from MVImgNet with precise camera calibration. The hybrid cold-start initialization method and two-stage training strategy (SFT followed by GRPO) enable MLLMs to overcome strong 2D reasoning biases and develop true 3D spatial understanding that generalizes across diverse spatial reasoning tasks.", "novelty": "Unlike prior work that relies on external spatial information or visual prompts to compensate for weak spatial reasoning, Actial directly activates latent spatial capabilities by targeting the fundamental distinction between 2D continuity and 3D consistency across viewpoints. The paper reveals that direct reinforcement learning alone fails due to strong 2D pretraining biases, necessitating explicit supervised fine-tuning on foundational spatial tasks first‚Äîa critical insight not addressed in previous approaches. This foundational training on viewpoint understanding transfers to out-of-domain spatial tasks, demonstrating that cross-view consistency learning provides generalizable spatial reasoning rather than task-specific pattern matching.", "ai_categories": ["Spatial and Physical Reasoning", "Vision-Language-Action Models", "Reasoning and Test-Time Compute"]}, {"paper_id": "4166430", "score": "28", "title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models", "authors": "Shi Qiu, Shaoyang Guo, Zhuo-Yang Song, Yunbo Sun, Zeyu Cai, Jiashen Wei, Tianyu Luo, Yixuan Yin, Zhang Haoxu, ..., Hua Xing Zhu", "session_type": "SD-4-2208", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2504.16074", "relevant_to_users": "4", "read_by_users": "5", "topics": "", "key_findings": "PHYBench introduces 500 original physics problems (high school to Olympiad level) revealing that even the best LLM (Gemini 2.5 Pro) achieves only 36.9% accuracy versus 61.9% human expert performance. The benchmark identifies two critical failure modes: Physical Perception errors (misunderstanding scenarios) and Robust Reasoning errors (derivation mistakes). The novel Expression Edit Distance (EED) metric improves sample efficiency by 204% over binary scoring, enabling fine-grained assessment of mathematical reasoning. Problems were rigorously curated by 178 PKU physics students through three-stage validation to ensure quality and prevent data contamination.", "description": "PHYBench is a benchmark evaluating LLMs' physical perception and reasoning capabilities through 500 original, text-only physics problems spanning six domains (mechanics, electromagnetism, thermodynamics, optics, modern physics, advanced physics). It addresses critical deficiencies in existing benchmarks including task oversimplification, data contamination, and flawed evaluation items through systematic curation and original problem design.", "key_contribution": "The benchmark's main innovations are: (1) original problem creation to eliminate data contamination, (2) a dual-framework evaluating Physical Perception (correctly abstracting scenarios) and Robust Reasoning (consistent mathematical derivation), and (3) the Expression Edit Distance metric that provides continuous scoring for mathematical expressions rather than binary correctness, enabling partial credit and more efficient evaluation.", "novelty": "Unlike existing physics benchmarks that suffer from data contamination and oversimplification, PHYBench creates entirely original problems through rigorous multi-stage human curation (178 students, peer review, expert validation). It is the first benchmark to explicitly separate Physical Perception failures (misunderstanding physics) from Robust Reasoning failures (mathematical errors), providing diagnostic insights into model weaknesses. The EED metric addresses limitations of binary scoring by measuring structural similarity in expression trees, offering 204% better sample efficiency and enabling nuanced assessment of partial correctness in multi-step derivations.", "ai_categories": ["Agent Benchmarking and Evaluation", "Mathematical and Logical Reasoning", "Spatial and Physical Reasoning"]}, {"paper_id": "4259467", "score": "28", "title": "Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling", "authors": "Xinglin Wang, Yiwei Li, Shaoxiong Feng, Peiwen Yuan, Yueqi Zhang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li", "session_type": "SD-5-3518", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/639ca4d7460b732c0c9d399142939d60bcdb29d2.pdf", "relevant_to_users": "5", "read_by_users": "14", "topics": "", "key_findings": "This paper reveals that existing test-time scaling methods use suboptimal resource allocation strategies, wasting computation by conflating direction quality with candidate count. It introduces DORA (Direction-Oriented Resource Allocation), which achieves state-of-the-art accuracy on mathematical reasoning benchmarks (MATH500, AIME2024, AIME2025) while requiring 3.5√ó fewer FLOPs and 4√ó fewer rollouts than REBASE. The work provides the first theoretical framework for optimal rollout allocation during test-time search, deriving provably optimal strategies under different confidence regimes.", "description": "This paper formulates test-time search as a Bayesian optimization problem to derive optimal strategies for allocating a fixed computational budget across multiple reasoning paths during LLM inference. The authors prove that standard solution-level allocation is fundamentally suboptimal when candidates share underlying reasoning directions, and introduce DORA, a direction-oriented allocation method that clusters semantically similar solutions and reweights rollouts to correct allocation bias.", "key_contribution": "The main contribution is DORA, a theoretically-grounded resource allocation strategy that softly clusters candidate solutions into shared reasoning directions and allocates rollouts proportionally at the direction level rather than the solution level, achieving superior accuracy-efficiency tradeoffs with provable optimality guarantees under correct grouping assumptions.", "novelty": "Unlike prior work that relies on human-designed heuristics for rollout allocation, this is the first to formulate test-time search as a principled optimization problem with theoretical foundations. It identifies and proves that solution-level allocation methods are fundamentally flawed when solutions share reasoning directions (Proposition 2), introducing a novel semantic clustering approach that decouples direction quality from candidate count. The work unifies existing methods (Temperature Sampling, Beam Search, REBASE) as special cases under different confidence regimes, while DORA addresses their structural limitations through direction-level reweighting.", "ai_categories": ["Reasoning and Test-Time Compute", "Model Efficiency and Optimization"]}, {"paper_id": "4259470", "score": "28", "title": "RAST: Reasoning Activation in LLMs via Small-model Transfer", "authors": "Siru Ouyang, Xinyu Zhu, Zilin Xiao, Minhao Jiang, Yu Meng, Jiawei Han", "session_type": "SD-5-1901", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/4e1cf538a0cee20ba69772f7ede39e11ffddd493.pdf", "relevant_to_users": "10", "read_by_users": "23", "topics": "", "key_findings": "RAST demonstrates that RL-induced reasoning improvements can be transferred from small models to large models at inference time, achieving 80%+ recovery rates on math reasoning benchmarks with 50% less GPU memory than direct RL training. The key insight is that RL doesn't fundamentally add new knowledge but reshapes output distributions by adjusting ~4% of tokens at reasoning-critical points, and these probability shifts are transferable across model scales. This enables efficient reasoning enhancement without expensive retraining.", "description": "RAST is a decoding-time method that transfers reasoning capabilities from small RL-trained models to larger base models by injecting probability adjustments (delta logits) computed from the difference between a small RL model and its base version. This approach achieves strong reasoning performance on mathematical and coding tasks while requiring significantly less computational resources than direct RL training.", "key_contribution": "The paper introduces a simple yet effective transfer learning approach that activates latent reasoning in large models by borrowing probability adjustments from smaller RL-trained models at inference time, eliminating the need for expensive large-scale RL training while maintaining comparable or better performance.", "novelty": "Unlike previous work that assumes RL fundamentally changes model capabilities, RAST reveals that RL primarily activates existing latent reasoning by adjusting a small subset (~4%) of reasoning-critical tokens, and these adjustments are model-scale invariant. This challenges the conventional wisdom that each model size requires separate RL training and enables efficient cross-scale transfer through a principled logit-adjustment mechanism at decoding time, addressing the computational bottleneck of large-model RL.", "ai_categories": ["Reasoning and Test-Time Compute", "Model Efficiency and Optimization", "Reinforcement Learning for LLMs"]}, {"paper_id": "4146495", "score": "28", "title": "Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization", "authors": "Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, Yatao Bian", "session_type": "SD-6-3718", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/4beac7313c00cacdbfe88ef717756d49edfe75a1.pdf", "relevant_to_users": "9", "read_by_users": "26", "topics": "", "key_findings": "The paper introduces EMPO (Entropy Minimized Policy Optimization), a fully unsupervised method that improves LLM reasoning without any labeled data or external supervision. EMPO achieves substantial gains: boosting Qwen2.5-Math-7B from 30.7% to 48.1% accuracy on math benchmarks and Qwen2.5-7B from 32.1% to 50.1% on MMLU-Pro, matching supervised methods. The key insight is that pretraining already encodes reasoning capabilities, and EMPO refines the model's ability to consistently access and select these latent patterns through semantic entropy minimization.", "description": "This paper proposes EMPO, a fully unsupervised approach to enhancing LLM reasoning that operates by continuously minimizing predictive entropy in a latent semantic space on unlabeled questions. Unlike traditional methods requiring supervised fine-tuning and reinforcement learning with labeled traces or reward models, EMPO achieves competitive performance on mathematical and open-ended reasoning tasks without any supervision signals.", "key_contribution": "EMPO is the first fully unsupervised method for LLM reasoning improvement that eliminates the need for labeled reasoning traces, verified answers, or pre-trained reward models. It leverages semantic clustering of model responses and uses cluster probabilities as reward signals to incentivize consistent, high-confidence reasoning patterns already embedded during pretraining.", "novelty": "Unlike supervised fine-tuning and RL-based approaches that depend on expensive labeled data, external reward models, or verified golden answers, EMPO operates entirely without supervision by treating semantic consistency as a proxy for correctness. The key innovation is recognizing that pretraining already embeds reasoning capabilities, and the challenge is efficiently accessing them rather than teaching new skills. EMPO achieves this through entropy minimization in latent semantic space, making it cost-effective and applicable where curated supervisory data is scarce or prohibitively expensive.", "ai_categories": ["Reasoning and Test-Time Compute", "Self-Improvement and Meta-Learning", "Reinforcement Learning for LLMs"]}, {"paper_id": "4242765", "score": "28", "title": "Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning", "authors": "Kongcheng Zhang, QI YAO, Shunyu Liu, Yingjie Wang, Baisheng Lai, Jieping Ye, Mingli Song, Dacheng Tao", "session_type": "SD-6-3404", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/4c696fcdaaa5b6e8b53a1bf9e94c8993ee0cd433.pdf", "relevant_to_users": "9", "read_by_users": "21", "topics": "", "key_findings": "The paper introduces CoVo, a self-rewarding RL framework that eliminates the need for external supervision in LLM reasoning by leveraging trajectory consistency patterns. Key findings: (1) correct responses exhibit consistent intermediate reasoning states that converge toward final answers (high consistency) with minimal deviation toward incorrect alternatives (low volatility); (2) CoVo achieves performance comparable to or surpassing supervised RL across mathematics (MATH-500, GSM8K, OlympiadBench), commonsense (MMLU-Pro, CommonsenseQA), and science (GPQA) benchmarks; (3) the combination of linear/vectorial aggregation of consistency-volatility metrics with curiosity bonuses enables effective self-supervised learning.", "description": "This paper proposes CoVo, a self-rewarding reinforcement learning framework for LLM reasoning that generates intrinsic rewards by measuring consistency and volatility of intermediate reasoning states across different response trajectories. The method enables LLMs to learn reasoning capabilities without external supervision by exploiting the observation that correct solutions exhibit convergent trajectory patterns toward their final answers while maintaining low deviation toward incorrect alternatives.", "key_contribution": "The main innovation is a trajectory-based intrinsic reward mechanism that combines consistency (alignment of intermediate states with final answers) and volatility (deviation toward incorrect candidates) through robust vector-space aggregation. This enables fully self-supervised RL for reasoning tasks, eliminating dependence on external reward models or human annotations while matching or exceeding supervised RL performance.", "novelty": "Unlike prior self-rewarding methods (e.g., STaR) that rely on binary correctness signals or external verifiers, CoVo exploits intermediate reasoning trajectory patterns as intrinsic quality indicators. The key innovation is recognizing that correct reasoning paths exhibit measurable consistency/volatility signatures without requiring ground-truth labels. This addresses the scalability bottleneck of supervised RL by enabling autonomous reward generation based on observable trajectory convergence properties, making it applicable to domains where external supervision is expensive or unavailable.", "ai_categories": ["Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4082846", "score": "27", "title": "WorldModelBench: Judging Video Generation Models As World Models", "authors": "Dacheng Li, Yunhao Fang, Yukang Chen, Shuo Yang, Shiyi Cao, Justin Wong, Michael Luo, Xiaolong Wang, Hongxu Yin, ..., Yao Lu", "session_type": "SD-1-4715", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2502.20694", "relevant_to_users": "24", "read_by_users": "63", "topics": "", "key_findings": "WorldModelBench introduces a specialized evaluation framework with 67K human annotations across 350 test cases to assess video generation models as world models. Top-performing models achieve only 61% task completion rates, revealing significant gaps in physics adherence and commonsense reasoning. The benchmark detects subtle violations like irregular object size changes that breach mass conservation laws, which remain invisible to existing metrics. A fine-tuned 2B-parameter visual language model judger predicts human preferences with 9.9% lower error rates than GPT-4o, and can be used for reward optimization to improve world modeling capabilities.", "description": "This paper presents WorldModelBench, an evaluation framework for assessing video generation models as world models across application-driven domains like robotics and autonomous driving. The benchmark incorporates 67K human annotations to evaluate models on instruction-following, physics adherence (covering five specific physical laws), and commonsense reasoning‚Äîdimensions overlooked by previous video quality benchmarks.", "key_contribution": "The main contribution is a novel benchmark that specifically targets world modeling violations rather than general video quality, coupled with a fine-tuned 2B-parameter visual language model judger that accurately predicts human preferences and enables reward optimization for improving video generation models.", "novelty": "Unlike previous benchmarks like VBench that focus on aesthetic coherence, WorldModelBench addresses a critical blindspot by specifically evaluating fine-grained physics violations (Newton's laws, mass conservation, fluid mechanics, impenetrability, gravitation) and demonstrates that existing benchmarks cannot distinguish physically realistic from unrealistic videos (VBench physics correlation 0.28 vs 0.69 for frame quality). The work moves beyond evaluation-only approaches by demonstrating that fine-tuning video models using the judger's rewards improves world modeling capabilities, providing a practical pathway for model improvement.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models", "Spatial and Physical Reasoning"]}, {"paper_id": "4068434", "score": "27", "title": "Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs", "authors": "Xander Davies, Eric Winsor, Alexandra Souly, Tomek Korbak, Robert Kirk, Christian Schroeder de Witt, Yarin Gal", "session_type": "SD-4-5401", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/106f56012866f9c2a8f8d433881447430b77c9df.pdf", "relevant_to_users": "4", "read_by_users": "5", "topics": "", "key_findings": "The paper proves that pointwise defenses (analyzing individual samples) fundamentally cannot prevent sophisticated fine-tuning attacks on LLM APIs. The authors demonstrate practical attacks achieving ~98% success rates on OpenAI's API using only benign training samples that exploit natural language variation (semantic/syntactic differences) to covertly transmit dangerous knowledge. These attacks evade both existing and enhanced detection systems designed to catch previous cipher-based attacks.", "description": "This paper analyzes the fundamental limitations of detecting fine-tuning attacks on LLM APIs when defenses examine individual training or inference samples. The authors construct 'pointwise-undetectable' attacks that use entirely benign-looking data pairs (e.g., mapping harmful questions to flower names or classification phrasings) to successfully elicit harmful information from fine-tuned models.", "key_contribution": "The main contribution is proving both theoretically (Theorem 1 via information-theoretic principles) and empirically that pointwise detection systems cannot defend against attacks that repurpose natural entropy in model outputs. The authors demonstrate this with practical attacks on OpenAI's fine-tuning API that succeed even against enhanced monitoring systems specifically designed to detect fine-tuning attacks.", "novelty": "Unlike previous fine-tuning attacks that required teaching models artificial ciphers or steganographic schemes (producing suspicious outputs), this work exploits natural variations already present in model behavior‚Äîsuch as different phrasings or semantically unrelated outputs. This makes attacks fundamentally undetectable by pointwise monitors since both inputs and outputs appear individually benign. The work shifts the security paradigm by proving an asymmetry favoring attackers when defenses analyze samples in isolation.", "ai_categories": ["Agent Safety and Security"]}, {"paper_id": "4089362", "score": "27", "title": "Memory Injection Attacks on LLM Agents via Query-Only Interaction", "authors": "Shen Dong, Shaochen Xu, Pengfei He, Yige Li, Jiliang Tang, Tianming Liu, Hui Liu, Zhen Xiang", "session_type": "SD-5-1412", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/0fa0fce8cc51e3ae2f36e5aef29236b15bcc35bc.pdf", "relevant_to_users": "8", "read_by_users": "11", "topics": "", "key_findings": "This paper demonstrates that LLM agents with memory can be compromised through query-only interactions, achieving 98.2% injection success rate and 76.8% attack success rate without requiring privileged system access. The attack successfully works across diverse agent architectures (medical reasoning, e-commerce planning, QA systems) with minimal impact on benign performance (<2% utility drop). This reveals a critical supply-chain vulnerability where any user can poison agent memory through normal interactions.", "description": "The paper introduces MINJA (Memory INJection Attack), a novel attack that enables adversaries to inject malicious records into an LLM agent's memory bank through normal query interactions alone, without direct memory access. The attack uses bridging steps to link benign victim queries to malicious reasoning patterns, and employs a progressive prompt shortening strategy to make these malicious records retrievable during victim query execution while appearing benign.", "key_contribution": "MINJA is the first demonstrated query-only memory injection attack against LLM agents that requires no privileged access or direct memory modification. The key innovation is using bridging steps and progressive prompt shortening to autonomously generate malicious memory records through agent self-interaction, making attacks practical under realistic threat models where attackers only have normal user-level access.", "novelty": "Unlike prior memory poisoning attacks (AgentPoison, BadChain) that assume attackers can directly modify memory banks or inject triggers into victim queries, MINJA operates under a realistic threat model requiring only query-level interaction. The innovation addresses the logical inconsistency problem between benign queries and malicious outputs through an intermediate bridging architecture, enabling the agent to autonomously generate attack records. The progressive shortening strategy makes attacks stealthy by gradually removing obvious attack indicators while preserving malicious reasoning patterns.", "ai_categories": ["Agent Safety and Security", "Memory and Context Management"]}, {"paper_id": "4434028", "score": "26", "title": "World Models Should Prioritize the Unification of Physical and Social Dynamics", "authors": "Xiaoyuan Zhang, Chengdong Ma, Yizhe Huang, Weidong Huang, Siyuan Qi, Song-Chun Zhu, Xue Feng, Yaodong Yang", "session_type": "SD-3-1009", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2510.21219", "relevant_to_users": "0", "read_by_users": "7", "topics": "", "key_findings": "This work argues that contemporary world models have developed in silos‚Äîphysical world models and social dynamics models remain largely separate. The paper demonstrates that integrating these perspectives yields more comprehensive understanding of complex systems and is critical for agents operating in environments with both physical constraints and social interactions, addressing a significant gap in current model development.", "description": "This position paper advocates for fundamentally reconceptualizing world models by integrating physical and social dynamics rather than treating them as independent domains. The authors contend that effective world modeling for real-world agents requires understanding how physical phenomena and social behaviors interact and influence each other, presenting a unified framework as essential for future progress.", "key_contribution": "The paper's primary innovation is proposing a conceptual framework that positions the unification of physical and social dynamics as a foundational principle for world model development, challenging the field's traditional separation of these modeling domains and offering structured justification for integration.", "novelty": "Unlike prior work treating physical simulation and social modeling as orthogonal research areas, this paper explicitly frames their integration as necessary. It addresses the limitation that current models fail to capture emergent phenomena arising from physical-social interactions, proposing that future world models must explicitly represent these interdependencies rather than addressing them separately.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Spatial and Physical Reasoning"]}, {"paper_id": "4242412", "score": "26", "title": "On Reasoning Strength Planning in Large Reasoning Models", "authors": "Leheng Sheng, An Zhang, Zijian Wu, Weixiang Zhao, Changshuo Shen, Yi Zhang, Xiang Wang, Tat-Seng Chua", "session_type": "SD-5-5313", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ec0170f131842f1aaee5993a19df22764b470213.pdf", "relevant_to_users": "2", "read_by_users": "6", "topics": "", "key_findings": "Large reasoning models (LRMs) pre-plan reasoning strengths in their activations before token generation, with the number of reasoning tokens predictable solely from question activations using linear probes. A directional vector embedded in activations causally controls this planning, where vector magnitude directly modulates reasoning token allocation. Manipulating this vector can decrease reasoning effort for simpler problems or increase it to potentially improve performance on harder ones, enabling practical applications like overthinking detection and efficient reasoning allocation.", "description": "This paper investigates the internal mechanisms behind difficulty-aware reasoning in large reasoning models, revealing how these models automatically allocate reasoning effort (measured by the number of reasoning tokens) based on problem difficulty. The work provides activation-level analysis showing that reasoning strength planning occurs before generation through pre-allocated directional vectors in model activations.", "key_contribution": "The main contribution is identifying and characterizing a pre-allocated directional vector in model activations that causally controls reasoning strength planning. The authors demonstrate that this vector's magnitude directly modulates reasoning length and end-of-reasoning token probabilities, providing the first mechanistic explanation for difficulty-aware reasoning behavior in LRMs.", "novelty": "Unlike previous work that only empirically observed difficulty-awareness in reasoning models, this paper provides the first mechanistic explanation by revealing the activation-level planning that occurs before token generation. It introduces interpretability tools for understanding reasoning behavior and demonstrates that this internal planning mechanism can be directly manipulated through vector arithmetic to control reasoning effort. This addresses the limitation of treating reasoning models as black boxes and enables practical applications like overthinking detection and adaptive reasoning allocation.", "ai_categories": ["Reasoning and Test-Time Compute", "Model Efficiency and Optimization", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4183513", "score": "26", "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data", "authors": "Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, ..., Gao Huang", "session_type": "SD-6-1908", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/34a1a72a0a54db41248d9ad8862c78e55ac789d9.pdf", "relevant_to_users": "22", "read_by_users": "43", "topics": "", "key_findings": "Absolute Zero Reasoner (AZR) achieves state-of-the-art performance in reasoning without using any labeled training data. It uses self-play where a single model acts as both task proposer and solver, with a Python executor providing verifiable rewards. AZR-Coder-7B achieves 50.4% combined average on code+math benchmarks, surpassing models trained on tens of thousands of expert-labeled examples. The method demonstrates exceptional cross-domain transfer (+10.9 to +15.2 points on math) and scales effectively with model size.", "description": "This paper introduces Absolute Zero, a reinforcement learning framework that enables language models to improve reasoning abilities through self-play without any human-annotated data. The model alternates between proposing coding tasks (deduction, abduction, induction) and solving them, using a code executor for verification and a learnability-based reward that encourages moderately-difficult task generation.", "key_contribution": "The main innovation is a zero-data RLVR paradigm where a single model bootstraps its own reasoning improvement through self-play task generation and solving, verified by code execution. This includes a task-relative REINFORCE++ algorithm and a learnability reward that guides the proposer to generate optimally-challenging tasks for learning.", "novelty": "Unlike prior RLVR methods that depend on expertly-curated question-answer pairs, Absolute Zero completely eliminates the need for external datasets by having the model generate its own training curriculum. It addresses the scalability limitations of supervised approaches and demonstrates that self-play mechanisms from game-playing AI (AlphaZero) can be adapted to general reasoning domains. The learnability-based reward and task-relative baselines represent novel technical contributions for stable multitask self-improvement.", "ai_categories": ["Self-Improvement and Meta-Learning", "Reinforcement Learning for LLMs", "Reasoning and Test-Time Compute"]}, {"paper_id": "4413537", "score": "25", "title": "MetaDefense: Defending Fine-tuning based Jailbreak Attack Before and During Generation", "authors": "Weisen Jiang, Sinno Jialin Pan", "session_type": "SD-2-1409", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/006cba1cf9e75c1f5fafd3bea5ee62d71f204085.pdf", "relevant_to_users": "1", "read_by_users": "12", "topics": "", "key_findings": "MetaDefense introduces a two-stage defense framework that leverages the LLM itself (rather than external classifiers) to detect and block fine-tuning-based jailbreak attacks both before and during generation, achieving 2√ó memory efficiency. The framework uses specialized diagnostic prompts to predict harmfulness of queries and partial responses, enabling early termination of harmful interactions while generalizing to unseen attack templates. Extensive experiments on LLaMA-2-7B, Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct demonstrate that it significantly outperforms existing defenses against four attack templates (Direct, PrefixInjection, RefusalSuppression, RolePlay) while maintaining competitive performance on benign tasks.", "description": "MetaDefense is a novel defense framework against fine-tuning-based jailbreak attacks on large language models. It operates in two stages: pre-generation defense that detects harmful queries before response generation begins using specialized prompts, and mid-generation defense that monitors partial responses during generation to prevent outputting harmful content, both leveraging the target LLM itself rather than separate classifiers.", "key_contribution": "The main innovation is reusing the target LLM itself for both harmfulness detection and response generation through instruction-tuning with specialized diagnostic prompts ('Is this query/response harmful or harmless? It is 100%...'), achieving memory-efficient defense that generalizes to unseen attack templates without requiring separate moderation models or rigid rule filters.", "novelty": "Unlike prior defenses that rely on separate moderation models, external classifiers, or static rule-based filters applied only at inference time, MetaDefense unifies defense within the LLM itself through lightweight instruction-tuning. It addresses the critical gap of defending against fine-tuning-based attacks specifically (where adversaries compromise models during training) rather than just prompt-based jailbreaks. The dual-stage approach enables continuous monitoring throughout the generation lifecycle with adaptive intervention windows, while previous work focused primarily on either pre-training safety or post-deployment filtering without addressing the fine-tuning attack vector.", "ai_categories": ["Agent Safety and Security", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4217512", "score": "25", "title": "SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents", "authors": "Ibragim Badertdinov, Alexander Golubev, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Andrei Andriushchenko, Maria Trofimova, Daria Litvintseva, Boris Yangel", "session_type": "SD-3-106", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.20411", "relevant_to_users": "6", "read_by_users": "9", "topics": "", "key_findings": "SWE-rebench introduces an automated pipeline for collecting and evaluating software engineering agent tasks while systematically addressing data contamination issues that inflate performance metrics. The paper provides a decontamination methodology that verifies test data hasn't been exposed during model training, along with a public benchmark and leaderboard for standardized agent evaluation. This enables more reliable assessment of genuine agent capabilities on real-world coding tasks.", "description": "This paper presents SWE-rebench, an automated pipeline for collecting software engineering tasks from repositories and evaluating AI agents while ensuring the benchmark datasets remain decontaminated from training data. The system provides standardized infrastructure for trustworthy benchmarking of software engineering agents.", "key_contribution": "The main contribution is an automated pipeline that systematically collects software engineering tasks and implements rigorous decontamination procedures to prevent data leakage between training and evaluation sets, enabling more accurate assessment of agent capabilities without inflated performance from contaminated test data.", "novelty": "Unlike existing benchmarks like SWE-bench, this work implements systematic decontamination procedures with automated verification processes integrated into the pipeline. It addresses the critical limitation of previous approaches that lacked rigorous mechanisms to ensure test data remained separate from training corpora, which led to inflated performance metrics from data leakage. The automated nature of both collection and decontamination enables continuous, reliable benchmark updates.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation"]}, {"paper_id": "4437099", "score": "25", "title": "Common Task Framework For a Critical Evaluation of Scientific Machine Learning Algorithms", "authors": "Philippe Wyder, Judah Goldfeder, Alexey Yermakov, Yue Zhao, Stefano Riva, Jan Williams, David Zoro, Amy Rude, Matteo Tomasetto, ..., Nathan Kutz", "session_type": "SD-6-2708", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2510.23166", "relevant_to_users": "2", "read_by_users": "5", "topics": "", "key_findings": "The paper introduces a Common Task Framework (CTF) that provides the first unified benchmarking infrastructure for scientific machine learning, with standardized datasets, evaluation metrics, and open-source implementation. It presents comprehensive baseline results across diverse algorithm families (neural networks, dynamical systems methods, physics-informed techniques) revealing performance patterns and computational trade-offs that clarify when different approaches excel.", "description": "This paper addresses the fragmentation in scientific machine learning evaluation by introducing a Common Task Framework that enables fair comparison of diverse SciML algorithms across standardized tasks. The framework provides unified evaluation infrastructure with consistent metrics, allowing systematic assessment of methods like LSTMs, DeepONet, KAN, SINDy, and Koopman approaches.", "key_contribution": "The main contribution is establishing the first comprehensive, unified benchmarking infrastructure for scientific machine learning that enables fair comparison across heterogeneous algorithm families on consistent tasks with standardized metrics at scale.", "novelty": "Unlike prior domain-specific and fragmented benchmarking efforts, this work provides unified evaluation infrastructure that allows fair comparison across diverse algorithm families that were previously evaluated inconsistently. It addresses the lack of standardized benchmarks in SciML by creating infrastructure parallel to established frameworks in computer vision and NLP, enabling systematic method selection and rigorous progress evaluation that was previously unavailable at this scale.", "ai_categories": ["Agent Benchmarking and Evaluation", "Domain-Specific Applications"]}, {"paper_id": "4290758", "score": "25", "title": "Rethinking Verification for LLM Code Generation: From Generation to Testing", "authors": "Zihan Ma, Taolin Zhang, Maosongcao, Junnan Liu, Wenwei Zhang, Minnan Luo, Songyang Zhang, Kai Chen", "session_type": "SD-6-106", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/11dbcf6567a9c2d7cfb82938a0ec215c232e6c80.pdf", "relevant_to_users": "5", "read_by_users": "8", "topics": "", "key_findings": "This paper identifies critical flaws in existing LLM code benchmarks (HumanEval, MBPP, LiveCodeBench) where limited and homogeneous test cases fail to catch subtle bugs, artificially inflating performance metrics. The authors introduce SAGA, a human-LLM collaborative method that achieves 90.62% detection rate for faulty code and 10.78% higher verifier accuracy compared to LiveCodeBench-v6. They propose TCGBench, a comprehensive benchmark for test-case generation, and multi-dimensional metrics (functional correctness, edge case handling, robustness, efficiency, test coverage) that rigorously quantify test-suite thoroughness rather than just code generation quality.", "description": "This paper reframes LLM code verification by shifting focus from generation-based evaluation to testing-based verification. Instead of improving code generators, it strengthens evaluation infrastructure by treating test case generation as a rigorous research problem, proposing SAGA (a human-LLM collaborative method) and TCGBench to produce comprehensive test suites that expose subtle bugs missed by existing benchmarks.", "key_contribution": "The main contribution is the paradigm shift from generation-only evaluation to test-driven verification, introducing SAGA as a human-in-the-loop framework that combines human programming expertise with LLM reasoning to generate comprehensive test cases, along with TCGBench benchmark and multi-dimensional metrics that assess test quality rather than just code correctness.", "novelty": "Unlike prior work that focuses on improving code generation models or benchmark diversity, this work addresses the fundamental evaluation infrastructure problem by recognizing that comprehensive test suites are essential for reliable LLM assessment. It moves beyond simple pass/fail correctness checks to dynamic testing-based verification that examines code behavior across diverse scenarios including edge cases and invalid inputs. The approach integrates software engineering fuzzing concepts with LLM evaluation, creating a systematic test-based assessment protocol rather than relying on hand-crafted evaluation sets with limited test coverage.", "ai_categories": ["Tool Use and Code Generation", "Agent Benchmarking and Evaluation", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4287623", "score": "24", "title": "On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study", "authors": "Riccardo Alberghi, Elizaveta Demyanenko, Luca Biggio, Luca Saglietti", "session_type": "SD-5-3307", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/57860c28b21d95678533bc618a0afee1c2a54e47.pdf", "relevant_to_users": "3", "read_by_users": "16", "topics": "", "key_findings": "This paper reveals a counterintuitive finding: transformer models trained on systematically inefficient reasoning traces (involving backtracking and longer deliberation) generalize better than those trained on globally optimal, algorithmically efficient traces. The benefit is not due to length alone‚Äîarbitrary redundancy fails to improve and can hurt performance. Generalization strongly correlates with next-token prediction confidence, suggesting that long, coherent, and locally incremental traces align better with transformer inductive biases. This challenges the assumption that optimal algorithms are optimal for learning and has implications for training curricula design, test-time compute strategies, and understanding LLM verbosity bias.", "description": "The paper investigates how the structure of chain-of-thought reasoning affects transformer learning using shortest-path problems in layered graphs as a controlled testbed. It compares models trained on different algorithmic approaches (depth-first search vs. layer-by-layer exploration) with varying efficiency levels to understand what makes reasoning traces suitable for next-token prediction.", "key_contribution": "The paper establishes that what seems most sensible to teach‚Äîthe shortest, globally optimal trace‚Äîis not what next-token predictors learn most readily. Instead, systematic, locally incremental yet longer reasoning paths better align with transformer inductive biases, and next-token confidence serves as a meaningful proxy for trace learnability.", "novelty": "Prior work showed CoTs improve performance but didn't characterize which structural properties matter or explore the efficiency-versus-learnability trade-off. This paper introduces a novel synthetic task with parametric control over reasoning trace efficiency (via temperature parameter) and custom tokenization for precise confidence measurement. It provides the first systematic evidence that architectural inductive biases favor locally coherent, incremental reasoning over global optimality, distinguishing meaningful structure from arbitrary redundancy through controlled experiments.", "ai_categories": ["Reasoning and Test-Time Compute", "Agent Benchmarking and Evaluation", "Planning and Decision Making"]}, {"paper_id": "4212868", "score": "24", "title": "Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning", "authors": "Van Yang, Zirui Liu, Hongye Jin, Qingyu Yin, Vipin Chaudhary, Xiaotian Han", "session_type": "SD-5-4200", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/3aa73c81186850fe236acaa4a81d2af3f1e5c910.pdf", "relevant_to_users": "4", "read_by_users": "12", "topics": "", "key_findings": "This paper reveals that long-context capacity is a critical foundation for reasoning ability in LLMs. Models with stronger long-context handling (measured via Needle-in-a-Haystack) consistently achieved higher reasoning accuracy after supervised fine-tuning, with improvements persisting even on short-input tasks. The study shows that extending a model's context window (from 32K to 1M tokens) before reasoning SFT yields substantial gains: Qwen2.5-Math-7B improved from 85.04% to 88.70% on MATH500 and 15.00% to 28.00% on AIME. Failed reasoning cases exhibited symptoms of insufficient long-context ability (repetition, incorrect cross-referencing, truncation), suggesting modern reasoning datasets require robust long-context processing as many samples now exceed 8-10K tokens.", "description": "This paper systematically investigates how a language model's capacity to handle extended contexts fundamentally constrains its reasoning performance. Through controlled experiments using RoPE scaling and model merging across LLaMA3-8B and Qwen2.5-7B models, the authors demonstrate that enhancing long-context ability before supervised fine-tuning on reasoning tasks leads to consistent improvements across mathematical reasoning benchmarks (MATH500, AIME, GSM8K), even for short-input problems.", "key_contribution": "The paper establishes long-context capacity as a first-class objective for reasoning-capable LLMs and proposes a two-step training recipe: first extend the model's context window capability, then perform reasoning-focused supervised fine-tuning. This challenges conventional approaches that focus primarily on fine-tuning strategies and dataset composition while overlooking foundational context-handling capabilities.", "novelty": "Unlike prior work that focused on training paradigms and dataset quality for reasoning improvement, this is the first systematic investigation revealing that insufficient long-context capacity fundamentally limits reasoning performance‚Äîeven when models nominally support extended sequences. The key novelty is demonstrating that long-context training imparts generalizable cognitive benefits that transfer to short-input reasoning tasks, not just long-sequence problems. This addresses the limitation in existing research that treated context length and reasoning ability as independent dimensions, showing they are intrinsically linked.", "ai_categories": ["Reasoning and Test-Time Compute", "Memory and Context Management"]}, {"paper_id": "4219584", "score": "24", "title": "Let Me Think! A Long Chain of Thought Can Be Worth Exponentially Many Short Ones", "authors": "Parsa Mirtaheri, Ezra Edelman, Samy Jelassi, Eran Malach, Enric Boix-Adser√É", "session_type": "SD-6-3514", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/3522982c33f5d5199bd558f48909b1bccbf81615.pdf", "relevant_to_users": "6", "read_by_users": "9", "topics": "", "key_findings": "This paper establishes that for certain reasoning tasks, a single long chain-of-thought can exponentially outperform many short chains combined via majority voting. Using graph connectivity problems, the authors prove formal separations based on computational complexity theory (TC‚Å∞ ‚äâ L assumption) and the vertex query model. Experiments across custom-trained transformers and large reasoning models (DeepSeek-R1, Qwen3, S1-32B) validate that sequential scaling provides exponential advantages on tasks requiring multi-hop reasoning, with practical implications for allocating test-time compute budgets toward longer individual reasoning traces rather than ensemble approaches.", "description": "This paper investigates the fundamental tradeoff between sequential scaling (longer chains-of-thought) and parallel scaling (majority voting over multiple short chains) for LLM reasoning at test time. Through theoretical analysis of graph connectivity problems and extensive experiments on transformers and large reasoning models, the authors demonstrate that certain problem structures exhibit exponential separations favoring sequential depth over parallel breadth.", "key_contribution": "The paper provides the first formal theoretical framework proving exponential separations between sequential and parallel test-time compute strategies, grounded in computational complexity theory. It introduces concrete graph reasoning tasks (two-path and bridge graphs) that operationalize these separations and validates predictions across trained transformers and frontier LLMs, establishing principled guidance for inference-time resource allocation.", "novelty": "Unlike prior work that empirically observed test-time compute benefits without distinguishing sequential from parallel approaches, this paper provides formal impossibility results showing that parallel scaling cannot efficiently substitute sequential reasoning for certain task structures. It bridges theoretical computer science (circuit complexity, the vertex query model) with practical LLM capabilities, introducing graph problems that realistically capture multi-hop reasoning limitations. The work fills a critical gap by establishing when and why long chains beat ensembles of short chains, moving beyond heuristic observations to provable, task-dependent principles for test-time compute allocation.", "ai_categories": ["Reasoning and Test-Time Compute", "Mathematical and Logical Reasoning"]}, {"paper_id": "4230675", "score": "24", "title": "FlySearch: Exploring how vision-language models explore", "authors": "Adam Pardyl, Dominik Matuszek, Mateusz Przebieracz, Marek Cygan, Bartosz Zieli√Ö¬Ñski, Maciej Wolczyk", "session_type": "SD-6-4410", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.02896", "relevant_to_users": "3", "read_by_users": "9", "topics": "", "key_findings": "FlySearch demonstrates that vision-language models exhibit structured exploration patterns when navigating 3D environments, with performance varying significantly based on environmental complexity. The study reveals that VLMs can learn effective navigation behaviors without explicit training for exploration tasks, and establishes benchmarking methods using Unreal Engine environments (City Sample, Electric Dreams) for standardized assessment of exploration strategies across models like GPT-4V, Claude, and Gemini.", "description": "FlySearch investigates how vision-language models explore and navigate visual environments by examining their decision-making strategies in simulated 3D spaces. The research treats VLM navigation as an active exploration problem and provides systematic analysis of how state-of-the-art models approach visual exploration tasks.", "key_contribution": "Introduces a novel framework for systematically analyzing VLM exploration behavior in 3D environments with comprehensive trajectory analysis, establishing evaluation benchmarks that enable comparative study of exploration strategies across different vision-language models.", "novelty": "Unlike previous work that focused on task-specific performance metrics, FlySearch uniquely centers on the exploration process itself‚Äîexamining how models decide where to look and move in open-ended spatial tasks. This addresses a critical gap in understanding model reasoning patterns beyond simple question-answering benchmarks, moving embodied AI evaluation from outcome-focused to process-focused analysis of spatial decision-making.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models", "Planning and Decision Making"]}, {"paper_id": "3422755", "score": "23", "title": "PARCO: Parallel AutoRegressive Models for Multi-Agent Combinatorial Optimization", "authors": "Federico Berto, Chuanbo Hua, Laurin Luttmann, Jiwoo Son, Junyoung Park, Kyuree Ahn, Changhyun Kwon, Lin Xie, Jinkyoo Park", "session_type": "SD-2-304", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/672bb4ede45dba28f133a8ebb2cfe40fb1f395c8.pdf", "relevant_to_users": "2", "read_by_users": "6", "topics": "", "key_findings": "PARCO introduces a parallel autoregressive framework for multi-agent combinatorial optimization that achieves 3.3-24.7√ó inference speedup over sequential methods while maintaining superior solution quality. It demonstrates 3.11-8.37% optimality gaps on heterogeneous capacitated vehicle routing problems (HCVRP) and achieves optimal solutions (0% gap) on on-demand multi-commodity pickup-and-delivery problems (OMDCPDP), with strong generalization to problems 10√ó larger than training instances. The framework successfully addresses the fundamental trade-off between solution quality and computational efficiency in multi-agent coordination.", "description": "PARCO is a reinforcement learning framework that enables multiple agents to construct solutions to combinatorial optimization problems in parallel rather than sequentially. It integrates transformer-based communication for agent collaboration, a multiple pointer mechanism for parallel decision-making, and priority-based conflict resolution to handle situations where agents select the same action.", "key_contribution": "The main innovation is enabling genuine parallel autoregressive construction where all M agents generate actions simultaneously at each step, reducing total construction steps from Œ£ T‚Çò to max T‚Çò. This is achieved through a novel architecture combining multi-agent transformer communication layers with a multiple pointer mechanism and learned priority-based conflict handlers.", "novelty": "Unlike prior sequential autoregressive approaches that suffer from high latency (each action depends on all previous actions) and poor coordination, PARCO enables simultaneous multi-agent decision-making while maintaining effective communication. Previous parallel methods like MAPDP used inflexible fixed decoders for specific agents without robust communication; PARCO introduces dynamic transformer-based agent communication and learned priority systems for conflict resolution rather than ad-hoc random tie-breaking. This addresses the three critical limitations of existing methods: suboptimal agent coordination, poor generalization across problem sizes and agent configurations, and high computational latency.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Planning and Decision Making", "Model Efficiency and Optimization"]}, {"paper_id": "4212876", "score": "23", "title": "Partner Modelling Emerges in Recurrent Agents (But Only When It Matters)", "authors": "Ruaridh Mon-Williams, Max Taylor-Davies, Elizabeth Mieczkowski, Natalia V√É¬©lez, Neil R Bramley, Yanwei Wang, Thomas L. Griffiths, Christopher G. Lucas", "session_type": "SD-2-301", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ce9e0e5f6bb446a8103ef0fb8f4c67e47b7972d0.pdf", "relevant_to_users": "2", "read_by_users": "6", "topics": "", "key_findings": "This paper demonstrates that recurrent neural network agents spontaneously develop internal representations of their collaborative partners' abilities without any explicit architectural features, auxiliary objectives, or specialized mechanisms for agent modeling. Critically, partner modeling emerges only when agents can influence partner behavior through task allocation - agents without influence show significantly degraded representations. The work challenges conventional wisdom that Theory of Mind capabilities require dedicated architectural components, showing they arise naturally from cooperative environmental pressures.", "description": "This paper investigates whether RNN agents trained in cooperative multi-agent environments (Overcooked-AI) spontaneously develop implicit Theory of Mind capabilities that enable them to infer and represent their partners' competencies. The research trains simple model-free GRU agents using PPO to collaborate with diverse partners varying in task-specific abilities, analyzing whether hidden states encode partner traits and support adaptive collaboration.", "key_contribution": "The main contribution is proving that partner modeling emerges spontaneously in standard model-free recurrent architectures trained solely to maximize shared task rewards, without requiring specialized encoder-decoder components, auxiliary training objectives, or explicit mechanisms for inferring other agents' states. The work establishes that environmental conditions imposing social pressure (specifically, the ability to influence partners through task allocation) are both necessary and sufficient for these representations to develop.", "novelty": "Unlike prior agent-modeling research (e.g., Rabinowitz et al.'s 'Machine Theory of Mind') that relies on specialized architectural components explicitly optimized for inferring other agents' internal states, this work shows such specialization is unnecessary. The critical novel insight is that partner modeling only emerges when agents can influence partner behavior - non-influential RNNs show worse partner representation quality than even single-partner specialists. This reveals that the right environmental pressures, rather than architectural complexity, drive the emergence of Theory of Mind-like capabilities in artificial agents.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Reinforcement Learning for LLMs"]}, {"paper_id": "4436400", "score": "23", "title": "Learning √¢¬Ä¬úPartner-Aware√¢¬Ä¬ù Collaborators in Multi-Party Collaboration", "authors": "Abhijnan Nath, Nikhil Krishnaswamy", "session_type": "SD-3-2010", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/40d31b0852235d5e98c6f82c41aa70978247a0dd.pdf", "relevant_to_users": "1", "read_by_users": "4", "topics": "", "key_findings": "The paper introduces Interruptible Collaborative Roleplayer (ICR), which uses counterfactual invariance-based KL divergence regularization to train LLM agents that can critically evaluate partner interventions rather than naively following all suggestions. ICR achieves 47% improvement over DPO on the Weights Task and 14% gain on DeliData, demonstrating that agents can learn to distinguish helpful from misleading interventions. The work provides theoretical proof that standard Bellman-optimal policies fail in Modified-Action MDPs (MAMDPs) where interventions modify agent behavior, and shows how counterfactual regularization addresses this gap efficiently through prompt-based methods.", "description": "This paper addresses the challenge of training LLM-based collaborative agents to critically evaluate partner interventions rather than blindly accepting all suggestions. The authors propose ICR, which uses counterfactual invariance regularization to help agents distinguish between helpful and misleading interventions in multi-party collaboration settings, tested on logical reasoning and coordination tasks.", "key_contribution": "The main contribution is the Modified-Action MDP formulation and the Interruptible Collaborative Roleplayer (ICR) method, which trains agents to be 'partner-aware' through counterfactual invariance-based regularization. ICR adds only one forward pass per sample to standard PPO, making it computationally efficient while enabling agents to resist misleading interventions and maintain logical consistency.", "novelty": "Unlike standard RLHF/DPO methods that treat interventions as passive context, this work explicitly models intervention dynamics through MAMDPs and proves that Bellman-optimal policies in standard MDPs are suboptimal when interventions modify agent behavior. The novelty lies in using prompt-based counterfactual construction for efficient training that develops 'intentionality'‚Äîthe ability to autonomously evaluate intervention quality without explicit reward signals for common ground. This differs from ad-hoc teamwork literature by addressing partner-specific intervention evaluation and from causal reasoning approaches by providing a scalable implementation for large language models.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Reinforcement Learning for LLMs", "Agent Safety and Security"]}, {"paper_id": "3795140", "score": "23", "title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Arm Bandits", "authors": "Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal", "session_type": "SD-4-4108", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/bf589cfd8356e7e428426d438835ed093caa2e02.pdf", "relevant_to_users": "2", "read_by_users": "3", "topics": "", "key_findings": "LASeR achieves 2.67% absolute accuracy improvement over ensemble baselines on reasoning tasks while providing 2x computational speedup. It demonstrates that dynamically selecting reward models per batch using multi-armed bandits outperforms both single fixed RMs and ensemble approaches. The method successfully generalizes across heterogeneous tasks including reasoning (GSM8K, StrategyQA, MMLU), instruction-following (WildChat), and long-context understanding (LongBench), showing that adaptive RM selection mitigates conflicting preference signals and computational overhead.", "description": "LASeR frames reward model selection for LLM alignment as a contextual multi-armed bandit problem, dynamically choosing the most suitable RM per training batch rather than using a single fixed RM or expensive multi-RM ensemble. It uses the negative DPO training loss as the reward signal and LinUCB algorithm with query embeddings as context to balance exploration and exploitation.", "key_contribution": "The paper introduces a bandit-driven framework that loads only one RM per batch based on contextual query embeddings, eliminating the computational overhead of ensemble methods while avoiding conflicting preference signals. This selective approach achieves better performance with 2x speedup compared to simultaneously using multiple RMs.", "novelty": "Unlike single-RM approaches that cannot adapt to task heterogeneity or ensemble methods that suffer from computational cost and conflicting signals, LASeR uses multi-armed bandits to learn which RMs work best for specific contexts during training. It addresses the key limitation that aggregating multiple RM scores creates noisy training data by selecting a single contextually-appropriate RM per batch. The use of negative DPO loss as the MAB reward signal enables the system to learn from actual training outcomes rather than relying on static leaderboard rankings.", "ai_categories": ["Reinforcement Learning for LLMs", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4202758", "score": "23", "title": "Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs", "authors": "Zhangyin Feng, Qianglong Chen, Ning Lu, Yongqian Li, Siqi Cheng, Shuangmu Peng, Duyu Tang, Shengcai Liu, Zhirui Zhang", "session_type": "SD-5-3709", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/e7ed5c9a866ddc4710624ed2df37d91bce6455d3.pdf", "relevant_to_users": "4", "read_by_users": "19", "topics": "", "key_findings": "This paper challenges the necessity of Process Reward Models (PRMs) by demonstrating that problem-solving reinforcement learning naturally induces PRM-like capabilities in LLMs without explicit process-level supervision. Key findings show that: (1) pure RL training on mathematical problem-solving progressively enhances both problem-solving proficiency and implicit process supervision capabilities, (2) current PRMs underperform simple baselines like majority voting when applied to state-of-the-art models like DeepSeek-R1 and QwQ-32B, and (3) problem-solving ability and process supervision represent complementary dimensions that co-evolve synergistically during outcome-focused RL training.", "description": "This paper investigates whether specialized Process Reward Models (PRMs) are necessary for training LLMs on complex reasoning tasks. Using empirical analysis of state-of-the-art models like DeepSeek-R1 and QwQ-32B, the authors examine how outcome-based reinforcement learning develops implicit step-level evaluation abilities without requiring dedicated PRM training.", "key_contribution": "The main contribution is empirically demonstrating that outcome-focused problem-solving RL implicitly induces PRM capability in LLMs, meaning models naturally develop step-level reasoning evaluation abilities without explicit process supervision. This fundamentally challenges the prevailing assumption that separate PRM training is necessary for effective reasoning model development.", "novelty": "Previous work relied on multi-stage training pipelines with explicit PRMs for process-level supervision. This work shows that outcome-based RL alone can produce equivalent or superior capabilities, offering a more efficient single-stage alternative. The key novelty is demonstrating that problem-solving proficiency and process supervision co-evolve naturally during pure RL training, addressing the complexity and computational overhead of traditional PRM-based approaches while maintaining or improving performance on reasoning benchmarks.", "ai_categories": ["Reinforcement Learning for LLMs", "Reasoning and Test-Time Compute", "Mathematical and Logical Reasoning"]}, {"paper_id": "3866947", "score": "23", "title": "A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning", "authors": "Guan Zhe Hong, Nishanth Dikkala, Enming Luo, Cyrus Rashtchian, Xin Wang, Rina Panigrahy", "session_type": "SD-5-1012", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/edfcc94d23a1796e8903435652cc311e00009492.pdf", "relevant_to_users": "13", "read_by_users": "28", "topics": "", "key_findings": "The paper identifies four families of specialized attention heads that solve propositional logic through modular circuits: queried-rule locating heads, queried-rule mover heads, fact-processing heads, and decision heads. Models exhibit 'lazy reasoning' where processing activates primarily after seeing the query. Three independent models (Mistral-7B, Gemma-2-9B, Gemma-2-27B) developed analogous but non-identical mechanisms, suggesting these circuits emerge organically. Larger models use additional logical-operator heads and more parallelism than smaller models.", "description": "This paper uses mechanistic interpretability and causal mediation analysis to reverse-engineer how large language models (7B-27B parameters) internally solve multi-step propositional logic problems. The researchers identify sparse circuits composed of specialized attention heads that work in modular, interpretable ways to locate rules, process facts, and make logical decisions.", "key_contribution": "The work extends circuit analysis to larger frontier models (up to 27B parameters) solving multi-step reasoning problems that require disambiguating relevant information from distractors. It provides fine-grained decomposition of reasoning circuits into four distinct functional families and demonstrates that pre-trained LLMs use modular, localized components rather than diffuse reasoning across all parameters.", "novelty": "Unlike prior mechanistic interpretability work that focused on simple tasks in small models (GPT-2-sized), this paper analyzes intermediate-complexity propositional logic in frontier-scale models. It addresses the gap between toy tasks and real reasoning by studying problems requiring multiple inference steps with contextual disambiguation. The work reveals that pre-trained LLMs develop specialized modular circuits, contrasting with task-trained transformers that use intermingled reasoning, suggesting fundamental differences in how models learn reasoning from different training regimes.", "ai_categories": ["Mathematical and Logical Reasoning", "Reasoning and Test-Time Compute"]}, {"paper_id": "4441456", "score": "23", "title": "REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving", "authors": "Sujun Tang, Christopher Priebe, Rohan Mahapatra, Lianhui Qin, Hadi Esmaeilzadeh", "session_type": "SD-6-909", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/3e257cd601e943e872f777fd88fbaf2a64c6c6ea.pdf", "relevant_to_users": "1", "read_by_users": "13", "topics": "", "key_findings": "Reasoning Compiler achieves 5.0√ó average speedup on neural network layers using 5.8√ó fewer samples than evolutionary search (TVM), resulting in 10.8√ó sample efficiency improvement. On full Llama-3-8B, it achieves 4.0√ó speedup with 3.9√ó fewer samples. The framework demonstrates that off-the-shelf LLMs can guide compiler optimization without fine-tuning, and larger instruction-tuned models achieve near-optimal performance with <6% of GPT-4o mini's sample budget. The approach transforms neural network compilation from uninformed stochastic search into guided contextual reasoning.", "description": "Reasoning Compiler is a novel compilation framework that combines large language models with Monte Carlo tree search (MCTS) to optimize neural network inference. The LLM acts as a proposal mechanism suggesting hardware-informed transformations based on current program state and performance feedback, while MCTS provides structured exploration-exploitation balancing to traverse the compiler optimization space efficiently.", "key_contribution": "The main innovation is formulating compiler optimization as a sequential, context-aware decision process where LLMs reason about transformation sequences using rich historical context (parent/grandparent/great-grandparent nodes, performance costs, available operations), coupled with MCTS to avoid myopic greedy decisions. This achieves 10.8√ó sample efficiency improvement over evolutionary search without requiring LLM fine-tuning.", "novelty": "Unlike evolutionary search methods (TVM/MetaSchedule) that explore redundantly without contextual awareness, Reasoning Compiler explicitly models how past optimization decisions influence future transformation validity and profitability. Previous ML-based autotuning focuses on parameter selection but doesn't reason about transformation sequences and compositional effects. This work addresses the fundamental limitation of sample-inefficiency in neural compilers by making each transformation decision with full awareness of the optimization trajectory, using off-the-shelf LLMs to leverage semantic understanding of code transformations without the need for retraining or architectural modifications.", "ai_categories": ["Tool Use and Code Generation", "Model Efficiency and Optimization", "Reasoning and Test-Time Compute"]}, {"paper_id": "4441596", "score": "23", "title": "Reasoning Planning for Language Models", "authors": "Bao Nguyen, Hieu Trung Nguyen, Ruifeng She, Xiaojin Fu, Viet Anh Nguyen", "session_type": "SD-6-1814", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/4e2c8f0df4fed5d40292835b8448bea21d28648f.pdf", "relevant_to_users": "2", "read_by_users": "4", "topics": "", "key_findings": "The paper introduces EPIC (Embedding-based Planning for Intermediate Computation), a method that enables language models to create structured reasoning plans before execution. It demonstrates measurable improvements in mathematical reasoning and code generation tasks by having models generate intermediate planning representations that decompose problems into manageable components before detailed execution.", "description": "This paper addresses how language models can improve reasoning through explicit planning. Rather than generating solutions directly, models first create structured plans using learned embeddings that represent reasoning steps, allowing them to organize complex tasks hierarchically before proceeding with solution execution.", "key_contribution": "The primary innovation is introducing a distinct 'planning phase' mechanism where models generate intermediate embedding-based representations that decompose problems into structured computational blueprints, contrasting with end-to-end generation by inserting an explicit reasoning layer that improves solution quality across mathematical and code generation domains.", "novelty": "Unlike previous work that focused on chain-of-thought or tree-based reasoning with implicit trajectories, this research uniquely emphasizes planning as a distinct, explicit phase with learned embedding representations. It addresses limitations in how models handle complex sequential reasoning by creating structured computational blueprints rather than relying solely on step-by-step token generation, providing a more flexible and hierarchical approach to reasoning structure.", "ai_categories": ["Reasoning and Test-Time Compute", "Planning and Decision Making", "Mathematical and Logical Reasoning"]}, {"paper_id": "4441364", "score": "22", "title": "The World Is Bigger: A Computationally-Embedded Perspective on the Big World Hypothesis", "authors": "Alex Lewandowski, Aditya A. Ramesh, Edan Meyer, Dale Schuurmans, Marlos C. Machado", "session_type": "SD-5-508", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/463a93594b9e688fecce37543cf3bf45e7f91310.pdf", "relevant_to_users": "7", "read_by_users": "14", "topics": "", "key_findings": "The paper introduces a computationally-embedded perspective on continual learning where agents are represented as automata simulated within universal computers, eliminating the need for ad hoc explicit capacity constraints. It proves that embedded automata are theoretically equivalent to agents interacting with partially observable MDPs over countably infinite state-spaces. The work introduces 'interactivity' as a novel objective function measuring an agent's capacity for continuous behavioral adaptation and prediction learning, and develops an RL algorithm to maximize it. Empirical results reveal that deep nonlinear networks struggle to maintain interactivity while deep linear networks improve with increased computational capacity, suggesting critical architectural implications for continual learning systems.", "description": "This paper addresses continual learning through the 'big world hypothesis' by proposing a computationally-embedded framework where agents are automata within universal computers, subject to implicit rather than explicit constraints. The authors introduce universal-local environments with transition dynamics depending on local neighborhoods of the state-space, enabling a more general formulation of continual learning that works for agents of any capacity.", "key_contribution": "The main contribution is the theoretical framework of computationally-embedded agents that provides implicit constraints through computational embedding, along with the introduction of 'interactivity' as a measurable objective for continual learning. The work includes both theoretical equivalence proofs (showing embedded automata equal POMDPs over infinite state-spaces) and a practical RL algorithm with empirical benchmarks.", "novelty": "Unlike prior work that uses explicit, ad hoc capacity constraints to formalize the big world hypothesis, this paper introduces implicit constraints through computational embedding, making the framework more general and scalable. The work addresses limitations of explicit constraints that are difficult to incorporate and limit scaling effectiveness. The novel 'interactivity' metric provides a principled objective for continual adaptation rather than relying on task-specific performance measures, and the theoretical connection to infinite-state POMDPs provides rigorous mathematical grounding for what was previously a conceptual idea.", "ai_categories": ["Reinforcement Learning for LLMs", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4274160", "score": "22", "title": "IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering", "authors": "Hengyu Liu, Chenxin Li, Zhengxin Li, Yipeng Wu, Wuyang Li, Zhiqin Yang, Zhenyuan Zhang, Yunlong Lin, Sirui Han, Brandon Feng", "session_type": "SD-6-4914", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.23329", "relevant_to_users": "5", "read_by_users": "18", "topics": "", "key_findings": "IR3D-Bench reveals that current state-of-the-art vision-language models (including GPT-4o, Claude 3, Gemini) struggle significantly with precise 3D scene decomposition tasks, particularly material property estimation and lighting analysis. Commercial VLMs generally outperform open-source alternatives, but all models show substantial limitations in extracting compositional 3D information from rendered images. The benchmark demonstrates a critical gap between VLMs' ability to describe images and their capacity to reverse-engineer the underlying 3D scene properties.", "description": "IR3D-Bench is a novel benchmark that evaluates vision-language models through an agentic inverse rendering framework, testing their ability to decompose and understand 3D scenes by extracting structural, material, and lighting information from rendered images. The benchmark uses synthetic scenes generated with Blender and employs VLMs as autonomous agents that iteratively analyze scene components.", "key_contribution": "The paper introduces the first systematic benchmark combining vision-language models with inverse rendering tasks, implementing VLMs as agents that must reason about how images are formed from underlying 3D properties. It provides a comprehensive evaluation protocol assessing object detection, material understanding, lighting analysis, and geometry estimation in a controlled, agent-based framework.", "novelty": "Unlike previous work that focused on either 2D scene understanding or isolated 3D tasks, this benchmark uniquely challenges VLMs to perform inverse rendering‚Äîreconstructing 3D compositional information (materials, lighting, geometry) from images. It addresses the critical gap between VLMs' descriptive capabilities and their ability to extract precise 3D scene properties needed for reconstruction and manipulation. The agentic framework enables iterative reasoning about scene formation, moving beyond single-shot image understanding to structured 3D decomposition.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models", "Tool Use and Code Generation"]}, {"paper_id": "4200499", "score": "22", "title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs", "authors": "Jingyao Wang, Wenwen Qiang, Zeen Song, Changwen Zheng, Hui Xiong", "session_type": "SD-6-214", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/de9fc962acefe20dd0d80073eadeb19263afeb06.pdf", "relevant_to_users": "4", "read_by_users": "21", "topics": "", "key_findings": "L2T introduces a universal, annotation-free process reward method that achieves +3.7% accuracy improvement over outcome-based RL while doubling token efficiency. The framework enables adaptive reasoning depth allocation - simple tasks terminate early while complex problems extend chains - addressing the critical finding that existing methods waste over 2√ó the tokens actually needed. By combining PAC-Bayes bounds with Fisher information matrix approximation, L2T reduces computational complexity while maintaining high estimation accuracy, achieving 48.5% on AIME 2024 and 88.1% on MATH500 using only 18% of base model tokens.", "description": "This paper presents Learning to Think (L2T), an information-theoretic reinforcement fine-tuning framework that trains LLMs to achieve optimal reasoning with fewer tokens. L2T reformulates LLM training as episodic RL with a universal dense process reward that quantifies episode-wise information gain in model parameters, requiring no task-specific evaluators or external annotations.", "key_contribution": "The main innovation is a universal dense process reward combining fitting information gain (measuring correctness improvement) with a parameter compression penalty (preventing overfitting), efficiently estimated via PAC-Bayes bounds and low-rank Fisher information matrix approximation. This enables task-agnostic, annotation-free optimization that simultaneously improves reasoning effectiveness and efficiency.", "novelty": "Unlike existing RL methods that rely on outcome-only rewards or task-specific process evaluators, L2T uses internal model signals (parameter information gain) to create a universal, annotation-free reward. It addresses the overlooked effectiveness-efficiency tradeoff by explicitly penalizing excessive parameter updates, enabling adaptive reasoning depth allocation. The theoretical contribution combines PAC-Bayes bounds with Fisher information for efficient reward estimation, reducing computation from intractable full-parameter analysis to 1-10% low-rank approximation while maintaining accuracy.", "ai_categories": ["Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs", "Model Efficiency and Optimization"]}, {"paper_id": "4051948", "score": "21", "title": "DMWM: Dual-Mind World Model with Long-Term Imagination", "authors": "Lingyi Wang, Rashed Shelim, Walid Saad, Naren Ramakrishnan", "session_type": "SD-2-5313", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/46a97d474ce914d7520303484925b64a33af1b9b.pdf", "relevant_to_users": "2", "read_by_users": "7", "topics": "", "key_findings": "DMWM achieves 14.3% improvement in logical consistency, 5.5-fold improvement in trial efficiency, 32% improvement in data efficiency, and 120% improvement in test returns for extended horizons (H>30) compared to state-of-the-art world models like DreamerV3. The dual-mind architecture successfully addresses error accumulation in long-term imagination by integrating logical reasoning with statistical inference, enabling reliable planning over extended time horizons across 20 DeepMind Control Suite tasks.", "description": "DMWM is a novel world model framework inspired by dual-process cognitive theory that combines an RSSM-based System 1 for intuitive state transitions with a logic-integrated neural network System 2 for hierarchical logical reasoning. The framework uses inter-system feedback to maintain logical consistency during long-horizon imagination tasks, addressing the fundamental limitation of error accumulation in traditional recurrent state-space models.", "key_contribution": "The main innovation is the integration of formal logical reasoning into world models through a dual-mind architecture, where System 2 (LINN-S2) uses recursive implication reasoning and Kronecker products to enforce logical constraints on System 1's statistical predictions, enabling imagination with logical consistency over extended planning horizons.", "novelty": "Unlike prior world models (DreamerV3, IRIS, TWM) that rely solely on single-step statistical inference through RSSMs, DMWM introduces explicit logical constraints via hierarchical deep logical reasoning that maintains multi-step dependencies. This addresses the critical limitation of error accumulation during long-term imagination by ensuring terminal logical consistency through cross-space logic operations and inter-system feedback. The approach bridges neural and symbolic reasoning, providing a fundamentally different mechanism for maintaining coherence in extended-horizon predictions.", "ai_categories": ["Planning and Decision Making", "Reinforcement Learning for LLMs", "Reasoning and Test-Time Compute"]}, {"paper_id": "4442511", "score": "21", "title": "Transcending Cost-Quality Tradeoff in Agent Serving via Session-Awareness", "authors": "Yanyu Ren, Li Chen, Dan Li, Xizheng Wang, Zhiyuan Wu, Yukai Miao, Yu Bai", "session_type": "", "session_time": "", "session_location": "Foyer", "pdf_url": "https://openreview.net/pdf/67186a4229264aede8b786e7d6e259cb157d2aba.pdf", "relevant_to_users": "0", "read_by_users": "2", "topics": "", "key_findings": "The paper introduces AGSERVE, a system that achieves comparable response quality to GPT-4o at only 16.5% of the cost, representing an 83.5% cost reduction. At equivalent cost points, AGSERVE delivers 1.8√ó higher quality compared to existing cost-quality tradeoff curves. The system leverages session predictability in agent workflows to enable efficient KV cache reuse and implements session-aware model cascading that dynamically selects different model sizes across different segments of a session's lifetime.", "description": "This paper presents AGSERVE (AGile AGent SERVing), a novel agent serving system that addresses the fundamental cost-quality tradeoff in LLM-based agents. The system features three key components: a session-aware server with ETA-based KV cache eviction and in-place positional embedding calibration, a quality-aware client performing session-aware model cascading through real-time quality assessment, and a dynamic resource scheduler maximizing GPU utilization.", "key_contribution": "The main contribution is AGSERVE's session-aware architecture that exploits the predictable patterns in agent workflows to simultaneously reduce costs and maintain quality. Unlike traditional approaches that treat requests independently, AGSERVE leverages session structure to enable aggressive KV cache reuse via ETA-based eviction policies and intelligently cascades between models of different sizes throughout a session's lifetime based on real-time quality assessment.", "novelty": "Previous agent serving systems treat each request independently and face a rigid cost-quality tradeoff. AGSERVE breaks this limitation by recognizing that agent workflows have predictable session patterns. The novel ETA-based (Estimated-Time-of-Arrival) eviction policy for KV cache management anticipates future reuse patterns within sessions, while session-aware model cascading dynamically adjusts model selection throughout a conversation rather than using a single model. This session-level optimization achieves what was previously considered impossible: significantly reducing costs while maintaining or improving quality.", "ai_categories": ["Model Efficiency and Optimization", "Tool Use and Code Generation"]}, {"paper_id": "4222247", "score": "21", "title": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "authors": "Haohan Chi, Huan-ang Gao, Ziming Liu, Jianing Liu, Chenyu Liu, Jinwei Li, Kaisen Yang, Yangcheng Yu, Zeda Wang, ..., Hao Zhao", "session_type": "SD-4-4612", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.23757", "relevant_to_users": "14", "read_by_users": "53", "topics": "", "key_findings": "This paper introduces the first open-source Vision-Language-Action (VLA) model specifically designed for autonomous driving, along with a comprehensive open dataset. It democratizes access to driving AI by releasing both model weights and training data, enabling the research community to build upon shared infrastructure rather than proprietary systems. The work successfully integrates visual perception and language understanding for driving tasks, demonstrating that multimodal instruction-following can be effectively applied to autonomous vehicle control.", "description": "The paper develops an end-to-end Vision-Language-Action system that processes visual input from driving scenarios and natural language instructions to generate driving actions. The model architecture integrates vision-language understanding with driving-specific action prediction, enabling vehicles to understand scene context and respond to multimodal commands.", "key_contribution": "The primary innovation is releasing the first open-source VLA framework for autonomous driving, including publicly available model weights and a large-scale driving dataset with diverse scenarios and multimodal annotations.", "novelty": "Unlike previous proprietary driving models that operated in isolation without language understanding, this work introduces the first open framework that bridges vision-language models with autonomous driving. It addresses the lack of publicly available large-scale multimodal driving datasets and the limited integration of language understanding in driving systems. The approach enables researchers to access and build upon shared infrastructure, moving away from the previous monopoly of closed proprietary systems.", "ai_categories": ["Vision-Language-Action Models", "Domain-Specific Applications"]}, {"paper_id": "4217361", "score": "21", "title": "Lifelong Safety Alignment for Language Models", "authors": "Haoyu Wang, Yifei Zhao, Zeyu Qin, Chao Du, Min Lin, Xueqian Wang, Tianyu Pang", "session_type": "SD-4-1906", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/8b599899e65f157c99b0a6ac3e8b45afbc4fee1a.pdf", "relevant_to_users": "10", "read_by_users": "16", "topics": "", "key_findings": "The paper demonstrates that standard fine-tuning procedures cause safety degradation in 15-40% of tested scenarios, with models experiencing catastrophic forgetting of safety constraints when adapted to new tasks post-deployment. The proposed lifelong learning framework substantially reduces safety violations while preserving task performance through continual verification, memory mechanisms, and dynamic adjustment strategies that treat safety as an ongoing process rather than one-time alignment.", "description": "This paper addresses catastrophic forgetting of safety constraints in language models during post-deployment adaptation. It proposes a lifelong learning framework that combines experience replay, constraint-based optimization, and continual safety verification to maintain alignment across sequential learning phases while balancing task performance with safety preservation.", "key_contribution": "The introduction of a lifelong learning framework that treats safety alignment as an ongoing process throughout a model's lifecycle, combining memory mechanisms to preserve learned safety constraints with dynamic monitoring and corrective interventions to prevent safety regression during continual adaptation.", "novelty": "Unlike prior work that treats safety alignment as a one-time pre-deployment objective, this work frames safety as a lifelong learning problem that must be actively maintained across sequential task adaptations. It addresses the critical gap where standard fine-tuning causes catastrophic forgetting of safety constraints, introducing mechanisms for continual verification and constraint preservation that enable safe adaptation without requiring full retraining from scratch‚Äîessential for real-world deployment of autonomous agents.", "ai_categories": ["Agent Safety and Security", "Reinforcement Learning for LLMs"]}, {"paper_id": "4175670", "score": "21", "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example", "authors": "Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, ..., yelong shen", "session_type": "SD-6-415", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/6488aab2ea1a4b2423d232a17c9fcd1545659f8c.pdf", "relevant_to_users": "13", "read_by_users": "27", "topics": "", "key_findings": "One-shot reinforcement learning with verifiable reward (1-shot RLVR) can dramatically improve LLM mathematical reasoning, boosting Qwen2.5-Math-1.5B from 36.0% to 73.6% on MATH500 and average performance across six benchmarks from 17.6% to 35.7% using just one training example. The work reveals that base models already possess significant latent reasoning capabilities that can be unlocked with minimal training signal. The method identifies novel phenomena including post-saturation generalization where test performance continues improving for 1,400+ steps after training accuracy saturates, and cross-category generalization where training on one problem type improves performance across diverse mathematical domains.", "description": "This paper investigates the data efficiency limits of reinforcement learning with verifiable rewards for mathematical reasoning in LLMs, demonstrating that a single strategically-selected training example can match the performance of training with 1,200+ examples. The work shows that policy gradient-based RL with proper exploration mechanisms can unlock latent reasoning capabilities in base models through minimal training signals.", "key_contribution": "The main contribution is a data-centric insight showing that one carefully-selected training example combined with RLVR can achieve performance comparable to training with thousands of examples, challenging assumptions about data requirements for RL fine-tuning of LLMs. The work demonstrates that the base model already possesses reasoning capabilities and RL serves primarily to concentrate probability mass on correct solutions the model can already generate stochastically.", "novelty": "Unlike prior work focusing on algorithmic refinements or dataset reduction to hundreds of examples, this work demonstrates extreme data efficiency‚Äîreducing to a single example while maintaining performance. The novelty lies in revealing that RLVR doesn't teach new reasoning skills but amplifies existing latent capabilities through policy gradients that concentrate probability mass on correct solutions. This fundamentally differs from few-shot/in-context learning by requiring actual training with gradient updates, and from standard supervised fine-tuning by using RL's exploration mechanisms to discover and reinforce correct reasoning paths the model can already generate but doesn't reliably select.", "ai_categories": ["Reinforcement Learning for LLMs", "Reasoning and Test-Time Compute", "Mathematical and Logical Reasoning"]}, {"paper_id": "4405444", "score": "20", "title": "Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs", "authors": "Zhixin Xie, Xurui Song, Jun Luo", "session_type": "SD-6-1410", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/2a60bb227d356283cc8b6eddcfeda9a0ef466282.pdf", "relevant_to_users": "4", "read_by_users": "11", "topics": "", "key_findings": "This paper demonstrates that LLMs can be jailbroken with 94.84% average success rate using only 10 benign QA pairs through a two-phase fine-tuning attack. The attack exploits overfitting dynamics: Phase 1 fine-tunes models on identical refusal answers creating narrow loss minima, then Phase 2 uses standard answers causing catastrophic forgetting of refusal behaviors. Critically, the attack uses 100% benign content (Harmful Score = 1), making it undetectable by content moderation while achieving effectiveness comparable to malicious fine-tuning (97.25% ASR). Tested on 10 models including GPT-4o (97.50% ASR), Llama3-8b (95.21% ASR), and Llama2-7b (92.57% ASR).", "description": "This paper presents a novel jailbreak attack that compromises LLM safety alignment through minimal fine-tuning with entirely benign data. By exploiting overfitting-induced sensitivity in loss landscapes, the attack causes models to forget refusal behaviors without using any harmful training content, exposing a critical vulnerability in current fine-tuning practices.", "key_contribution": "The main innovation is demonstrating that jailbreak attacks can be executed using only benign training data by strategically exploiting overfitting dynamics and catastrophic forgetting. This two-phase approach (overfit on identical refusals, then fine-tune with helpful answers) achieves high attack success rates while remaining completely undetectable by content moderation systems.", "novelty": "Unlike prior jailbreak methods that rely on adversarial prompts, identity-shifting, encryption, or toxic training data, this work reveals that the fine-tuning process itself can be weaponized using entirely benign content. It addresses the limitation that existing attacks are detectable by content moderation by achieving maximum stealth (100% benign data) while maintaining effectiveness. The key insight is exploiting sharp loss landscape geometry created by overfitting on identical refusal answers, causing small parameter updates to trigger large behavioral shifts that erase safety alignment.", "ai_categories": ["Agent Safety and Security", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4434111", "score": "20", "title": "Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning", "authors": "Sanghyun Ahn, Wonje Choi, Junyong Lee, Jinwoo Park, Honguk Woo", "session_type": "SD-6-2212", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/8a135640e90e68e7dd192021ba6a8fdff76f596f.pdf", "relevant_to_users": "6", "read_by_users": "25", "topics": "", "key_findings": "The paper introduces a neuro-symbolic framework that improves code-as-policies task success rates by 46.2% over baselines through symbolic verification and interactive validation. It achieves 86.8% executability for task-relevant actions on RLBench and real-world robotic tasks. The framework actively generates exploratory code to acquire missing observations in dynamic, partially observable environments while preserving task-relevant states.", "description": "This paper addresses the limited environmental grounding problem in LLM-based code-as-policies for embodied task planning, particularly in dynamic or partially observable settings. The proposed neuro-symbolic framework incorporates explicit symbolic verification during code generation and an interactive validation phase that actively explores the environment to gather missing information.", "key_contribution": "The main contribution is a two-phase neuro-symbolic framework combining symbolic verification with interactive validation that generates exploratory code to actively query the environment for missing observations while maintaining task progress, significantly improving reliability and success rates in embodied task planning.", "novelty": "Unlike previous code-as-policies approaches that rely solely on LLMs to generate code from initial observations, this work introduces symbolic verification mechanisms and interactive exploration capabilities. It addresses the critical limitation of handling incomplete observations and environmental uncertainty in dynamic settings by actively generating exploratory code that gathers missing information while preserving task-relevant states, rather than assuming all necessary information is available upfront.", "ai_categories": ["Tool Use and Code Generation", "Planning and Decision Making", "Spatial and Physical Reasoning"]}, {"paper_id": "4093709", "score": "19", "title": "WritingBench: A Comprehensive Benchmark for Generative Writing", "authors": "Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, ..., Fei Huang", "session_type": "SD-1-1805", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2503.05244", "relevant_to_users": "2", "read_by_users": "12", "topics": "", "key_findings": "WritingBench introduces a comprehensive benchmark spanning 6 core writing domains (Academic/Engineering, Finance/Business, Politics/Law, Literature/Art, Education, Advertising/Marketing) and 100 subdomains with 1,000 queries. The paper's query-dependent evaluation framework achieves 84% human alignment, significantly outperforming static-criteria baselines (67%, 58%). Remarkably, the framework enables a 7B-parameter model to outperform GPT-4o in writing through effective data curation, demonstrating the practical impact of criteria-aware evaluation.", "description": "WritingBench addresses the gap in LLM evaluation for writing tasks by introducing a comprehensive benchmark that evaluates models across diverse writing domains with varying style, format, and length requirements. The paper proposes a query-dependent evaluation framework that dynamically generates instance-specific assessment criteria for each writing task, complemented by a fine-tuned critic model for criteria-aware scoring.", "key_contribution": "The main contribution is a two-phase query-dependent evaluation framework that empowers LLMs to dynamically generate instance-specific assessment criteria (5 criteria per task with detailed rubrics on a 10-point scale) rather than applying static metrics, enabling more accurate evaluation of writing quality across diverse domains and demonstrating 84% human alignment.", "novelty": "Unlike existing benchmarks that use generic text generation metrics or static evaluation criteria, WritingBench dynamically generates tailored assessment criteria for individual writing tasks, adapting to specific style, format, and length requirements. This addresses the limitation that prior work failed to capture the diverse requirements of high-quality written content across domains. The framework's ability to enable smaller models (7B parameters) to outperform larger models like GPT-4o demonstrates its effectiveness in identifying superior writing quality beyond conventional benchmark approaches.", "ai_categories": ["Agent Benchmarking and Evaluation", "Domain-Specific Applications"]}, {"paper_id": "4441552", "score": "19", "title": "MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning", "authors": "Sizhe Tang, Jiayu Chen, Tian Lan", "session_type": "SD-1-300", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/8e363493ff9d9b3fa837f8d6bb3198fa13ba65f4.pdf", "relevant_to_users": "1", "read_by_users": "3", "topics": "", "key_findings": "MALinZero achieves 2-3√ó speedup over MAZero baseline, reaching the same winning rate with 50-70% fewer samples in SMAC environments. It nearly doubles winning rates on challenging SMACv2 heterogeneous scenarios and demonstrates 11% improvements on complex matrix games with 8^10 joint action spaces. The method provides theoretical regret bounds of O(n¬∑d¬∑‚àö(ŒºT)¬∑ln(T)) that scale linearly with agent count rather than exponentially with action space size, and proves (1-1/e)-approximation guarantees for joint action selection via submodular optimization.", "description": "MALinZero addresses the exponential action space explosion problem in multi-agent planning by modeling joint-action returns through low-dimensional linear combinations of per-agent action rewards. The method integrates Linear Upper Confidence Bound (LinUCT) with Monte Carlo Tree Search, projecting high-dimensional joint action spaces into tractable contextual linear bandit problems solved with strongly-convex, Œº-smooth loss functions.", "key_contribution": "The core innovation is LinUCT (Linear Upper Confidence Bound applied to Trees), which extends contextual linear bandit techniques to multi-agent MCTS by representing joint returns as X_t = ‚ü®Œ∏*, A_t‚ü© + Œµ_t where Œ∏* is an n¬∑d dimensional parameter vector. This enables efficient action selection in low-dimensional space while maintaining exploration-exploitation balance, with submodular joint action selection providing (1-1/e)-approximation guarantees.", "novelty": "Unlike MAZero which only addresses distributed state transitions, MALinZero specifically tackles combinatorial action space explosion by learning that not all dimensions of joint action spaces are equally important. Previous methods like MAZero require evaluating exponentially many joint actions (d^n), while MALinZero reduces search complexity to linear in n¬∑d through learned low-dimensional projections. The key insight is using strongly-convex loss functions that avoid underestimating good actions while not overestimating poor ones, combined with submodular maximization theory absent in prior heuristic multi-agent MCTS approaches.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Planning and Decision Making", "Reinforcement Learning for LLMs"]}, {"paper_id": "4219525", "score": "19", "title": "Don√¢¬Ä¬ôt Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models", "authors": "Sohyun An, Ruochen Wang, Tianyi Zhou, Cho-Jui Hsieh", "session_type": "SD-1-4203", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/1b9aa688f3d241f228fbd9b8694bdffb938d1d5f.pdf", "relevant_to_users": "5", "read_by_users": "11", "topics": "", "key_findings": "The paper introduces Dynamic Thinking pattern Optimization (DTO), a framework that reduces computational overhead in large reasoning models by 47% (attention FLOPs) while improving accuracy by up to 15.6%. It addresses the 'overthinking' problem where models generate unnecessarily long reasoning paths that waste computation. The method identifies the earliest point where sufficient confidence is reached, prunes unhelpful intermediate segments, and uses preference optimization to train models to prefer efficient trajectories. Results show token reduction from ~5,000 to ~3,000 on mathematical benchmarks while maintaining or improving accuracy.", "description": "This paper tackles inefficiency in large reasoning models caused by overthinking‚Äîunnecessarily complex reasoning paths that waste computation. The authors propose DTO, a framework that segments reasoning into distinct thinking patterns, identifies the earliest point of sufficient confidence, and trains models to prefer optimized trajectories that remove detrimental patterns while preserving beneficial ones.", "key_contribution": "The main contribution is the DTO framework that performs segment-level evaluation of reasoning components rather than aggregate-level metrics, enabling precise identification and removal of unhelpful reasoning segments while preserving beneficial patterns through preference optimization.", "novelty": "Unlike prior approaches that rely on heuristic truncation or length-based cutoffs, DTO uniquely addresses reasoning efficiency through segment-level analysis of individual cognitive components' contributions. It systematically identifies when models have accumulated sufficient confidence rather than applying blunt aggregate metrics. This addresses the key limitation of existing test-time compute scaling methods that focus solely on final answer accuracy without optimizing the reasoning process itself.", "ai_categories": ["Reasoning and Test-Time Compute", "Model Efficiency and Optimization", "Reinforcement Learning for LLMs"]}, {"paper_id": "4064202", "score": "19", "title": "STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models", "authors": "Narun Krishnamurthi Raman, Taylor Lundy, Kevin Leyton-Brown, Jesse Perla", "session_type": "SD-1-1510", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ce44a9975c0bc8b6ace090a98fc92829bdf5fece.pdf", "relevant_to_users": "2", "read_by_users": "4", "topics": "", "key_findings": "STEER-ME reveals critical gaps in LLM microeconomic reasoning capabilities through a comprehensive 58-element benchmark. Even state-of-the-art models like GPT-4o and Claude 3.5 Sonnet perform at near-random accuracy (51-54%) on value-of-information decisions, despite achieving mid-80% conditional accuracy on computational components. The benchmark uses auto-STEER, an LLM-assisted data generation protocol that creates fresh questions by adapting templates across domains and perspectives, mitigating training data contamination. Testing 27 LLMs revealed that only o3 achieved near-flawless performance through native code execution, while other competitive models consistently miscalculated fundamental concepts like deadweight loss and failed to properly assess information purchase decisions.", "description": "STEER-ME is a comprehensive benchmark that taxonomizes non-strategic microeconomic reasoning into 58 distinct elements covering competitive market analysis, optimal consumption, utility maximization, and information economics. Unlike the original STEER benchmark which focused on strategic game-theoretic reasoning, STEER-ME targets computational microeconomic skills requiring precise mathematical computation and sequential reasoning across decision-making under uncertainty, Bayes' rule belief updating, and value-of-information analysis.", "key_contribution": "The main innovation is the creation of a systematic taxonomy of 58 microeconomic reasoning elements combined with auto-STEER, a novel automated benchmark generation protocol that produces fresh questions across multiple domains and perspectives to prevent overfitting. The benchmark separates conceptual reasoning from computational execution, revealing that headline accuracy scores can be misleading when multiple-choice structures embed partial credit.", "novelty": "STEER-ME addresses the gap left by strategic reasoning benchmarks by focusing specifically on non-strategic microeconomic reasoning requiring precise computation. Unlike prior economic evaluations that narrowly assess specific applications, STEER-ME rigorously evaluates foundational computational reasoning skills essential for economic decision-making. The auto-STEER protocol introduces a sustainable approach to benchmark generation that can produce fresh questions dynamically, preventing models from overfitting to static evaluation sets. Most critically, the decomposed analysis approach reveals that models can achieve high computational accuracy while failing completely at the conceptual level of determining when information has value.", "ai_categories": ["Agent Benchmarking and Evaluation", "Mathematical and Logical Reasoning"]}, {"paper_id": "4173672", "score": "19", "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning", "authors": "Jiaqi Chen, Bang Zhang, Ruotian Ma, Peisong Wang, Xiaodan Liang, Zhaopeng Tu, Xiaolong Li, Kwan-Yee K. Wong", "session_type": "SD-4-4007", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/37589115324975c5657483b37546cdd4916aeeed.pdf", "relevant_to_users": "3", "read_by_users": "15", "topics": "", "key_findings": "SPC introduces a self-play adversarial framework that eliminates the need for expensive manual step-level annotations in training process reward models. Through iterative games between a 'sneaky generator' producing hard-to-detect errors and a 'critic' identifying them, the model progressively improves error detection (e.g., 70.8% to 77.7% accuracy on ProcessBench). When used to guide test-time search, SPC outperforms state-of-the-art process reward models on mathematical reasoning benchmarks (MATH500, AIME2024), demonstrating superior scalability and performance compared to annotation-dependent approaches.", "description": "This paper presents Self-Play Critic (SPC), a framework for training process reward models through adversarial self-play games rather than human annotations. Two model copies compete: a generator that produces deceptive erroneous reasoning steps, and a critic that learns to detect these errors, with both improving through competitive RL-based dynamics.", "key_contribution": "The main innovation is replacing supervised learning with human annotations by an adversarial game-theoretic approach where a critic and generator engage in competitive self-play. This creates a self-improving system that discovers challenging failure modes and edge cases autonomously, scaling better than annotation-based methods while achieving superior performance.", "novelty": "Unlike traditional process reward models (PRMs) that require extensive human annotation of step-level correctness, SPC eliminates this bottleneck through adversarial self-play dynamics. The 'sneaky generator' automatically creates challenging examples designed to fool the critic, enabling dynamic hard example mining without manual labeling. This addresses the scalability limitations of annotation-dependent approaches while achieving better generalization through exposure to adversarially-generated boundary conditions.", "ai_categories": ["Reasoning and Test-Time Compute", "Self-Improvement and Meta-Learning", "Reinforcement Learning for LLMs"]}, {"paper_id": "4203858", "score": "19", "title": "Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges", "authors": "Pengrui Quan, Brian Wang, Kang Yang, Liying Han, Mani Srivastava", "session_type": "SD-4-2411", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.11618", "relevant_to_users": "0", "read_by_users": "1", "topics": "", "key_findings": "The paper introduces STARK, a 10,000-instance benchmark for evaluating spatiotemporal reasoning in LLMs. Key findings show that reasoning models (O3, DeepSeek) significantly outperform standard LLMs, but all models struggle with combined spatiotemporal tasks involving multiple interacting objects over time. Models handle isolated spatial or temporal reasoning better than their combination, with substantial performance degradation on transitive spatial relationships across time steps and tracking moving objects.", "description": "This paper presents a comprehensive benchmark for evaluating how well large language models and reasoning models handle spatiotemporal reasoning‚Äîtasks requiring simultaneous understanding of spatial relationships (e.g., 'to the left of') and temporal relationships (e.g., 'before,' 'after'). The work systematically assesses model capabilities on scenarios involving dynamic scenes with moving objects and changing spatial configurations over time.", "key_contribution": "The main contribution is the STARK benchmark, a large-scale structured evaluation framework specifically designed for LLMs that systematically tests spatiotemporal reasoning across varying complexity levels. The work provides the first comprehensive comparison of standard LLMs versus specialized reasoning models on combined spatial-temporal understanding tasks.", "novelty": "Unlike prior work that adapted computer vision datasets or focused on either spatial or temporal reasoning separately, this paper creates a benchmark specifically designed for language models that integrates both dimensions. It addresses the limitation of existing evaluations by including controlled complexity variations for diagnostic analysis and evaluating both standard and reasoning-enhanced LLMs. The work reveals that scale alone is insufficient and that enhanced reasoning processes are critical for spatiotemporal understanding.", "ai_categories": ["Agent Benchmarking and Evaluation", "Spatial and Physical Reasoning", "Reasoning and Test-Time Compute"]}, {"paper_id": "4243068", "score": "19", "title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering", "authors": "Yuki Imajuku, Kohki Horie, Yoichi Iwata, Kensho Aoki, Naohiro Takahashi, Takuya Akiba", "session_type": "SD-4-702", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.09050", "relevant_to_users": "1", "read_by_users": "2", "topics": "", "key_findings": "ALE-Bench introduces a comprehensive benchmark for evaluating AI systems on algorithm engineering tasks requiring iterative refinement to meet specific performance criteria. The empirical evaluation reveals significant variance in current model performance (GPT-4, Claude, Gemini) across different task complexities, demonstrating that long-horizon algorithmic optimization remains challenging for existing AI systems.", "description": "ALE-Bench is a benchmark designed to assess how well language models and AI agents can tackle long-horizon, objective-driven algorithm optimization challenges. It spans competitive programming, optimization, and heuristic search domains, requiring sustained reasoning capabilities for iterative algorithmic refinement.", "key_contribution": "The benchmark bridges the gap between single-shot problem-solving benchmarks (HumanEval, MBPP) and real-world algorithm engineering by focusing on iterative refinement cycles with structured evaluation criteria tied to algorithmic efficiency and correctness, rather than just functional correctness.", "novelty": "Unlike existing code generation benchmarks that focus on single-shot solutions, ALE-Bench specifically evaluates long-horizon objectives requiring sustained iterative refinement to achieve target performance metrics. It addresses the limitation of prior benchmarks by emphasizing algorithmic optimization over time rather than immediate correctness, and provides a unified framework integrating multiple problem domains with clear success criteria for sustained problem-solving sequences.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation"]}, {"paper_id": "4233174", "score": "19", "title": "Contextual Integrity in LLMs via Reasoning and Reinforcement Learning", "authors": "Guangchen Lan, Huseyin A Inan, Sahar Abdelnabi, Janardhan Kulkarni, Lukas Wutschitz, Reza Shokri, Christopher Brinton, Robert Sim", "session_type": "SD-4-416", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/b39de1061902f7e2d23191a93a6d94f945bff28e.pdf", "relevant_to_users": "3", "read_by_users": "6", "topics": "", "key_findings": "The paper introduces the first RL-based post-training framework to teach LLMs contextual integrity reasoning, reducing inappropriate information disclosure by up to 40% while maintaining task performance. Using only ~700 synthetic examples, the approach successfully transfers to human-annotated benchmarks like PrivacyLens across multiple model families. The method combines structured Chain-of-Thought reasoning with GRPO reinforcement learning to internalize privacy norms within the model rather than relying on external constraints.", "description": "This paper addresses how autonomous LLM agents can inappropriately disclose confidential or contextually inappropriate information when making decisions on behalf of users. The authors develop a reinforcement learning framework that teaches models to explicitly reason about contextual integrity (CI) - determining what information is appropriate to share in specific contexts - through structured reasoning and reward-based training.", "key_contribution": "The main contribution is the first explicit RL-based approach (CI-RL) for instilling contextual integrity reasoning in LLMs through post-training, combined with a novel synthetic dataset generation pipeline. This achieves significant privacy protection improvements while maintaining utility, with successful transfer from synthetic training data to real-world benchmarks.", "novelty": "Unlike prior inference-time defenses (gatekeepers, firewalls, access controls) that constrain models externally, this work fundamentally changes model behavior by internalizing CI norms through training. The approach is novel in combining explicit structured reasoning (CI-CoT) with RL optimization, and demonstrates that automatically generated synthetic data spanning diverse domains and transmission principles can effectively train models for contextual integrity without requiring expensive human annotation at scale. This shifts from external constraint-based privacy protection to intrinsic model alignment.", "ai_categories": ["Agent Safety and Security", "Reinforcement Learning for LLMs", "Reasoning and Test-Time Compute"]}, {"paper_id": "3897384", "score": "19", "title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Cultural Learning", "authors": "Mircea Tudor Lic√Ñ¬É, Ojas Shirekar, Baptiste Colle, Chirag Raman", "session_type": "SD-4-401", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/aaff34bc5f2395383d58c98d38975df26262971c.pdf", "relevant_to_users": "5", "read_by_users": "18", "topics": "", "key_findings": "MindForge demonstrates that weaker open-weight LLM-powered agents can match GPT-4 performance through collaborative social learning rather than model scaling, achieving 5√ó improvement on basic tasks (37.5% vs 7% success) and 3√ó more milestones on complex tech-tree progression. The framework introduces explicit theory of mind representations and natural language communication that enable agents to diagnose and correct false beliefs and faulty code generation through multi-turn dialogue. This challenges the assumption that only large proprietary models can perform complex embodied tasks, reframing knowledge distillation as a social, test-time process rather than requiring gradient-based fine-tuning.", "description": "MindForge is a generative agent framework for embodied AI in Minecraft that combines structured belief-desire-intention representations grounded in causal theory of mind templates, natural language communication channels, and multi-component memory systems (episodic, semantic, procedural). The framework enables agents to learn through collaboration with peers and expert guidance, achieving significant performance improvements on both elementary and complex tasks without requiring model fine-tuning.", "key_contribution": "The main innovation is reframing knowledge transfer as a social, test-time learning process using explicit theory of mind reasoning and perspective-taking, enabling weaker open-weight models to improve through multi-turn dialogue rather than gradient-based training. MindForge identifies and addresses two primary failure modes in open-weight agents‚Äîfalse beliefs and faulty code generation‚Äîthrough partner-specific mental state modeling and collaborative error correction.", "novelty": "Unlike previous approaches that rely on model scaling or fine-tuning (which provides minimal benefit), MindForge introduces explicit belief modeling and theory of mind reasoning for embodied multi-agent collaboration. It addresses Voyager's limitations where open-weight LLMs fail on elementary tasks (7% success) by enabling expert agents to diagnose novice misconceptions through perspective-taking and provide targeted corrections via natural language. The framework demonstrates that social learning at test-time can achieve performance gains comparable to using much larger proprietary models, without requiring domain-specific gradient updates.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Reasoning and Test-Time Compute", "Memory and Context Management"]}, {"paper_id": "4271374", "score": "19", "title": "SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents", "authors": "Wanxin Tian, Shijie Zhang, Kevin Zhang, Xiaowei Chi, Chun-Kai Fan, Junyu Lu, Yulin Luo, Qiang Zhou, Yiming Zhao, ..., Jian Tang", "session_type": "SD-6-303", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/d934e1858b206a0ba8fe1fb5281ee9f117238785.pdf", "relevant_to_users": "4", "read_by_users": "17", "topics": "", "key_findings": "SEEA-R1 achieves state-of-the-art performance on ALFWorld with 85.07% (textual) and 46.27% (multi-modal) success rates, outperforming GPT-4o. Critically, it maintains competitive performance (80.30%/44.03%) without ground-truth rewards, demonstrating genuine self-evolution. Real robotics experiments show 34.72% absolute improvement. The framework successfully converts sparse environmental rewards into dense intermediate signals through tree search, enabling effective multi-step reasoning without hand-crafted reward functions.", "description": "SEEA-R1 is the first reinforcement fine-tuning framework for self-evolving embodied agents, addressing the challenge of learning from sparse rewards in long-horizon interactive tasks. It combines Tree-GRPO (Monte Carlo Tree Search integrated with policy optimization) and MGRM (a multi-modal generative reward model) to enable agents to autonomously generate training signals and refine reasoning through closed-loop learning without relying on task-specific reward engineering.", "key_contribution": "The paper introduces Tree-GRPO, which integrates MCTS into policy optimization to convert sparse outcome rewards into structured step-wise process signals, and MGRM, a task-agnostic learned reward model that generalizes across diverse scenarios. Together, these create a self-evolution loop where improved reasoning enhances reward accuracy, which in turn advances policy learning.", "novelty": "Unlike prior reinforcement fine-tuning work focused on symbolic domains (math, code) or embodied approaches relying on predefined modules, SEEA-R1 is the first to enable autonomous self-improvement for embodied agents through learned, generalizable reward models rather than simulator-specific signals. It addresses the fundamental limitation of sparse rewards in multi-step reasoning by using tree search to densify feedback, and eliminates dependence on hand-crafted reward functions through its self-supervised reward modeling paradigm that uses trajectory diversity and majority voting.", "ai_categories": ["Self-Improvement and Meta-Learning", "Reinforcement Learning for LLMs", "Vision-Language-Action Models"]}, {"paper_id": "4246391", "score": "18", "title": "Detecting High-Stakes Interactions with Activation Probes", "authors": "Alex McKenzie, Urja Pawar, Phil Blandfort, William Bankes, David Krueger, Ekdeep Singh Lubana, Dmitrii Krasheninnikov", "session_type": "SD-3-1112", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/6291cf20df7b3579af7a0739c773db78424d18cd.pdf", "relevant_to_users": "7", "read_by_users": "10", "topics": "", "key_findings": "The paper demonstrates that lightweight activation probes trained on synthetic data achieve robust out-of-distribution generalization (AUROC > 0.91) for detecting high-stakes LLM interactions while offering computational savings of six orders-of-magnitude compared to prompted or finetuned medium-sized LLM monitors. Six probe architectures were tested, with the Attention and Softmax probes performing best. Adding just 32-256 labeled examples from the target distribution can boost performance to >0.97 AUROC, exceeding prompted Llama-70B baselines. The work introduces a novel synthetic dataset construction methodology with deconfounding techniques and validates the approach across diverse real-world test sets.", "description": "This paper studies using activation probes (lightweight classifiers operating on LLM internal representations) to detect 'high-stakes interactions'‚Äîcontexts where LLM outputs could lead to substantial real-world consequences like medical/financial harm. The authors evaluate six probe architectures trained on synthetic data and demonstrate they generalize robustly to real-world scenarios while being dramatically more computationally efficient than traditional LLM-based monitoring approaches.", "key_contribution": "The main contribution is demonstrating that simple linear probes on model activations can serve as highly efficient first-stage filters in cascaded monitoring systems, matching the performance of medium-sized LLM monitors (8-12B parameters) at a million times lower computational cost. The work introduces novel probe architectures (Softmax and Max of Rolling Means) and provides the first rigorous benchmarking of probes against finetuned LLM monitors for safety monitoring.", "novelty": "Unlike prior work that focused on in-distribution interpretability tasks, this paper addresses synthetic-to-real-world generalization for safety monitoring and rigorously benchmarks probes against state-of-the-art LLM-based monitors. It introduces 'high-stakes interaction detection' as a new monitoring target distinct from existing work on deception or truthfulness detection. The key innovation is demonstrating that cascaded systems combining cheap probes with expensive classifiers can exceed the performance of either approach alone within fixed computational budgets, solving the effectiveness-vs-cost trade-off in deployment monitoring.", "ai_categories": ["Agent Safety and Security", "Tool Use and Code Generation"]}, {"paper_id": "4120946", "score": "18", "title": "LLM Generated Persona is a Promise with a Catch", "authors": "Leon Li, Haozhe Chen, Hongseok Namkoong, Tianyi Peng", "session_type": "SD-6-1109", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2503.16527", "relevant_to_users": "2", "read_by_users": "5", "topics": "", "key_findings": "The paper reveals that LLM-generated personas exhibit systematic biases in downstream tasks, with more LLM-generated content producing increasingly left-leaning, overly positive results that deviate significantly from real-world distributions. In large-scale experiments using ~1 million personas across 6 LLMs and 500+ opinion questions, personas with more LLM-generated content predicted a Democratic sweep of all U.S. states in the 2024 election and consistently showed sentiment inflation, missing realistic portrayals of life challenges. The study demonstrates that current ad hoc persona generation techniques lack methodological rigor and are non-representative of actual population distributions.", "description": "This paper systematically evaluates bias in LLM-generated synthetic personas used to simulate human behavior for social science, marketing, and economic research. Through extensive experiments generating approximately one million personas using six open-source LLMs across three persona types, the researchers test their validity on presidential election forecasting and 500+ opinion survey questions, revealing substantial systematic biases.", "key_contribution": "The paper provides the first large-scale, systematic evaluation exposing critical flaws in current LLM persona generation practices, demonstrating that the generation process itself‚Äînot just simulation‚Äîintroduces substantial bias. It contributes a framework categorizing persona types, an open-sourced dataset of one million personas, semantic analysis revealing bias sources, and a roadmap for developing more rigorous persona generation science.", "novelty": "Unlike prior work that focused on simulation bias or assumed adequate persona generation, this research explicitly dissects how the persona generation process itself introduces systematic distortions across diverse domains and populations. The study addresses the gap between theoretical feasibility demonstrations and practical validation by conducting unprecedented scale evaluation (1 million personas, 500+ questions, 6 LLMs) with rigorous comparison against real demographic distributions. It shifts the field's focus from treating persona generation as a given to recognizing it as a critical source of bias requiring methodological rigor and calibration.", "ai_categories": ["Agent Benchmarking and Evaluation", "Agent Safety and Security"]}, {"paper_id": "4442470", "score": "18", "title": "ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking", "authors": "Lequan Lin, Dai Shi, Andi Han, Feng Chen, Qiuzheng Chen, Jiawen Li, Zhaoyang Li, Jiyuan Zhang, Zhenbang Sun, Junbin Gao", "session_type": "SD-6-3300", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/2d4818decec8bd80bc1421d0101a796c541e0194.pdf", "relevant_to_users": "0", "read_by_users": "5", "topics": "", "key_findings": "ACT introduces a three-stage pipeline (annotation, error estimation, human review) that reduces the performance gap between machine and human annotation to less than 2% on most benchmarks while saving 70-90% of human costs. Key insights include: cross-criticism (using different models for annotation vs criticism) outperforms self-criticism by up to 22.46%, chain-of-thought prompting helps criticism more than annotation, and exponential weighting sampling rules outperform normalization-based approaches. The method achieved 87.95% accuracy on CIFAR-10 with only 11.52% human budget, just 0.71% below full human annotation.", "description": "This paper presents ACT (Annotation with Critical Thinking), a training-free framework that uses multimodal LLMs as both annotators and error detectors (criticizers) to identify suspicious cases for strategic human review. The approach works across NLP, computer vision, and multimodal domains, using a second MLLM to estimate error probability without ground truth and directing limited human budget to high-uncertainty cases.", "key_contribution": "The main innovation is the dual-role MLLM architecture with zero-shot error detection through cross-criticism, combined with theoretical contributions including adapted active M-estimation with exponential weighting/thresholding sampling rules. This provides provable bounds on parameter gaps between ACT-trained and fully human-annotated models while eliminating the need for task-specific training of error detectors.", "novelty": "Unlike prior work requiring task-specific training for error detection (e.g., CDI with XGBoost) or relying on self-criticism, ACT uses zero-shot cross-criticism where different MLLMs serve as annotator and criticizer, eliminating training overhead while working across diverse domains. The theoretical contribution of exponential weighting sampling rules addresses fundamental limitations of normalization-based active M-estimation in prior work, which performs poorly under limited budgets. ACT also systematically addresses documented bias in LLM-as-judge literature where models favor their own outputs, demonstrating that strategic model pairing (best annotator with second-best criticizer) significantly outperforms self-evaluation approaches.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models"]}, {"paper_id": "4301873", "score": "17", "title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning", "authors": "Yuncong Yang, Jiageng Liu, Zheyuan Zhang, Siyuan Zhou, Reuben Tan, Jianwei Yang, Yilun Du, Chuang Gan", "session_type": "SD-1-4307", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/5ed386610c8657ff319e5833b8272c6459dd85a4.pdf", "relevant_to_users": "14", "read_by_users": "36", "topics": "", "key_findings": "MindJourney introduces test-time scaling for spatial reasoning by coupling VLMs with controllable world models based on video diffusion, achieving 7.7-8% improvement on SAT benchmark without fine-tuning. The framework enables VLMs to actively explore simulated 3D environments through iterative camera trajectory planning and multi-view synthesis, outperforming even VLMs trained with reinforcement learning. It uses spatial beam search to efficiently prioritize promising exploration paths, providing a plug-and-play solution that addresses VLMs' fundamental lack of internal 3D dynamic models.", "description": "MindJourney is a test-time scaling framework that enhances VLMs' spatial reasoning by coupling them with video diffusion-based world models. The VLM iteratively sketches camera trajectories while the world model synthesizes corresponding views, enabling multi-view evidence gathering through interactive 3D exploration guided by spatial beam search.", "key_contribution": "The main innovation is enabling test-time scaling for spatial reasoning through world models without requiring fine-tuning. The framework introduces spatial beam search‚Äîan efficient pruning algorithm that balances breadth and depth to prioritize promising exploration paths in simulated 3D environments, providing VLMs with dynamic multi-view evidence for spatial queries.", "novelty": "Unlike traditional methods that rely on static 2D image analysis or require extensive retraining, MindJourney provides a plug-and-play test-time solution that grants VLMs missing 3D reasoning capabilities through external world modeling. It addresses the fundamental limitation that VLMs perceive 2D images but lack internal models of 3D dynamics by enabling active exploration and anticipation of scene changes after egocentric motion. The approach differs from prior test-time scaling methods by specifically targeting spatial reasoning through controllable video generation and outperforms VLMs trained with reinforcement learning while requiring no parameter updates.", "ai_categories": ["Reasoning and Test-Time Compute", "Spatial and Physical Reasoning", "Vision-Language-Action Models"]}, {"paper_id": "4436783", "score": "17", "title": "Once Upon an Input: Reasoning via Per-Instance Program Synthesis", "authors": "Adam Stein, Neelay Velingker, Mayur Naik, Eric Wong", "session_type": "SD-2-3717", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/7b4a6ca902e7228c2855e6b864e4699825b54723.pdf", "relevant_to_users": "2", "read_by_users": "12", "topics": "", "key_findings": "PIPS introduces per-instance program synthesis with structural feedback, achieving 8.6-9.4% absolute accuracy improvements over Program of Thought and Chain of Thought baselines across 30 benchmarks. The method reduces undesirable program generations by 65.1% through iterative refinement using structural checks for trivial programs, syntax errors, type errors, and placeholders. A novel confidence metric dynamically selects between program synthesis and direct reasoning per instance with 65.3% accuracy, while decoupled symbolic extraction enables robust handling of multimodal inputs.", "description": "This paper presents Per-Instance Program Synthesis (PIPS), a method that generates and refines executable programs at the instance level using structural feedback without requiring task-specific guidance or explicit test cases. The approach separates perceptual understanding (symbolic extraction) from algorithmic reasoning (program execution), and uses a confidence metric to dynamically choose between program synthesis and direct reasoning for each input.", "key_contribution": "The main innovation is a specification-free program synthesis framework that iteratively refines programs using structural feedback rather than test cases, coupled with a learned confidence metric that enables per-instance adaptive selection between synthesis and reasoning. This allows the system to address complex algorithmic problems while avoiding the common pitfalls of prior program-generation approaches.", "novelty": "Unlike Chain of Thought and Program of Thought methods that use single-pass generation, PIPS introduces iterative refinement with structural feedback loops that detect and correct trivial programs, syntax errors, and type mismatches without requiring test cases. The work addresses the fundamental limitation that prior methods generate programs for all inputs regardless of suitability, introducing a calibrated confidence metric for adaptive per-instance method selection. The decoupled symbolic extraction phase prevents programs from attempting raw data processing (especially for multimodal inputs), a critical failure mode where PoT attempts to use image processing libraries 12.7% of the time on visual tasks versus 0% for PIPS.", "ai_categories": ["Tool Use and Code Generation", "Reasoning and Test-Time Compute", "Planning and Decision Making"]}, {"paper_id": "4245953", "score": "17", "title": "Can We Infer Confidential Properties of Training Data from LLMs?", "authors": "Pengrun Huang, Chhavi Yadav, Kamalika Chaudhuri, Ruihan Wu", "session_type": "SD-4-1313", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/cd0fb86716237f2897625fe1354d6af3d7b0ec5b.pdf", "relevant_to_users": "3", "read_by_users": "15", "topics": "", "key_findings": "This paper demonstrates that fine-tuned LLMs leak confidential dataset-level properties through property inference attacks. Two novel attacks achieve MAE as low as 0.61-3.28% for gender ratio inference and 1.43-7.95% for medical diagnosis prevalence. The word-frequency shadow-model attack and prompt-based generation attack successfully extract sensitive information like patient demographics and disease prevalence from medical LLMs. The work reveals a previously unrecognized vulnerability where properties embedded indirectly in text (e.g., gender implied through 'my gynecologist') can be inferred with modest computational resources (500-1,000 generated samples).", "description": "This paper investigates property inference attacks on fine-tuned large language models, focusing on whether adversaries can extract confidential dataset-level properties such as patient demographics or disease prevalence from models deployed in sensitive domains like healthcare. The authors introduce PropInfer, a benchmark built on the ChatDoctor medical dataset, and demonstrate successful attacks across multiple pretrained LLMs using two fine-tuning paradigms: question-answering and chat-completion.", "key_contribution": "The main contribution is the PropInfer benchmark and two tailored property inference attacks for LLMs: a black-box prompt-based generation attack and a grey-box shadow-model attack leveraging word frequency signals. These attacks successfully reveal confidential dataset properties from fine-tuned models, achieving mean absolute errors below 7% for gender ratios and medical diagnosis prevalence across Llama and Pythia models.", "novelty": "Unlike prior property inference research that focused on discriminative models (image classifiers) or generative models (GANs), this work is the first to systematically study property inference attacks specifically for large language models. The novelty lies in adapting attacks to handle properties indirectly embedded in text rather than explicit labels, proposing word-frequency features instead of loss/probability vectors, and differentiating between two fine-tuning paradigms (SFT vs. CLM-FT) which exhibit different vulnerability patterns. This addresses the gap where it was unclear whether property inference attacks transfer to LLMs that blend discriminative and generative capabilities.", "ai_categories": ["Agent Safety and Security", "Domain-Specific Applications"]}, {"paper_id": "4208817", "score": "17", "title": "Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs", "authors": "Jie Ma, Ning Qu, Zhitao Gao, Xing Rui, Jun Liu, Hongbin Pei, Jiang Xie, Lingyun Song, Pinghui Wang, ..., su zhou", "session_type": "SD-6-3616", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/33aae90c6ae97c2a878758d797cf8196f7ca09de.pdf", "relevant_to_users": "4", "read_by_users": "12", "topics": "", "key_findings": "This paper achieves state-of-the-art performance on knowledge graph question answering with a 13% Hit@1 improvement on ComplexWebQuestions while requiring only 2.9 LLM calls per question (vs. 12.7-22.6 for competitors). It introduces Kahneman-Tversky optimization (KTO) for handling severely imbalanced training data (25% positive, 75% negative paths) in relation path generation, treating it as a preference optimization problem that models human utility and risk perception. The framework systematically exploits both structural priors (relation paths) and constraint priors (type, temporal, ordinal, multi-entity constraints) that previous retrieval-augmented generation methods overlooked, enabling both faithful reasoning and reliable verification.", "description": "The paper proposes Deliberation over Priors (DP), a trustworthy reasoning framework for knowledge graph question answering that addresses LLM hallucinations by systematically leveraging structural patterns and constraints embedded in knowledge graphs. The framework combines progressive knowledge distillation (using supervised fine-tuning and Kahneman-Tversky optimization) with a reasoning-introspection strategy across four modules: Distillation, Planning, Instantiation, and Introspection.", "key_contribution": "The main innovation is the dual-strategy framework that (1) uses Kahneman-Tversky optimization to distill structural priors into LLMs for faithful relation path generation despite severe class imbalance, and (2) employs constraint-based introspection for deliberative verification and backtracking. This creates bidirectional control where structural priors guide what reasoning paths to generate while constraint priors determine which paths to accept or reject.", "novelty": "Unlike previous KGQA and RAG approaches that retrieve facts in an end-to-end or step-by-step manner but fail to systematically exploit KG structure, this work comprehensively leverages both structural information (relation path patterns) and constraint priors (explicit/implicit constraints) for simultaneous faithfulness and reliability. The application of Kahneman-Tversky optimization (borrowed from human utility modeling and risk perception) to preference learning under severe class imbalance in KG path generation is novel. The introspection-backtracking mechanism that verifies paths against extracted constraints and iteratively refines reasoning is a new approach to ensuring trustworthy generation.", "ai_categories": ["Reasoning and Test-Time Compute", "Tool Use and Code Generation", "Agent Safety and Security"]}, {"paper_id": "4431142", "score": "16", "title": "From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction", "authors": "Zhida Zhao, Talas Fu, Yifan Wang, Lijun Wang, Huchuan Lu", "session_type": "SD-2-4809", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/dbe78cc8fdf28b0340af74010ec4ad766aca831b.pdf", "relevant_to_users": "5", "read_by_users": "33", "topics": "", "key_findings": "PWM achieves state-of-the-art autonomous driving performance (0.04% collision rate on nuScenes, 88.1 PDMS on NAVSIM) using only single front-camera input, matching methods that require multi-view cameras and LiDAR. The collaborative state-action prediction mechanism enables action-free future state forecasting at 40 FPS with only 0.28s latency for 10-frame predictions. The approach demonstrates training scalability by pretraining on 1747 hours of unlabeled video data from OpenDV-YouTube, eliminating dependency on expensive action-labeled datasets.", "description": "This paper introduces Policy World Model (PWM), a unified architecture for autonomous driving that integrates world modeling and trajectory planning through collaborative state-action prediction. The system uses action-free future state forecasting to enable human-like anticipatory perception, where predicted future states actively inform action selection rather than being generated independently.", "key_contribution": "The main innovation is collaborative state-action prediction that creates synergistic connections between world modeling and planning: P_Œ∏(future, action|observation) instead of separate P_w(future|observation) and P_Œ∏(action|observation). This enables the model to forecast future states before action prediction, with forecasted frames actively informing trajectory planning decisions.", "novelty": "Previous world models either required separate policy models for planning or unified architecture without true synergy between forecasting and planning. PWM addresses this by enabling action-free video forecasting that eliminates dependency on action-labeled training data and allows flexible temporal rollout before action decisions. Unlike action-conditioned generation approaches, PWM's forecasting mechanism can predict future states independently first, then use these predictions to guide planning, mimicking human anticipatory perception where we envision scenarios before deciding actions.", "ai_categories": ["Planning and Decision Making", "Vision-Language-Action Models", "Reinforcement Learning for LLMs"]}, {"paper_id": "4163509", "score": "16", "title": "CodeCrash: Exposing LLM Fragility to Misleading Natural Language in Code Reasoning", "authors": "Man Ho LAM, Chaozheng Wang, Jen-tse Huang, Michael Lyu", "session_type": "SD-4-2610", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/9b9883a392dca607cfb2b26eccc24136b0e5cf90.pdf", "relevant_to_users": "5", "read_by_users": "17", "topics": "", "key_findings": "CodeCrash reveals that LLMs suffer 23.2% average performance degradation when code contains misleading natural language cues (comments, print statements, hints), demonstrating that models prioritize textual descriptions over executable semantics. Large Reasoning Models (LRMs) like QwQ-32B exhibit superior initial robustness but experience 'reasoning collapse'‚Äîa pathological phenomenon where plausible but incorrect hints trigger 2-3x token consumption (up to 32,000+ tokens) and quadratic growth in confusion tokens due to cognitive dissonance. The study establishes that current LLMs lack critical reasoning capability to distinguish functional code logic from non-executable natural language, creating reliability risks for real-world code understanding applications.", "description": "This paper introduces CodeCrash, a stress-testing framework with 1,279 questions from CruxEval and LiveCodeBench that systematically evaluates LLM robustness in code reasoning under structural perturbations and misleading natural language contexts. The benchmark tests whether models can maintain accuracy when code behavior contradicts accompanying comments, docstrings, or hints, revealing fundamental weaknesses in how LLMs prioritize executable semantics versus textual cues.", "key_contribution": "CodeCrash is the first benchmark to systematically utilize natural language-embedded misleading perturbations to evaluate code reasoning robustness, exposing that LLMs over-rely on textual cues rather than performing genuine code execution tracing. The framework identifies 'reasoning collapse' in advanced Large Reasoning Models‚Äîa novel phenomenon where cognitive dissonance from plausible contradictions causes pathological overthinking and token explosion.", "novelty": "Unlike prior work focused on structural mutations (CCTest, ReCode) or natural language variations in code generation, CodeCrash uniquely combines both to stress-test code understanding rather than generation, specifically examining whether models distinguish executable semantics from misleading natural language. Previous benchmarks evaluated robustness through syntactic transformations or adversarial inputs, but CodeCrash is the first to systematically inject contextually plausible but semantically incorrect hints (comments, prints, docstrings) that mimic real-world scenarios of outdated documentation or malicious comments. The discovery of reasoning collapse in LRMs addresses an unexplored limitation where enhanced reasoning capabilities paradoxically create vulnerability to cognitive dissonance.", "ai_categories": ["Tool Use and Code Generation", "Agent Safety and Security", "Reasoning and Test-Time Compute"]}, {"paper_id": "4442549", "score": "16", "title": "Learning and Planning Multi-Agent Tasks via an MoE-based World Model", "authors": "Zijie Zhao, Zhongyue Zhao, Kaixuan Xu, Yuqian Fu, Jiajun Chai, Yuanheng Zhu, Dongbin Zhao", "session_type": "SD-4-303", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/09e9ae881c16219816d4e101be5c5634163ae0b6.pdf", "relevant_to_users": "3", "read_by_users": "8", "topics": "", "key_findings": "M3W is the first MoE-based multi-task world model for multi-agent reinforcement learning, demonstrating superior performance, sample efficiency, and multi-task adaptability on Bi-DexHands (14 tasks) and MA-Mujoco (24 tasks). The paper shows that applying MoE to world models rather than policies enables effective knowledge reuse across similar tasks while avoiding gradient conflicts across dissimilar tasks, achieving better results than policy-centric multi-task MARL methods.", "description": "This paper introduces M3W, a novel approach that applies mixture-of-experts (MoE) architecture to world models for multi-task multi-agent reinforcement learning. The system uses a SoftMoE-based dynamics model with a SparseMoE-based predictor for learning, and performs planning by evaluating and optimizing actions through predicted rollouts without relying on explicit policy models.", "key_contribution": "The main innovation is applying MoE to world models instead of policies for MT-MARL, enabling both learning and planning in a unified framework. The dual architecture combines SoftMoE dynamics modeling for smooth task representation and SparseMoE prediction for efficient task-specific rollouts, allowing model-based planning without policy networks.", "novelty": "Unlike existing multi-task MARL methods that apply MoE to policies (which struggle due to substantial variation in optimal policies across tasks), M3W exploits the bounded similarity in task dynamics‚Äîtasks within groups share similar dynamics while differing significantly between groups. This world model-centric approach with MoE addresses gradient conflicts in multi-task learning while enabling policy-free planning through learned dynamics, overcoming limitations of policy-centric methods that must balance conflicting policy objectives across diverse tasks.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Planning and Decision Making", "Reinforcement Learning for LLMs"]}, {"paper_id": "4434258", "score": "15", "title": "MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection", "authors": "Shengtian Yang, Yue Feng, Yingshi Liu, Jingrou Zhang, Jie Qin", "session_type": "", "session_time": "", "session_location": "Foyer", "pdf_url": "https://openreview.net/pdf/12eb1e4189682fe7b8223e53327f9e2ada20bd59.pdf", "relevant_to_users": "1", "read_by_users": "8", "topics": "", "key_findings": "MoniTor is the first training-free online video anomaly detection framework leveraging LLMs, achieving real-time detection with 0.6-second intervals and 5-second end-to-end latency. It introduces a hierarchical dual-memory architecture with LSTM-inspired mechanisms and a scoring queue that mitigates LLM instruction dependency. On UCF-Crime dataset, MoniTor achieves 82.57% AUC, outperforming online baselines by 6.51% and competing with offline methods without requiring any training data or labels.", "description": "MoniTor addresses real-time video anomaly detection in streaming surveillance scenarios by applying Vision-Language Models and LLMs in a training-free paradigm. The system uses dynamic memory gating to capture temporal dependencies, behavior prediction to anticipate events, and a scoring queue with anomaly priors to guide LLM decision-making without requiring labeled training data.", "key_contribution": "The main innovation is the first training-free online VAD framework that integrates LLMs with a novel hierarchical memory architecture combining episodic and working memory, an adaptive forgetting gate to filter irrelevant historical information, and a standard scoring queue that provides context-aware guidance to LLMs for distinguishing normal from abnormal behaviors in real-time streaming video.", "novelty": "Unlike prior work limited to offline processing (LAVAD, EventVAD) or requiring extensive training data, MoniTor uniquely combines training-free operation with online streaming capability. It addresses critical LLM limitations in temporal reasoning through memory gating that prevents historical anomalies from corrupting current assessments‚Äîa problem that naive LLM approaches suffer from. The scoring queue mechanism fundamentally solves LLM instruction dependency by providing dynamic historical context, enabling real-time anomaly assessment without pre-trained models or labeled data.", "ai_categories": ["Vision-Language-Action Models", "Domain-Specific Applications"]}, {"paper_id": "4204607", "score": "15", "title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks", "authors": "Yinghao Zhu, Ziyi He, Haoran Hu, Xiaochen Zheng, Xichen Zhang, Wang, Junyi Gao, Liantao Ma, Lequan Yu", "session_type": "SD-4-1809", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.12371", "relevant_to_users": "0", "read_by_users": "3", "topics": "", "key_findings": "MedAgentBoard reveals that multi-agent collaboration does not universally outperform single LLMs or conventional methods in medical AI. While multi-agent systems show benefits in specific scenarios like clinical workflow automation (improving task completeness), they are outperformed by advanced single LLMs in textual medical QA and critically underperform specialized conventional methods in medical visual QA and EHR-based prediction tasks. The research emphasizes that task-specific, evidence-based selection of AI approaches is essential, and the complexity overhead of multi-agent systems must be carefully weighed against tangible performance gains.", "description": "MedAgentBoard is a comprehensive benchmark for systematic evaluation of multi-agent collaboration, single-LLM, and conventional approaches across four diverse medical task categories: medical (visual) question answering, lay summary generation, structured EHR predictive modeling, and clinical workflow automation. It addresses the lack of generalizability in existing evaluations and provides rigorous comparisons across text, medical images, and structured EHR data to guide evidence-based AI solution selection in medicine.", "key_contribution": "The paper introduces the first unified comparative framework that systematically evaluates multi-agent systems alongside single LLMs and conventional methods across diverse medical tasks with multiple data modalities. It provides evidence-based insights challenging the assumption of multi-agent superiority, demonstrating that specialized conventional methods and task-appropriate approaches often outperform complex multi-agent architectures in medical AI applications.", "novelty": "Unlike prior work that evaluated medical AI approaches in isolation or narrow domains, MedAgentBoard is the first to provide rigorous head-to-head comparisons of multi-agent systems, single LLMs, and conventional methods across diverse clinical applications. It addresses the critical gap of unvalidated claims about multi-agent effectiveness by establishing an evidence-based framework that reveals when multi-agent collaboration provides genuine value versus when simpler approaches suffice. The benchmark uniquely combines multiple task categories reflecting real clinical workflows with task-specific metrics, moving beyond performance-only evaluation to consider the practical trade-offs between architectural complexity and actual performance gains.", "ai_categories": ["Agent Benchmarking and Evaluation", "Multi-Agent Systems and Collaboration", "Domain-Specific Applications"]}, {"paper_id": "4219301", "score": "14", "title": "Caption This, Reason That: VLMs Caught in the Middle", "authors": "Zihan Weng, Lucas Gomez, Taylor Whittington Webb, Pouya Bashivan", "session_type": "SD-1-2112", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/756a05a421ed180f3aedfcaa7a55a73f50e83d30.pdf", "relevant_to_users": "8", "read_by_users": "35", "topics": "", "key_findings": "This paper reveals a fundamental trade-off in Vision-Language Models between captioning (descriptive) and reasoning (analytical) capabilities. The research demonstrates that optimizing VLMs for one capability often degrades performance on the other, creating a design tension. It provides empirical evidence that architectural choices and training procedures inherently favor one task type over the other, explaining variable VLM performance across different applications.", "description": "The paper investigates the inherent conflict between image captioning and visual reasoning capabilities in Vision-Language Models. It formally characterizes how VLMs are 'caught in the middle' when trying to balance descriptive generation tasks with complex analytical reasoning tasks.", "key_contribution": "The main contribution is formally identifying and systematically analyzing the previously under-explored trade-off between captioning and reasoning in VLMs, demonstrating that architectural and training choices optimal for one capability are often suboptimal for the other.", "novelty": "Unlike previous work that evaluated VLMs on individual tasks in isolation, this paper takes a comparative approach to examine how capability in one domain affects another. It is the first to explicitly frame and systematically analyze this fundamental tension, moving beyond the assumption that VLMs can seamlessly handle both descriptive and analytical tasks equally well. This reveals important insights about why certain architectural and training decisions create performance degradation patterns across task types.", "ai_categories": ["Vision-Language-Action Models", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4442314", "score": "14", "title": "Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders", "authors": "Ali Rasekh, Erfan Bagheri Soula, Omid Daliran, Simon Gottschalk, Mohsen Fayyaz", "session_type": "", "session_time": "", "session_location": "Foyer", "pdf_url": "https://openreview.net/pdf/8983a85d88c67728d26c47fa75cdce08379b3561.pdf", "relevant_to_users": "1", "read_by_users": "20", "topics": "", "key_findings": "The paper introduces STAVEQ2, which integrates stacked temporal attention (STA) modules directly into the vision encoder of Video-LLMs. This architectural modification achieves up to +5.5% improvement on video understanding benchmarks (VITATECS, MVBench, Video-MME), with particularly strong gains in action recognition and temporal reasoning tasks. The approach uses divided space-time attention with 4x fewer heads for temporal processing and demonstrates that existing Video-LLMs have fundamental architectural limitations in temporal understanding rather than just training data issues.", "description": "This paper addresses critical limitations in Video-LLMs' temporal understanding by introducing stacked temporal attention modules within the vision encoder itself. The approach processes spatial features first, then applies temporal attention across frames for each patch independently, enabling better capture of action progression and temporal dynamics before visual tokens reach the language model.", "key_contribution": "The main contribution is the STAVEQ2 architecture that embeds temporal reasoning capabilities directly into the vision encoder through stacked temporal attention modules, rather than relying solely on the LLM to infer temporal relationships. This includes a parameter-efficient design using 4x fewer attention heads for temporal processing and a two-stage training strategy with zero-initialized output projections.", "novelty": "Unlike previous Video-LLM architectures like Qwen2-VL that delegate temporal understanding entirely to the LLM or InternVideo2 that uses joint spatiotemporal attention, this work introduces sequential divided space-time attention within the vision encoder. It addresses the fundamental limitation where models show performance drops with few-shot examples, proving the issue is architectural rather than data-related. The approach differs from naive temporal token augmentation by systematically processing temporal relationships throughout the encoding process with reduced computational overhead through fewer attention heads.", "ai_categories": ["Vision-Language-Action Models", "Reasoning and Test-Time Compute"]}, {"paper_id": "4441566", "score": "14", "title": "Predictable Scale (Part II) --- Farseer: A Refined Scaling Law in LLMs", "authors": "Houyi Li, Wenzhen Zheng, Qiufeng Wang, Zhenyu Ding, Haoying Wang, Zili Wang, Shijie Xuyang, Ning Ding, Shuigeng Zhou, ..., Daxin Jiang", "session_type": "", "session_time": "", "session_location": "Foyer", "pdf_url": "https://openreview.net/pdf/b2d2dae1fc87cdd4510e8c2672fcf585d1c653f2.pdf", "relevant_to_users": "3", "read_by_users": "10", "topics": "", "key_findings": "Farseer introduces a refined scaling law that improves extrapolation accuracy by 433% compared to Chinchilla's law, achieving only 0.47% relative error when predicting loss for a 25.1B parameter model beyond the training range. The work is validated through training ~1,000 LLMs consuming 3 million H100 GPU hours. Critically, Farseer predicts that optimal data-to-parameter ratios increase with larger compute budgets, contradicting Chinchilla's static D/N‚âà20 recommendation and better aligning with modern LLM training practices (Llama, Qwen). The approach demonstrates generalization across different data compositions and maintains accuracy as model sizes scale.", "description": "This paper introduces Farseer, an improved scaling law for predicting Large Language Model performance across different model sizes and training dataset scales. The work addresses the critical 'scaling gap' where insights from small-scale experiments fail to transfer reliably to production-scale training, proposing a novel functional form that explicitly models N-dependent data scaling effects.", "key_contribution": "Farseer's main innovation is a mathematically rigorous functional form L(N,D)=e^(a‚ÇÉ¬∑N^Œ≥+b‚ÇÉ)+e^(a‚ÇÇ¬∑N^Œ≤+b‚ÇÇ)¬∑D^(-e^(a‚ÇÅ¬∑N^Œ±+b‚ÇÅ)) that captures how optimal data scaling varies across model sizes through N-dependent parameters A(N) and B(N), replacing Chinchilla's constant parameters. This is achieved through a differential analysis approach and three-stage fitting method that proves the necessity of interaction terms between model size and data volume.", "novelty": "Unlike Chinchilla's law which uses constant data-scaling parameters (B and Œ≤) that only capture 'average' behavior and perform poorly at distribution extremes, Farseer introduces N-dependent scaling factors that adapt across the full model size spectrum. The work employs differential analysis to mathematically prove that loss changes depend non-trivially on both N and D, requiring an interaction term H(N,D) that additive formulations fundamentally omit. This addresses Chinchilla's core limitation of assuming static optimal data-to-parameter ratios, which contradicts empirical observations in modern large-scale LLM training.", "ai_categories": ["Model Efficiency and Optimization", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4213246", "score": "14", "title": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models", "authors": "Zekai Zhao, Qi Liu, Kun Zhou, Zihan Liu, Yifei Shao, Zhiting Hu, Biwei Huang", "session_type": "SD-2-1813", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/2b62626f959a636707dfb61efe094e4dcd7bd32f.pdf", "relevant_to_users": "4", "read_by_users": "18", "topics": "", "key_findings": "The paper discovers that long chain-of-thought (CoT) abilities already exist dormant in base language models and can be activated without training. Key findings: (1) Long-CoT related activations concentrate in the last few layers (>50% in final layer) and follow predictable logarithmic decay patterns after trigger tokens, (2) Simply amplifying these activations and inserting 'wait' tokens at strategic points (e.g., after mathematical expressions) elicits long-form reasoning at inference time, achieving 45‚Üí57.5% accuracy gains on AMC23 and 10‚Üí49% self-reflection rates on Math500 without any training, (3) Training only 1.51% of parameters (activation amplification modules in last layers with reduced LoRA) matches full fine-tuning performance while being 4√ó more parameter-efficient, generating 50% fewer tokens on AMC23.", "description": "This paper presents a training-free method to elicit long chain-of-thought reasoning in language models by analyzing and amplifying specific activations in the final layers. The authors discover that base models already possess latent long-CoT abilities that can be 'awakened' through targeted activation control and strategic insertion of 'wait' tokens, eliminating the need for costly reinforcement learning or supervised fine-tuning.", "key_contribution": "The main innovation is discovering that long-CoT abilities are dormant in base models and can be activated through surgical intervention on a small set of high-impact activations following predictable logarithmic decay patterns. This enables both training-free inference-time control and highly parameter-efficient fine-tuning (1.51% parameters) that matches full model fine-tuning performance.", "novelty": "Unlike prior work requiring expensive RL or supervised fine-tuning on large datasets, this paper reveals long-CoT ability already exists in base models and demonstrates how to access it through inference-time activation amplification. The key novel insight is the empirical discovery that long-CoT reasoning is governed by sparse, predictable activation patterns in final layers following logarithmic decay‚Äîenabling a training-free method using only 160 contrastive examples. This fundamentally shifts from 'teaching models to reason' to 'awakening latent reasoning abilities' through mechanistic understanding of activation dynamics.", "ai_categories": ["Reasoning and Test-Time Compute", "Model Efficiency and Optimization"]}, {"paper_id": "4227128", "score": "14", "title": "zip2zip: Inference-Time Adaptive Tokenization via Online Compression", "authors": "Saibo Geng, Nathan Ranchin, Yunzhen Yao, Maxime Peyrard, Chris Wendler, Michael Gastpar, Robert West", "session_type": "SD-2-3615", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/0f10815a6be342f3a8137fe6510088e422bd3a6b.pdf", "relevant_to_users": "5", "read_by_users": "19", "topics": "", "key_findings": "zip2zip achieves 15-40% token sequence reduction and up to 102% prefilling speedup on H100 GPUs by dynamically adapting vocabulary at inference time using Lempel-Ziv-Welch compression. The method demonstrates 48-70% bytes-per-token efficiency gains in code domains and 48-61% in math, while maintaining reasonable performance across seven NLP benchmarks. Existing LLMs can be adapted to zip2zip in just 10 GPU-hours via parameter-efficient fine-tuning, making it a practical drop-in replacement for static tokenization.", "description": "This paper introduces zip2zip, a method that enables language models to adaptively expand their tokenization vocabulary during inference using online LZW compression. The approach continuously merges frequently co-occurring base tokens into reusable 'hypertokens' on-the-fly, computing their embeddings dynamically via a lightweight hyper-encoder network, addressing the inefficiency of static tokenizers on domain-specific text.", "key_contribution": "The main innovation is combining LZW online compression with a dynamic embedding architecture and compression-based pretraining to enable inference-time vocabulary adaptation that compresses both input and output tokens. The work provides theoretical grounding through an entropy invariance theorem showing optimal performance is theoretically achievable under lossless compression, along with practical implementation achieving substantial inference speedups with minimal architectural changes.", "novelty": "Unlike prior domain-adapted tokenizers that require separate fixed vocabularies per domain or lossy prompt compression methods that only compress inputs, zip2zip performs lossless online compression that adapts during generation and compresses both inputs and outputs simultaneously. The use of LZW enables immediate hypertoken reuse during decoding (unlike offline BPE), while the dynamic embedding layer eliminates the need for massive static vocabulary matrices. This addresses the fundamental limitation that static tokenizers optimized on general corpora become inefficient on specialized domains, providing 2-3x cost reduction for domain-specific inference without requiring domain-specific model training.", "ai_categories": ["Model Efficiency and Optimization", "Tool Use and Code Generation"]}, {"paper_id": "4232577", "score": "14", "title": "OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis", "authors": "Junting Chen, Haotian Liang, Lingxiao Du, Weiyun Wang, Mengkang Hu, Yao Mu, Wenhai Wang, Jifeng Dai, Ping Luo, ..., Lin Shao", "session_type": "SD-2-5507", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/b83bcc6b13bf3bed81ebb73be9bae7cc2be710e7.pdf", "relevant_to_users": "3", "read_by_users": "33", "topics": "", "key_findings": "OWMM-Agent introduces the first dedicated foundation model for open-world mobile manipulation that unifies global scene understanding, robot state tracking, and multi-modal action generation in a single VLM (Vision-Language Model). The model achieves 97.85% decision-making success and 21.90% full task completion in simulation (vs. 0.33% for GPT-4o baselines), with 90% success rate on physical robots using zero-shot transfer. The work demonstrates that unified VLM architectures drastically outperform modular pipelines (GPT-4o + specialized grounding modules) while reducing inference time from ~160s to ~37s. A novel agentic data synthesis pipeline generates 235k high-quality training examples from simulation without human annotation, addressing the critical data bottleneck in embodied AI.", "description": "This paper tackles open-world mobile manipulation (OWMM) where robots must execute complex compositional instructions like 'move object A from location B to C' in unseen environments. The authors propose OWMM-VLM, a unified foundation model that processes multiple pre-mapped scene images and current observations to generate executable actions with spatial coordinates, replacing traditional modular pipelines that chain separate models for perception, reasoning, and grounding.", "key_contribution": "The main innovation is a unified VLM architecture that integrates high-level decision-making with precise spatial grounding for mobile manipulators, trained via an agentic data synthesis pipeline that generates 235k instruction-action pairs from simulation. This eliminates the need for separate visual grounding modules and achieves superior performance with 4x faster inference compared to modular baselines.", "novelty": "Unlike prior work that chains multiple specialized models (VLMs + grounding modules like RoboPoint/PIVOT) or relies on computationally expensive 3D semantic mapping, this work demonstrates that a single fine-tuned VLM can directly output executable spatial actions while maintaining multi-image reasoning. It addresses critical limitations of base VLMs in rare robotics-specific tasks (navigable area grounding, embodiment constraints) through targeted data synthesis, proving that embodied priors can be learned efficiently with limited scene diversity but sufficient data volume. The approach reframes mobile manipulation as a multi-turn vision-language reasoning problem rather than a geometric reconstruction challenge.", "ai_categories": ["Vision-Language-Action Models", "Tool Use and Code Generation", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4192316", "score": "14", "title": "Temporal Logic-Based Multi-Vehicle Backdoor Attacks against Offline RL Agents in End-to-end Autonomous Driving", "authors": "Xuan Chen, Shiwei Feng, Zikang Xiong, Shengwei An, Yunshu Mao, Lu Yan, Guanhong Tao, Wenbo Guo, Xiangyu Zhang", "session_type": "SD-2-1201", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/26313d9a67de648ba12a7315cbcaa87c2787a4f5.pdf", "relevant_to_users": "1", "read_by_users": "3", "topics": "", "key_findings": "This paper introduces the first trajectory-based backdoor attack against end-to-end autonomous driving systems, demonstrating that coordinated multi-vehicle trajectories can serve as practical, real-world triggers. The attack achieves near-100% mission violation rates while maintaining stealthiness through a novel negative training strategy that prevents false activations. Experiments across 5 offline RL algorithms show that existing defenses remain largely ineffective, revealing significant vulnerabilities in autonomous driving systems trained via offline reinforcement learning.", "description": "The paper proposes a novel backdoor attack against offline RL-based autonomous driving systems that uses coordinated trajectories of attacker-controlled vehicles as triggers, instead of impractical pixel-level patches. Using temporal logic specifications to generate sophisticated multi-vehicle behaviors and a negative training strategy to enhance stealthiness, the attack demonstrates high effectiveness across multiple RL algorithms while revealing critical security gaps in current AD systems.", "key_contribution": "The main innovation is shifting from pixel-level triggers to trajectory-based triggers for backdoor attacks, leveraging temporal logic specifications to automatically generate coordinated multi-vehicle behaviors as stealthy, real-world deployable attack vectors. The negative training strategy that incorporates patch trajectories (similar but non-trigger patterns) is critical for preventing false activations and maintaining attack stealthiness.", "novelty": "Unlike prior backdoor attacks that rely on physical patches with viewpoint and lighting constraints, this work exploits the contextual awareness of autonomous vehicles through coordinated multi-vehicle trajectories‚Äîa realistic and practical attack vector previously unexplored in end-to-end driving. The use of temporal logic for systematic trigger generation and the negative training strategy to refine trigger specificity represent significant methodological advances. This is the first work to demonstrate that complex spatiotemporal patterns in multi-vehicle interactions can be weaponized against offline RL agents, addressing the limitation that attackers cannot directly modify ego vehicle camera inputs.", "ai_categories": ["Agent Safety and Security", "Reinforcement Learning for LLMs", "Planning and Decision Making"]}, {"paper_id": "4432167", "score": "14", "title": "SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models for Time Series", "authors": "Qitai Tan, Yiyun Chen, Mo Li, Ruiwen Gu, Yilin Su, Xiao-Ping (Steven) Zhang", "session_type": "SD-2-2304", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2510.20273", "relevant_to_users": "3", "read_by_users": "9", "topics": "", "key_findings": "SynTSBench reveals that no single deep learning model achieves optimal performance across all temporal patterns in time series forecasting. MLP architectures (PaiFilter, DLinear) dominate trend and periodic forecasting, while Transformer-based models show significant limitations. Model robustness varies dramatically: TimesNet exhibits best noise resilience while N-HiTS shows catastrophic performance collapse under anomalies (TSMiker's error increases 342% with anomaly density). Even specialized architectures underperform on long-range dependencies and multivariate relationships, indicating fundamental trade-offs in model design that necessitate task-specific model selection rather than universal solutions.", "description": "SynTSBench introduces a synthetic data-driven evaluation framework for systematically assessing deep learning models' fundamental capabilities in time series forecasting. The framework generates controlled synthetic datasets with isolated temporal features (trends, seasonality, dependencies), noise conditions, anomalies, and multivariate relationships, establishing mathematical performance boundaries for precise capability assessment across 12 deep learning and 3 zero-shot forecasting models.", "key_contribution": "The paper's main innovation is a controllable synthetic benchmark that enables feature-isolated evaluation with theoretical optimum baselines, addressing the black-box nature of existing evaluation frameworks that rely on real-world data with intertwined components and no mathematical performance boundaries. This enables quantifiable gap analysis between model predictions and optimal solutions across three analytical dimensions: temporal feature decomposition, robustness analysis, and theoretical optimum benchmarking.", "novelty": "Unlike traditional benchmarks (ProbTS, TFB, BasicTS) that use real-world observational data without feature isolation, SynTSBench introduces controlled synthetic generation with known optimal solutions and isolated variables to eliminate confounding factors. This addresses the fundamental limitation that real-world datasets contain intertwined temporal components that cannot be separated for targeted evaluation, and lack theoretical boundaries for determining whether improvements represent genuine learning or noise overfitting. The framework's programmable feature configuration enables systematic assessment of 11 trend patterns, 10 periodic patterns, multiple noise distributions, anomaly types, stochastic processes, and multivariate relationships with quantifiable performance gaps.", "ai_categories": ["Agent Benchmarking and Evaluation"]}, {"paper_id": "4224371", "score": "14", "title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM", "authors": "Bowen Dong, Minheng Ni, Zitong Huang, Guanglei Yang, Wangmeng Zuo, Lei Zhang", "session_type": "SD-5-4110", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/5c8396fbd51956694870ead9c44afcff4e5253cb.pdf", "relevant_to_users": "5", "read_by_users": "12", "topics": "", "key_findings": "The paper introduces MIRAGE, a benchmark that isolates reasoning-induced hallucinations (errors in logical inference despite accurate visual perception) in MLLMs. Key findings reveal that while model scaling reduces logical, factual, and fabrication hallucinations, spatial hallucinations remain unaffected, indicating fundamental limitations in visual reasoning capabilities. The proposed Logos method reduces logical hallucinations by 15.4% through curriculum reinforcement fine-tuning and collaborative hint inference. Question types correlate with distinct hallucination patterns, and correcting reasoning chains can improve accuracy from ~10% to ~70%.", "description": "This paper presents MIRAGE, a diagnostic benchmark with 1,329 questions designed to evaluate reasoning-induced hallucinations in multimodal large language models (MLLMs) by isolating cases where visual perception is correct but logical inference fails. The work includes multi-granular evaluation metrics (accuracy, factuality, and LLM hallucination scores) and proposes Logos, a training method that combines curriculum reinforcement learning with collaborative hint inference to reduce reasoning hallucinations.", "key_contribution": "The main contribution is the MIRAGE benchmark that uniquely isolates reasoning-induced hallucinations from perception errors through careful curation of questions where MLLMs correctly perceive visual input but fail in logical reasoning. Additionally, the paper introduces a hierarchical evaluation framework spanning answer, step, and thought levels, and demonstrates that the Logos method can reduce logical hallucinations by 15.4% in 7B models while improving overall accuracy by 8.3 percentage points.", "novelty": "Unlike existing benchmarks that focus on perception accuracy or final answer correctness, MIRAGE provides fine-grained evaluation of the reasoning process by isolating reasoning failures from perceptual errors. This addresses the critical limitation that prior work fails to distinguish between perception-induced and reasoning-induced hallucinations. The work introduces multi-level annotations (final answers, intermediate steps, claims, and complete reasoning chains) and reveals that spatial hallucinations represent a fundamental limitation unaffected by current scaling approaches, suggesting the need for novel architectural solutions rather than simply larger models or more data.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute", "Vision-Language-Action Models"]}, {"paper_id": "4212687", "score": "14", "title": "LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions", "authors": "Chaochen Gao, Xing W, Zijia Lin, Debing Zhang, Songlin Hu", "session_type": "SD-6-4007", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ed87797f943163aff0337ae4824c26f2e347ea6a.pdf", "relevant_to_users": "1", "read_by_users": "4", "topics": "", "key_findings": "LongMagpie introduces a self-synthesis framework that automatically generates large-scale long-context instruction data by exploiting aligned LLMs' internalized document-query patterns without human annotation. The method achieves leading performance on long-context benchmarks (HELMET, RULER, LongBench v2) while being 10-13√ó more token-efficient than existing methods like ChatQA and LongAlign. The p-Mix strategy successfully balances long and short-context capabilities, outperforming ChatQA by 2.28 points and LongAlign by 6.44 points on long-context tasks.", "description": "LongMagpie is a self-synthesis framework that generates long-context instruction data by feeding documents followed by special tokens to aligned LLMs, which then auto-regressively generate contextually relevant queries. The method includes query filtering mechanisms and extends to multi-document scenarios, producing diverse, high-quality training data without manual annotation or templates.", "key_contribution": "The main innovation is leveraging the inherent document-query relationship patterns that aligned LLMs internalize during long-context instruction training, enabling automatic generation of contextually relevant queries without seed questions or complex prompting. The p-Mix probabilistic mixing strategy balances long and short-context capabilities during training.", "novelty": "Unlike the original Magpie which generates general instructions from template prefixes alone, LongMagpie specifically exploits document-query patterns learned during long-context training to generate contextually grounded queries. It addresses limitations of template-based synthesis methods (limited scale, diversity, quality) and eliminates the high cost of human annotation for long-context data. The approach is significantly more token-efficient (10-13√ó fewer tokens per instruction) than existing methods while achieving superior performance.", "ai_categories": ["Memory and Context Management", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4216518", "score": "14", "title": "Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers", "authors": "Zhengliang Shi, Lingyong Yan, Dawei Yin, Suzan Verberne, Maarten de Rijke, Zhaochun Ren", "session_type": "SD-6-4006", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/f6244ed69adb4a0f1c375503deb1e23c39a8ac15.pdf", "relevant_to_users": "2", "read_by_users": "6", "topics": "", "key_findings": "ExSearch achieves +7.8% exact match improvement on HotpotQA over strong baselines, outperforming GPT-4o and LLaMA-3.3-70B with a smaller 7B model. The iterative self-incentivization process creates a positive feedback loop where the LLM progressively learns from its own generated data, with theoretical guarantees of convergence through the Monotone Convergence Theorem. The framework demonstrates substantial improvements in retrieval recall over traditional query decomposition and re-ranking approaches, while unifying document retrieval, evidence extraction, and answer generation in an end-to-end learnable system.", "description": "This paper introduces ExSearch, a framework that enables LLMs to act as agentic searchers by treating search trajectories as latent variables and optimizing them through a Generalized Expectation-Maximization (GEM) algorithm. The LLM learns to interleave three core actions‚Äîthinking (query generation), search (retriever activation), and recording (evidence extraction)‚Äîas reasoning unfolds, creating a self-incentivized learning loop.", "key_contribution": "The main contribution is a unified self-incentivization framework using GEM that treats search trajectories as latent variables, alternating between trajectory exploration (E-step) and re-weighted trajectory learning (M-step). This enables end-to-end optimization of retrieval and reasoning, avoiding the misalignment problems of cascaded approaches while providing theoretical convergence guarantees.", "novelty": "Unlike prior cascaded approaches (e.g., Self-RAG) that fine-tune LLMs for isolated stages with separate supervision, ExSearch unifies all components through iterative self-incentivization where importance weights reflect trajectory utility for answer generation. This addresses the fundamental misalignment problem of stage-by-stage optimization by creating an end-to-end learnable system where the LLM learns from its own generated data. The framework also provides theoretical convergence guarantees absent in previous agentic search methods.", "ai_categories": ["Web and Computer-Use Agents", "Self-Improvement and Meta-Learning", "Reasoning and Test-Time Compute"]}, {"paper_id": "4242273", "score": "13", "title": "RADAR: Benchmarking Language Models on Imperfect Tabular Data", "authors": "Ken Gu, Zhihan Zhang, Kate Lin, Yuwei Zhang, Akshay Paruchuri, Hong Yu, Mehran Kazemi, Kumar Ayush, A. Ali Heydari, ..., Xin Liu", "session_type": "SD-1-2402", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.08249", "relevant_to_users": "1", "read_by_users": "5", "topics": "", "key_findings": "RADAR benchmark reveals that language models experience significant performance degradation when handling imperfect tabular data with realistic issues like missing values and inconsistent formatting. The work systematically identifies which types of data imperfections most challenge current models, showing that existing LLMs are not robust to the messy, real-world data conditions they encounter in practice.", "description": "This paper introduces RADAR, a benchmark for evaluating language models on tabular data with realistic imperfections such as missing values, inconsistent formatting, and data quality issues. The benchmark systematically tests how well various LLMs handle messy real-world data compared to clean datasets.", "key_contribution": "RADAR provides the first comprehensive benchmark specifically designed to evaluate LLM robustness on imperfect tabular data, moving beyond existing benchmarks that assume clean inputs and enabling more realistic assessment of model capabilities for practical applications.", "novelty": "Unlike prior benchmarks that evaluate LLMs on pristine, clean tabular data, RADAR systematically introduces realistic data imperfections to mirror real-world conditions. This addresses a critical gap where existing evaluations fail to capture how models perform on the messy data they actually encounter in practice, providing insights into model vulnerabilities and robustness that clean-data benchmarks miss.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation"]}, {"paper_id": "4441820", "score": "13", "title": "Bootstrap Off-policy with World Model", "authors": "Guojian Zhan, Likun Wang, Xiangteng Zhang, Jiaxin Gao, Masayoshi Tomizuka, Shengbo Eben Li", "session_type": "SD-1-304", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/41b2f866d682b3c56d82ba0f291b84901efb52d3.pdf", "relevant_to_users": "10", "read_by_users": "44", "topics": "", "key_findings": "BOOM introduces a bootstrap loop architecture that tightly couples planning and off-policy learning, achieving state-of-the-art results on DMC Suite (877.7 average return, +17.7% over TD-MPC2) and Humanoid-Bench (820.6 average return, +60.5% over BMPC). The method addresses the fundamental actor divergence problem in model-based RL where data is collected by a planner but the policy is trained assuming it generated the data itself. The likelihood-free alignment loss enables direct policy learning from non-parametric planner action distributions by minimizing forward KL divergence without requiring intractable likelihood computations. A soft Q-weighted mechanism prioritizes high-return behaviors while managing variable action quality from historical replay data.", "description": "BOOM (Bootstrap Off-policy with World Model) is a model-based reinforcement learning framework that creates a bidirectional feedback loop where a policy initializes a planner, and the planner refines actions to bootstrap the policy through behavior alignment. Using a jointly learned world model, the method employs a likelihood-free alignment loss combined with value-weighted prioritization to align the policy with the planner's non-parametric action distribution.", "key_contribution": "The main innovation is the likelihood-free alignment loss that enables direct policy bootstrapping from a planner's intractable non-parametric action distribution by minimizing forward KL divergence, requiring only the policy's log-probabilities. This is combined with a soft Q-weighting mechanism that prioritizes high-value actions from replay data, creating a tight planning-learning integration that maintains distribution consistency throughout training.", "novelty": "Unlike prior model-based RL methods (TD-MPC, Dreamer, MuZero) that separate planning from policy learning or use imagination-based training, BOOM directly addresses the actor divergence problem where the planner collects data but the policy is trained assuming self-generated trajectories. The forward KL formulation avoids mode collapse of reverse KL approaches, capturing multimodal planner distributions while remaining computationally tractable by eliminating the need to compute the planner's likelihood. This resolves the cascading failures of distribution shift in value learning and unreliable policy improvement that plague planning-driven MBRL approaches.", "ai_categories": ["Planning and Decision Making", "Reinforcement Learning for LLMs"]}, {"paper_id": "4117904", "score": "13", "title": "The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement", "authors": "Ruihan Yang, Fanghua Ye, Jian Li, Siyu Yuan, Yikai Zhang, Zhaopeng Tu, Xiaolong Li, Deqing Yang", "session_type": "SD-2-4009", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ae3bf83043c88b4e28668b095624b19cc07ed197.pdf", "relevant_to_users": "4", "read_by_users": "11", "topics": "", "key_findings": "This paper demonstrates that LLM agents can be significantly enhanced through systematic critique-guided improvement frameworks. The work shows that providing structured evaluative feedback during agent execution enables more effective self-correction and planning refinement, achieving measurable improvements across multiple benchmarks. This suggests that guiding agents toward better decision-making through structured critique is a promising direction beyond standard prompting approaches.", "description": "The paper introduces a framework for enhancing language model agents by incorporating critique-guided improvement mechanisms into the agent loop. The approach generates structured evaluative feedback that helps agents identify and correct errors in their reasoning and planning processes, leading to improved performance on complex reasoning and decision-making tasks.", "key_contribution": "The primary contribution is a novel critique-guided improvement framework that leverages iterative evaluative feedback to enhance LLM agent performance. This framework treats critique as a first-class component of agent execution, providing explicit guidance for refinement and moving beyond reactive error correction to proactive improvement.", "novelty": "Unlike traditional single-pass prompting methods, this approach introduces iterative improvement cycles where critique is systematically integrated throughout agent execution rather than being a post-hoc validation step. It addresses the limitation that agents typically lack explicit feedback mechanisms for identifying problematic reasoning patterns. The novel treatment of critique as a core component enables agents to refine their planning and decision-making through structured evaluative signals.", "ai_categories": ["Self-Improvement and Meta-Learning", "Planning and Decision Making", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4381163", "score": "13", "title": "Imagined Autocurricula", "authors": "Ahmet H. G√É¬ºzel, Matthew Thomas Jackson, Jarek Luca Liesen, Tim Rockt√É¬§schel, Jakob Nicolaus Foerster, Ilija Bogunovic, Jack Parker-Holder", "session_type": "SD-5-300", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/3facb7ccf15168534e07f4990a5bd009d1afea45.pdf", "relevant_to_users": "6", "read_by_users": "19", "topics": "", "key_findings": "IMAC (Imagined Autocurricula) achieves 17-48% improvements over model-free baselines across Procgen environments by combining diffusion-based world models with Prioritized Level Replay (PLR) for automatic curriculum learning. The method demonstrates strong transfer to held-out environments despite training only within learned world models from narrow datasets. Ablations show 38-56% improvements from the autocurriculum component alone, proving that strategic selection of imagined initial states substantially outperforms random sampling.", "description": "IMAC trains embodied agents entirely within diffusion-based world models learned from offline data, without requiring accurate simulators or vast online data collection. The method adapts PLR‚Äîtraditionally used for procedural environment generation‚Äîto work within learned world models, creating an automatic curriculum over imagined rollouts by prioritizing initial states with high learning potential based on temporal difference errors.", "key_contribution": "The first method to successfully combine foundation world models with unsupervised environment design (UED) for autocurriculum learning, operating entirely within imagined rollouts rather than requiring direct environment control. This enables training generally capable agents in learned simulations while maintaining training efficiency through automatic curriculum generation.", "novelty": "Unlike prior autocurriculum methods (e.g., Neural Auto-Curricula) that require direct control over procedural environment generators, IMAC operates entirely within learned world models by treating them as procedural generators of agent experience. It addresses the critical challenge of ensuring agents train on useful generated data by adapting PLR to select high-value imagined initial states rather than random sampling. The variable-length rollout approach and world-model-agnostic design represent new paradigms for curriculum learning in model-based RL.", "ai_categories": ["Self-Improvement and Meta-Learning", "Reinforcement Learning for LLMs"]}, {"paper_id": "4209401", "score": "12", "title": "Reverse Engineering Human Preferences with Reinforcement Learning", "authors": "Lisa Alazraki, Yi-Chern Tan, Jon Ander Campos, Maximilian Mozes, Marek Rei, Max Bartolo", "session_type": "SD-1-1909", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/e7f264f17bfbaac2f1357558dddf2cceae14cd6e.pdf", "relevant_to_users": "3", "read_by_users": "9", "topics": "", "key_findings": "This paper demonstrates that LLM-as-a-judge evaluation systems are vulnerable to systematic adversarial manipulation through upstream preamble generation. The work shows that reinforcement learning can be used to reverse-engineer judge models' preference functions by generating text preambles that boost evaluation scores while leaving responses unchanged, making the manipulation virtually undetectable. The approach transfers across different model pairs and reveals critical security implications for LLM evaluation frameworks widely used in practice.", "description": "This paper examines vulnerabilities in LLM-as-a-judge evaluation systems by using reinforcement learning to generate adversarial text preambles that manipulate judge models into giving higher scores. Unlike post-hoc response editing, this upstream manipulation strategy operates on frozen LLMs and remains covert, demonstrating that human preferences embedded in judge models can be systematically reverse-engineered.", "key_contribution": "The main contribution is introducing Reinforcement Learning for Reverse Engineering (RLRE), a method that optimizes upstream text preambles using judge-LLM signals as rewards to manipulate evaluation scores in a virtually undetectable manner. This approach reveals a new attack vector on LLM evaluation systems that is more covert than existing adversarial methods and transfers across different model combinations.", "novelty": "Unlike previous work that focused on post-hoc response editing, this paper introduces an upstream manipulation strategy that generates adversarial preambles while keeping model responses unchanged, making attacks virtually undetectable. The method addresses limitations of existing approaches by demonstrating transferability across unseen model pairs and revealing a fundamental vulnerability in widely-deployed LLM-as-a-judge frameworks. This represents a paradigm shift from modifying outputs to manipulating the evaluation context itself.", "ai_categories": ["Agent Safety and Security", "Reinforcement Learning for LLMs"]}, {"paper_id": "4483846", "score": "12", "title": "SciArena: An Open Evaluation Platform for Non-Verifiable Scientific Literature-Grounded Tasks", "authors": "Yilun Zhao, Kaiyan Zhang, Tiansheng Hu, Sihong Wu, Ronan Le Bras, Yixin Liu, Robert Tang, Joseph Chee Chang, Jesse Dodge, ..., Arman Cohan", "session_type": "SD-1-5316", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "", "relevant_to_users": "2", "read_by_users": "2", "topics": "", "key_findings": "SciArena introduces a crowdsourced evaluation platform using community voting (similar to Chatbot Arena) with 13,000+ votes from trusted researchers across scientific domains. The paper reveals that current automated evaluation methods struggle with scientific literature tasks, with the best-performing model (o3) achieving only 65.1% accuracy in predicting human preferences on SciArena-Eval‚Äîsignificantly lower than general benchmarks. Model performance varies substantially by scientific domain, with o3 leading overall while Claude-4-Opus excels in Healthcare and DeepSeek-R1-0528 in Natural Science. The platform demonstrates strong inter-annotator agreement and question diversity aligned with real-world research needs, validating community-driven evaluation for open-ended scientific tasks.", "description": "SciArena is an open, collaborative platform for evaluating foundation models on scientific literature tasks through community-driven voting rather than static benchmarks. It engages trusted researchers to provide pairwise comparisons of model responses on literature-grounded, long-form questions spanning diverse scientific domains, with 23 models evaluated and over 13,000 votes collected.", "key_contribution": "The platform introduces a dynamic, community-driven evaluation paradigm specifically for scientific literature understanding, moving beyond static benchmarks to leverage collective intelligence of domain experts. It includes SciArena-Eval, a meta-evaluation benchmark built from human preference data that reveals significant challenges in automated evaluation of scientific literature tasks.", "novelty": "Unlike traditional static benchmarks with verifiable answers (e.g., MMLU, ScienceQA), SciArena addresses the unique challenge of evaluating non-verifiable, open-ended scientific tasks where answers lack single correct responses and require domain expertise to judge quality. It pioneers the application of Chatbot Arena's crowdsourced voting approach to scientific literature, creating a continuously updating evaluation system that assesses models' ability to synthesize and ground responses in actual research papers. This addresses limitations of existing benchmarks that are static, limited in scale, quickly outdated, and unable to capture the nuanced reasoning and literature synthesis required for real-world scientific inquiry.", "ai_categories": ["Agent Benchmarking and Evaluation", "Domain-Specific Applications"]}, {"paper_id": "4197019", "score": "12", "title": "Lost in Transmission: When and Why LLMs Fail to Reason Globally", "authors": "Tobias Schnabel, Kiran Tomlinson, Adith Swaminathan, Jennifer Neville", "session_type": "SD-2-3907", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/93d0795dfa82fdff6b6b121fce6307ed161915ba.pdf", "relevant_to_users": "10", "read_by_users": "22", "topics": "", "key_findings": "The paper identifies systematic 'transmission failures' in LLMs where models lose critical information and logical consistency when reasoning requires integrating information across distant text spans. It develops a diagnostic framework to categorize failure modes and reveals that LLMs frequently lose track of constraints, contradict earlier statements, and fail to maintain global coherence despite strong local performance. The research isolates root causes including attention mechanism limitations, training biases toward local optimization, and architectural constraints in maintaining long-range dependencies.", "description": "This paper investigates fundamental failures in how LLMs process and reason about global information, examining scenarios where models struggle to maintain coherence and accuracy across entire problem spaces despite performing well on isolated components. It systematically analyzes when and why LLMs experience 'transmission failures' that cause them to lose critical information or logical consistency across complex reasoning sequences.", "key_contribution": "The paper provides a diagnostic framework to identify and categorize specific failure modes in global reasoning, distinguishing between local competence and holistic understanding gaps. It offers empirical analysis isolating root causes of failures in attention mechanisms, training procedures, and architectural limitations.", "novelty": "Unlike previous work that benchmarks performance, this research dissects the underlying mechanisms of why and when reasoning failures occur, focusing on the understudied gap between apparent capabilities and actual reasoning limitations. It introduces the concept of 'transmission failures' to characterize how models lose information across distant spans, providing a new lens for understanding LLM limitations. The work addresses the lack of systematic understanding of global vs. local reasoning failures, offering actionable insights for architectural and training improvements.", "ai_categories": ["Reasoning and Test-Time Compute", "Agent Benchmarking and Evaluation", "Model Efficiency and Optimization"]}, {"paper_id": "3956362", "score": "12", "title": "GRIP: A Graph-Based Reasoning Instruction Producer", "authors": "Jiankang Wang, Jianjun Xu, Xiaorui Wang, Yuxin Wang, Mengting Xing, Shancheng Fang, Hongtao Xie", "session_type": "SD-4-1909", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/9ef2967c43fa97f6b4cfdddaf146abf156250794.pdf", "relevant_to_users": "1", "read_by_users": "4", "topics": "", "key_findings": "GRIP introduces a graph-based synthetic data generation method that produces 2.1M high-quality reasoning instructions from just 7.5K seed samples. Models trained on GRIP-MATH achieve significant improvements on mathematical reasoning benchmarks: Mistral-7B reaches 57.3 average (vs 51.2 for best prior method MathScale) and 41.6 on MATH (vs 31.0 for WizardMath, 28.2 for MetaMath). GRIP-trained models match or surpass proprietary models like DeepSeekMath-RL and Qwen2-Math-7B on challenging out-of-domain datasets (GK II, SVAMP), demonstrating superior scalability, diversity, and cost-efficiency in reasoning instruction synthesis.", "description": "GRIP is a graph-based framework for synthesizing diverse, high-quality reasoning instructions at scale by constructing knowledge graphs from seed data concepts and leveraging both explicit (direct logical connections) and implicit (underlying contextual patterns) relationships. The system uses multi-model supervision for quality control and addresses scalability limitations of prior synthetic data methods like Evol-Instruct.", "key_contribution": "The key innovation is using graph-based representations to structurally encode reasoning problem dependencies, systematically exploring relationship graphs to generate diverse, logically coherent complex reasoning tasks rather than iteratively mutating existing instructions. This enables generation of 2.1M instruction pairs from 7.5K seeds while maintaining quality and ensuring structural validity.", "novelty": "Unlike prior evolution-based methods (Evol-Instruct, self-instruct) that produce shallow instruction variations through iterative mutation, GRIP fundamentally changes the approach by using graph structures to capture both explicit logical constraints and implicit semantic dependencies between concepts. This addresses the core limitation of previous methods struggling with multi-step logical inference and complex reasoning tasks, while dramatically improving scalability (280x expansion ratio), diversity, and reducing computational costs through systematic graph-based exploration rather than iterative refinement.", "ai_categories": ["Reasoning and Test-Time Compute", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4442584", "score": "12", "title": "Generalizing Experience for Language Agents with Hierarchical MetaFlows", "authors": "Shengda Fan, Xin Cong, Zhong Zhang, Yuepeng Fu, Yesai Wu, Hao Wang, Xinyu Zhang, Enrui Hu, Yankai Lin", "session_type": "SD-4-2001", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/e5e7cd3c074c77adae516f8d392136a8fe8657f5.pdf", "relevant_to_users": "0", "read_by_users": "3", "topics": "", "key_findings": "The paper introduces MetaFlowLLM, a novel experience reuse framework that achieves significant performance improvements: 32.3% average success rate improvement on AppWorld and 6.2% on WorkBench. The framework enables language agents to effectively leverage historical completed tasks through a hierarchical experience tree structure. It works seamlessly with existing agent architectures (ReAct, Reflexion) while reducing execution costs.", "description": "This paper addresses the limitation that existing language model-based agents lack effective mechanisms to reuse experiences from historically completed tasks. The authors propose MetaFlowLLM, which constructs a hierarchical experience tree where each node represents a 'MetaFlow' containing static workflows and dynamic subtasks.", "key_contribution": "The main contribution is a Hierarchical MetaFlow Merging algorithm that organizes experience into a tree structure, combined with a reinforcement learning pipeline that generates valid MetaFlows from historical data. This enables agents to generalize and reuse past task experiences effectively.", "novelty": "Unlike prior work that lacks systematic experience reuse mechanisms for language agents, this work introduces a hierarchical structure that captures both static workflows and dynamic subtasks from completed tasks. The novelty lies in the merging algorithm that creates reusable experience abstractions and the RL-based approach to ensure MetaFlow validity, addressing the limitation of agents starting from scratch on similar tasks.", "ai_categories": ["Memory and Context Management", "Self-Improvement and Meta-Learning", "Planning and Decision Making"]}, {"paper_id": "4429589", "score": "12", "title": "PlanU: Large Language Model Reasoning through Planning under Uncertainty", "authors": "Ziwei Deng, Mian Deng, Chenjing Liang, Zeming Gao, Chennan Ma, Chenxing Lin, Haipeng Zhang, Songzhu Mei, Siqi Shen, Cheng Wang", "session_type": "SD-6-1906", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/a9771087543396ca7e59296ca5d3d68429e5a708.pdf", "relevant_to_users": "0", "read_by_users": "5", "topics": "", "key_findings": "PlanU achieves substantial improvements on planning under uncertainty benchmarks: 100% success on Blocksworld 2-step tasks, 55.9% on 6-step tasks (vs RAP's 25.5%), 37.8% on TravelPlanner (vs LATS's 23.4%), and 73% average reward on WebShop (vs LATS's 57%). The method is the only one to successfully complete complex stochastic tasks like 'Tomato Lettuce Salad' in Overcooked and certain VirtualHome tasks. The key innovation is modeling MCTS node returns as quantile distributions rather than mean values, capturing both LLM and environmental uncertainty simultaneously.", "description": "PlanU integrates Monte Carlo Tree Search with LLMs to handle reasoning tasks under uncertainty by modeling return distributions at each MCTS node using quantiles. The method addresses both LLM uncertainty (from stochastic sampling) and environmental uncertainty (from stochastic state transitions) in multi-step interactive planning scenarios.", "key_contribution": "The main innovation is the quantile distribution modeling of MCTS node returns combined with an Upper Confidence Bounds with Curiosity (UCC) score that balances exploitation with curiosity-driven exploration. This enables LLMs to reason effectively in stochastic environments by maintaining full return distributions rather than collapsing uncertainty into single expected values.", "novelty": "Unlike prior work that either uses multiple reasoning chains while missing environmental uncertainty (ToT, CoT) or forecasts unknown probabilities unsuitable for multi-step tasks (DeLLMa), PlanU simultaneously models both LLM and environmental uncertainty through quantile distributions. Previous MCTS-based methods like RAP and LATS average stochasticity into single expected values, losing critical uncertainty information. PlanU's quantile regression approach preserves distribution spread, enabling the system to identify and appropriately handle high-uncertainty states through curiosity-driven exploration.", "ai_categories": ["Planning and Decision Making", "Reasoning and Test-Time Compute"]}, {"paper_id": "4441646", "score": "12", "title": "LILO: Learning to Reason at the Frontier of Learnability", "authors": "Thomas Foster, Anya Sims, Johannes Forkel, Jakob Nicolaus Foerster", "session_type": "SD-6-4905", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/81c8ea42739bd8be2077f7a518eeedc3ad8b7d5c.pdf", "relevant_to_users": "3", "read_by_users": "9", "topics": "", "key_findings": "LILO introduces Sampling for Learnability (SFL), a curriculum learning method that prioritizes training on questions with high variance of success (measured as p(1-p) where p is success probability). The method addresses a critical inefficiency in RL training of LLMs where many questions are either solved by all attempts (already learned) or by none (no training signal). LILO consistently improves final test accuracy across multiple base models, algorithms (PPO and VinePPO), and reasoning datasets (MATH, GSM8K), yielding up to 3x reduction in training steps required to reach target performance.", "description": "LILO (Learnability Improves LLMs Optimally) is a curriculum learning approach for RL fine-tuning of language models that prioritizes training on questions where the model achieves intermediate success rates‚Äîneither consistently solved nor consistently failed. The method adapts sampling for learnability from RL literature to efficiently improve reasoning capabilities by focusing computational resources on problems at the frontier of what the model can learn.", "key_contribution": "The main contribution is adapting sampling for learnability from classical RL to LLM training, with theoretical proof that LILO maximizes expected improvement of the model. The method provides a practical framework with negligible computational overhead that continuously adapts the curriculum based on the model's evolving learning state, achieving 3x faster convergence while improving final test accuracy.", "novelty": "Unlike traditional RL training that treats all questions equally, LILO addresses the binary outcome problem where computational effort is wasted on problems already mastered or too difficult to learn from. Previous approaches used random sampling or difficulty-based curricula, but LILO specifically targets variance in success rates (the 'sweet spot' where genuine learning occurs) rather than difficulty estimation alone. This represents a shift from uniform sampling to adaptive, learnability-driven curriculum learning that is theoretically grounded and empirically validated to maximize learning efficiency.", "ai_categories": ["Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4426614", "score": "11", "title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation", "authors": "Jiani Huang, Amish Sethi, Matthew Kuo, Mayank Keoliya, Neelay Velingker, JungHo Jung, Ser-Nam Lim, Ziyang Li, Mayur Naik", "session_type": "SD-1-4908", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/32e384d1b7bff0817b05af22b3999df7789e5b8d.pdf", "relevant_to_users": "2", "read_by_users": "10", "topics": "", "key_findings": "ESCA demonstrates that grounding embodied AI perception through spatial-temporal scene graphs significantly enhances agent performance. SGClip, trained on 87K+ open-domain videos via a neurosymbolic learning pipeline, achieves state-of-the-art results on scene graph generation and action localization benchmarks. ESCA significantly reduces agent perception errors and enables open-source models to surpass proprietary baselines across two embodied environments, addressing weak visual-semantic grounding in existing MLLMs.", "description": "ESCA is a framework that augments embodied agent perception using SGClip, a novel CLIP-based, open-domain, and promptable foundation model for generating scene graphs from video. The approach addresses the critical limitation of multi-modal large language models' inability to reliably connect visual features with textual semantics, providing richer spatial-temporal context for improved embodied agent decision-making.", "key_contribution": "SGClip, a novel foundation model for generating scene graphs trained on 87K+ videos using a neurosymbolic pipeline that eliminates manual annotation requirements. This enables both zero-shot inference and task-specific fine-tuning for embodied agents, providing structured spatial-temporal understanding that significantly improves agent perception.", "novelty": "Unlike prior work that relies on human-annotated data, ESCA uses a self-supervised training strategy with automatically-generated captions aligned with model-produced scene graphs. This fundamentally shifts how embodied agents perceive and reason about their environments by introducing structured scene understanding through scene graphs, addressing the weak visual-semantic grounding problem in existing MLLMs without requiring expensive manual annotations.", "ai_categories": ["Vision-Language-Action Models", "Planning and Decision Making", "Spatial and Physical Reasoning"]}, {"paper_id": "4483820", "score": "11", "title": "ML4CO-Bench-101: Benchmark Machine Learning for Classic Combinatorial Problems on Graphs", "authors": "Jiale Ma, Wenzheng Pan, Yang Li, Junchi Yan", "session_type": "SD-1-4507", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "", "relevant_to_users": "0", "read_by_users": "0", "topics": "", "key_findings": "ML4CO-Bench-101 reveals that many neural CO methods rely heavily on heuristic pre- and post-processing tricks that mask weak learning components. Through fair evaluation isolating learning from heuristics, the benchmark shows neural methods achieve speed advantages over traditional solvers but with varying performance across problem types. The tri-leveled taxonomy (paradigm-model-learning) exposes that Global Prediction, Local Construction, and Adaptive Expansion paradigms excel on different problem classes, providing clear guidelines for method selection and future development in the ML4CO community.", "description": "ML4CO-Bench-101 is a comprehensive benchmarking framework that systematically evaluates machine learning approaches for combinatorial optimization on graphs. It introduces a tri-leveled taxonomy to categorize neural CO solvers, implements three unified solver architectures (GP4CO, LC4CO, AE4CO), and provides 34 standardized datasets across 7 classic problems including TSP, ATSP, CVRP, MIS, MCut, MVC for fair and transparent comparison.", "key_contribution": "The benchmark establishes the first comprehensive framework with unified taxonomy, reproducible implementations, and transparent evaluation protocols for neural combinatorial optimization. It introduces three canonical solver architectures that integrate diverse existing methods and provides fair performance assessment by isolating learning components from compensatory heuristic tricks.", "novelty": "Unlike previous ML4CO work that suffers from methodological fragmentation and inconsistent evaluation protocols, this benchmark pioneers a systematic tri-leveled categorization scheme that dissects design choices across paradigm, model, and learning dimensions. It addresses the critical limitation of heuristic tricks masking true learning capability by enforcing evaluation protocols that reveal raw contribution of data-driven components. The work also uniquely bridges both edge-oriented and node-oriented CO problems within a single unified framework, enabling cross-problem generalization analysis previously unavailable in the field.", "ai_categories": ["Agent Benchmarking and Evaluation", "Planning and Decision Making"]}, {"paper_id": "4442696", "score": "11", "title": "Efficiently Scaling LLM Reasoning Programs with Certaindex", "authors": "Yichao Fu, Junda Chen, Siqi Zhu, Zheyu Fu, Zhongdongming Dai, Yonghao Zhuang, Yian Ma, Aurick Qiao, Tajana Rosing, ..., Hao Zhang", "session_type": "SD-2-3706", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/4ca50f6ea693aeda7b95d22f1929d6a5d49cf4ff.pdf", "relevant_to_users": "0", "read_by_users": "2", "topics": "", "key_findings": "This paper reveals that reasoning models generate 4.5x more tokens than necessary to maintain accuracy due to 'self-doubt' - unnecessarily verifying correct answers. The authors introduce Certaindex, an algorithm-agnostic metric that detects when reasoning has stabilized, enabling safe early termination. Integrated into their Dynasor serving system, this achieves 11-29% token savings in batch inference and 3.3x throughput gains in online serving with no accuracy loss, addressing a critical inefficiency in test-time compute scaling.", "description": "The paper presents Certaindex, a universal certainty metric for LLM reasoning programs that quantifies when models have 'settled' on answers, and Dynasor, a reasoning-aware serving system that uses this metric for intelligent early termination. The system works across diverse reasoning algorithms (Chain-of-Thought, Self-Consistency, MCTS, REBASE) and achieves substantial compute savings without sacrificing accuracy.", "key_contribution": "The main innovation is the first algorithm-agnostic certainty metric that works across fundamentally different reasoning strategies, combined with a lightweight non-invasive scheduling layer that optimizes serving performance without requiring model modifications or retraining.", "novelty": "Unlike prior work requiring model modifications, hidden state analysis, or algorithm-specific approaches, Certaindex provides a unified framework applicable to both iterative refinement and multi-path search methods. It addresses the limitation that traditional uncertainty measures aren't practical for production serving, and introduces reasoning-aware gang scheduling that coordinates dependent subqueries rather than treating them independently. The approach is theoretically grounded with proofs showing early exit preserves accuracy when answer distributions converge.", "ai_categories": ["Reasoning and Test-Time Compute", "Model Efficiency and Optimization"]}, {"paper_id": "4149522", "score": "11", "title": "SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning", "authors": "Rui Pan, Yinwei Dai, Zhihao Zhang, Gabriele Oliaro, Zhihao Jia, Ravi Netravali", "session_type": "SD-2-1900", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/7c62b5e4703772251c504c1cdc203fa7a96cd873.pdf", "relevant_to_users": "4", "read_by_users": "14", "topics": "", "key_findings": "SpecReason achieves 1.5-2.5√ó speedup over vanilla LRM inference while improving accuracy by 1.0-9.9% across reasoning benchmarks (MATH, AIME, GPQA). The system demonstrates that LRM inference is inherently more tolerant of approximations than standard language models, with step acceptance rates of 40-80%. When combined with speculative decoding, it provides an additional 19-44% latency reduction, showing these techniques are complementary rather than competing.", "description": "SpecReason is a system that accelerates Large Reasoning Model (LRM) inference by using a lightweight model to speculatively generate intermediate reasoning steps while the base model only assesses their utility through single-token scoring. This approach exploits the observation that reasoning progress depends on semantic insights rather than exact token sequences, making LRM inference naturally tolerant of approximations.", "key_contribution": "The main innovation is operating at the semantic similarity level rather than requiring exact token matching like traditional speculative decoding. SpecReason specifically targets intermediate thinking tokens where approximation tolerance exists, using a lightweight assessment mechanism (~70 tokens per step) that allows the base model to verify or correct speculated reasoning steps.", "novelty": "Unlike speculative decoding which requires exact token-level equivalence, SpecReason accepts semantically equivalent reasoning steps and focuses specifically on intermediate thinking tokens. This addresses the limitation that prior approaches either sacrifice quality through reduced reasoning depth (Sky-T1-Flash) or operate at token-level granularity. SpecReason uniquely exploits the tolerance of reasoning processes to approximate intermediate steps, recognizing that occasional mistakes can be self-corrected, enabling a fundamentally different optimization strategy for reasoning workloads.", "ai_categories": ["Reasoning and Test-Time Compute", "Model Efficiency and Optimization"]}, {"paper_id": "4442210", "score": "11", "title": "Dependency Matters: Enhancing LLM Reasoning with Explicit Knowledge Grounding", "authors": "Xiangyu Wen, Min Li, Junhua Huang, Jianyuan Zhong, Zhijian Xu, Zeju Li, Yongxiang Huang, Mingxuan Yuan, Qiang Xu", "session_type": "SD-4-3603", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/88669d564fe12a96458ad3ff7025be1f74506a21.pdf", "relevant_to_users": "0", "read_by_users": "2", "topics": "", "key_findings": "The paper introduces GRiD (Grounded Reasoning in Dependency), a framework that explicitly grounds LLM reasoning steps in structured knowledge graphs to address hallucinations and inconsistencies. Key innovations include: (1) a dual-node architecture with knowledge extraction nodes that retrieve factual premises and reasoning nodes that perform logical inferences, (2) a self-contained approach requiring no external databases or larger auxiliary models, and (3) performance matching GPT-4o and DeepSeek-R1 on benchmarks like StrategyQA, CommonsenseQA, GPQA, and TruthfulQA using only 8B-14B parameter models. The framework can function as both a training paradigm and an inference-time verification module.", "description": "This paper addresses the problem of LLMs producing superficially coherent but internally inconsistent reasoning due to poorly-grounded knowledge extraction. GRiD introduces a dependency-aware reasoning framework that transforms reasoning traces into knowledge-enhanced graphs where each reasoning step explicitly depends on verified premise knowledge, with step-wise verification ensuring logical consistency throughout the reasoning chain.", "key_contribution": "The main contribution is a scalable, self-contained reasoning framework that explicitly separates knowledge extraction from reasoning steps through a structured graph architecture. Unlike existing methods, GRiD relies solely on the base model itself for both graph construction and verification, eliminating dependencies on external knowledge bases or larger teacher models while achieving state-of-the-art performance.", "novelty": "GRiD differs from Chain-of-Thought and existing structured reasoning methods by explicitly enforcing logical dependencies between reasoning steps through a graph-based architecture with separate knowledge extraction and reasoning nodes. It addresses the critical limitation that standard LLM reasoning lacks explicit tracking of how facts and inferences depend on each other, which leads to hallucinations and circular reasoning. The novel approach of using only the base model for self-verification through dependency-correctness checking makes it highly scalable and applicable as a post-training inference module, unlike methods requiring external knowledge sources or model distillation.", "ai_categories": ["Reasoning and Test-Time Compute", "Tool Use and Code Generation", "Agent Safety and Security"]}, {"paper_id": "4438224", "score": "11", "title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "authors": "Zhenyu Zhang, Tianyi Chen, Weiran Xu, Alex Pentland, Jiaxin Pei", "session_type": "SD-5-4103", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/311841e491eca81acc49d5288503e9b7819788d6.pdf", "relevant_to_users": "4", "read_by_users": "6", "topics": "", "key_findings": "ReCAP achieves substantial improvements on long-horizon reasoning benchmarks: 32% gain on synchronous Robotouille and 29% on asynchronous Robotouille under strict pass@1 protocol. The framework consistently outperforms ReAct across all evaluated models (Qwen2.5-32B/72B, LLaMA-4-400B, DeepSeek-V3-671B, GPT-4o) without requiring model-specific tuning. ReCAP introduces a training-free hierarchical framework that addresses context drift and goal information loss in sequential prompting methods while avoiding the runtime overhead of traditional hierarchical approaches through memory-efficient execution that scales linearly with task depth.", "description": "ReCAP is a hierarchical framework for LLM agents that combines plan-ahead decomposition, structured re-injection of parent plans, and memory-efficient context management to improve long-horizon reasoning and planning. The framework generates full subtask lists, executes items sequentially while refining remaining tasks, and maintains consistent multi-level context during recursive execution without requiring training or fine-tuning.", "key_contribution": "The main contribution is a training-free recursive framework that unifies the benefits of sequential and hierarchical prompting methods by aligning high-level goals with low-level actions, reducing redundant prompting, and preserving coherent context updates across recursion levels. This enables structured subtask decomposition, dynamic memory tracking, and observation-driven plan adaptation while maintaining linear cost scaling with task depth.", "novelty": "Unlike sequential prompting methods that suffer from context drift and recurrent failure cycles, and hierarchical methods that weaken cross-level continuity or incur substantial runtime overhead, ReCAP introduces structured re-injection of parent plans to maintain multi-level context consistency during recursive returns. The framework addresses the fundamental limitation of existing approaches by enabling full plan-ahead decomposition with dynamic refinement while keeping memory bounded through selective context management, eliminating the need for external validation modules or training.", "ai_categories": ["Planning and Decision Making", "Reasoning and Test-Time Compute", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4441592", "score": "11", "title": "Retro-R1: LLM-based Agentic Retrosynthesis", "authors": "Wei Liu, Jiangtao Feng, Hongli Yu, Yuxuan Song, Yuqiang Li, Shufei Zhang, LEI BAI, Wei-Ying Ma, Hao Zhou", "session_type": "SD-6-1511", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/470a5ef177e20c474a9e358c1bab0c1cf082f1de.pdf", "relevant_to_users": "3", "read_by_users": "7", "topics": "", "key_findings": "RETRO-R1 achieves 55.79% pass@1 success rate, surpassing previous state-of-the-art by 8.95%. It demonstrates strong generalization to out-of-domain test cases where existing methods fail despite high in-domain performance. The framework enables data-efficient, generalizable scientific problem-solving through RL-trained LLMs that interact dynamically with plug-in single-step retrosynthesis tools and learn from environmental feedback.", "description": "RETRO-R1 is an LLM-based agentic retrosynthesis system that uses reinforcement learning to navigate the retrosynthesis search space through multi-turn interactions with the environment and strategic tool use. The LLM acts as a policy network mapping current states to action distributions, enabling it to design multi-step molecular synthesis pathways by dynamically interacting with plug-in single-step retrosynthesis tools.", "key_contribution": "The main innovation is training an LLM via reinforcement learning to act as an interactive agent for retrosynthesis planning, achieving superior performance on long-sequence tasks without requiring additional continued pretraining or supervised fine-tuning. Any single-step model can serve as a plug-in for the framework without training.", "novelty": "Unlike prior retrosynthesis approaches that rely on single-turn, question-answering formats and lead to exponential search space growth, RETRO-R1 uses multi-turn interactions where the LLM dynamically learns from environmental feedback. This addresses the fundamental limitations of traditional computer-aided synthesis planning methods that apply iterative single-step predictions inefficiently. The work marks a significant advance in equipping LLMs with chemist-like reasoning abilities through reinforcement learning for scientific problem-solving.", "ai_categories": ["Tool Use and Code Generation", "Reinforcement Learning for LLMs", "Domain-Specific Applications"]}, {"paper_id": "4244202", "score": "11", "title": "Know What You Don't Know: Uncertainty Calibration of Process Reward Models", "authors": "Young-Jin Park, Kristjan Greenewald, Kaveh Alim, Hao Wang, Navid Azizan", "session_type": "SD-6-1912", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/18533f919a9403854c7955628ad3488bf682b694.pdf", "relevant_to_users": "3", "read_by_users": "6", "topics": "", "key_findings": "State-of-the-art process reward models (PRMs) are poorly calibrated and systematically overestimate success probabilities, especially for challenging out-of-distribution problems. The paper introduces a quantile regression-based calibration method that dramatically reduces calibration error (e.g., Brier scores improved from 0.29-0.41 to 0.01-0.08 on AIME problems). The proposed instance-adaptive scaling strategy achieves 60-75% computational savings during inference while maintaining accuracy, by dynamically allocating compute proportional to problem difficulty based on calibrated uncertainty estimates.", "description": "This paper addresses the calibration problem in process reward models (PRMs) used for guiding inference-time scaling in large language models. The authors propose a quantile regression-based fine-tuning approach that calibrates PRMs to provide accurate uncertainty estimates, enabling instance-adaptive scaling strategies that dynamically adjust computational budgets based on estimated problem difficulty.", "key_contribution": "The main innovation is a quantile regression calibration method for PRMs that outputs confidence-bounded predictions rather than single overconfident estimates, combined with an instance-adaptive scaling framework that uses these calibrated probabilities to allocate inference compute proportionally to problem difficulty, achieving significant efficiency gains without sacrificing accuracy.", "novelty": "Unlike prior work that focused on PRM accuracy for step-ranking, this is the first to treat PRM outputs as interpretable probability estimates requiring proper calibration. The paper introduces instance-adaptive sampling to LLM inference-time scaling, drawing connections to classical adaptive particle filtering principles. Standard calibration techniques (temperature scaling, isotonic regression) are shown to fail on out-of-distribution tasks, necessitating the proposed quantile regression approach that captures posterior uncertainty through Monte Carlo rollout estimation.", "ai_categories": ["Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs", "Mathematical and Logical Reasoning"]}, {"paper_id": "4393192", "score": "11", "title": "VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception", "authors": "Ziang Yan, Yinan He, Xinhao Li, Zhengrong Yue, Xiangyu Zeng, Yali Wang, Yu Qiao, Limin Wang, Yi Wang", "session_type": "SD-6-5200", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/7cf4479fc77f583f1cdede042be97389be035f9a.pdf", "relevant_to_users": "8", "read_by_users": "47", "topics": "", "key_findings": "VideoChat-R1.5 introduces Visual Test-Time Scaling (VTTS), achieving 5%+ average improvements across 15+ benchmarks compared to Qwen2.5VL baselines. The method demonstrates that reinforcement learning with spatio-temporal rewards outperforms supervised fine-tuning for continuous localization tasks, with particularly dramatic gains on grounded understanding (e.g., +61.9% on NextGQA). The work establishes a scaling law for visual perception at test-time, showing monotonic performance improvements with additional iterative refinement passes, and introduces the VTTS-80K dataset with fine-grained spatio-temporal annotations.", "description": "This paper presents a novel test-time scaling approach for multimodal large language models that enables dynamic, iterative visual perception during inference rather than static preprocessing. The Iterative Perception (ITP) mechanism progressively refines attention on high-confidence spatio-temporal regions guided by the model's evolving textual predictions, trained via reinforcement learning with rewards for spatial/temporal accuracy, answer correctness, and format compliance.", "key_contribution": "The main contribution is Visual Test-Time Scaling (VTTS), the first systematic approach to extend compute scaling to the visual modality in MLLMs through iterative perception during inference. This includes a reinforcement learning framework with spatio-temporal rewards that enables sub-pixel precision in localization, and demonstrates that perceiving iteratively under learned reward guidance fundamentally outperforms single-pass visual encoding followed by language-only reasoning.", "novelty": "Unlike prior work that perceives visual input once before reasoning (including methods like Best-of-N sampling, MCTS, and vision-aided CoT), VTTS dynamically generates spatio-temporal clues and selectively adds visual information iteratively during inference. It addresses the limitation of VideoChat-R1's language-only CoT approach by introducing explicit visual iteration as a core mechanism. The use of reinforcement learning with continuous metric-aware penalties (IoU for localization) rather than supervised fine-tuning is novel for spatio-temporal reasoning, as SFT was shown to degrade performance by overfitting to annotation distributions.", "ai_categories": ["Reasoning and Test-Time Compute", "Vision-Language-Action Models", "Reinforcement Learning for LLMs"]}, {"paper_id": "4442209", "score": "10", "title": "Thoughts Are All Over the Place: On the Underthinking of Long Reasoning Models", "authors": "Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, ..., Dong Yu", "session_type": "SD-1-5518", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/52cab7bd6214e5b9d63addbfb0411f3ce8f517f4.pdf", "relevant_to_users": "4", "read_by_users": "9", "topics": "", "key_findings": "The paper identifies 'underthinking' - a critical inefficiency where long reasoning models (LRMs) like o1 abandon promising reasoning paths prematurely, switching between thoughts 418% more frequently in incorrect responses. Over 70% of incorrect responses contain at least one correct thought that is abandoned. The authors propose TIP (Thought Switching Penalty), a training-free decoding strategy that penalizes premature thought transitions, improving accuracy on AIME2024 from 38.3% to 44.1% (pass@1) without requiring model retraining. The method works orthogonally with existing sampling strategies like self-consistency.", "description": "This paper investigates why long reasoning models (o1-like LLMs) produce longer incorrect responses with frequent thought switching, identifying a phenomenon called 'underthinking' where models abandon promising reasoning paths prematurely. The authors analyze this behavior across challenging math benchmarks (MATH500, GPQA Diamond, AIME2024) using open-source models like QwQ-32B and DeepSeek-R1-671B.", "key_contribution": "The paper introduces the first systematic analysis and quantitative metric for 'underthinking' in long reasoning models, and proposes TIP (Thought Switching Penalty) - a practical training-free decoding strategy that modifies logits to discourage premature thought transitions, achieving significant accuracy improvements without fine-tuning while remaining compatible with existing sampling methods.", "novelty": "Unlike prior work on 'overthinking' (wasted computation on trivial paths), this is the first to study the opposite problem of 'underthinking' where models fail to sufficiently explore promising reasoning paths. It reveals that 225% more tokens in incorrect responses stem from 418% more thought-switching rather than deeper reasoning, and that models often discover correct solutions but abandon them. The training-free TIP approach addresses reasoning inefficiency at the decoding level rather than through model architecture or training changes.", "ai_categories": ["Reasoning and Test-Time Compute", "Mathematical and Logical Reasoning", "Model Efficiency and Optimization"]}, {"paper_id": "4348861", "score": "10", "title": "Steering When Necessary: Flexible Steering Large Language Models with Backtracking", "authors": "Zifeng Cheng, Jinwei Gan, Zhiwei Jiang, Cong Wang, Yafeng Yin, Xiang Luo, Yuchen Fu, Qing Gu", "session_type": "SD-2-1912", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/795a175c3b3b1e9de873069086e754159a4e855b.pdf", "relevant_to_users": "1", "read_by_users": "9", "topics": "", "key_findings": "This paper introduces FASB (Flexible Activation Steering with Backtracking), a dynamic steering method that monitors LLM generation quality in real-time and adaptively intervenes only when necessary. The key innovation is a backtracking mechanism that regenerates deviated tokens and adjusts intervention strength proportional to deviation severity. On TruthfulQA, it achieves 80.56% True√óInfo score (vs. 76.11% for prior work ITI), with MC1 accuracy of 48.71% (vs. 38.31%), demonstrating significant improvements in truthfulness while generalizing across six different LLMs and multiple benchmarks.", "description": "The paper presents a framework for steering LLMs that dynamically determines when and how strongly to intervene during generation by tracking both input questions and generated content. Unlike prior methods that apply uniform interventions indiscriminately, FASB uses probing classifiers to detect deviations in real-time and implements a backtracking mechanism to regenerate tokens when the model strays from desired behavior (e.g., truthfulness).", "key_contribution": "The main contribution is the introduction of content-aware, adaptive steering with backtracking‚Äîa two-stage system that identifies truthfulness-correlated attention heads through probing, then dynamically monitors generation quality and adaptively adjusts intervention strength proportional to detected deviation severity, with the ability to backtrack and regenerate previous tokens when deviations occur.", "novelty": "Unlike previous activation steering methods that apply fixed-strength interventions uniformly to all generations or make decisions based solely on input questions, FASB addresses the limitation that deviation detection during generation requires monitoring both questions and generated content. The novel backtracking mechanism solves the critical problem that \"intervening after detecting a deviation is often too late\"‚Äîby regenerating previous tokens when deviations are detected, it can correct course mid-generation. This dynamic, content-aware approach avoids wasting computational resources on already-correct outputs while providing stronger interventions only where needed.", "ai_categories": ["Agent Safety and Security", "Reasoning and Test-Time Compute"]}, {"paper_id": "4441291", "score": "10", "title": "Reasoning Models Hallucinate More: Factuality-Aware Reinforcement Learning for Large Reasoning Models", "authors": "Junyi Li, Hwee Tou Ng", "session_type": "SD-2-314", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/31f57f113b7a0a8d53b00020bbcdabe6ac8a82cf.pdf", "relevant_to_users": "6", "read_by_users": "18", "topics": "", "key_findings": "The paper reveals a critical paradox: while RL fine-tuning dramatically improves reasoning capabilities in LLMs, it simultaneously increases hallucination rates by 2-3x. Through theoretical analysis, the authors identify three root causes: high-variance gradients from binary rewards, entropy-induced randomness during exploration, and susceptibility to spurious local optima. The proposed FSPO method achieves substantial improvements across benchmarks: 89.5% on GSM8K, 58.4% on TruthfulQA, and 83.0% on HaluEval-QA, while maintaining strong reasoning performance. Most importantly, the work demonstrates that explicit step-wise factuality verification during RL training can simultaneously enhance reasoning accuracy and reduce hallucinations.", "description": "This paper provides the first systematic analysis of why reasoning-oriented reinforcement learning increases hallucinations in large language models, identifying that outcome-driven reward modeling ignores intermediate reasoning steps where most errors occur. The authors propose Factuality-aware Step-wise Policy Optimization (FSPO), which integrates explicit factuality verification at each reasoning step during RL training, using entailment scores against evidence to provide denser feedback signals.", "key_contribution": "The main contribution is FSPO, a novel RL fine-tuning algorithm that combines answer correctness rewards with step-wise factuality rewards, adjusting token-level advantages during training to reinforce factually correct tokens and penalize hallucinated ones. This approach provides theoretical grounding (three theorems explaining RL-induced hallucinations) and empirical validation showing simultaneous improvements in both reasoning accuracy and factuality across multiple benchmarks and base models.", "novelty": "Unlike prior work on hallucination mitigation through pre-training data curation, post-processing verification, or supervised fine-tuning, this is the first work to analyze and address hallucinations specifically in reasoning models during RL training. The novelty lies in integrating step-wise factuality verification directly into the RL optimization process rather than treating verification as a separate post-hoc step, and providing theoretical characterization of why outcome-based RL amplifies hallucinations. The approach addresses the fundamental limitation of sparse reward signals by introducing dense, step-level feedback that guides the policy away from hallucinated reasoning paths during training.", "ai_categories": ["Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs", "Agent Safety and Security"]}, {"paper_id": "4442328", "score": "10", "title": "GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning", "authors": "Haolong Yan, Yeqing Shen, Xin Huang, Jia Wang, Kaijun Tan, Zhixuan Liang, Hongxin Li, Zheng Ge, Osamu Yoshie, ..., Daxin Jiang", "session_type": "SD-4-415", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ce35fb684e3b11b9c0f1fcc38598cfb3504c728e.pdf", "relevant_to_users": "3", "read_by_users": "8", "topics": "", "key_findings": "This paper introduces GUI Exploration Lab (GE-Lab), a simulation environment for training GUI navigation agents, and demonstrates a three-stage training approach that significantly improves performance. The key findings show that: (1) supervised fine-tuning provides foundational knowledge but has limited generalization (98.89% in-distribution vs 55.45% out-of-distribution), (2) single-turn RL improves OOD performance by 13.7% relative, and (3) multi-turn RL enables exploration strategies and error recovery, achieving 3.5√ó improvement on complex 7-step tasks (2.92% vs 0.83%). The approach transfers to real-world scenarios with 72.03% average performance across eight benchmarks after continual training.", "description": "This paper addresses the challenge of training GUI navigation agents for complex multi-screen tasks by introducing GE-Lab, a flexible simulation environment engine that enables controlled training and evaluation with full access to environment metadata. The work demonstrates that a progressive training approach‚Äîsupervised fine-tuning followed by single-turn then multi-turn reinforcement learning‚Äîsignificantly improves agent performance on both simulated and real-world GUI navigation benchmarks.", "key_contribution": "The main contribution is the introduction of multi-turn reinforcement learning for GUI navigation agents, combined with a customizable simulation environment (GE-Lab) that overcomes the proprietary constraints of real-world GUI systems. The three-stage training paradigm enables agents to develop exploration strategies and error recovery capabilities that transfer to real-world scenarios, demonstrating that interactive RL training with sparse rewards produces agents that discover novel navigation paths not present in supervised training data.", "novelty": "Unlike prior work that focuses on single-step action correction or operates on pre-constructed trajectories, this work introduces true multi-turn interactive RL for GUI navigation where agents generate action sequences dynamically and learn through trial-and-error with sparse goal-based rewards. The GE-Lab environment addresses the critical limitation that real-world GUI systems are proprietary and lack comprehensive metadata, enabling reproducible research with controllable out-of-distribution testing. The work demonstrates that MT-RL agents discover entirely new efficient navigation paths unseen during training and exhibit strategic error recovery behaviors, representing a shift from memorization-based approaches to genuine exploration-based learning.", "ai_categories": ["Web and Computer-Use Agents", "Reinforcement Learning for LLMs", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4441366", "score": "10", "title": "On Evaluating LLM Alignment by Evaluating LLMs as Judges", "authors": "Yixin Liu, Pengfei Liu, Arman Cohan", "session_type": "SD-4-2000", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/cefd5dffe2958e9dbfba77cc4764ee04a8cc5a95.pdf", "relevant_to_users": "7", "read_by_users": "17", "topics": "", "key_findings": "The paper discovers a strong generation-evaluation consistency (GE-consistency) among LLMs, showing a 0.971 Spearman correlation between an LLM's ability to generate aligned outputs and its ability to evaluate alignment. This insight enables a new evaluation paradigm that assesses LLMs as judges rather than evaluating their generated outputs. The proposed AlignEval benchmark achieves 0.946 Spearman correlation with ChatBot Arena rankings while eliminating the need for expensive LLM-judge API calls (saving $10-$20 per model evaluation).", "description": "This paper investigates whether LLMs' generation capabilities correlate with their evaluation capabilities for alignment tasks. The authors demonstrate strong generation-evaluation consistency and propose AlignEval, a benchmark that evaluates LLM alignment by testing models as judges on fixed pairwise comparison instances, rather than evaluating their generated outputs.", "key_contribution": "AlignEval is a cost-efficient alignment benchmark containing 2,671 pairwise comparison instances with gold-standard labels that assesses LLMs based on their evaluation capabilities rather than generation quality. This approach matches or surpasses existing benchmarks (AlpacaEval, Arena-Hard) in capturing human preferences while requiring zero API costs for evaluating new models.", "novelty": "Unlike existing evaluation methods that require expensive LLM judges to assess each new model's outputs, this work fundamentally shifts the paradigm by leveraging the discovered generation-evaluation consistency to assess alignment through evaluation capability instead. This addresses the scalability limitations of current approaches by reusing fixed preference annotations, eliminating per-model evaluation costs while maintaining strong correlation with human preferences (0.946 with ChatBot Arena).", "ai_categories": ["Agent Benchmarking and Evaluation", "Reinforcement Learning for LLMs"]}, {"paper_id": "4292006", "score": "10", "title": "Scaling RL to Long Videos", "authors": "Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, ..., Song Han", "session_type": "SD-5-4709", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/086f9eed5c2d342130b7d5c3c1f80a2cc8f3594f.pdf", "relevant_to_users": "20", "read_by_users": "59", "topics": "", "key_findings": "This paper introduces LongVILA-R1-7B, a vision-language model that achieves 65.1% accuracy on VideoMME (71.1% with subtitles), processing up to 8,192 frames per video. It presents LongVideo-Reason, a dataset of 104K long video QA pairs with reasoning annotations across sports, games, and vlogs. The Multi-modal Reinforcement Sequence Parallelism (MR-SP) training infrastructure achieves 2.1x speedup for long video RL training, enabling efficient processing of hour-long videos on standard hardware.", "description": "This paper presents a full-stack framework for scaling reinforcement learning to long video understanding in vision-language models. The work combines a large-scale annotated dataset (LongVideo-Reason), a two-stage training pipeline using chain-of-thought supervised fine-tuning followed by reinforcement learning, and a specialized training infrastructure (MR-SP) designed for efficient long-sequence video processing.", "key_contribution": "The main contribution is a complete system for applying RL to long video reasoning in VLMs, consisting of: (1) LongVideo-Reason dataset with 104K high-quality annotated video QA pairs, (2) a two-stage CoT-SFT + RL training pipeline, and (3) MR-SP infrastructure that enables efficient long video RL training with cached embeddings and sequence parallelism.", "novelty": "Previous VLMs struggled with reasoning over extended video sequences due to lack of suitable datasets, inefficient training methods, and computational constraints. This work addresses these limitations by creating the first large-scale long video reasoning dataset with CoT annotations, combining supervised and RL training stages specifically for video reasoning, and introducing MR-SP‚Äîa novel training infrastructure that uses cached visual embeddings and sequence parallelism to make RL feasible on hour-long videos with standard hardware (8xA100).", "ai_categories": ["Reinforcement Learning for LLMs", "Vision-Language-Action Models", "Reasoning and Test-Time Compute"]}, {"paper_id": "4086027", "score": "9", "title": "EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test", "authors": "Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang", "session_type": "SD-1-3606", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/28c4c8cf58b0086a2136d73f6059ada87ac33e53.pdf", "relevant_to_users": "19", "read_by_users": "26", "topics": "", "key_findings": "EAGLE-3 achieves up to 6.5x speedup for LLM inference (1.4x improvement over EAGLE-2) by introducing training-time test, which simulates multi-step generation during training to prevent error accumulation. The method demonstrates observable scaling laws‚Äîthe first speculative sampling approach where acceleration improvements scale linearly with training data, addressing a fundamental limitation of previous EAGLE versions. It achieves 1.38x throughput improvement in production systems (SGLang framework) at batch size 64.", "description": "EAGLE-3 is a speculative sampling method that accelerates large language model inference by using a draft model with direct token prediction and multi-layer feature fusion. Unlike previous EAGLE versions that predict intermediate features and rely only on top-layer representations, EAGLE-3 eliminates feature prediction constraints and integrates features from multiple layers through a training-time test technique that simulates inference conditions during training.", "key_contribution": "The introduction of training-time test, which feeds the draft model's own predictions back as inputs during training to match inference conditions, combined with direct token prediction and multi-layer feature fusion. This enables the draft model to scale effectively with increased training data while maintaining stable acceptance rates across multiple prediction steps.", "novelty": "EAGLE-3 is the first speculative sampling method to demonstrate scaling laws where acceleration improvements increase linearly with training data, which was impossible in EAGLE-1/2 due to feature prediction constraints. It addresses the critical distribution shift problem by simulating multi-step generation during training rather than single-step prediction, preventing the error accumulation that plagued previous methods. The shift from feature-level to token-level prediction with multi-layer fusion removes architectural bottlenecks that prevented previous EAGLE versions from benefiting from larger datasets.", "ai_categories": ["Model Efficiency and Optimization", "Reasoning and Test-Time Compute"]}, {"paper_id": "4239642", "score": "9", "title": "How Benchmark Prediction from Fewer Data Misses the Mark", "authors": "Guanhua Zhang, Florian E. Dorner, Moritz Hardt", "session_type": "SD-1-207", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/a4097f48740ba588b5ffe5a4fd3f7d88b8eb0a70.pdf", "relevant_to_users": "5", "read_by_users": "11", "topics": "", "key_findings": "This paper reveals that simple random sampling with regression outperforms most sophisticated benchmark prediction methods, challenging the assumption that careful subset selection is necessary. All existing methods heavily depend on model similarity and fail during extrapolation to stronger models‚Äîprecisely when evaluation of frontier models is most needed. The authors introduce an AIPW (Augmented Inverse Propensity Weighting) estimator that provides modest but consistent improvements over random sampling, even in extrapolation scenarios, though it still depends on model similarity.", "description": "This paper systematically evaluates 11 benchmark prediction methods across 19 diverse benchmarks to assess approaches that aim to accelerate costly LLM evaluation by selecting small subsets of evaluation data and predicting overall performance. The study reveals fundamental limitations in existing methods, particularly their reliance on model similarity and failure when extrapolating to novel, high-performing models.", "key_contribution": "The paper provides the first comprehensive systematic evaluation exposing that sophisticated subset selection methods are unnecessary (random sampling works as well or better), and that all existing approaches fundamentally fail at the evaluation frontier where they're most needed. It introduces an AIPW-based estimator that provides the only consistent improvement during extrapolation, though gains remain modest.", "novelty": "Unlike previous work that proposed individual methods without comprehensive comparison, this paper systematically evaluates the entire landscape of benchmark prediction approaches and reveals their shared dependency on model similarity‚Äîa critical limitation not previously identified. It challenges the field's core assumption about the necessity of careful subset selection and introduces AIPW as a theoretically-grounded alternative from statistical inference that provides consistency guarantees absent in learning-based methods. The work shifts focus from developing new selection strategies to understanding fundamental constraints on when benchmark prediction can succeed.", "ai_categories": ["Agent Benchmarking and Evaluation", "Model Efficiency and Optimization"]}, {"paper_id": "4441438", "score": "9", "title": "SpecMAS: A Multi-Agent System for Self-Verifying System Generation via Formal Model Checking", "authors": "Rishabh Agrawal, Kaushik Tushar Ranade, Aja Khanal, Kalyan Shankar Basu, Apurva Narayan", "session_type": "SD-1-2802", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/d2baf726e03709dc05beda305bd6ede14d1a9b1b.pdf", "relevant_to_users": "0", "read_by_users": "1", "topics": "", "key_findings": "SpecMAS demonstrates that agentic AI can be successfully combined with formal verification methods to create trustworthy, self-correcting systems. The framework achieves end-to-end automation of specification verification by parsing natural language SOPs, translating them to NuSMV formal models, verifying correctness via temporal logic model checking, and iteratively debugging until all properties are satisfied. On MiniSpecBench, it outperforms strong LLM baselines across fidelity, compliance, and execution reliability.", "description": "SpecMAS is a multi-agent system that automatically converts natural language Standard Operating Procedures (SOPs) into formally verified system models. It integrates model synthesis and verification in a unified pipeline, using NuSMV temporal logic model checking to ensure correctness and iteratively self-correcting based on counterexample analysis until all formal properties are satisfied.", "key_contribution": "The main contribution is an end-to-end multi-agent framework that bridges natural language specifications and formal verification through iterative, self-correcting model generation. SpecMAS introduces agentic debugging using counterexample analysis to automatically identify and fix specification violations until formal correctness is achieved.", "novelty": "Unlike prior work that treats specification generation and verification as separate tasks, SpecMAS unifies them in a closed-loop agentic system that can self-correct. Previous approaches typically require manual intervention when formal properties fail; SpecMAS automates the debugging cycle by analyzing counterexamples and iteratively refining the model. This convergence of agentic AI with formal methods enables automated generation of provably correct systems from natural language, addressing the gap between informal requirements and formally verified implementations.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Tool Use and Code Generation", "Agent Safety and Security"]}, {"paper_id": "4220390", "score": "9", "title": "LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for Scientific Embodied Agents", "authors": "Rui Li, Zixuan Hu, Wenxi Qu, Jinouwen Zhang, Zhenfei Yin, Sha Zhang, Xuantuo Huang, Hanqing Wang, Tai WANG, ..., SHIXIANG TANG", "session_type": "SD-2-2204", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.22634", "relevant_to_users": "3", "read_by_users": "4", "topics": "", "key_findings": "LabUtopia introduces the first comprehensive simulation platform specifically designed for scientific embodied agents in laboratory environments. It uniquely combines GPU-accelerated fluid physics with a chemical reasoning engine (using GPT-4o) to simulate reaction-driven transformations. Experimental results reveal that current state-of-the-art models (ACT and Diffusion Policy) achieve >70% success on atomic actions but experience sharp performance degradation on long-horizon tasks and near-zero success on out-of-domain generalization, highlighting critical limitations in sequential reasoning and robustness. The platform provides 30 distinct tasks across 5 hierarchical complexity levels with over 200 lab-specific assets, establishing a rigorous testbed for advancing embodied AI in high-stakes scientific settings.", "description": "LabUtopia is a high-fidelity simulation and hierarchical benchmarking suite for developing embodied AI agents capable of automating complex laboratory workflows. It integrates LabSim (physics + chemistry simulator), LabScene (procedural scene generator with 60+ equipment categories), and LabBench (5-level hierarchical benchmark from atomic actions to mobile manipulation tasks). The platform addresses the unique challenges of scientific environments including perceiving physical-chemical transformations, precise instrument handling, and long-horizon experimental protocols.", "key_contribution": "The main contribution is creating the first simulation platform that models both physical dynamics and chemical reactions for laboratory automation, coupled with a hierarchical benchmark that systematically evaluates embodied agents from basic manipulation to complex long-horizon mobile tasks. This fills a critical gap by providing infrastructure specifically designed for the high-complexity demands of scientific experimentation, which previous household-focused simulators could not address.", "novelty": "Unlike existing embodied AI simulators designed for household environments, LabUtopia uniquely integrates a chemical reasoning engine that models reaction-driven state changes (color transformations, product formation) essential for laboratory perception and reasoning. Previous platforms lacked the ability to simulate chemically meaningful interactions, had limited laboratory-specific asset diversity, and provided no comprehensive evaluation protocols spanning atomic to long-horizon tasks. LabUtopia addresses these limitations by combining multi-physics simulation with chemical knowledge reasoning, providing extensive lab-specific assets (~60 equipment types, ~80 glassware types), and establishing a 5-level hierarchical benchmark that reveals critical generalization and long-horizon planning failures in current models.", "ai_categories": ["Agent Benchmarking and Evaluation", "Domain-Specific Applications", "Spatial and Physical Reasoning"]}, {"paper_id": "4219361", "score": "9", "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing", "authors": "Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang", "session_type": "SD-4-5317", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/14de671242c3654992d79919aba5d312e05f7347.pdf", "relevant_to_users": "3", "read_by_users": "10", "topics": "", "key_findings": "R2R discovers that only 11% of tokens genuinely diverge reasoning paths between small and large language models, while most differences are neutral variations. By training a lightweight 56M-parameter neural router to identify and selectively route only path-divergent tokens to the LLM, R2R achieves 92% of LLM accuracy using only 17% average parameters. The method delivers 2.76√ó speedup over the 32B LLM baseline while maintaining near-identical performance, and improves the 1.5B SLM's accuracy by 4.6√ó with only 12.9% LLM token usage.", "description": "R2R (Roads to Rome) is a token-level routing system that combines small and large language models for efficient inference. Unlike speculative decoding which requires identical token distributions, R2R tolerates neutral differences and uses a neural router to identify only path-divergent tokens that require LLM correction, achieving significant efficiency gains while preserving reasoning quality.", "key_contribution": "The paper introduces a token-level routing approach with three key innovations: (1) a path-following labeling strategy that distinguishes path-divergent tokens from neutral variations through automated continuation and verification, (2) a lightweight neural router trained on 7.6M token labels to predict divergence, and (3) an immediate correction scheme that avoids the rollback overhead of speculative decoding by routing tokens to the LLM at prediction time.", "novelty": "Unlike speculative decoding which rejects any token difference and suffers from low acceptance rates and rollback overhead, R2R tolerates neutral variations and only corrects genuinely divergent tokens. Compared to query-level or step-level routing methods, R2R operates at fine-grained token granularity with learned divergence prediction rather than heuristics. The automated labeling pipeline using continuation-based verification eliminates the need for manual annotation while ensuring semantic correctness, and the immediate routing strategy prevents cascading token rejections that plague verification-based approaches.", "ai_categories": ["Reasoning and Test-Time Compute", "Model Efficiency and Optimization"]}, {"paper_id": "4051512", "score": "9", "title": "Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning", "authors": "Feng Chen, Allan Raventos, Nan Cheng, Surya Ganguli, Shaul Druckmann", "session_type": "SD-6-3719", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/d5b025da459325ca88dbc0f0f8c3dc0c23384640.pdf", "relevant_to_users": "4", "read_by_users": "14", "topics": "", "key_findings": "The paper reveals a critical misalignment between standard cross-entropy (CE) fine-tuning and test-time compute scaling: while pass@1 accuracy improves with training, pass@N performance actually degrades‚Äîa novel overfitting phenomenon where models become overconfident in incorrect solutions. The proposed Direct Coverage Optimization (DCO) method directly optimizes pass@N coverage during training by limiting model confidence to approximately 1/N, creating an emergent regularization effect. Experiments demonstrate substantial improvements on MATH (Llama-3-8B) and theorem proving tasks (LeanDojo, MiniF2F with 4-5% gains), establishing a Pareto-optimal performance frontier when training N matches test-time N.", "description": "This paper identifies that standard fine-tuning with cross-entropy loss creates overconfident models that perform worse at test-time compute scaling (pass@N degrades despite pass@1 improving). The authors propose Direct Coverage Optimization (DCO), a modified training objective that directly optimizes the probability of finding the correct answer in N samples by automatically limiting model confidence, aligning training with test-time sampling strategies.", "key_contribution": "The introduction of Direct Coverage Optimization (DCO), a theoretically-grounded training loss that directly optimizes pass@N coverage metrics by incorporating an overconfidence regularization factor that attenuates gradients for high-confidence predictions (above ~1/N threshold). This creates proper alignment between fine-tuning objectives and test-time compute scaling strategies.", "novelty": "Unlike prior work that treats training and test-time compute as decoupled processes, this paper reveals and addresses the fundamental misalignment in standard fine-tuning: CE loss optimizes for confident single-sample predictions, which paradoxically hurts multi-sample performance. The key innovation is proving theoretically that optimal confidence should decrease with N (not increase), and providing a practical training objective with emergent regularization properties that automatically prevents excessive confidence. This addresses the previously unrecognized problem that extended fine-tuning can degrade pass@N performance even while improving pass@1.", "ai_categories": ["Reasoning and Test-Time Compute", "Mathematical and Logical Reasoning", "Reinforcement Learning for LLMs"]}, {"paper_id": "4400022", "score": "9", "title": "Automated Model Discovery via Multi-modal & Multi-step Pipeline", "authors": "Lee Jung-Mok, Nam Hyeon-Woo, Moon Ye-Bin, Junhyun Nam, Tae-Hyun Oh", "session_type": "SD-6-4613", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/9677940f4812871bd634d12cccadc3598687a9ac.pdf", "relevant_to_users": "1", "read_by_users": "2", "topics": "", "key_findings": "The paper introduces a novel multi-modal, multi-step framework for automated model discovery using two vision-language model (VLM) agents: AnalyzerVLM performs iterative multi-step analysis through code execution and reasoning to propose candidate models, while EvaluatorVLM introduces a Visual Information Criterion (VIC) that combines quantitative and perceptual assessment to ensure generalizability. Experiments on Gaussian process kernel discovery and symbolic regression tasks demonstrate superior generalization performance compared to existing methods like Automatic Statistician and BoxLM, with ablation studies confirming that both multi-modality (using visual plots) and multi-step reasoning are crucial for discovering favorable models that avoid overfitting.", "description": "This paper presents an automated model discovery framework that uses two VLM agents working in tandem: one performs iterative multi-step analysis to propose candidate models by dynamically choosing analytical approaches, and another evaluates models using a novel Visual Information Criterion that assesses both visual fitness and generalizability. The framework applies to different model classes including Gaussian processes and symbolic regression, demonstrating strong performance across real-world datasets.", "key_contribution": "The main innovation is reframing automated model discovery from exhaustive search over predefined spaces to intelligent, iterative analysis that mimics expert scientific reasoning, combined with a novel Visual Information Criterion (VIC) that incorporates perceptual model assessment to better capture generalization beyond training data.", "novelty": "Unlike prior work (Automatic Statistician, BoxLM) that uses predefined grammars, greedy search, and fixed evaluation criteria, this approach employs dynamic adaptive analysis where the agent conducts multi-step investigations before proposing models. The Visual Information Criterion addresses a critical limitation of traditional metrics (BIC) by penalizing models with poor visual extrapolation, preventing overfitting that standard quantitative metrics miss. The multi-modal integration of visual plots with numerical data enables fewer analytical steps and better alignment with human judgment compared to text-only approaches.", "ai_categories": ["Tool Use and Code Generation", "Self-Improvement and Meta-Learning", "Reasoning and Test-Time Compute"]}, {"paper_id": "4264670", "score": "8", "title": "Inference-Time Reward Hacking in Large Language Models", "authors": "Hadi Khalaf, Claudio Mayrink Verdun, Alex Oesterling, Himabindu Lakkaraju, Flavio Calmon", "session_type": "SD-1-1403", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/8d477ae3e38043ed738ea2aa59e110eec3f49a44.pdf", "relevant_to_users": "11", "read_by_users": "27", "topics": "", "key_findings": "The paper provides the first formal mathematical proof that inference-time reward hacking is structurally inevitable under standard conditions‚Äîwhen optimizing proxy rewards, true performance either increases monotonically or exhibits exactly one peak before declining (the 'winner's curse'). It introduces Best-of-Poisson (BoP), a novel inference-time sampling method with a single tunable parameter that achieves near-optimal KL divergence (10^-4 gap) from the theoretically optimal distribution, and HedgeTune, a principled algorithm for identifying optimal inference-time parameters. Experiments across math reasoning (MMLU-Pro, MATH, GPQA) and human preference tasks demonstrate that hedging successfully mitigates reward hacking and achieves superior reward-distortion tradeoffs, even with highly capable reward models.", "description": "This paper formalizes inference-time reward hacking in LLMs‚Äîthe phenomenon where overoptimizing imperfect proxy rewards initially improves but eventually degrades true performance. The authors prove this hacking pattern is mathematically inevitable under common conditions and introduce Best-of-Poisson sampling and the HedgeTune algorithm to systematically mitigate it while extracting useful signals from proxy rewards.", "key_contribution": "The main contributions are: (1) the first formal theoretical characterization proving inference-time reward hacking is structurally inevitable using variation-diminishing theory, (2) Best-of-Poisson (BoP), a novel single-parameter sampling method that provides efficient near-exact approximation to optimal reward-KL policies, and (3) HedgeTune, a principled parameter-tuning framework that identifies optimal operating points to prevent overoptimization.", "novelty": "Unlike prior work that only empirically observed reward hacking in Best-of-n sampling, this paper provides the first rigorous mathematical proof of its inevitability using variation-diminishing kernels and monotone-likelihood-ratio theory. It moves beyond observation to principled mitigation by introducing BoP‚Äîwhich elegantly bridges sampling-based and distribution-based approaches through Poisson randomization‚Äîand a unified hedging framework applicable across multiple inference-time methods. Previous work lacked both the theoretical characterization of when/why hacking occurs and systematic approaches for parameter selection to prevent it.", "ai_categories": ["Reinforcement Learning for LLMs", "Agent Safety and Security", "Reasoning and Test-Time Compute"]}, {"paper_id": "4253083", "score": "8", "title": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "authors": "Zewei Zhou, Tianhui Cai, Seth Z. Zhao, Yun Zhang, Zhiyu Huang, Bolei Zhou, Jiaqi Ma", "session_type": "SD-1-2208", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/ac3e0d2216d650bf65be2b1559d68dc79c32c6ed.pdf", "relevant_to_users": "24", "read_by_users": "55", "topics": "", "key_findings": "AutoVLA introduces a unified vision-language-action model that adaptively switches between fast trajectory-only generation and slow chain-of-thought reasoning based on scenario complexity. Using Group Relative Policy Optimization (GRPO) for reinforcement fine-tuning, it achieves 66.8% runtime reduction while improving planning performance by 10.6%. The model ranks highly in the 2025 Waymo Vision-based End-to-End Driving Challenge, achieving the top RFS Spotlight score on the most challenging scenarios, and demonstrates competitive performance across nuPlan, nuScenes, Waymo, and CARLA benchmarks.", "description": "AutoVLA is a unified autoregressive vision-language-action model for end-to-end autonomous driving that performs semantic reasoning and trajectory planning directly from raw visual inputs and language instructions. It tokenizes continuous trajectories into discrete, physically feasible actions integrated into a language model framework, enabling both reasoning and action generation in a single architecture.", "key_contribution": "The main innovation is the adaptive dual-thinking framework that dynamically switches between fast trajectory-only generation and slow chain-of-thought reasoning based on scenario complexity, trained through a two-stage approach combining supervised fine-tuning with GRPO-based reinforcement fine-tuning that optimizes the reasoning-efficiency trade-off.", "novelty": "Unlike previous VLA models that suffer from physically infeasible outputs, complex multi-component architectures, or unnecessarily long reasoning for all scenarios, AutoVLA addresses these limitations through a unified single-model approach with adaptive reasoning. It eliminates the need for separate reasoning and action modules while intelligently reducing computational overhead by learning when full reasoning is unnecessary for straightforward driving scenarios. The GRPO-based reinforcement fine-tuning specifically optimizes this adaptive behavior, enabling the model to self-select the appropriate thinking mode rather than applying uniform reasoning across all situations.", "ai_categories": ["Vision-Language-Action Models", "Reinforcement Learning for LLMs", "Reasoning and Test-Time Compute"]}, {"paper_id": "4425576", "score": "8", "title": "CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs", "authors": "Gucongcong Fan, Chaoyue Niu, chengfei lv, Fan Wu, Guihai Chen", "session_type": "SD-2-4111", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/5b9c63b9b600fd293798deb51960a50373bb0faf.pdf", "relevant_to_users": "0", "read_by_users": "2", "topics": "", "key_findings": "CORE achieves 55.6% reduction in UI exposure (29.97% on AndroidLab) with only 4.9% accuracy loss (3.06% on AndroidLab) compared to cloud-only baselines. It reduces sensitive UI element exposure by 70.49% (DroidTask) and 38.84% (AndroidLab). The framework demonstrates that collaborative cloud-local architectures can achieve practical privacy-accuracy tradeoffs in mobile automation, cutting cloud inference time by up to 44% while maintaining near-baseline task success rates.", "description": "CORE is a collaborative framework combining cloud and local LLMs to reduce unnecessary UI information exposure in mobile agents. It uses layout-aware block partitioning, co-planning between local and cloud models, and multi-round co-decision-making to upload only minimal sufficient UI content to cloud LLMs while maintaining task accuracy.", "key_contribution": "The first system to explicitly minimize UI exposure in mobile agents through asymmetric cloud-local collaboration. Introduces layout-aware XML-based block partitioning, collaborative planning where models jointly identify sub-tasks, and dynamic multi-round accumulation that requests additional UI blocks only when needed for confident decisions.", "novelty": "Unlike previous approaches using either cloud-only agents (full UI upload, high privacy risk) or local-only agents (privacy-preserving but poor performance), CORE creates an asymmetric collaboration model where a strong cloud LLM operates without direct UI access while a weak local LLM has full UI visibility. This exploits complementary strengths rather than having multiple cloud LLMs with full access. The framework is training-free, modality-agnostic, and addresses the previously unstudied problem of minimizing unnecessary information exposure in mobile automation.", "ai_categories": ["Agent Safety and Security", "Multi-Agent Systems and Collaboration", "Web and Computer-Use Agents"]}, {"paper_id": "4427385", "score": "8", "title": "Zero-Shot Performance Prediction for Probabilistic Scaling Laws", "authors": "Viktoria Schram, Markus Hiller, Daniel Beck, Trevor Cohn", "session_type": "SD-4-608", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/5656a3e0c168ce620e714e942c21cc42662e5dcc.pdf", "relevant_to_users": "0", "read_by_users": "2", "topics": "", "key_findings": "The paper introduces a zero-shot prediction framework for learning curves that drastically reduces the computational cost of deriving scaling laws for NLP models. Using multi-output Gaussian Processes with a hierarchical structure, it achieves accurate predictions on small datasets (30 learning curves) where previous methods required 256+ configurations. The active learning strategy efficiently reduces predictive uncertainty, with MaGP achieving MSE of 0.02¬±0.01 versus DHGP's 0.07¬±0.00 on nanoGPT experiments, enabling cost-efficient scaling law determination with uncertainty quantification.", "description": "This paper presents a probabilistic framework for predicting learning curves and scaling laws of NLP models without exhaustive training. It formulates the problem as multitask learning using latent variable multi-output Gaussian Processes organized in a two-layer hierarchy, capturing correlations among tasks (e.g., model sizes, architectures) to enable zero-shot predictions for unseen configurations.", "key_contribution": "The main innovation is enabling zero-shot learning curve prediction on genuinely small datasets by explicitly modeling hierarchical task relationships through multi-output Gaussian Processes. This is combined with an active learning strategy that queries the most uncertain curves to efficiently reduce predictive variance and provide accurate scaling law predictions at significantly reduced computational cost.", "novelty": "Previous scaling law research required exhaustively training all model configurations, which is computationally prohibitive. This work addresses this by leveraging hierarchical task correlations to enable zero-shot predictions on small datasets (30 curves vs. 256+ in prior work). The novelty lies in the latent variable multi-output GP formulation that explicitly models bi-level hierarchies (e.g., embedding dimensions and layer counts) with separate kernels at different organizational levels, enabling genuine zero-shot inference for unseen model sizes while providing principled uncertainty quantification through the probabilistic framework.", "ai_categories": ["Model Efficiency and Optimization", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4238198", "score": "7", "title": "STSBench: A Spatio-temporal Scenario Benchmark for Multi-modal Large Language Models in Autonomous Driving", "authors": "Christian Fruhwirth-Reisinger, Du√Ö¬°an Mali√Ñ¬á, Wei Lin, David Schinagl, Samuel Schulter, Horst Possegger", "session_type": "SD-1-4618", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.06218", "relevant_to_users": "5", "read_by_users": "21", "topics": "", "key_findings": "STSBench reveals critical shortcomings in existing VLMs' ability to reason about fundamental traffic dynamics in complex environments. The benchmark evaluation shows that current models struggle with spatio-temporal reasoning across multi-view sensors (6 cameras and LiDAR) and temporal sequences. The paper demonstrates an urgent need for architectural advances that explicitly model spatio-temporal reasoning, as existing VLMs fail at holistic understanding of ego-vehicle actions and multi-agent interactions despite succeeding at simpler semantic tasks like object recognition.", "description": "STSBench is a scenario-based benchmark framework for evaluating vision-language models in autonomous driving contexts. Applied to the NuScenes dataset (STSnu), it provides 971 human-verified multiple-choice questions across 43 diverse traffic scenarios that assess models' ability to understand both ego-vehicle actions and complex interactions among traffic participants using multi-view camera and LiDAR data over temporal sequences.", "key_contribution": "The first benchmark to evaluate spatio-temporal reasoning capabilities of VLMs for autonomous driving using comprehensive 3D perception across multi-view sensors and temporal sequences. It introduces an automated framework for mining predefined traffic scenarios from datasets with ground-truth annotations, human verification interface, and generation of contrastive multiple-choice questions that test holistic driving understanding rather than isolated semantic tasks.", "novelty": "Unlike existing benchmarks that focus on single-viewpoint images/videos and semantic tasks (object recognition, dense captioning), STSBench evaluates end-to-end driving VLMs on multi-modal sensor fusion (6-view cameras + LiDAR) across temporal sequences (6 timesteps). It addresses the gap in assessing spatio-temporal reasoning about ego-vehicle decision-making and multi-agent interactions through 43 scenario types with contrastive negatives. Previous benchmarks lacked evaluation of dynamic traffic reasoning and complex agent interactions essential for safe autonomous systems.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models", "Spatial and Physical Reasoning"]}, {"paper_id": "4426713", "score": "7", "title": "Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs", "authors": "Guiyao Tie, Zenghui Yuan, Zeli Zhao, Chaoran Hu, Tianhe Gu, Ruihang Zhang, Sizhe Zhang, Junran Wu, Xiaoyue Tu, ..., Lichao Sun", "session_type": "SD-2-5100", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2510.16062", "relevant_to_users": "4", "read_by_users": "10", "topics": "", "key_findings": "The paper reveals that current LLMs have limited intrinsic self-correction abilities without external feedback, with performance varying significantly by task complexity and model architecture. The benchmark shows that while LLMs can correct errors when provided with external verification signals, they struggle to autonomously identify and fix their own mistakes. The research provides empirical evidence of substantial performance gaps between models and identifies specific conditions under which self-correction fails, which is critical for deploying trustworthy AI systems.", "description": "This paper introduces CorrectBench, a comprehensive benchmark for systematically evaluating self-correction capabilities in large language models. The research assesses multiple state-of-the-art LLMs including GPT-4, Claude, Llama 3, and Qwen 2.5 across diverse tasks, comparing both intrinsic correction (models fixing their own errors) and external correction methods (correction with feedback).", "key_contribution": "The primary innovation is establishing the first systematic benchmark framework for evaluating self-correction in LLMs, providing empirical evidence about when and why models fail to correct themselves. This addresses a critical gap in understanding model reliability and trustworthiness for real-world deployment.", "novelty": "Unlike previous work that examined specific correction techniques anecdotally, CorrectBench provides a comprehensive evaluation framework that systematically compares multiple correction approaches (intrinsic vs. external) across numerous models and diverse tasks. It moves beyond anecdotal evidence to provide rigorous empirical data about the concrete limitations of current self-correction capabilities, identifying specific failure modes and conditions that were not previously well-understood in the literature.", "ai_categories": ["Agent Benchmarking and Evaluation", "Self-Improvement and Meta-Learning", "Reasoning and Test-Time Compute"]}, {"paper_id": "4071743", "score": "7", "title": "AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling", "authors": "Zhining Zhang, Chuanyang Jin, Mung Yao Jia, Shunchi Zhang, Tianmin Shu", "session_type": "SD-4-2203", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/97d508cdb6040e0326e1f3e82a7473020de10424.pdf", "relevant_to_users": "2", "read_by_users": "6", "topics": "", "key_findings": "AutoToM introduces an automated agent modeling framework that achieves 82.43% accuracy across five Theory of Mind benchmarks, substantially outperforming large reasoning models (o3-mini-high: 73.94%, Gemini 2.0 Flash Thinking: 74.18%) and prior model-based methods. The system produces human-like confidence estimates (0.93, 0.88, 0.73 correlation with human judgments) and achieves 27.7% speedup on embodied assistance tasks. It successfully scales mental inference by automating the construction and refinement of agent models, eliminating the manual engineering bottleneck that limited previous approaches.", "description": "AutoToM addresses Theory of Mind reasoning by automating agent model construction through LLM-powered Bayesian inverse planning. The system observes agent actions, generates hypotheses about mental states (beliefs, goals, constraints), and iteratively refines models by introducing additional mental variables or incorporating more timesteps when inference uncertainty exceeds thresholds. This enables scalable, robust, and interpretable mental state inference across diverse benchmarks and modalities.", "key_contribution": "The core innovation is automated agent modeling that integrates LLM flexibility with Bayesian inverse planning robustness. Unlike previous approaches requiring manual model specification or relying on implicit reasoning, AutoToM autonomously constructs explicit, testable computational models through hypothesis sampling, likelihood estimation, and uncertainty-guided iterative refinement of mental variables and temporal context.", "novelty": "Previous ToM approaches required manual specification of agent models or assumed agents followed known planning algorithms, limiting scalability beyond toy domains. AutoToM eliminates this bottleneck by enabling LLMs to autonomously construct and dynamically refine agent models through observation, using uncertainty-guided adjustments to introduce relevant mental variables and temporal context. This systematic automation of model construction‚Äîtreating it as an explicit computational problem rather than implicit reasoning‚Äîenables the framework to handle diverse agent types and complex interaction dynamics without predefined assumptions, while maintaining interpretability through explicit model representations.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Reasoning and Test-Time Compute", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4205670", "score": "7", "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "authors": "Xiaoyuan Liu, Tian Liang, Zhiwei He, Jiahao Xu, Wenxuan Wang, Pinjia He, Zhaopeng Tu, Haitao Mi, Dong Yu", "session_type": "SD-5-306", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/f28db2a8d244c8994006bd065afbd5a061c42feb.pdf", "relevant_to_users": "2", "read_by_users": "6", "topics": "", "key_findings": "The paper demonstrates that LLMs trained with traditional RL on verifiable rewards suffer from 'superficial self-reflection'‚Äîthey can generate correct answers but cannot reliably verify their own outputs. RISE addresses this by simultaneously training both problem-solving and self-verification within a unified RL process, achieving a 47.7 percentage point improvement in verification accuracy (26.8% to 74.5%) while maintaining strong reasoning performance. The approach shows that verification skills are learnable through direct RL training and develop faster than problem-solving abilities, enabling better test-time scaling and model calibration.", "description": "This paper introduces RISE (Reinforcing Reasoning with Self-Verification), an online reinforcement learning framework that jointly trains LLMs to both solve problems and verify their own solutions using a single integrated process. The method leverages verifiable rewards from outcome verifiers to provide real-time feedback for both generation and verification tasks, addressing the limitation of decoupled learning in existing RLVR approaches.", "key_contribution": "RISE's main innovation is the tight coupling of problem-solving and self-verification learning within unified RL training steps, where models critique their own on-policy generations and both trajectories contribute jointly to policy updates using PPO with a shared critic. This enables explicit training of verification as a primary objective with direct reward signals, rather than treating it as a post-hoc process.", "novelty": "Previous work either trains models to solve OR verify problems separately, or adds self-critique as decoupled post-processing. RISE is novel in explicitly training verification as a primary RL objective alongside problem-solving within the same training loop, using online verification that provides contemporary feedback during training. This unified approach overcomes the limitation of 'superficial self-reflection' where models trained only on solution generation fail to develop robust verification capabilities, demonstrating that true self-verification emerges when directly optimized with verifiable reward signals rather than learned implicitly.", "ai_categories": ["Reinforcement Learning for LLMs", "Self-Improvement and Meta-Learning", "Mathematical and Logical Reasoning"]}, {"paper_id": "3927919", "score": "7", "title": "Best-of-N Jailbreaking", "authors": "John Hughes, Sara Price, Aengus Lynch, Rylan Schaeffer, Fazl Barez, Arushi Somani, Sanmi Koyejo, Henry Sleight, Erik Jones, ..., Mrinank Sharma", "session_type": "SD-5-3913", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/214b0cbe5fe5a3a56ddfd1977e1acfb9c721c50a.pdf", "relevant_to_users": "12", "read_by_users": "16", "topics": "", "key_findings": "Best-of-N Jailbreaking achieves 89% attack success rate on GPT-4o and 78% on Claude 3.5 Sonnet by sampling 10,000 augmented prompts. The method works across modalities (text, vision, audio) and reveals that attack success rates follow power-law-like scaling behavior with the number of samples. When combined with optimized prefix attacks, it yields up to 35% additional improvement. Critically, the paper demonstrates that successful jailbreaks only work ~30% of the time when resampled, indicating the attack exploits system randomness rather than discovering persistent vulnerabilities.", "description": "This paper introduces Best-of-N Jailbreaking, a simple black-box algorithm that jailbreaks frontier AI systems across modalities by repeatedly sampling variations of harmful prompts with simple augmentations (like random shuffling, capitalization for text, or font/speed variations for vision/audio) until a harmful response is elicited. The method requires no gradient access or model internals, making it applicable to closed-source systems.", "key_contribution": "The main contribution is demonstrating that simple, semantically-preserving input perturbations combined with repeated sampling can reliably jailbreak state-of-the-art safety-aligned models across text, vision, and audio modalities. This reveals a fundamental vulnerability: frontier models' sensitivity to innocuous input variations combined with stochastic decoding creates an exploitable probability landscape that scaling defenses alone cannot address.", "novelty": "Unlike prior gradient-based attacks (GCG, AutoDAN) that optimize adversarial prompts or LLM-based methods (PAIR) that craft specific jailbreak strategies, Best-of-N exploits the high-dimensional probability landscape of stochastic models through simple, innocuous augmentations rather than engineered adversarial patterns. The key insight is that jailbreak success stems from exploiting sampling randomness across the input space rather than discovering persistent vulnerabilities or perfect adversarial prompts. This black-box approach works across modalities without transfer or gradients, revealing that semantic-preserving perturbations are sufficient when combined with sufficient sampling, exposing a fundamental robustness limitation in current safety alignment approaches.", "ai_categories": ["Agent Safety and Security", "Reasoning and Test-Time Compute"]}, {"paper_id": "4239206", "score": "7", "title": "Multi-step Visual Reasoning with Visual Tokens Scaling and Verification", "authors": "Tianyi Bai, Zengjie Hu, Fupeng Sun, Qiu Jiantao, Yizhen Jiang, Guangxin He, Bohan Zeng, Conghui He, Binhang Yuan, Wentao Zhang", "session_type": "SD-5-4813", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/a48656c1e2d8fb5cee742d21b8b328458f483707.pdf", "relevant_to_users": "7", "read_by_users": "21", "topics": "", "key_findings": "The paper introduces inference-time visual token scaling with verifier-guided multi-step reasoning for MLLMs. Key findings include: (1) A framework that enables dynamic, iterative visual perception through 10 visual tools (grounding, depth, zoom, OCR, segmentation, etc.) rather than static token encoding; (2) A verifier trained via multi-step DPO that evaluates reasoning steps and determines optimal termination; (3) Significant performance gains across benchmarks‚ÄîGPT-4o improved 6.9% on BLINK average, with up to 18.33% gains on compositional counting tasks; (4) The VTS dataset providing supervised trajectories and preference comparisons for training. The work demonstrates that MLLMs can achieve more interpretable, grounded reasoning through adaptive computation at inference time.", "description": "This paper presents a framework that formulates visual reasoning as a Markov Decision Process, enabling MLLMs to dynamically scale visual tokens during inference through iterative, verifier-guided exploration. Rather than encoding entire images into fixed tokens upfront, a reasoner proposes visual actions from a set of 10 tools while a verifier evaluates these actions and determines when to terminate reasoning.", "key_contribution": "The main innovation is inference-time visual token scaling with a verifier-guided reasoning loop that enables MLLMs to iteratively refine visual understanding through selective tool use. The verifier, trained via multi-step DPO, provides adaptive computation by evaluating reasoning steps and determining optimal termination points, moving from static single-pass inference to dynamic, feedback-driven visual perception.", "novelty": "Unlike existing MLLMs that use static inference with fixed visual tokens encoded upfront, this work enables dynamic, context-aware visual reasoning that mirrors human selective attention. It addresses the limitation that previous approaches cannot iteratively refine understanding or adapt during inference. The novel MDP formulation with verifier-guided termination allows variable-length reasoning chains with bounded steps, giving models agency to autonomously decide what to observe next, how to focus attention, and when to stop‚Äîfundamentally different from rigid, predetermined visual processing pipelines.", "ai_categories": ["Reasoning and Test-Time Compute", "Tool Use and Code Generation", "Vision-Language-Action Models"]}, {"paper_id": "4068425", "score": "7", "title": "Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models", "authors": "Vlad Sobal, Wancong Zhang, Kyunghyun Cho, Randall Balestriero, Tim G. J. Rudner, Yann LeCun", "session_type": "SD-6-212", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/5233ea1b8598dfd1eed771e087de919731ced5a1.pdf", "relevant_to_users": "26", "read_by_users": "80", "topics": "", "key_findings": "This paper systematically compares model-free RL and model-based planning approaches on offline, reward-free data across 23 datasets with controlled quality variations. Key findings show that model-free RL requires large amounts of high-quality data to succeed, while the proposed PLDM (Planning with Latent Dynamics Model) approach demonstrates superior generalization to unseen environments, better data efficiency (achieving ~80% success with only thousands of transitions), and robustness to suboptimal data quality. PLDM is the only method that reliably succeeds on held-out mazes and can transfer to new tasks by simply modifying planning objectives without retraining.", "description": "This paper addresses the question of whether reinforcement learning or optimal control is better suited for learning from offline, reward-free trajectories. The authors conduct a systematic evaluation of six methods (five model-free RL approaches and one model-based planning approach) across navigation tasks with varying dataset qualities, and introduce PLDM, which uses Joint Embedding Predictive Architecture (JEPA) to learn latent dynamics models for planning without reconstruction.", "key_contribution": "The main contribution is a systematic empirical comparison revealing that model-based planning with latent dynamics (PLDM) achieves comparable trajectory stitching to leading model-free methods while providing superior generalization, data efficiency, and robustness to suboptimal offline data. The paper also introduces two new navigation environments with granular control over dataset generation and provides practical guidelines for method selection based on data availability.", "novelty": "Unlike prior work that focused on offline RL with rewards or limited reward-free scenarios, this paper uniquely provides fine-grained analysis of how data quality, diversity, and trajectory characteristics independently affect different methods across 23 controlled datasets. It demonstrates that reconstruction-free self-supervised learning (via JEPA) outperforms reconstruction-based approaches for control, and shows that model-based planning can handle distribution shift and suboptimal data better than model-free methods. Previous offline RL work didn't systematically isolate these factors or compare RL versus control paradigms in reward-free settings.", "ai_categories": ["Planning and Decision Making", "Reinforcement Learning for LLMs", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4210986", "score": "6", "title": "R$^2$ec: Towards Large Recommender Models with Reasoning", "authors": "Runyang You, Yongqi Li, Xinyu Lin, Xin Zhang, Wenjie Wang, Wenjie Li, Liqiang Nie", "session_type": "SD-1-2412", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/f29b278eccadd4b83c9cca979b6b35476c14e3a8.pdf", "relevant_to_users": "1", "read_by_users": "4", "topics": "", "key_findings": "R¬≤ec introduces a unified large recommender model with intrinsic reasoning capability through a dual-head architecture that generates reasoning chains and performs efficient single-step item prediction. The paper presents RecPO, a novel reinforcement learning framework that jointly optimizes reasoning and recommendation without human-annotated reasoning data using a fused reward mechanism combining NDCG and similarity rewards. Extensive experiments show R¬≤ec outperforms traditional, LLM-based, and reasoning-augmented baselines with 35-77% improvements on Hit@5, demonstrating that reasoning consistently improves recommendations across model sizes with gains increasing at scale (21.7% higher N@5 for larger models).", "description": "R¬≤ec addresses the challenge of integrating reasoning capabilities into large recommender models while maintaining computational efficiency. Unlike prior approaches that use separate reasoning and recommendation modules or lack reasoning entirely, R¬≤ec proposes a unified dual-head architecture with a language modeling head for reasoning and a recommendation head for efficient single-step item prediction, trained end-to-end using the RecPO reinforcement learning framework.", "key_contribution": "The main innovation is the dual-head architecture that unifies reasoning and recommendation in a single model with true end-to-end optimization, combined with RecPO‚Äîa reinforcement learning framework that enables joint training of reasoning and recommendation without human-annotated reasoning data through a novel fused reward mechanism blending discrete ranking signals (NDCG) with continuous similarity rewards.", "novelty": "Unlike previous reasoning-augmented recommenders that use separate parallel modules requiring gradient-free alternating optimization, R¬≤ec achieves true end-to-end gradient flow through its unified architecture, enabling fine-grained alignment between reasoning and ranking objectives. It overcomes the data scarcity problem by using RL instead of requiring human-annotated reasoning chains, and replaces expensive autoregressive item ID generation with efficient single-step prediction, reducing inference latency by 66-68% compared to prior LLM-based recommenders while delivering superior performance. The work demonstrates that reasoning capability can be intrinsically embedded in recommender models rather than bolted on as external modules.", "ai_categories": ["Reasoning and Test-Time Compute", "Tool Use and Code Generation", "Reinforcement Learning for LLMs"]}, {"paper_id": "4442501", "score": "6", "title": "Triplets Better Than Pairs: Towards Stable and Effective Self-Play Fine-Tuning for LLMs", "authors": "Yibo Wang, Hai-Long Sun, Guangda Huzhang, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Lijun Zhang", "session_type": "SD-5-3608", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/e0596e6e39072aed73b22d2c2c86772c2aae52a0.pdf", "relevant_to_users": "1", "read_by_users": "5", "topics": "", "key_findings": "TSPIN addresses critical stability issues in self-play fine-tuning by introducing historical advantages (comparing current responses to initial policy responses) that maintain optimization signal even when current advantages diminish, and entropy constraints that enable reference-free fine-tuning. The method achieves comparable or superior performance to supervised fine-tuning while using only 25% of annotated samples, with stable performance across training iterations.", "description": "This paper proposes TSPIN (Triplet-based Self-Play Fine-Tuning), which improves upon SPIN by using triplet comparisons instead of pairs to address unstable optimization and misalignment between training rewards and generation metrics in self-play fine-tuning of large language models.", "key_contribution": "The main innovation is a triplet-based comparison framework that incorporates both current advantages (between current and previous iteration responses) and historical advantages (between current responses and initial policy responses), combined with entropy constraints that enable reference-free fine-tuning without training-generation discrepancy.", "novelty": "Unlike SPIN which only uses pairwise comparisons between consecutive iterations and suffers from vanishing reward advantages, TSPIN introduces a third anchor point (the initial policy's responses) to maintain stable optimization signals throughout training. It also uniquely incorporates entropy regularization theoretically justified for reference-free fine-tuning, eliminating the need for reference policies during generation and resolving the train-test mismatch problem inherent in SPIN.", "ai_categories": ["Reinforcement Learning for LLMs", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4421055", "score": "5", "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems", "authors": "Hancheng Ye, Zhengqi Gao, Mingyuan Ma, Qinsi Wang, Yuzhe Fu, Ming-Yu Chung, Yueqian Lin, Zhijian Liu, Jianyi Zhang, ..., Yiran Chen", "session_type": "SD-1-810", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/81561154949bf17e7f12ee6dc0485c10a2415686.pdf", "relevant_to_users": "2", "read_by_users": "4", "topics": "", "key_findings": "KVCOMM achieves up to 7.8x speedup in multi-agent LLM systems by enabling dynamic KV-cache reuse across agents with different context prefixes. It maintains 70-87.6% cache reuse rates across diverse tasks (retrieval-augmented generation, mathematical reasoning, collaborative coding) while preserving accuracy within ~2% of baseline. The training-free framework reduces time-to-first-token from 430ms to 55ms in five-agent configurations, solving the critical multi-context redundancy problem that plagued previous multi-agent systems.", "description": "KVCOMM is a training-free, prompt-adaptive KV-cache sharing framework for LLM-based multi-agent systems that addresses computational inefficiency caused by redundant context reprocessing. The paper introduces an anchor pool mechanism that dynamically tracks and predicts KV-cache offsets across agents with diverging prefix contexts, enabling efficient cache reuse without model retraining.", "key_contribution": "The first framework to enable cross-agent KV-cache reuse despite varying context prefixes by solving the offset variance problem through an anchor-based online learning mechanism. It uses RoPE de-rotation/re-rotation for positional alignment and embedding-based interpolation to predict cache offsets dynamically, eliminating the need for full context recomputation.", "novelty": "Unlike prior KV-caching methods that assume static prefixes and apply uniform reuse policies, KVCOMM uniquely addresses the offset variance problem where identical shared text produces different KV-cache positions under varying agent-specific prefixes. Previous approaches like CacheBlend used selective recomputation with fixed policies, while KVCOMM introduces runtime-adaptive decisions through anchor-based offset prediction, enabling complete cache reuse without profiling or training. This is the first work to make KV-cache sharing practical for multi-agent systems with dynamic, heterogeneous contexts.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Memory and Context Management", "Model Efficiency and Optimization"]}, {"paper_id": "4441434", "score": "5", "title": "Length Generalization via Auxiliary Tasks", "authors": "Pranjal Awasthi, Anupam Gupta, Ravi Kumar", "session_type": "SD-2-5402", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/a5e5e4faf09290faa591d33b29401917374054fa.pdf", "relevant_to_users": "0", "read_by_users": "0", "topics": "", "key_findings": "This paper introduces 'extrapolation by association,' showing that transformers can transfer length generalization capabilities across related tasks through attention head reuse. Training on a longer auxiliary task (e.g., length ‚â§32) enables models to generalize to longer sequences on a target task trained only on short sequences (e.g., length ‚â§16). Key findings: (1) Transfer occurs across arithmetic (reverse add, carry operations), string manipulation (copy, reverse, capitalize), and maze navigation tasks. (2) Transfer is most effective when auxiliary-to-main task length ratios are between 0.5-2. (3) Mechanistic analysis reveals that shared attention heads between tasks correlate with successful transfer‚Äîmodels with more computational overlap show better generalization. (4) Similar effects appear in pretrained language models, suggesting pretraining provides reusable computational scaffolding for downstream extrapolation.", "description": "This paper investigates length generalization in transformers‚Äîthe ability to handle longer sequences than seen during training‚Äîthrough the lens of task association. The authors demonstrate that training jointly with a longer, related auxiliary task enables models to extrapolate to longer inputs on a target task, tested across algorithmic domains including arithmetic, string manipulation, and maze navigation.", "key_contribution": "The main contribution is demonstrating that length generalization is transferable across related tasks through attention head reuse, rather than being task-specific. This introduces 'extrapolation by association' as a mechanism where multi-task training with strategically chosen auxiliary tasks induces implicit length generalization without architectural modifications.", "novelty": "Unlike previous work focusing on position encoding modifications or task-specific architectural changes for length generalization, this paper reveals that multi-task learning naturally facilitates length transfer through compositional attention head reuse. It addresses the limitation of prior approaches that treated length generalization as isolated to individual tasks, showing instead that transformers learn task-agnostic compositional strategies. The work provides the first mechanistic evidence linking attention head sharing between tasks to successful length generalization transfer, demonstrating that relatedness of tasks (not just architectural design) is crucial for extrapolation.", "ai_categories": ["Reasoning and Test-Time Compute", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4271278", "score": "5", "title": "Representation Consistency for Accurate and Coherent LLM Answer Aggregation", "authors": "Junqi Jiang, Tom Bewley, Salim I. Amoukou, Francesco Leofante, Antonio Rago, Saumitra Mishra, Francesca Toni", "session_type": "SD-2-4008", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/a3ecdca64c27eaba285b6683681268faca747be5.pdf", "relevant_to_users": "0", "read_by_users": "6", "topics": "", "key_findings": "This paper introduces Representation Consistency (RC), a test-time scaling method that improves LLM answer aggregation by analyzing internal model activations rather than just counting answer frequencies. RC achieves consistent accuracy improvements of up to 4% over self-consistency baselines across four open-source LLMs and four reasoning datasets. The method uses cached activations and lightweight similarity computations, requiring no additional model queries. RC-S (using sparse autoencoders) demonstrates >90% agreement with human judgments about reasoning coherence.", "description": "The paper addresses limitations in existing test-time scaling methods like self-consistency (SC), which rely solely on answer frequency for aggregation. RC enhances answer aggregation by weighting answers based on the consistency of internal model activations during response generation, distinguishing between coherent and incoherent reasoning paths that lead to the same answer.", "key_contribution": "The main innovation is combining frequency-based voting with activation consistency metrics to create a unified scoring function: V(q,a) = Œª¬∑consistency + (1-Œª)¬∑frequency. This allows the method to downweight answers that appear frequently but arise from inconsistent internal reasoning patterns, without requiring additional model queries or modifications to prompting strategies.", "novelty": "Unlike self-consistency and other test-time scaling methods that only count answer occurrences, RC is the first to leverage internal model activations to assess reasoning coherence during aggregation. It addresses the critical limitation that multiple responses reaching the same answer may do so through wildly different and potentially incoherent reasoning paths. The method works with any response generation strategy (varied prompts, sampling methods) and operates entirely on cached activations, making it computationally efficient and broadly applicable.", "ai_categories": ["Reasoning and Test-Time Compute", "Mathematical and Logical Reasoning"]}, {"paper_id": "4205663", "score": "5", "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "authors": "Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin", "session_type": "SD-4-5418", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/8136e4668a09f8c47a2454d9e72728d4fdea055e.pdf", "relevant_to_users": "1", "read_by_users": "3", "topics": "", "key_findings": "The paper introduces AnytimeReasoner, a framework that enables LLMs to provide high-quality reasoning solutions under varying computational budgets. It proposes Budget Relative Policy Optimization (BRPO), a variance reduction technique that outperforms GRPO across all thinking budgets. Key results show that on 1.5B and 7B parameter models, AnytimeReasoner achieves 32.7% accuracy on AIME2024 (vs GRPO's 28.9%) and improves average anytime accuracy to 46.0% (vs 43.0%) across five benchmarks, enabling more flexible and token-efficient deployment in resource-constrained services.", "description": "This paper addresses the challenge of optimizing LLM reasoning under varying computational constraints. Unlike existing approaches that optimize for a single fixed token budget, AnytimeReasoner enables models to be interrupted at any point during reasoning while still providing best-effort solutions, making them suitable for resource-constrained deployment scenarios.", "key_contribution": "The main contribution is Budget Relative Policy Optimization (BRPO), a novel variance reduction technique that leverages verifiable dense rewards through budget-sampled truncation and decoupled optimization of thinking and summary policies. This enables models to maintain strong performance across all thinking budgets rather than optimizing for a single fixed budget.", "novelty": "Unlike GRPO which optimizes only final performance under maximum budget with sparse rewards, AnytimeReasoner introduces intermediate verifiable rewards at sampled budgets for better credit assignment during RL training. The novel BRPO baseline exploits the sequential nature of LLM generation where current context serves as a prefix for future context, interpolating between progress-aware and group-based variance reduction. This addresses fundamental limitations in training efficiency, deployment flexibility, and token waste by enabling the model to reach correct answers efficiently at any computational budget rather than requiring full trace generation.", "ai_categories": ["Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs", "Model Efficiency and Optimization"]}, {"paper_id": "4311460", "score": "5", "title": "Checklists Are Better Than Reward Models For Aligning Language Models", "authors": "Vijay Viswanathan, Yanchao Sun, Xiang Kong, Meng Cao, Graham Neubig, Tongshuang Wu", "session_type": "SD-4-103", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/490597cf8f353f8b01b8474e2f98c045eba8f5f4.pdf", "relevant_to_users": "6", "read_by_users": "19", "topics": "", "key_findings": "The paper introduces Reinforcement Learning from Checklist Feedback (RLCF), which replaces scalar reward models with structured checklists that evaluate whether responses satisfy specific, interpretable criteria. RLCF consistently outperforms standard reward model-based methods across multiple benchmarks including AlpacaEval, ArenaHard, FollowBench, XSTest, GSM8K, and TruthfulQA. The approach preserves interpretability throughout training by maintaining explicit criterion evaluations rather than collapsing complex alignment goals into single scalar scores.", "description": "This paper proposes RLCF (Reinforcement Learning from Checklist Feedback) as an alternative to traditional reward models for aligning language models. Instead of using scalar rewards that collapse quality into a single number, RLCF decomposes alignment goals into discrete, verifiable checklist items that are evaluated independently, preserving semantic meaning and interpretability throughout the training process.", "key_contribution": "The main innovation is replacing holistic scalar reward signals with structured, criterion-level checklist evaluations for reinforcement learning alignment. This preserves interpretability, reduces gaming potential, and demonstrates empirical improvements across diverse benchmarks while maintaining explicit feedback about why responses succeed or fail.", "novelty": "Unlike RLHF and reward model approaches that rely on pairwise comparisons or scalar ratings, RLCF maintains interpretable criterion-level feedback throughout training. This addresses the fundamental limitation of reward models that oversimplify multidimensional quality into single scores, leading to gaming behaviors and lack of explainability. The structured decomposition approach is harder to exploit than abstract scalar signals while providing clearer guidance on what constitutes alignment.", "ai_categories": ["Reinforcement Learning for LLMs", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4385508", "score": "5", "title": "See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model", "authors": "Pengteng Li, Pinhao Song, Wuyang Li, Huizai Yao, Weiyu Guo, Yijie Xu, Dugang Liu, Hui Xiong", "session_type": "SD-5-5206", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/bcef03fe8da8c459269e00eeb6626d6584a0ba4c.pdf", "relevant_to_users": "5", "read_by_users": "37", "topics": "", "key_findings": "See&Trek demonstrates consistent improvements across spatial reasoning benchmarks (VSI-Bench and STI-Bench), achieving up to +3.5% performance gains on various MLLMs without any training. The framework operates entirely on CPU and requires only a single forward pass, making it highly practical. It achieves particularly strong results on numerical reasoning tasks (+10.8% on Appearance Order) and dynamic understanding, proving that training-free spatial prompting can significantly enhance MLLM spatial comprehension.", "description": "See&Trek is the first training-free and GPU-free spatial prompting framework designed to enhance spatial understanding in Multimodal Large Language Models (MLLMs) for video processing. It addresses two critical limitations: visual homogeneity from uniform temporal sampling and unknown motion information that prevents models from inferring spatial relationships.", "key_contribution": "The paper introduces a plug-and-play framework combining Maximum Semantic Richness Sampling (using YOLO-based detection with Balanced-TopK strategy), Motion Reconstruction via Visual Odometry, and Spatiotemporal Encoding through temporal markers and color-coding. This enables MLLMs to better understand spatial relationships without requiring model retraining, additional modalities, or GPU resources.", "novelty": "Unlike previous approaches that require fine-tuning, retraining, or additional input modalities (depth maps, point clouds, camera poses), See&Trek is completely training-free, GPU-free, and works with vision-only RGB input. It addresses the fundamental limitations of visual homogeneity and unknown motion through perception-guided keyframe selection and explicit motion reconstruction, making spatial reasoning enhancement accessible to any existing MLLM without modification. This represents a paradigm shift from model-centric to prompt-centric spatial understanding improvement.", "ai_categories": ["Vision-Language-Action Models", "Spatial and Physical Reasoning", "Model Efficiency and Optimization"]}, {"paper_id": "4243007", "score": "5", "title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning", "authors": "Xiao Liang, Zhong-Zhi Li, Yeyun Gong, Yang Wang, Hengyuan Zhang, yelong shen, Ying Nian Wu, Weizhu Chen", "session_type": "SD-6-1802", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/c3403842de341324f63358f1732f1518761661f2.pdf", "relevant_to_users": "1", "read_by_users": "2", "topics": "", "key_findings": "SwS introduces a self-aware framework that identifies model weaknesses during preliminary RL training (problems with <50% accuracy and declining performance trends), then synthesizes targeted problems to address these deficiencies. Achieves substantial gains of 10.0% (7B models) and 7.7% (32B models) across eight reasoning benchmarks including AIME, GSM8K, and MATH-500, with up to 20% improvement on previously failed problems in weak domains. The framework uses category-weighted budget allocation to focus synthetic problem generation on the weakest domains, with rigorous answer verification through self-consistency.", "description": "This paper presents SwS, a framework that enhances Reinforcement Learning with Verifiable Rewards (RLVR) for LLM mathematical reasoning by identifying model weaknesses during preliminary training and synthesizing targeted problems to address those specific deficiencies. The approach uses concept extraction from failure cases, recombines concepts based on co-occurrence patterns and embeddings, and generates augmented training problems with verified reference answers to enable self-directed improvement without external knowledge distillation.", "key_contribution": "The main innovation is the weakness-driven problem synthesis approach that explicitly identifies persistent failure patterns during RL training (accuracy <50% with negative trend), extracts core concepts from these failures, and generates targeted synthetic problems with difficulty calibration and rigorous answer verification, enabling models to systematically address their specific reasoning deficiencies rather than training indiscriminately on generic problem sets.", "novelty": "Unlike prior synthetic data methods (MetaMath, MathScale, PRIME) that indiscriminately expand problem sets or generate problems without considering model capabilities, SwS addresses the key limitation that existing approaches ignore model-specific weaknesses and produce problems that are either too trivial or too challenging. It introduces a two-phase training process with explicit weakness identification (negative accuracy slopes), category-weighted budget allocation focusing resources on weakest domains, and difficulty calibration to ensure synthetic problems remain within learnable ranges. This contrasts with previous RLVR approaches that suffer from gradient vanishing on uniformly difficult or easy problems and lack mechanisms for targeted self-improvement.", "ai_categories": ["Reinforcement Learning for LLMs", "Self-Improvement and Meta-Learning", "Mathematical and Logical Reasoning"]}, {"paper_id": "4442635", "score": "5", "title": "ChatVLA-2: Vision-Language-Action Model with Open-World Reasoning", "authors": "Zhongyi Zhou, Yichen Zhu, Xiaoyu Liu, Zhibin Tang, Junjie Wen, Yaxin Peng, Chaomin Shen, Yi Xu", "session_type": "SD-6-2201", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/c88d737915ea445cb600d21cb0c7125912b7053b.pdf", "relevant_to_users": "12", "read_by_users": "58", "topics": "", "key_findings": "ChatVLA-2 solves the critical problem of VLM capability erosion during robotics fine-tuning by using a two-stage training pipeline with dynamic mixture-of-experts architecture. It achieves 82.7% success on open-world math reasoning tasks (vs. 0% for baselines like OpenVLA, DexVLA, œÄ‚ÇÄ) and 81.4% on novel object placement tasks with unseen objects. The model successfully preserves mathematical reasoning, OCR, and spatial understanding capabilities from the pre-trained VLM while adapting to robotic action control, demonstrating genuine open-world generalization without explicit training on these skills.", "description": "ChatVLA-2 is a vision-language-action model that addresses the fundamental challenge of preserving pre-trained VLM capabilities during robotics fine-tuning through a novel two-stage training approach with dynamic mixture-of-experts. The system maintains open-world reasoning abilities (math, OCR, spatial understanding) while learning robotic control, achieving state-of-the-art performance on both novel reasoning tasks and traditional manipulation benchmarks.", "key_contribution": "The main innovation is a two-stage training pipeline combining (1) co-training on image-text and robotics data to establish knowledge-action connections, and (2) a reasoning-following stage that freezes the VLM backbone while training only action experts. The dynamic mixture-of-experts architecture with adaptive routing preserves the LLM structure while enabling task-specific adaptation without feature competition.", "novelty": "Unlike prior VLA models that suffer from catastrophic forgetting of pre-trained capabilities during robotics fine-tuning, ChatVLA-2 explicitly preserves and leverages VLM knowledge through staged training and selective expert activation. Previous work like OpenVLA and DexVLA cannot handle open-world reasoning tasks (achieving 0% on novel math problems), while ChatVLA-2 demonstrates genuine transfer of untrained capabilities (82.7% success). The reasoning-following mechanism uniquely ensures actions align with internal model reasoning rather than just adding reasoning as a separate capability.", "ai_categories": ["Vision-Language-Action Models", "Reasoning and Test-Time Compute", "Tool Use and Code Generation"]}, {"paper_id": "4279550", "score": "4", "title": "Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab", "authors": "Haonan Duan, Stephen Lu, Caitlin F Harrigan, Nishkrit Desai, Jiarui Lu, Micha√Ö¬Ç Koziarski, Leonardo Cotta, Chris Maddison", "session_type": "SD-2-1615", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2507.02083", "relevant_to_users": "2", "read_by_users": "8", "topics": "", "key_findings": "The paper demonstrates that current language models can engage with complex scientific workflows but reveals significant performance gaps in systems biology tasks. It establishes that evaluating LMs through executable scientific workflows with real biological models (SBML format) provides more meaningful assessment than traditional benchmarks. The benchmark shows measurable differences in how various LM architectures handle biological reasoning, simulation tools, and domain-specific experimental protocols.", "description": "This paper introduces SciGym, a benchmark framework designed to evaluate language models' scientific capabilities in systems biology by creating a comprehensive 'dry lab' environment where LMs perform experiments using computational tools, biological simulation environments, and real biological data from SBML model repositories.", "key_contribution": "The primary innovation is establishing a systems biology-specific evaluation framework that grounds assessment in executable scientific workflows rather than static question-answering, integrating real biological models, simulation tools, and domain-specific experimental protocols to assess scientific reasoning capabilities rather than just knowledge retrieval.", "novelty": "Unlike previous LM benchmarks that focus on general knowledge or isolated tasks, this work evaluates models through complete executable scientific workflows in a realistic dry lab setting. It addresses the gap between general-purpose benchmarks and domain-specific scientific capability assessment by incorporating intermediate task feedback mechanisms typical of actual scientific research. The approach moves evaluation from static Q&A to dynamic interaction with biological simulation environments and real models.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation", "Domain-Specific Applications"]}, {"paper_id": "4236952", "score": "4", "title": "MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark", "authors": "Junjie Xing, Yeye He, Mengyu Zhou, Haoyu Dong, Shi Han, Lingjiao Chen, Dongmei Zhang, Surajit Chaudhuri, H. V. Jagadish", "session_type": "SD-2-1804", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.05587", "relevant_to_users": "0", "read_by_users": "2", "topics": "", "key_findings": "MMTU introduces a comprehensive benchmark evaluating LLMs on diverse table understanding tasks including SQL generation, table QA, column type annotation, entity linking, spreadsheet automation, and long-context reasoning. Evaluation of state-of-the-art models (GPT-4, Claude, Gemini) reveals significant challenges particularly with long-context scenarios and complex multi-step operations, establishing performance baselines and identifying areas needing improvement in table understanding capabilities.", "description": "This paper presents MMTU, a large-scale benchmark designed to evaluate large language models on multiple table understanding and reasoning tasks across various domains including database querying, information extraction, semantic understanding, and spreadsheet automation. The benchmark provides a unified framework for assessing diverse table-related capabilities of LLMs.", "key_contribution": "MMTU creates the first comprehensive multi-task benchmark that unifies diverse table understanding capabilities in a single framework, spanning natural language to SQL, table QA, column annotation, entity linking, spreadsheet automation, and novel long-context table reasoning tasks that reflect real-world usage scenarios.", "novelty": "Unlike prior benchmarks focusing on isolated table tasks, MMTU addresses the gap by integrating multiple table understanding capabilities into one unified evaluation framework. It introduces novel long-context reasoning tasks requiring models to process extensively long tables, testing practical real-world scenarios that existing benchmarks overlook. The benchmark also incorporates spreadsheet-specific tasks reflecting actual user interactions with tools like Excel, providing a more comprehensive and realistic assessment of LLM table understanding capabilities.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation"]}, {"paper_id": "4101176", "score": "4", "title": "MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?", "authors": "Zhe Xu, Daoyuan Chen, Zhenqing Ling, Yaliang Li, Ying Shen", "session_type": "SD-6-113", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2503.09499", "relevant_to_users": "1", "read_by_users": "8", "topics": "", "key_findings": "The paper identifies critical factors in question synthesis for reasoning-focused fine-tuning, demonstrating that specific characteristics of synthesized questions (such as complexity, specificity, and reasoning requirements) significantly impact model reasoning capabilities. Through comprehensive ablation studies, it establishes that thoughtful question design matters substantially for thinking-centric fine-tuning, moving beyond simple question quantity to focus on quality metrics that enhance reasoning performance.", "description": "This paper systematically investigates what properties make training questions effective for improving AI models' reasoning abilities. It provides empirical analysis of question synthesis parameters and their relationship to downstream model performance on complex, thinking-intensive tasks.", "key_contribution": "The main contribution is a systematic empirical analysis of question synthesis dimensions for reasoning-focused fine-tuning, identifying specific question characteristics that drive improvements in model reasoning capabilities rather than treating question generation as a generic scaling process.", "novelty": "Unlike previous work that treats question generation generically and focuses on scaling quantity, this research provides granular analysis of what specific question properties (complexity, specificity, reasoning requirements) actually drive reasoning improvements. It fills a methodological gap by systematically examining the quality dimensions of synthesized questions rather than just increasing volume, addressing the limitation that existing instruction-tuning research lacks deep investigation into which synthesis parameters are most effective.", "ai_categories": ["Reasoning and Test-Time Compute", "Self-Improvement and Meta-Learning"]}, {"paper_id": "4210264", "score": "4", "title": "Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models", "authors": "Ilgee Hong, Changlong Yu, Liang Qiu, Weixiang Yan, Zhenghao Xu, Haoming Jiang, Qingru Zhang, Qin Lu, Xin Liu, ..., Tuo Zhao", "session_type": "SD-6-3613", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/eec1920da8921f9ebab8a70513b7531b0b8281d3.pdf", "relevant_to_users": "5", "read_by_users": "12", "topics": "", "key_findings": "Think-RM achieves state-of-the-art results on RM-Bench, outperforming Bradley-Terry RMs and vertically scaled GenRMs by 8%. The framework demonstrates that depth-oriented long-horizon reasoning (extending single trajectories from hundreds to thousands of tokens) outperforms breadth-oriented vertical scaling approaches on reasoning-intensive tasks. The novel pairwise RLHF pipeline that directly optimizes policies using pairwise preference rewards shows superior end-policy performance compared to traditional pointwise RLHF methods.", "description": "Think-RM is a training framework that enables long-horizon reasoning in generative reward models by modeling an internal thinking process. Rather than producing shallow, structured rationales, it generates flexible, self-guided reasoning traces supporting self-reflection, hypothetical reasoning, and divergent reasoning capabilities for evaluating complex responses in RLHF.", "key_contribution": "The paper introduces a two-stage training approach (SFT on long CoT data followed by rule-based RL) that extends reward model reasoning from hundreds to thousands of tokens, and presents a novel pairwise RLHF pipeline that directly optimizes policies using pairwise preference rewards without converting to pointwise signals, eliminating information loss and enabling more effective use of GenRM outputs.", "novelty": "Unlike previous generative reward models that rely on shallow, vertically scaled reasoning with structured external rationales, Think-RM enables deep, long-horizon reasoning through self-guided internal thinking processes. It addresses the fundamental limitation of existing GenRMs' inability to handle nuanced, reasoning-intensive tasks by scaling depth rather than breadth. The pairwise RLHF pipeline solves the technical challenge of pointwise-pairwise incompatibility in standard RLHF algorithms, allowing direct optimization from preference structures.", "ai_categories": ["Reinforcement Learning for LLMs", "Reasoning and Test-Time Compute"]}, {"paper_id": "4227522", "score": "3", "title": "Janus-Pro-R1: Advancing Collaborative Visual Comprehension and Generation via Reinforcement Learning", "authors": "Kaihang Pan, Yang Wu, Wendong Bu, Kai Shen, Juncheng Li, Yingting Wang, liyunfei, Siliang Tang, Jun Xiao, ..., Yueting Zhuang", "session_type": "SD-1-5401", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/4aad34acb1fe54ce3d709604ec7d81ff8ce11b11.pdf", "relevant_to_users": "4", "read_by_users": "11", "topics": "", "key_findings": "Janus-Pro-R1 achieves genuine unification of visual comprehension and generation through reinforcement learning, enabling 'Aha moments' where models self-correct image generation errors through iterative introspection. The work demonstrates that RL (not just SFT) is essential for authentic reasoning - achieving 7.5% improvement on GenEval (outperforming GPT-4o at 0.86 vs 0.85), 47.0% on T2I-CompBench, and 1.7% on DPG-Bench. Critically, it shows mutual enhancement: both generation quality and comprehension capabilities improve simultaneously through collaborative training, with the model becoming a reliable semantic evaluator (91.1% reliability) while generating superior images.", "description": "This paper introduces Janus-Pro-R1, a multimodal LLM that unifies visual comprehension and generation through a two-stage training framework: supervised fine-tuning to decompose visual generation into sub-skills (generate, evaluate, regenerate), followed by reinforcement learning (GRPO) that enables spontaneous self-reflection. The approach transforms image generation from a single-pass process into an iterative introspective loop where the model detects and corrects its own errors without ground-truth images.", "key_contribution": "The main innovation is enabling genuine chain-of-thought reasoning for visual generation through RL-driven co-evolution of comprehension and generation capabilities. Unlike prior work that treats these as separate functions or uses forced textual planning, Janus-Pro-R1 achieves spontaneous self-correction where the model naturally evaluates generated images and regenerates improved versions through authentic reflection rather than mechanistic imitation.", "novelty": "Previous MLLMs like Janus-Pro treated visual comprehension and generation as separate encapsulated functions without synergy, limiting even basic text-to-image performance. This work addresses this by embedding CoT directly into token-level decisions through RL rather than cascading external models or using intermediate diffusion steps. The key insight is that SFT alone produces only marginal gains through memorization (0.81 vs 0.80 baseline), while RL unlocks generalization and genuine unification - demonstrating that the same model simultaneously improves both generation quality AND becomes a stronger evaluator, representing true capability synergy rather than mere parameter-sharing.", "ai_categories": ["Reinforcement Learning for LLMs", "Vision-Language-Action Models", "Reasoning and Test-Time Compute"]}, {"paper_id": "4214784", "score": "3", "title": "Dynamic Risk Assessments for Offensive Cybersecurity Agents", "authors": "Boyi Wei, Benedikt Stroebl, Jiacen Xu, Joie Zhang, Zhou Li, Peter Henderson", "session_type": "SD-2-1109", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.18384", "relevant_to_users": "5", "read_by_users": "7", "topics": "", "key_findings": "This paper introduces a dynamic risk assessment framework for offensive cybersecurity agents that enables continuous, context-aware evaluation throughout agent execution. The work demonstrates that real-time risk monitoring with adaptive intervention strategies can balance agent capability with safety constraints, moving beyond static pre-deployment assessments. This approach shows practical applicability across vulnerability discovery and penetration testing scenarios while enabling safer deployment of increasingly autonomous AI security tools.", "description": "The paper addresses how to safely deploy AI agents for offensive cybersecurity tasks by developing dynamic risk assessment methods that adapt as agents autonomously execute security operations. It proposes a framework that continuously evaluates threat levels throughout an agent's operation, enabling real-time intervention based on offensive technique severity, target criticality, and operational context.", "key_contribution": "The main innovation is a continuous, adaptive risk assessment framework that performs real-time monitoring and graduated intervention for offensive cybersecurity agents, replacing binary allow/block decisions with context-aware, dynamic threat evaluation throughout multi-step autonomous operations.", "novelty": "Unlike prior work that treated offensive AI capability evaluation as a one-time, static pre-deployment assessment, this research implements ongoing, context-aware risk monitoring that adapts to an agent's current actions, objectives, and environmental conditions. It addresses the fundamental limitation that static evaluation cannot account for unforeseen agent behaviors during execution, introducing graduated intervention strategies that balance operational effectiveness with safety constraints in real-time rather than through fixed predetermined limits.", "ai_categories": ["Agent Safety and Security", "Agent Benchmarking and Evaluation", "Domain-Specific Applications"]}, {"paper_id": "4207585", "score": "3", "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation", "authors": "Jiajun Shi, Jian Yang, Jiaheng Liu, Xingyuan Bu, Jiangjie Chen, Junting Zhou, Kaijing Ma, Zhoufutu Wen, Bingli Wang, ..., Ge Zhang", "session_type": "SD-4-2014", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/a3f1d4df6324abb0ea9576e5fe2da1d467238283.pdf", "relevant_to_users": "0", "read_by_users": "0", "topics": "", "key_findings": "KORGym evaluates 19 LLMs and 8 VLMs across 50+ games spanning six reasoning dimensions (mathematical, puzzle, spatial, strategic, control interaction, and multimodal). Key findings include: (1) closed-source models like O3-mini (82%) significantly outperform open-source alternatives, (2) reasoning abilities show consistent patterns within model families, (3) 'thinking' models form distinct performance clusters, (4) thinking mechanisms can outweigh parameter count (DeepSeek-R1-Distill-Qwen-32B beats Qwen2.5-72B-Instruct), and (5) strong correlation exists between response length and reasoning performance. The platform reveals that RL-trained models (like Doubao-1.5-thinking-pro) excel particularly in puzzle reasoning.", "description": "KORGym is a dynamic game-based evaluation platform that assesses LLM and VLM reasoning capabilities through interactive, multi-turn game environments. It introduces knowledge orthogonal reasoning evaluation‚Äîdecoupling reasoning assessment from memorized pretraining knowledge‚Äîacross 50+ games in textual and visual formats with reinforcement learning support.", "key_contribution": "The main innovation is a unified, gymnasium-style platform combining 50+ diverse games with multi-turn interaction, RL integration, and multimodal support, moving beyond static, domain-specific benchmarks. It introduces a modular architecture with standardized APIs and the Capability Dimension Aggregated Mean metric that normalizes heterogeneous game scores for fair cross-task comparison.", "novelty": "Unlike previous benchmarks that conflate memorization with reasoning (limiting assessment to single-turn, domain-specific tasks), KORGym implements knowledge orthogonal evaluation through rule-based games requiring systematic reasoning independent of pretraining data. It addresses critical limitations: LogicGame's single-turn constraints preventing long-term planning assessment, TextArena's lack of robustness, and gg-bench's poor gameplay fidelity. The platform uniquely enables controlled difficulty, multi-epoch evaluation, and robust RL training without the opponent-interference confounds of competitive multi-agent setups.", "ai_categories": ["Agent Benchmarking and Evaluation", "Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs"]}, {"paper_id": "4144293", "score": "3", "title": "Among Us: A Sandbox for Measuring and Detecting Agentic Deception", "authors": "Satvik Golechha, Adri√É¬† Garriga-Alonso", "session_type": "SD-5-1517", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/0ff014da0c71915ceb0a84e3977c85eaf1e134dd.pdf", "relevant_to_users": "12", "read_by_users": "16", "topics": "", "key_findings": "This paper introduces Among Us, a sandbox social deduction game for LLM agents where deception emerges naturally from game objectives rather than explicit prompting. Testing 18 models across 2,054 games, researchers found that frontier reasoning models (DeepSeek R1, Claude 3.7 Sonnet) excel significantly more at producing deception than detecting it compared to non-reasoning models. Linear probes trained on 'pretend dishonest' datasets achieved 95-99% AUROCs detecting deception out-of-distribution, demonstrating strong generalization. The work introduces 'Deception Elo' as an unbounded metric for measuring strategic deception capabilities.", "description": "The paper presents Among Us, a text-based social deduction game adapted for LLM agents to study long-term, strategic deception that emerges from game mechanics rather than explicit prompting. The sandbox enables systematic measurement of deception production and detection capabilities across frontier models using a novel Elo-based rating system.", "key_contribution": "The main contributions are: (1) an open-sourced sandbox environment enabling emergent, strategic deception without explicit prompting, (2) the Deception Elo metric providing unbounded measurement of deceptive capabilities, and (3) demonstration that linear probes trained on synthetic dishonesty data generalize remarkably well (95%+ AUROC) to detect out-of-distribution deception in multi-agent gameplay.", "novelty": "Unlike prior work that uses explicit prompting or constrained scenarios, this sandbox elicits long-term strategic deception emergently from winning conditions in open-ended multi-agent interactions. Previous benchmarks saturate quickly and measure binary success/failure, while Deception Elo provides unbounded tracking of evolving capabilities. The work also reveals a previously unidentified asymmetry in frontier reasoning models: they are significantly better at producing deception than detecting it, suggesting current training methods may prioritize offensive over defensive deception capabilities.", "ai_categories": ["Agent Benchmarking and Evaluation", "Agent Safety and Security", "Multi-Agent Systems and Collaboration"]}, {"paper_id": "4262419", "score": "3", "title": "VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models", "authors": "Chongkai Gao, Zixuan Liu, Zhenghao Chi, Junshan Huang, Xin Fei, Yiwen Hou, Yuxuan Zhang, Yudi Lin, Zhirui Fang, Lin Shao", "session_type": "SD-5-2213", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/05a810d8dce16f520e115b9ee80b8096e6512276.pdf", "relevant_to_users": "4", "read_by_users": "17", "topics": "", "key_findings": "This paper provides the first systematic, controlled comparison of planning paradigms and representations in Vision-Language-Action models. Key findings: (1) visually grounded planning representations outperform language-based planning with faster inference and lower training costs; (2) Hierarchical-VLA paradigm achieves superior or comparable performance across task execution, generalization, scalability, and continual learning compared to ActionOnly and Integrated paradigms, though at the cost of slower training and inference; (3) smaller models (0.5B parameters) trained from scratch can match fine-tuned larger models, suggesting the field should focus on data quality over model scaling; (4) policy learning is consistently more challenging than task planning itself.", "description": "VLA-OS addresses the lack of fair comparisons in Vision-Language-Action research by creating a unified, modular framework that enables controlled experiments across three VLA paradigms (ActionOnly, Integrated, and Hierarchical) and three planning representations (language reasoning, visual reasoning, and image foresight). The authors conduct exhaustive combinatorial experiments across six benchmarks to isolate which specific planning components drive performance gains in robotic manipulation tasks.", "key_contribution": "The main contribution is methodological: VLA-OS provides a composable system with interchangeable planning heads and a unified VLM backbone that enables the first fair, systematic comparison of planning paradigms in VLA models. This unified framework decouples planning design choices from architectural variations, allowing researchers to identify precise sources of performance improvements that were previously obscured by confounding variables.", "novelty": "Unlike prior VLA research where models varied significantly in architecture, datasets, and training procedures, VLA-OS introduces a unified experimental framework that isolates the impact of planning representations and paradigms through controlled comparisons. This addresses the field's critical limitation of being unable to determine whether performance gains come from planning strategies, model architecture, or dataset differences. The paper also introduces novel evaluation metrics that separate task planning performance from policy learning performance, revealing that explicit planning can hurt Integrated-VLA models while implicit planning improves them.", "ai_categories": ["Vision-Language-Action Models", "Planning and Decision Making", "Agent Benchmarking and Evaluation"]}, {"paper_id": "4207701", "score": "3", "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-world Sensory Perceptions", "authors": "Bufang Yang, Lilin Xu, Liekang Zeng, Kaiwei Liu, Siyang Jiang, Wenrui Lu, Hongkai Chen, Xiaofan Jiang, Guoliang Xing, Zhenyu Yan", "session_type": "SD-6-2515", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/8c61939b607693d9b13cc1df27793d844f3648f5.pdf", "relevant_to_users": "2", "read_by_users": "3", "topics": "", "key_findings": "ContextAgent introduces the first context-aware proactive agent that leverages extensive sensory contexts from wearable devices (egocentric video, audio, smartphone notifications) to anticipate user needs without explicit instructions. The system achieves up to 8.5% higher accuracy in proactive predictions and 6.0% improvement in tool calling compared to baselines. The authors curate ContextAgentBench, the first benchmark with 1,000 samples across 9 daily scenarios and 20 tools for evaluating proactive agents. Key insights include that persona contexts (user identity, preferences, historical behaviors) are crucial for proactive capabilities, with their removal causing up to 12.3% performance drops.", "description": "ContextAgent is a context-aware proactive LLM agent framework that processes multi-modal sensory data from wearables (video, audio, notifications) to autonomously anticipate and fulfill user needs without explicit instructions. The system uses a three-stage pipeline: extracting sensory and persona contexts from wearable sensors, predicting necessity for proactive intervention, and automatically invoking appropriate tool chains when assistance is warranted.", "key_contribution": "The main innovation is enabling LLM agents to be proactive rather than reactive by incorporating extensive open-world sensory contexts from wearables, combined with a context-aware reasoner that integrates both sensory and persona contexts to autonomously predict when users need assistance and execute multi-step tool chains without explicit user prompts.", "novelty": "Unlike existing agents that are reactive (requiring explicit user instructions) or limited to enclosed environments (desktop UIs), ContextAgent operates continuously in open-world settings using hands-free wearable sensors. It addresses the limitation of rule-based proactive systems by using LLM-driven decisions grounded in rich sensory contexts. The work introduces proactive-oriented context extraction using in-context learning rather than basic zero-shot vision-language models, and demonstrates that combining sensory contexts with persona contexts (user profiles and historical behaviors) is crucial for accurate proactive predictions‚Äîa capability not explored in prior reactive agent frameworks.", "ai_categories": ["Agent Benchmarking and Evaluation", "Tool Use and Code Generation", "Memory and Context Management"]}, {"paper_id": "4208762", "score": "2", "title": "R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization", "authors": "Yuante Li, Xu Yang, Xiao Yang, Xisen Wang, Weiqing Liu, Jiang Bian", "session_type": "SD-2-111", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2505.15155", "relevant_to_users": "0", "read_by_users": "1", "topics": "", "key_findings": "Introduces R&D-Agent-Quant, a multi-agent framework that jointly optimizes data-centric factors and ML models for quantitative trading. The key innovation is coordinated agent collaboration between factor discovery and model optimization, rather than treating them as sequential tasks. Demonstrates that simultaneous optimization of data quality and model architecture outperforms isolated approaches, with specialized agents handling factor engineering, analysis, and model tuning through iterative refinement cycles.", "description": "This paper presents a multi-agent framework for quantitative trading that coordinates specialized agents to simultaneously optimize financial factor discovery and machine learning model selection. The system integrates domain knowledge from quantitative finance with LLM-based agents to improve stock prediction through joint data-centric and model-centric optimization.", "key_contribution": "A collaborative multi-agent architecture that treats factor engineering and model optimization as interconnected processes rather than sequential steps, with specialized agents working iteratively to improve both data quality and model performance in coordination.", "novelty": "Unlike traditional quantitative finance pipelines that separate feature engineering from model selection, this work implements true multi-agent collaboration where agents jointly optimize factors and models based on shared performance feedback. It addresses the limitation of treating data quality as secondary to model architecture by making factor discovery a first-class optimization target. The framework applies LLM reasoning capabilities to automated financial factor engineering, enabling agents to discover and refine predictive features using domain knowledge.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Domain-Specific Applications", "Tool Use and Code Generation"]}, {"paper_id": "4227075", "score": "2", "title": "NavBench: Probing Multimodal Large Language Models for Embodied Navigation", "authors": "Yanyuan Qiao, Haodong Hong, Wenqi Lyu, Dong An, Siqi Zhang, Yutong Xie, Xinyu Wang, Qi Wu", "session_type": "SD-5-5412", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/1ef1a313c6a3eea3eea8cfe4ac568866df673dec.pdf", "relevant_to_users": "4", "read_by_users": "12", "topics": "", "key_findings": "NavBench reveals critical temporal reasoning weaknesses in MLLMs for embodied navigation, with even the best model (GPT-4o) achieving only 53.34% comprehension and 41.33% execution accuracy. The benchmark demonstrates that most open-source models can only reliably complete easy navigation episodes, and all models fail dramatically when sub-instructions are temporally reordered. Progress estimation (tracking multi-step instruction completion) remains a consistent weakness across all models except GPT-4o. The work establishes that comprehension and execution abilities are closely related, and simulation performance correlates with real-world robot deployment.", "description": "NavBench is a comprehensive benchmark for evaluating multimodal large language models on embodied navigation under zero-shot conditions. It consists of two components: (1) navigation comprehension with 3,200 question-answer pairs across global instruction alignment, temporal progress estimation, and local observation-action reasoning tasks, and (2) step-by-step navigation execution across 432 episodes in 72 indoor scenes, stratified by spatial, cognitive, and execution complexity.", "key_contribution": "The paper introduces the first cognitively-grounded benchmark that decomposes embodied navigation evaluation into comprehension and execution components, revealing that temporal reasoning is a fundamental weakness in current MLLMs. It provides fine-grained difficulty stratification and includes a real-world robot deployment pipeline, bridging the simulation-reality gap that prior benchmarks ignored.", "novelty": "Unlike traditional VLN benchmarks (R2R, ObjectNav) that rely on task-specific supervision and only measure final success rates, NavBench evaluates generalist MLLMs in zero-shot settings with cognitively-grounded diagnostic tasks inspired by human spatial reasoning studies. It introduces orthogonal complexity dimensions (spatial, cognitive, execution) for nuanced analysis rather than treating all episodes uniformly, and uniquely includes temporal progress estimation tasks that expose a critical weakness ignored by prior spatial reasoning benchmarks like SpatialBench and ScanReason. The work moves beyond passive perception to active decision-making evaluation with real-world robot deployment validation.", "ai_categories": ["Agent Benchmarking and Evaluation", "Vision-Language-Action Models", "Spatial and Physical Reasoning"]}, {"paper_id": "4239461", "score": "2", "title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling", "authors": "Yongsen Mao, Junhao Zhong, Chuan Fang, Jia Zheng, Rui Tang, Hao Zhu, Ping Tan, Zihan Zhou", "session_type": "SD-5-1612", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/22f930c6b44c852e1c53aa7784df786168735e5d.pdf", "relevant_to_users": "19", "read_by_users": "36", "topics": "", "key_findings": "SpatialLM achieves state-of-the-art performance on layout estimation (93.5 F1 @.5 IoU on Structured3D, significantly outperforming RoomFormer's 81.4) and competitive results on 3D object detection (65.6 F1 @.25 IoU on ScanNet). The model can process point clouds from diverse sources (monocular video, RGBD, LiDAR) without specialized equipment, and generates structured 3D scene understanding including architectural elements (walls, doors, windows) and oriented object bounding boxes. It introduces the capability for user-specified object categories through LLM flexibility, and is trained on a large-scale synthetic dataset of 12,328 indoor scenes (54,778 rooms) with high-quality annotations.", "description": "SpatialLM is a 3D large language model that processes 3D point cloud data to generate structured indoor scene understanding, including architectural elements and object bounding boxes. Unlike prior methods requiring task-specific architectures, it adapts standard multimodal LLM frameworks fine-tuned from open-source models to handle spatial reasoning tasks.", "key_contribution": "The main contribution is demonstrating that standard multimodal LLM architectures can be effectively adapted for 3D spatial understanding tasks without requiring specialized network designs, achieving state-of-the-art layout estimation while accepting point clouds from diverse input sources and enabling flexible, user-specified object detection through LLM capabilities.", "novelty": "Previous 3D scene understanding methods relied on task-specific network architectures and required specialized data collection equipment. SpatialLM breaks from this by showing that standard multimodal LLM frameworks can achieve superior performance on spatial reasoning tasks, introducing input source flexibility (monocular video, RGBD, LiDAR) and user-customizable detection categories. It addresses the limitation of rigid, pre-defined object categories in prior work by leveraging LLM flexibility to support arbitrary user-specified categories, and bridges the gap between unstructured geometric data and structured 3D representations through a unified LLM-based approach.", "ai_categories": ["Vision-Language-Action Models", "Spatial and Physical Reasoning", "Domain-Specific Applications"]}, {"paper_id": "4441833", "score": "2", "title": "BraVE: Offline Reinforcement Learning for Discrete Combinatorial Action Spaces", "authors": "Matthew Landers, Taylor W. Killian, Hugo Barnes, Thomas Hartvigsen, Afsaneh Doryab", "session_type": "SD-6-306", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/fb6e5db6d0511de2cbf926cf309aa9e0fbe40245.pdf", "relevant_to_users": "2", "read_by_users": "9", "topics": "", "key_findings": "BraVE achieves up to 20√ó better performance than prior offline RL methods in environments with over 4 million discrete actions. It successfully handles non-factorizable rewards and sub-action dependencies where factorized methods catastrophically fail (e.g., maintaining -42.9 returns vs. -902.6 for FAS in high-dependency scenarios). The method evaluates actions with linear rather than exponential complexity while preserving the ability to model interdependencies between sub-actions.", "description": "This paper introduces BraVE (Branch Value Estimation), an offline RL method for high-dimensional discrete combinatorial action spaces. BraVE imposes a tree structure over the action space and uses neural network-guided traversal to efficiently select actions, evaluating only a linear number of candidates while preserving sub-action dependencies through behavior-regularized TD loss and branch value propagation.", "key_contribution": "BraVE solves the scalability-expressivity tradeoff in offline RL for combinatorial action spaces by introducing tree-structured action traversal with Q-guided greedy selection. This enables linear-time action evaluation while maintaining full joint action representations, unlike factorized methods that sacrifice expressivity or exhaustive methods that are computationally infeasible.", "novelty": "Previous methods either exhaustively evaluate exponential action spaces (computationally prohibitive) or use factorized approaches that decompose Q-functions linearly, which fails when sub-actions have interdependencies. BraVE introduces a novel tree-based traversal mechanism that preserves full joint action modeling while achieving linear complexity, using data-driven tree sparsification for conservative action selection instead of learned generative models. This is the first method to effectively handle sparse but critical sub-action dependencies that cause factorized methods to catastrophically fail.", "ai_categories": ["Planning and Decision Making", "Reinforcement Learning for LLMs"]}, {"paper_id": "4226070", "score": "1", "title": "Scaling Physical Reasoning with the PHYSICS Dataset", "authors": "Shenghe Zheng, Qianjia Cheng, Junchi Yao, Mengsong Wu, haonan he, Ning Ding, Yu Cheng, Shuyue Hu, LEI BAI, ..., Peng Ye", "session_type": "SD-1-2106", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://arxiv.org/pdf/2506.00022", "relevant_to_users": "2", "read_by_users": "6", "topics": "", "key_findings": "Introduces PHYSICS, a large-scale dataset with thousands of physics problems spanning multiple difficulty levels and subdomains. Demonstrates that current LLMs show significant performance variations across different physics reasoning tasks, and that scaling dataset size and diversity improves physical reasoning capabilities. Provides systematic evaluation framework for identifying specific weakness areas in model reasoning across introductory to advanced physics concepts.", "description": "This paper presents PHYSICS, a comprehensive benchmark dataset designed to evaluate and improve physical reasoning capabilities in large language models. The dataset contains thousands of curated physics problems across various subfields and difficulty tiers, enabling systematic analysis of how models handle scientific reasoning in physics domains.", "key_contribution": "Creates the first large-scale, systematically structured physics reasoning benchmark that provides comprehensive coverage across physics subdomains and difficulty levels, enabling granular evaluation and targeted improvement of LLM physical reasoning capabilities.", "novelty": "Unlike existing physics benchmarks that were limited in scope or scale, PHYSICS offers significantly more expansive and diverse problem coverage across physics topics with systematic difficulty progression. This addresses previous limitations by enabling researchers to identify specific reasoning weaknesses at granular levels rather than just aggregate performance, and provides a more comprehensive resource for scaling physical reasoning through diverse problem exposure.", "ai_categories": ["Reasoning and Test-Time Compute", "Spatial and Physical Reasoning"]}, {"paper_id": "4095258", "score": "1", "title": "Validating LLM-as-a-Judge Systems under Rating Indeterminacy", "authors": "Luke Guerdan, Solon Barocas, Ken Holstein, Hanna Wallach, Steven Wu, Alexandra Chouldechova", "session_type": "SD-1-1515", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/2ad527d02fe920443fe79a516d7575f417103f99.pdf", "relevant_to_users": "2", "read_by_users": "5", "topics": "", "key_findings": "The paper reveals that standard validation approaches for LLM-as-a-judge systems are heavily biased by differences in how humans and LLMs resolve rating indeterminacy in forced-choice rating tasks. Through experiments with 11 real-world tasks and 9 commercial LLMs, it shows that conventional validation methods select judges performing up to 31% worse than judges selected using multi-label 'response set' ratings that explicitly account for indeterminacy. The work demonstrates that forced-choice selection effects are more impactful than rater error, and that fully specifying rating tasks can reduce annotation requirements by 66%.", "description": "This paper introduces a comprehensive framework for validating LLM-as-a-judge systems in scenarios where rating tasks lack single correct answers due to legitimate disagreement or ambiguity. The work decomposes rating variation into rater error, genuine disagreement, and forced-choice selection effects, showing how current validation practices systematically select suboptimal judge systems.", "key_contribution": "The paper's main contribution is a probabilistic framework that explicitly models rating indeterminacy by distinguishing between forced-choice and response-set rating schemes, along with empirical evidence that validation metric choices can reverse judge system rankings, leading to deployment of significantly suboptimal systems.", "novelty": "Unlike prior work that assumes gold labels exist and treats disagreement as noise, this paper explicitly models legitimate rating variation and quantifies how forced-choice selection effects confound validation. It is the first to systematically decompose rating variation sources and demonstrate empirically how validation choices impact downstream deployment performance. The work addresses the critical gap between agreement-based validation metrics and actual task performance, showing that asymmetric forced-choice effects are a more significant confound than rater error in judge system selection.", "ai_categories": ["Agent Benchmarking and Evaluation", "Agent Safety and Security"]}, {"paper_id": "4442700", "score": "1", "title": "To Think or Not To Think: A Study of Thinking in Rule-Based Visual Reinforcement Fine-Tuning", "authors": "Ming Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, Haoquan Zhang, Wang Bill Zhu, Kaipeng Zhang", "session_type": "SD-1-4915", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/8105c1360484fcffb35e03d8f791d0b437aa1589.pdf", "relevant_to_users": "0", "read_by_users": "4", "topics": "", "key_findings": "The paper challenges the assumption that explicit chain-of-thought reasoning is universally beneficial in reinforcement fine-tuning (RFT) for multimodal LLMs. Key findings show that No-Thinking-RFT consistently outperforms or matches Thinking-RFT on visual perception tasks across model sizes, while small models (2B) generate trivial reasoning that hurts performance. The work reveals a 'free-lunch phenomenon' where RFT on one dataset improves cross-dataset generalization, and demonstrates that MLLMs can adaptively learn when to think based on their capabilities and task complexity. Computational tasks like math word problems benefit from explicit thinking in larger models (7B), but visual perception and puzzle tasks perform better without reasoning steps.", "description": "This paper investigates when explicit reasoning processes help or hurt performance during rule-based reinforcement fine-tuning of multimodal large language models across diverse visual tasks including perception, math, puzzles, grounding, and detection. The authors test models from 2B to 7B parameters and propose multiple RFT variants: No-Thinking-RFT (eliminates reasoning entirely), Think-After-Answer (places reasoning after answers), and Adaptive-Thinking (lets models learn when to think).", "key_contribution": "The main contribution is systematically demonstrating that visual perception tasks do not require explicit thinking during RFT, introducing No-Thinking-RFT which achieves faster convergence and better performance by using simple equality-based accuracy rewards that discourage verbosity. The work also introduces Think-After-Answer and Adaptive-Thinking approaches that allow models to learn task-appropriate reasoning strategies aligned with their capabilities.", "novelty": "This work is the first to question and empirically challenge the necessity of explicit reasoning in RFT for visual tasks, countering the prevailing assumption from DeepSeek-R1 that chain-of-thought is universally beneficial. Unlike prior research that only examined overthinking at inference time, this reveals thinking's negative effects during fine-tuning itself, showing that small models generate trivial reasoning and mid-sized models produce inconsistent reasoning-answer pairs. The adaptive learning approach demonstrates MLLMs can autonomously converge to optimal reasoning strategies based on task complexity and model capacity, addressing the limitation that previous approaches forced explicit reasoning across all scenarios.", "ai_categories": ["Reasoning and Test-Time Compute", "Reinforcement Learning for LLMs", "Vision-Language-Action Models"]}, {"paper_id": "4442002", "score": "1", "title": "Analogy-based Multi-Turn Jailbreak against Large Language Models", "authors": "Mengjie Wu, Yihao Huang, Zhenjun Lin, Kangjie Chen, Yuyang zhang, Yuhan Huang, Run Wang, Lina Wang", "session_type": "SD-2-1300", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/90ec84387b8d7282640d625e2d28faef32f89000.pdf", "relevant_to_users": "4", "read_by_users": "10", "topics": "", "key_findings": "The paper demonstrates that analogies distributed across multiple conversational turns can effectively bypass LLM safety filters through accumulated contextual manipulation. It shows that progressive analogy refinement significantly degrades safety detection across multiple LLM architectures, revealing vulnerabilities in sequential reasoning defenses. The work provides empirical evidence that models struggle to track harmful intent when obscured through reasoning parallels rather than explicit requests.", "description": "This paper investigates how analogies can be weaponized across multiple conversational turns to circumvent safety mechanisms in large language models. It introduces a systematic framework for multi-turn analogical attacks where harmful requests are reformulated through parallel reasoning patterns, building arguments progressively to obscure prohibited intent.", "key_contribution": "The paper presents a systematic framework for multi-turn analogical jailbreak attacks that exploits LLMs' difficulty in tracking thematic consistency across dialogue exchanges. It demonstrates how distributing harmful intent temporally through analogical reasoning is more effective than compressed single-turn prompt injection.", "novelty": "Unlike previous jailbreak approaches that rely on single-turn prompt injection, this work uniquely emphasizes temporal distribution of attacks across multiple conversational turns. It addresses the limitation that existing safety defenses focus on single-exchange attacks, revealing that safety mechanisms fail when harmful intent requires integrating knowledge across separate dialogue instances. The approach exploits models' weakness in maintaining safety vigilance across progressive analogy refinement rather than detecting explicit harmful patterns.", "ai_categories": ["Agent Safety and Security", "Reasoning and Test-Time Compute"]}, {"paper_id": "4216004", "score": "1", "title": "Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling", "authors": "Hongtao Xu, Wenting Shen, Yuanxin Wei, Ang Wang, Guo Runfan, Tianxing Wang, Yong Li, Mingzhen Li, Weile Jia", "session_type": "SD-4-2002", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/a7327b197b07cb1df3b5e408eecd14e1276acc6d.pdf", "relevant_to_users": "2", "read_by_users": "8", "topics": "", "key_findings": "Skrull introduces dynamic data scheduling to address the efficiency challenges in long-context supervised fine-tuning (SFT), achieving 3.76x average speedup (up to 7.54x) over DeepSpeed. The key insight is that heterogeneous sequence length distributions create conflicting optimization demands: long sequences require context parallelism for memory management, but this compromises efficiency for short sequences through communication overhead and GPU underutilization. Skrull's two-level scheduling approach (Distributed-Aware Context Parallelism and Global Data Scheduling) balances these trade-offs while maintaining mathematical equivalence with standard optimizers.", "description": "Skrull is a dynamic data scheduler for efficient long-context supervised fine-tuning that adaptively determines whether each sequence should be processed locally or distributed across devices. The system formulates scheduling as a joint optimization problem to balance computational requirements across heterogeneous sequence lengths while maintaining training equivalence with mainstream optimizers like Adam and AdamW.", "key_contribution": "The main contribution is a lightweight dynamic scheduling algorithm with two levels: Distributed-Aware Context Parallelism (DACP) that selectively applies context parallelism only where needed, and Global Data Scheduling (GDS) that optimizes micro-batch composition for load balancing, achieving near-zero scheduling overhead.", "novelty": "Unlike existing systems like DeepSpeed that use fixed parallelism configurations, Skrull treats long-context training efficiency as a data scheduling problem rather than a system limitation. Previous approaches either broke training equivalence (like LongAlign's sorted batching) or suffered from uniform parallelism overhead, while Skrull dynamically adapts distribution per-sequence to maintain both long-sequence capability and short-sequence efficiency without compromising mathematical equivalence with standard optimizers.", "ai_categories": ["Model Efficiency and Optimization", "Memory and Context Management"]}, {"paper_id": "4210637", "score": "1", "title": "BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization", "authors": "Xueyang Zhou, Guiyao Tie, Guowen Zhang, Hecheng Wang, Pan Zhou, Lichao Sun", "session_type": "SD-4-2408", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/9ef98e483d9e324778e7116170aa4848fc6f67e3.pdf", "relevant_to_users": "1", "read_by_users": "8", "topics": "", "key_findings": "BadVLA demonstrates the first systematic backdoor attack on Vision-Language-Action models, achieving near-100% attack success rates across multiple benchmarks while maintaining clean task accuracy. The attack proves robust against input perturbations, task transfers, and model fine-tuning, exposing critical security vulnerabilities in embodied AI systems. This reveals urgent deployment risks for real-world robotic systems relying on multimodal learning architectures under Training-as-a-Service paradigms.", "description": "This paper investigates backdoor vulnerabilities in Vision-Language-Action (VLA) models used for robotic control. BadVLA introduces a backdoor attack method based on Objective-Decoupled Optimization that exposes how tightly coupled multimodal architectures can be exploited to enable malicious control deviations while preserving benign task performance.", "key_contribution": "The first systematic backdoor attack framework for VLA models using a two-stage Objective-Decoupled Optimization approach: (1) explicit feature-space separation to isolate trigger representations from benign inputs, and (2) conditional control deviations that activate only with triggers present.", "novelty": "Unlike previous work on traditional adversarial perturbations, BadVLA addresses persistent backdoor threats specifically within VLA architectures, filling a critical gap in security research for embodied AI. The Objective-Decoupled Optimization approach uniquely tackles the challenge of maintaining feature-space separation between triggered and clean inputs while preserving task performance, addressing the urgent need for secure embodied model design under Training-as-a-Service paradigms where model training may be outsourced.", "ai_categories": ["Agent Safety and Security", "Vision-Language-Action Models"]}, {"paper_id": "4221890", "score": "1", "title": "GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning", "authors": "Jusheng Zhang, Yijia Fan, Wenjun Lin, Ruiqi Chen, Haoyi Jiang, Wenhao Chai, Jian Wang, Keze Wang", "session_type": "SD-4-5314", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/55c4606303cc86c81ca99b5d5743cfe40d7fb140.pdf", "relevant_to_users": "0", "read_by_users": "5", "topics": "", "key_findings": "GAM-Agent introduces a game-theoretic multi-agent framework that significantly improves vision-language reasoning across multiple benchmarks. It achieves 5-6% accuracy improvements on small-to-mid scale models (Qwen2.5-VL-7B, InternVL3-14B) and 2-3% gains on GPT-4o. The framework's uncertainty-aware collaboration mechanism dynamically triggers multi-round debates when disagreement or ambiguity is detected, enabling specialized agents to verify logical consistency and factual correctness through structured communication.", "description": "This paper presents GAM-Agent, a multi-agent framework that formulates visual reasoning as a non-zero-sum game between specialized base agents handling visual perception subtasks and a critical agent verifying logic and factual correctness. Agents communicate through structured claims, evidence, and uncertainty estimates, with dynamic collaboration intensity adjusted by an uncertainty-aware controller.", "key_contribution": "The main innovation is the game-theoretic formulation of multi-agent vision-language reasoning combined with an uncertainty-aware controller that dynamically modulates collaboration intensity. Unlike fixed multi-agent approaches, this system quantifies uncertainty in agent outputs and uses these metrics to trigger collaborative debate mechanisms only when needed.", "novelty": "Previous vision-language systems relied on monolithic single-agent models or static multi-agent collaboration patterns. GAM-Agent addresses these limitations by introducing non-zero-sum game dynamics that encourage cooperation, explicitly modeling uncertainty to trigger adaptive collaboration, and using structured inter-agent verification to catch logical errors. The uncertainty-aware controller represents a departure from fixed collaboration patterns, enabling the system to recognize when ambiguity requires resolution through debate rather than applying uniform collaboration across all inputs.", "ai_categories": ["Multi-Agent Systems and Collaboration", "Vision-Language-Action Models", "Reasoning and Test-Time Compute"]}, {"paper_id": "4098154", "score": "1", "title": "TS-RAG: Retrieval-Augmented Generation based Time Series Foundation Models are Stronger Zero-Shot Forecaster", "authors": "Kanghui Ning, Zijie Pan, Yu Liu, Yushan Jiang, James Y. Zhang, Kashif Rasul, Anderson Schneider, Lintao Ma, Yuriy Nevmyvaka, Dongjin Song", "session_type": "SD-5-2308", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/e686d5b872f3c42cd96442359b22e23319fe0acb.pdf", "relevant_to_users": "2", "read_by_users": "14", "topics": "", "key_findings": "TS-RAG achieves state-of-the-art zero-shot time series forecasting by combining retrieval-augmented generation with time series foundation models, improving performance by up to 6.84% over existing TSFMs without requiring task-specific fine-tuning. The framework uses a 2.8 million pattern knowledge base and an Adaptive Retrieval Mixer (ARM) module with multi-head attention and gating networks to dynamically fuse retrieved patterns with the model's internal representations. Tested across seven benchmarks, it demonstrates consistent improvements while providing enhanced interpretability through explicit pattern matching.", "description": "TS-RAG addresses the limitations of existing Time Series Foundation Models (TSFMs) in zero-shot forecasting by introducing a retrieval-augmented generation framework. The system retrieves semantically relevant time series segments from a knowledge base and dynamically integrates them with the TSFM's predictions using an Adaptive Retrieval Mixer module, enabling better generalization across diverse domains without fine-tuning.", "key_contribution": "The main innovation is the Adaptive Retrieval Mixer (ARM) module that dynamically fuses retrieved historical patterns with frozen TSFM representations using multi-head attention and adaptive gating networks. This enables zero-shot forecasting improvements while maintaining the foundation model's generalization capabilities and providing interpretability through explicit pattern matching.", "novelty": "Unlike previous TSFMs (TimesFM, Moirai, Chronos) that lack inherent mechanisms for domain adaptation, TS-RAG introduces retrieval-augmented generation specifically for time series, allowing models to dynamically leverage external contextual knowledge without fine-tuning. The work addresses the limitation of handling non-stationary dynamics and distribution shifts by retrieving and adaptively integrating similar historical patterns from a 2.8 million pattern knowledge base. The architecture keeps the backbone TSFM frozen while only training the fusion module, representing a new paradigm for building adaptable yet generalizable time series systems.", "ai_categories": ["Domain-Specific Applications", "Model Efficiency and Optimization"]}, {"paper_id": "4341713", "score": "1", "title": "Bridging Human and LLM Judgments: Understanding and Narrowing the Gap", "authors": "Felipe Maia Polo, Xinhe Wang, Mikhail Yurochkin, Gongjun Xu, Moulinath Banerjee, Yuekai Sun", "session_type": "SD-5-1515", "session_time": "", "session_location": "Exhibit Hall C,D,E", "pdf_url": "https://openreview.net/pdf/3b238b3324d5ab3f3e5a672a12a2c7610ee13a48.pdf", "relevant_to_users": "7", "read_by_users": "14", "topics": "", "key_findings": "The paper introduces Bridge, a unified statistical framework that explicitly models and corrects systematic discrepancies between human and LLM judgments in evaluation tasks. Using the Bradley-Terry model, Bridge achieves substantially higher agreement with human ratings (in accuracy, calibration, and KL divergence) compared to baseline LLM judges, while requiring fewer human annotations than traditional fine-tuning. Evaluated on BigGen Bench and Chatbot Arena with six LLM judges, Bridge systematically exposes and reduces LLM biases (such as favoring longer responses) that diverge from human preferences.", "description": "This paper addresses the systematic gap between human and LLM evaluator judgments in LLM-as-a-judge scenarios. The authors present Bridge, a statistical framework grounded in the Bradley-Terry model that estimates latent quality scores explaining both human and LLM rankings, then applies calibration to align LLM judgments with human preferences without extensive human annotation.", "key_contribution": "Bridge provides a principled statistical approach to quantifying and correcting systematic biases in LLM evaluations through a unified framework that works for both absolute scoring and pairwise comparisons. It enables more reliable automatic evaluation with minimal human feedback while maintaining computational efficiency.", "novelty": "Unlike previous work that either uses LLMs as direct judges or ignores human-LLM discrepancies, Bridge explicitly models the gap between human and LLM judgments as a statistical phenomenon that can be systematically corrected. Rather than treating LLM bias as a limitation to work around, it provides a formal framework to quantify and calibrate for these biases, achieving better alignment with human preferences than simple prompting or reranking strategies while requiring fewer annotations than fine-tuning approaches.", "ai_categories": ["Agent Benchmarking and Evaluation", "Self-Improvement and Meta-Learning"]}];

        // Embedded author data
        const authors = [{"name": "Yaxin Luo", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 95.0, "max_score": 95, "total_relevant": 5, "total_reads": 13, "papers": [{"title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents", "score": 95, "session": "SD-4-2513", "pdf_url": "https://arxiv.org/pdf/2505.24878", "relevant": 5, "reads": 13}], "affiliation": "MBZUAI", "role": "PhD Student", "photo_url": "https://yaxin9luo.github.io/images/Yaxin.JPG", "profile_url": "https://yaxin9luo.github.io"}, {"name": "Zhaoyi Li", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 95.0, "max_score": 95, "total_relevant": 5, "total_reads": 13, "papers": [{"title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents", "score": 95, "session": "SD-4-2513", "pdf_url": "https://arxiv.org/pdf/2505.24878", "relevant": 5, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhiqiang Shen", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 95.0, "max_score": 95, "total_relevant": 5, "total_reads": 13, "papers": [{"title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents", "score": 95, "session": "SD-4-2513", "pdf_url": "https://arxiv.org/pdf/2505.24878", "relevant": 5, "reads": 13}], "affiliation": "Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)", "role": "Assistant Professor", "photo_url": "https://zhiqiangshen.com/imgs/me2.JPG", "profile_url": "https://zhiqiangshen.com/"}, {"name": "Div Garg", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 93.0, "max_score": 93, "total_relevant": 5, "total_reads": 11, "papers": [{"title": "REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites", "score": 93, "session": "SD-2-3602", "pdf_url": "https://arxiv.org/pdf/2504.11543", "relevant": 5, "reads": 11}], "affiliation": "AGI, Inc.", "role": "CEO", "photo_url": "https://media.licdn.com/dms/image/v2/D5603AQE_BPnukUYk7g/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1731214355451?e=2147483647&v=beta&t=-VkFwC_oOp43AoE4BBu5K1_V3R2fJ04nLVrBCeEpasg", "profile_url": "https://divyanshgarg.com/"}, {"name": "Diego Caples", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 93.0, "max_score": 93, "total_relevant": 5, "total_reads": 11, "papers": [{"title": "REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites", "score": 93, "session": "SD-2-3602", "pdf_url": "https://arxiv.org/pdf/2504.11543", "relevant": 5, "reads": 11}], "affiliation": "AGI, Inc.", "role": "Head of Research", "photo_url": "https://avatars.githubusercontent.com/u/20152429?s=64&v=4", "profile_url": "https://github.com/dCaples"}, {"name": "Sumeet Motwani", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 93.0, "max_score": 93, "total_relevant": 5, "total_reads": 11, "papers": [{"title": "REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites", "score": 93, "session": "SD-2-3602", "pdf_url": "https://arxiv.org/pdf/2504.11543", "relevant": 5, "reads": 11}], "affiliation": "University of Oxford", "role": "PhD Student", "photo_url": "https://sumeetmotwani.com/profile.jpg", "profile_url": "https://sumeetmotwani.com/"}, {"name": "Dheeraj Vattikonda", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.0, "max_score": 92, "total_relevant": 4, "total_reads": 14, "papers": [{"title": "How to Train Your LLM Web Agent: A Statistical Diagnosis", "score": 92, "session": "SD-1-4109", "pdf_url": "https://openreview.net/pdf/0252f1f5697c0153740a7438e473e45964e85102.pdf", "relevant": 4, "reads": 14}], "affiliation": "McGill University, Mila", "role": "Research Master's Student / Visiting Researcher at ServiceNow Research", "photo_url": "https://www.servicenow.com/research/author/dheeraj-vattikonda/avatar_hu51fec77321170e824e07aa6be30217a3_19867_270x270_fill_q75_lanczos_center.jpg", "profile_url": "https://www.servicenow.com/research/author/dheeraj-vattikonda.html"}, {"name": "Santhoshi Ravichandran", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.0, "max_score": 92, "total_relevant": 4, "total_reads": 14, "papers": [{"title": "How to Train Your LLM Web Agent: A Statistical Diagnosis", "score": 92, "session": "SD-1-4109", "pdf_url": "https://openreview.net/pdf/0252f1f5697c0153740a7438e473e45964e85102.pdf", "relevant": 4, "reads": 14}], "affiliation": "Mila - Quebec AI Institute (University of Montreal)", "role": "MSc Graduate / Alumni", "photo_url": "https://mila.quebec/sites/default/files/styles/member_full/public/member/10239/portrait-of-santhoshi-ravichandran.jpeg.webp?itok=m8BfF3DB", "profile_url": "https://scholar.google.com/citations?user=RHcgZP8AAAAJ&hl=en"}, {"name": "Massimo Caccia", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.0, "max_score": 92, "total_relevant": 4, "total_reads": 14, "papers": [{"title": "How to Train Your LLM Web Agent: A Statistical Diagnosis", "score": 92, "session": "SD-1-4109", "pdf_url": "https://openreview.net/pdf/0252f1f5697c0153740a7438e473e45964e85102.pdf", "relevant": 4, "reads": 14}], "affiliation": "ServiceNow Research", "role": "Senior Research Scientist", "photo_url": "https://optimass.github.io/images/massimo.jpg", "profile_url": "https://optimass.github.io/"}, {"name": "Frank (Fangzheng) Xu", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.0, "max_score": 92, "total_relevant": 15, "total_reads": 24, "papers": [{"title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks", "score": 92, "session": "SD-2-402", "pdf_url": "https://arxiv.org/pdf/2412.14161", "relevant": 15, "reads": 24}], "affiliation": "Microsoft AI", "role": "Member of Technical Staff", "photo_url": "https://frankxfz.me/me.jpg", "profile_url": "https://frankxfz.me/"}, {"name": "Yufan Song", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.0, "max_score": 92, "total_relevant": 15, "total_reads": 24, "papers": [{"title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks", "score": 92, "session": "SD-2-402", "pdf_url": "https://arxiv.org/pdf/2412.14161", "relevant": 15, "reads": 24}], "affiliation": "ByteDance", "role": "Software Development Engineer", "photo_url": "https://avatars.githubusercontent.com/u/33971064?v=4", "profile_url": "https://scholar.google.com/citations?user=cpZgsAUAAAAJ&hl=en"}, {"name": "Graham Neubig", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.0, "max_score": 92, "total_relevant": 15, "total_reads": 24, "papers": [{"title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks", "score": 92, "session": "SD-2-402", "pdf_url": "https://arxiv.org/pdf/2412.14161", "relevant": 15, "reads": 24}], "affiliation": "Carnegie Mellon University", "role": "Associate Professor", "photo_url": "https://www.phontron.com/images/neubig-headshot-2021.jpg", "profile_url": "https://www.phontron.com/"}, {"name": "Boyu Gou", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.0, "max_score": 92, "total_relevant": 9, "total_reads": 13, "papers": [{"title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge", "score": 92, "session": "SD-3-1915", "pdf_url": "https://arxiv.org/pdf/2506.21506", "relevant": 9, "reads": 13}], "affiliation": "The Ohio State University", "role": "PhD Student", "photo_url": "https://boyugou.github.io/assets/img/avator.jpg", "profile_url": "https://boyugou.github.io/"}, {"name": "Zanming Huang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.0, "max_score": 92, "total_relevant": 9, "total_reads": 13, "papers": [{"title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge", "score": 92, "session": "SD-3-1915", "pdf_url": "https://arxiv.org/pdf/2506.21506", "relevant": 9, "reads": 13}], "affiliation": "The Ohio State University", "role": "PhD Student", "photo_url": "https://tzmhuang.github.io/images/profile_pic.jpg", "profile_url": "https://tzmhuang.github.io"}, {"name": "Yu Su", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.0, "max_score": 92, "total_relevant": 9, "total_reads": 13, "papers": [{"title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge", "score": 92, "session": "SD-3-1915", "pdf_url": "https://arxiv.org/pdf/2506.21506", "relevant": 9, "reads": 13}], "affiliation": "The Ohio State University", "role": "Associate Professor", "photo_url": "https://ysu1989.github.io/images/YuSu.jpeg", "profile_url": "https://ysu1989.github.io/"}, {"name": "Yunjia Qi", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.0, "max_score": 92, "total_relevant": 2, "total_reads": 2, "papers": [{"title": "AGENTIF: Benchmarking Large Language Models Instruction Following Ability in Agentic Scenarios", "score": 92, "session": "SD-6-114", "pdf_url": "", "relevant": 2, "reads": 2}], "affiliation": "Tsinghua University", "role": "PhD Student", "photo_url": "https://cdn-avatars.huggingface.co/v1/production/uploads/648c48d8c0ddeee6df5b6d22/BlrYDv3eQxZ-Y5vtVGegX.jpeg", "profile_url": "https://scholar.google.com/citations?user=Xxiwr8YAAAAJ&hl=en"}, {"name": "Hao Peng", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.0, "max_score": 92, "total_relevant": 2, "total_reads": 2, "papers": [{"title": "AGENTIF: Benchmarking Large Language Models Instruction Following Ability in Agentic Scenarios", "score": 92, "session": "SD-6-114", "pdf_url": "", "relevant": 2, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Juanzi Li", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.0, "max_score": 92, "total_relevant": 2, "total_reads": 2, "papers": [{"title": "AGENTIF: Benchmarking Large Language Models Instruction Following Ability in Agentic Scenarios", "score": 92, "session": "SD-6-114", "pdf_url": "", "relevant": 2, "reads": 2}], "affiliation": "Tsinghua University", "role": "Professor", "photo_url": null, "profile_url": "https://keg.cs.tsinghua.edu.cn/persons/ljz/"}, {"name": "Yang JingYi", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 91.0, "max_score": 91, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "RiOSWorld: Benchmarking the Risk of Multimodal Computer-Use Agents", "score": 91, "session": "SD-1-1201", "pdf_url": "https://openreview.net/pdf/11ac207fff4f7439cb345f2e79ef2a6fb5611910.pdf", "relevant": 3, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shuai Shao", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 91.0, "max_score": 91, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "RiOSWorld: Benchmarking the Risk of Multimodal Computer-Use Agents", "score": 91, "session": "SD-1-1201", "pdf_url": "https://openreview.net/pdf/11ac207fff4f7439cb345f2e79ef2a6fb5611910.pdf", "relevant": 3, "reads": 6}], "affiliation": "Tencent Hunyuan", "role": "Principal Researcher", "photo_url": "https://www.sshao.com/images/9K7A4275.JPG", "profile_url": "https://www.sshao.com/"}, {"name": "Jing Shao", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 91.0, "max_score": 91, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "RiOSWorld: Benchmarking the Risk of Multimodal Computer-Use Agents", "score": 91, "session": "SD-1-1201", "pdf_url": "https://openreview.net/pdf/11ac207fff4f7439cb345f2e79ef2a6fb5611910.pdf", "relevant": 3, "reads": 6}], "affiliation": "Shanghai Artificial Intelligence Laboratory", "role": "Research Scientist", "photo_url": "https://amandajshao.github.io/assets/img/Profile.JPG", "profile_url": "https://amandajshao.github.io/"}, {"name": "Ivan Evtimov", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 91.0, "max_score": 91, "total_relevant": 6, "total_reads": 13, "papers": [{"title": "WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks", "score": 91, "session": "SD-2-1311", "pdf_url": "https://arxiv.org/pdf/2504.18575", "relevant": 6, "reads": 13}], "affiliation": "Meta FAIR", "role": "Research Scientist", "photo_url": "https://static.licdn.com/aero-v1/sc/h/bgaqk7x4ntjz0wg67d8u723eb", "profile_url": "https://ivanevtimov.eu/"}, {"name": "Arman Zharmagambetov", "paper_count": 2, "highly_relevant_count": 1, "avg_score": 84.0, "max_score": 91, "total_relevant": 10, "total_reads": 18, "papers": [{"title": "WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks", "score": 91, "session": "SD-2-1311", "pdf_url": "https://arxiv.org/pdf/2504.18575", "relevant": 6, "reads": 13}, {"title": "AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents", "score": 77, "session": "SD-4-1304", "pdf_url": "https://arxiv.org/pdf/2503.09780", "relevant": 4, "reads": 5}], "affiliation": "Meta AI (FAIR)", "role": "Research Scientist", "photo_url": "https://scontent-lax3-2.xx.fbcdn.net/v/t39.35477-6/431488050_709303461277781_9187849216738221759_n.jpg?_nc_cat=106&ccb=1-7&_nc_sid=e280be&_nc_ohc=9O8mW2h2XEMQ7kNvwH4Iuen&_nc_oc=AdkjcmA5QJHQycw1Bo2u90i60028p1fS6WKS1CRV4fMKcy5m5ARXXV70tDP4HU-P2_8&_nc_zt=14&_nc_ht=scontent-lax3-2.xx&_nc_gid=8UBjkn2aOeGz8JKYzEqhng&oh=00_Afnzwe3yQ4FycZrlnrOueM-cik0ZEo_AzaDR4wGsyhmD8g&oe=693AE100", "profile_url": "https://arman-z.github.io/"}, {"name": "Kamalika Chaudhuri", "paper_count": 2, "highly_relevant_count": 1, "avg_score": 84.0, "max_score": 91, "total_relevant": 10, "total_reads": 18, "papers": [{"title": "WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks", "score": 91, "session": "SD-2-1311", "pdf_url": "https://arxiv.org/pdf/2504.18575", "relevant": 6, "reads": 13}, {"title": "AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents", "score": 77, "session": "SD-4-1304", "pdf_url": "https://arxiv.org/pdf/2503.09780", "relevant": 4, "reads": 5}], "affiliation": "Meta (FAIR)", "role": "Director, Research Scientist", "photo_url": "https://jacobsschool.ucsd.edu/sites/default/files/groups/jsoe/img/people/faculty-profiles/322.jpg", "profile_url": "https://cseweb.ucsd.edu/~kamalika/"}, {"name": "Amartya Chakraborty", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 90.0, "max_score": 90, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning", "score": 90, "session": "SD-4-1912", "pdf_url": "https://arxiv.org/pdf/2505.16986", "relevant": 2, "reads": 6}], "affiliation": "Capital One", "role": "Senior Machine Learning Engineer", "photo_url": "https://static.licdn.com/aero-v1/sc/h/bgaqk7x4ntjz0wg67d8u723eb", "profile_url": "https://scholar.google.com/citations?user=gLOUVmEAAAAJ&hl=en"}, {"name": "Paresh Dashore", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 90.0, "max_score": 90, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning", "score": 90, "session": "SD-4-1912", "pdf_url": "https://arxiv.org/pdf/2505.16986", "relevant": 2, "reads": 6}], "affiliation": "Capital One", "role": "Applied Scientist", "photo_url": "https://media.licdn.com/dms/image/v2/C4E03AQG5pIW9vlWgtw/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1626896577434?e=2147483647&v=beta&t=Lhi30_KNnj6iEgpZZD7T99EUd4YpmGEFbkYI6qHSWsg", "profile_url": "https://www.linkedin.com/in/paresh-dashore-3b7111b0"}, {"name": "Genta Winata", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 90.0, "max_score": 90, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning", "score": 90, "session": "SD-4-1912", "pdf_url": "https://arxiv.org/pdf/2505.16986", "relevant": 2, "reads": 6}], "affiliation": "Capital One AI Foundations", "role": "Senior Applied Scientist", "photo_url": "https://gentawinata.com/assets/img/profile.jpg", "profile_url": "https://gentawinata.com/"}, {"name": "Xiaojun Guo", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 89.0, "max_score": 89, "total_relevant": 4, "total_reads": 12, "papers": [{"title": "$\\texttt{G1}$: Teaching LLMs to Reason on Graphs with Reinforcement Learning", "score": 89, "session": "SD-2-2800", "pdf_url": "https://openreview.net/pdf/eb95dfb11f1b0c705f2e368d650f0af1afbef02b.pdf", "relevant": 4, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ang Li", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 89.0, "max_score": 89, "total_relevant": 4, "total_reads": 12, "papers": [{"title": "$\\texttt{G1}$: Teaching LLMs to Reason on Graphs with Reinforcement Learning", "score": 89, "session": "SD-2-2800", "pdf_url": "https://openreview.net/pdf/eb95dfb11f1b0c705f2e368d650f0af1afbef02b.pdf", "relevant": 4, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yisen Wang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 89.0, "max_score": 89, "total_relevant": 4, "total_reads": 12, "papers": [{"title": "$\\texttt{G1}$: Teaching LLMs to Reason on Graphs with Reinforcement Learning", "score": 89, "session": "SD-2-2800", "pdf_url": "https://openreview.net/pdf/eb95dfb11f1b0c705f2e368d650f0af1afbef02b.pdf", "relevant": 4, "reads": 12}], "affiliation": "Peking University", "role": "Assistant Professor", "photo_url": "https://yisenwang.github.io/talk-photo-2.jpg", "profile_url": "https://yisenwang.github.io/"}, {"name": "Yifei Zhou", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 89.0, "max_score": 89, "total_relevant": 11, "total_reads": 20, "papers": [{"title": "Self-Challenging Language Model Agents", "score": 89, "session": "SD-2-3407", "pdf_url": "https://openreview.net/pdf/d7856a8a4e991dab71814a3f39f28fe17b6b93bd.pdf", "relevant": 11, "reads": 20}], "affiliation": "xAI", "role": "Member of Technical Staff", "photo_url": "https://yifeizhou02.github.io/images/profile.png", "profile_url": "https://yifeizhou02.github.io/"}, {"name": "Sergey Levine", "paper_count": 2, "highly_relevant_count": 1, "avg_score": 81.5, "max_score": 89, "total_relevant": 18, "total_reads": 42, "papers": [{"title": "Self-Challenging Language Model Agents", "score": 89, "session": "SD-2-3407", "pdf_url": "https://openreview.net/pdf/d7856a8a4e991dab71814a3f39f28fe17b6b93bd.pdf", "relevant": 11, "reads": 20}, {"title": "Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL", "score": 74, "session": "SD-3-5419", "pdf_url": "https://openreview.net/pdf/a27fd29bd9a99f5624d5362501cf5a6acc4c4e37.pdf", "relevant": 7, "reads": 22}], "affiliation": "UC Berkeley", "role": "Associate Professor", "photo_url": "https://people.eecs.berkeley.edu/~svlevine/images/portrait_lab_small.png", "profile_url": "https://people.eecs.berkeley.edu/~svlevine/"}, {"name": "Sainbayar Sukhbaatar", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 89.0, "max_score": 89, "total_relevant": 11, "total_reads": 20, "papers": [{"title": "Self-Challenging Language Model Agents", "score": 89, "session": "SD-2-3407", "pdf_url": "https://openreview.net/pdf/d7856a8a4e991dab71814a3f39f28fe17b6b93bd.pdf", "relevant": 11, "reads": 20}], "affiliation": "Meta AI (FAIR)", "role": "Research Scientist", "photo_url": "https://tesatory.github.io/me.png", "profile_url": "https://tesatory.github.io/"}, {"name": "Yu Shang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 89.0, "max_score": 89, "total_relevant": 3, "total_reads": 8, "papers": [{"title": "AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems", "score": 89, "session": "SD-3-2509", "pdf_url": "https://arxiv.org/pdf/2505.19623", "relevant": 3, "reads": 8}], "affiliation": "Tsinghua University", "role": "PhD Candidate", "photo_url": "https://shangy21.github.io/photo.jpg", "profile_url": "https://shangy21.github.io/"}, {"name": "Peijie Liu", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 89.0, "max_score": 89, "total_relevant": 3, "total_reads": 8, "papers": [{"title": "AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems", "score": 89, "session": "SD-3-2509", "pdf_url": "https://arxiv.org/pdf/2505.19623", "relevant": 3, "reads": 8}], "affiliation": "Tsinghua University", "role": "Master Student", "photo_url": null, "profile_url": "https://scholar.google.com/citations?user=NKWRhFoAAAAJ&hl=en"}, {"name": "Yong Li", "paper_count": 2, "highly_relevant_count": 1, "avg_score": 84.0, "max_score": 89, "total_relevant": 3, "total_reads": 26, "papers": [{"title": "AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems", "score": 89, "session": "SD-3-2509", "pdf_url": "https://arxiv.org/pdf/2505.19623", "relevant": 3, "reads": 8}, {"title": "LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models", "score": 79, "session": "SD-3-309", "pdf_url": "https://openreview.net/pdf/64ba671c860d8032b21824e28d00916c3e613964.pdf", "relevant": 0, "reads": 18}], "affiliation": "Tsinghua University", "role": "Professor", "photo_url": "https://fi.ee.tsinghua.edu.cn/img/people/liyong1.jpg", "profile_url": "https://fi.ee.tsinghua.edu.cn/~liyong/"}, {"name": "Fali Wang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 89.0, "max_score": 89, "total_relevant": 3, "total_reads": 12, "papers": [{"title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks", "score": 89, "session": "SD-4-3405", "pdf_url": "https://openreview.net/pdf/13ea5abf7135c2269571122e956e24009565de1f.pdf", "relevant": 3, "reads": 12}], "affiliation": "The Pennsylvania State University", "role": "PhD Student", "photo_url": "https://fairyfali.github.io/images/profile.jpg", "profile_url": "https://fairyfali.github.io"}, {"name": "Hui Liu", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 89.0, "max_score": 89, "total_relevant": 3, "total_reads": 12, "papers": [{"title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks", "score": 89, "session": "SD-4-3405", "pdf_url": "https://openreview.net/pdf/13ea5abf7135c2269571122e956e24009565de1f.pdf", "relevant": 3, "reads": 12}], "affiliation": "Amazon Ads", "role": "Senior Applied Scientist", "photo_url": "https://layneins.github.io/photo/img4.jpg", "profile_url": "https://layneins.github.io/"}, {"name": "Suhang Wang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 89.0, "max_score": 89, "total_relevant": 3, "total_reads": 12, "papers": [{"title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks", "score": 89, "session": "SD-4-3405", "pdf_url": "https://openreview.net/pdf/13ea5abf7135c2269571122e956e24009565de1f.pdf", "relevant": 3, "reads": 12}], "affiliation": "Pennsylvania State University", "role": "Associate Professor", "photo_url": "https://ist.psu.edu/assets/uploads/directoryImages/_480x480_crop_center-center_80_none/Wang-Suhang.jpg", "profile_url": "https://suhangwang.ist.psu.edu/"}, {"name": "Dongjie Yang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 88.0, "max_score": 88, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "Wide-Horizon Thinking and Simulation-Based Evaluation for Real-World LLM Planning with Multifaceted Constraints", "score": 88, "session": "SD-4-5407", "pdf_url": "https://openreview.net/pdf/4fe14c361104913e172e118fa05d358d3b030840.pdf", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chengqiang Lu", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 88.0, "max_score": 88, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "Wide-Horizon Thinking and Simulation-Based Evaluation for Real-World LLM Planning with Multifaceted Constraints", "score": 88, "session": "SD-4-5407", "pdf_url": "https://openreview.net/pdf/4fe14c361104913e172e118fa05d358d3b030840.pdf", "relevant": 1, "reads": 4}], "affiliation": "Xiaohongshu", "role": "Head of AI Search Generation Algorithms", "photo_url": "http://conference.boolan.com/FrbG3Sgs3Pjai0ZmU8Ng5SnPgOmc", "profile_url": "https://scholar.google.com/citations?user=FGMCzzQAAAAJ"}, {"name": "hai zhao", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 88.0, "max_score": 88, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "Wide-Horizon Thinking and Simulation-Based Evaluation for Real-World LLM Planning with Multifaceted Constraints", "score": 88, "session": "SD-4-5407", "pdf_url": "https://openreview.net/pdf/4fe14c361104913e172e118fa05d358d3b030840.pdf", "relevant": 1, "reads": 4}], "affiliation": "Shanghai Jiao Tong University", "role": "Professor", "photo_url": "https://bcmi.sjtu.edu.cn/~zhaohai/wallpics/DSC_2155small.png", "profile_url": "https://bcmi.sjtu.edu.cn/~zhaohai/"}, {"name": "Xueguang Ma", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 87.0, "max_score": 87, "total_relevant": 8, "total_reads": 25, "papers": [{"title": "General-Reasoner: Advancing LLM Reasoning Across All Domains", "score": 87, "session": "SD-2-308", "pdf_url": "https://openreview.net/pdf/59ca2ebad16c9f4c1d622f0bcd9007d31e1874bc.pdf", "relevant": 8, "reads": 25}], "affiliation": "University of Waterloo", "role": "PhD Student", "photo_url": "https://mxueguang.github.io/images/profile.jpg", "profile_url": "https://mxueguang.github.io/"}, {"name": "Qian Liu", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 87.0, "max_score": 87, "total_relevant": 8, "total_reads": 25, "papers": [{"title": "General-Reasoner: Advancing LLM Reasoning Across All Domains", "score": 87, "session": "SD-2-308", "pdf_url": "https://openreview.net/pdf/59ca2ebad16c9f4c1d622f0bcd9007d31e1874bc.pdf", "relevant": 8, "reads": 25}], "affiliation": "TikTok AI Innovation Center, Singapore", "role": "Researcher, Head of Code Research Team", "photo_url": "https://siviltaram.github.io/images/profile.png", "profile_url": "https://siviltaram.github.io/"}, {"name": "Wenhu Chen", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 87.0, "max_score": 87, "total_relevant": 8, "total_reads": 25, "papers": [{"title": "General-Reasoner: Advancing LLM Reasoning Across All Domains", "score": 87, "session": "SD-2-308", "pdf_url": "https://openreview.net/pdf/59ca2ebad16c9f4c1d622f0bcd9007d31e1874bc.pdf", "relevant": 8, "reads": 25}], "affiliation": "University of Waterloo", "role": "Assistant Professor", "photo_url": "https://wenhuchen.github.io/images/headshot.jpg", "profile_url": "https://wenhuchen.github.io/"}, {"name": "Yunxiang Zhang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 87.0, "max_score": 87, "total_relevant": 3, "total_reads": 8, "papers": [{"title": "MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?", "score": 87, "session": "SD-3-1910", "pdf_url": "https://arxiv.org/pdf/2504.09702", "relevant": 3, "reads": 8}], "affiliation": "University of Michigan", "role": "PhD Student", "photo_url": "https://yunx-z.github.io/images/profile.png", "profile_url": "https://yunx-z.github.io/"}, {"name": "Muhammad Khalifa", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 87.0, "max_score": 87, "total_relevant": 3, "total_reads": 8, "papers": [{"title": "MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?", "score": 87, "session": "SD-3-1910", "pdf_url": "https://arxiv.org/pdf/2504.09702", "relevant": 3, "reads": 8}], "affiliation": "University of Michigan", "role": "PhD Candidate", "photo_url": "https://i.ibb.co/xtbdfkDx/headshot-circle-2.png", "profile_url": "https://mukhal.github.io/"}, {"name": "Lu Wang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 87.0, "max_score": 87, "total_relevant": 3, "total_reads": 8, "papers": [{"title": "MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?", "score": 87, "session": "SD-3-1910", "pdf_url": "https://arxiv.org/pdf/2504.09702", "relevant": 3, "reads": 8}], "affiliation": "University of Michigan", "role": "Associate Professor", "photo_url": "https://web.eecs.umich.edu/~wangluxy/img/personal/personal-big.jpg", "profile_url": "https://web.eecs.umich.edu/~wangluxy/"}, {"name": "Jiabin Tang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.0, "max_score": 86, "total_relevant": 12, "total_reads": 18, "papers": [{"title": "AI-Researcher: Autonomous Scientific Innovation", "score": 86, "session": "SD-3-5417", "pdf_url": "https://openreview.net/pdf/a1c63cdd0495de94664b1513f7d95a3aedcb483a.pdf", "relevant": 12, "reads": 18}], "affiliation": "The University of Hong Kong", "role": "PhD Candidate", "photo_url": "https://tjb-tech.github.io/images/tjb_0708_final.jpg", "profile_url": "https://tjb-tech.github.io/"}, {"name": "Lianghao Xia", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.0, "max_score": 86, "total_relevant": 12, "total_reads": 18, "papers": [{"title": "AI-Researcher: Autonomous Scientific Innovation", "score": 86, "session": "SD-3-5417", "pdf_url": "https://openreview.net/pdf/a1c63cdd0495de94664b1513f7d95a3aedcb483a.pdf", "relevant": 12, "reads": 18}], "affiliation": "University of Hong Kong", "role": "Research Assistant Professor", "photo_url": "https://akaxlh.github.io/avatar2.jpeg", "profile_url": "https://akaxlh.github.io/"}, {"name": "Chao Huang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.0, "max_score": 86, "total_relevant": 12, "total_reads": 18, "papers": [{"title": "AI-Researcher: Autonomous Scientific Innovation", "score": 86, "session": "SD-3-5417", "pdf_url": "https://openreview.net/pdf/a1c63cdd0495de94664b1513f7d95a3aedcb483a.pdf", "relevant": 12, "reads": 18}], "affiliation": "The University of Hong Kong", "role": "Assistant Professor", "photo_url": "https://www.cs.hku.hk/images/Staff/chuang02_full.jpg", "profile_url": "https://sites.google.com/view/chaoh"}, {"name": "Fan LIU", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.0, "max_score": 86, "total_relevant": 5, "total_reads": 15, "papers": [{"title": "Bag of Tricks for Inference-time Computation of LLM Reasoning", "score": 86, "session": "SD-6-4913", "pdf_url": "https://arxiv.org/pdf/2502.07191", "relevant": 5, "reads": 15}], "affiliation": "HKUST(GZ)", "role": "Research Student", "photo_url": "https://luckyfan-cs.github.io//images/fan.png", "profile_url": "https://luckyfan-cs.github.io/"}, {"name": "Wen-Shuo Chao", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.0, "max_score": 86, "total_relevant": 5, "total_reads": 15, "papers": [{"title": "Bag of Tricks for Inference-time Computation of LLM Reasoning", "score": 86, "session": "SD-6-4913", "pdf_url": "https://arxiv.org/pdf/2502.07191", "relevant": 5, "reads": 15}], "affiliation": "The Hong Kong University of Science and Technology", "role": "PhD Student", "photo_url": "https://vincent40416.github.io/images/profile.jpg", "profile_url": "https://vincent40416.github.io/"}, {"name": "Hao Liu", "paper_count": 2, "highly_relevant_count": 1, "avg_score": 81.5, "max_score": 86, "total_relevant": 9, "total_reads": 24, "papers": [{"title": "Bag of Tricks for Inference-time Computation of LLM Reasoning", "score": 86, "session": "SD-6-4913", "pdf_url": "https://arxiv.org/pdf/2502.07191", "relevant": 5, "reads": 15}, {"title": "MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem", "score": 77, "session": "SD-3-5516", "pdf_url": "https://openreview.net/pdf/e01a39be0de98c2f808394f7affa2bfb004c68a5.pdf", "relevant": 4, "reads": 9}], "affiliation": "The Hong Kong University of Science and Technology (Guangzhou)", "role": "Assistant Professor", "photo_url": "https://RaymondHLIU.github.io/images/haoliu.jpg", "profile_url": "https://raymondhliu.github.io/"}, {"name": "Yufan Dang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 85.0, "max_score": 85, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "Multi-Agent Collaboration via Evolving Orchestration", "score": 85, "session": "SD-1-4019", "pdf_url": "https://openreview.net/pdf/9727f658d788c52f49f12ae4b230baf4cf0d4007.pdf", "relevant": 4, "reads": 9}], "affiliation": "Tsinghua University", "role": "Master's Student", "photo_url": "https://NA-Wen.github.io/images/avatar.png", "profile_url": "https://na-wen.github.io/"}, {"name": "Chen Qian", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 85.0, "max_score": 85, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "Multi-Agent Collaboration via Evolving Orchestration", "score": 85, "session": "SD-1-4019", "pdf_url": "https://openreview.net/pdf/9727f658d788c52f49f12ae4b230baf4cf0d4007.pdf", "relevant": 4, "reads": 9}], "affiliation": "Shanghai Jiao Tong University", "role": "Associate Professor", "photo_url": "https://qianc62.github.io/assets/media/qianchen.png", "profile_url": "https://qianc62.github.io/"}, {"name": "Maosong Sun", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 85.0, "max_score": 85, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "Multi-Agent Collaboration via Evolving Orchestration", "score": 85, "session": "SD-1-4019", "pdf_url": "https://openreview.net/pdf/9727f658d788c52f49f12ae4b230baf4cf0d4007.pdf", "relevant": 4, "reads": 9}], "affiliation": "Tsinghua University", "role": "Professor", "photo_url": "https://www.ae-info.org/attach/User/Sun_Maosong/Sun_Maosong_small.jpg", "profile_url": "https://scholar.google.com/citations?user=zIgT0HMAAAAJ&hl=en"}, {"name": "Augusto B. Corr√É¬™a", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 85.0, "max_score": 85, "total_relevant": 3, "total_reads": 7, "papers": [{"title": "Classical Planning with LLM-Generated Heuristics: Challenging the State of the Art with Python Code", "score": 85, "session": "SD-3-3210", "pdf_url": "https://openreview.net/pdf/924624fad0f2d9ea4637686b275593407c20b753.pdf", "relevant": 3, "reads": 7}], "affiliation": "University of Oxford", "role": "Junior Research Fellow", "photo_url": "https://chch.ox.ac.uk/sites/default/files/styles/portrait/public/2024-11/A_B_Correa_profile_crop_lowres.jpg", "profile_url": "https://abcorrea.github.io/"}, {"name": "Andr√É¬© Grahl Pereira", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 85.0, "max_score": 85, "total_relevant": 3, "total_reads": 7, "papers": [{"title": "Classical Planning with LLM-Generated Heuristics: Challenging the State of the Art with Python Code", "score": 85, "session": "SD-3-3210", "pdf_url": "https://openreview.net/pdf/924624fad0f2d9ea4637686b275593407c20b753.pdf", "relevant": 3, "reads": 7}], "affiliation": "Federal University of Rio Grande do Sul", "role": "Professor", "photo_url": "https://www.inf.ufrgs.br/site/wp-content/uploads/2017/03/Andr√©-Grahl-Pereira-164x220.png", "profile_url": "https://scholar.google.com/citations?user=uVd5njYAAAAJ&hl=en"}, {"name": "Jendrik Seipp", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 85.0, "max_score": 85, "total_relevant": 3, "total_reads": 7, "papers": [{"title": "Classical Planning with LLM-Generated Heuristics: Challenging the State of the Art with Python Code", "score": 85, "session": "SD-3-3210", "pdf_url": "https://openreview.net/pdf/924624fad0f2d9ea4637686b275593407c20b753.pdf", "relevant": 3, "reads": 7}], "affiliation": "Link√∂ping University", "role": "Senior Associate Professor", "photo_url": "https://liu.se/-/media/employeeimages/56/employee_image_jense56.jpeg?as=1&w=300&h=300&cr=1&crw=300&crh=300&bc=%23ffffff&hash=F569168F2A1D3414002A6CB905DB12AF", "profile_url": "https://mrlab.ai/jendrik-seipp/"}, {"name": "Ziyu Wan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 84.0, "max_score": 84, "total_relevant": 16, "total_reads": 38, "papers": [{"title": "ReMA: Learning to Meta-Think for LLMs with Multi-agent Reinforcement Learning", "score": 84, "session": "SD-2-400", "pdf_url": "https://openreview.net/pdf/4ef88667f9d6e59455e6805cd91df2e8e519f9d0.pdf", "relevant": 16, "reads": 38}], "affiliation": "Shanghai Jiao Tong University, Shanghai AI Lab", "role": "Unknown", "photo_url": null, "profile_url": "https://scholar.google.com/citations?user=VEtZ7gYAAAAJ&hl=en"}, {"name": "Yunxiang LI", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 84.0, "max_score": 84, "total_relevant": 16, "total_reads": 38, "papers": [{"title": "ReMA: Learning to Meta-Think for LLMs with Multi-agent Reinforcement Learning", "score": 84, "session": "SD-2-400", "pdf_url": "https://openreview.net/pdf/4ef88667f9d6e59455e6805cd91df2e8e519f9d0.pdf", "relevant": 16, "reads": 38}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ying Wen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 84.0, "max_score": 84, "total_relevant": 16, "total_reads": 38, "papers": [{"title": "ReMA: Learning to Meta-Think for LLMs with Multi-agent Reinforcement Learning", "score": 84, "session": "SD-2-400", "pdf_url": "https://openreview.net/pdf/4ef88667f9d6e59455e6805cd91df2e8e519f9d0.pdf", "relevant": 16, "reads": 38}], "affiliation": "Shanghai Jiao Tong University", "role": "Associate Professor", "photo_url": "https://yingwen.io/author/ying-wen/avatar_hude959322692e67e9b74e6796722f3509_66818_270x270_fill_q90_lanczos_center.jpg", "profile_url": "https://yingwen.io/"}, {"name": "Xiaoxi Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 84.0, "max_score": 84, "total_relevant": 5, "total_reads": 10, "papers": [{"title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability", "score": 84, "session": "SD-3-1905", "pdf_url": "https://openreview.net/pdf/4e3ecfb1260a3ed3a6f051cf994b3be14a8f904e.pdf", "relevant": 5, "reads": 10}], "affiliation": "Renmin University of China", "role": "PhD Student", "photo_url": "https://xiaoxi-li1.github.io/images/xiaoxi.jpg", "profile_url": "https://xiaoxi-li1.github.io/"}, {"name": "Jiajie Jin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 84.0, "max_score": 84, "total_relevant": 5, "total_reads": 10, "papers": [{"title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability", "score": 84, "session": "SD-3-1905", "pdf_url": "https://openreview.net/pdf/4e3ecfb1260a3ed3a6f051cf994b3be14a8f904e.pdf", "relevant": 5, "reads": 10}], "affiliation": "Gaoling School of Artificial Intelligence, Renmin University of China", "role": "PhD Student", "photo_url": "https://ignorejjj.github.io/assets/img/main.jpg", "profile_url": "https://ignorejjj.github.io"}, {"name": "Zhicheng Dou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 84.0, "max_score": 84, "total_relevant": 5, "total_reads": 10, "papers": [{"title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability", "score": 84, "session": "SD-3-1905", "pdf_url": "https://openreview.net/pdf/4e3ecfb1260a3ed3a6f051cf994b3be14a8f904e.pdf", "relevant": 5, "reads": 10}], "affiliation": "Renmin University of China", "role": "Professor", "photo_url": "https://dou.playbigdata.com/zhicheng_dou_2021.jpg", "profile_url": "https://dou.playbigdata.com/"}, {"name": "Periklis Mantenoglou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 84.0, "max_score": 84, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language", "score": 84, "session": "SD-6-3108", "pdf_url": "https://arxiv.org/pdf/2510.05972", "relevant": 0, "reads": 0}], "affiliation": "√ñrebro University", "role": "Postdoctoral Researcher", "photo_url": "https://periklismant.github.io/images/photo.jpg", "profile_url": "https://periklismant.github.io/"}, {"name": "Rishi Hazra", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 84.0, "max_score": 84, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language", "score": 84, "session": "SD-6-3108", "pdf_url": "https://arxiv.org/pdf/2510.05972", "relevant": 0, "reads": 0}], "affiliation": "FAIR (Meta AI), London", "role": "Postdoctoral Researcher", "photo_url": "https://rishihazra.github.io/assets/img/prof_pic.jpg", "profile_url": "https://rishihazra.github.io/"}, {"name": "Luc De Raedt", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 84.0, "max_score": 84, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language", "score": 84, "session": "SD-6-3108", "pdf_url": "https://arxiv.org/pdf/2510.05972", "relevant": 0, "reads": 0}], "affiliation": "KU Leuven", "role": "Full Professor of Computer Science, Director of Leuven.AI", "photo_url": "https://wms.cs.kuleuven.be/people/lucderaedt/luc-de-raedt.jpg/@@images/image-600-c4164adf422d6b637d90547f4a1fc057.jpeg", "profile_url": "https://wms.cs.kuleuven.be/people/lucderaedt/"}, {"name": "Xinji Mai", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 83.0, "max_score": 83, "total_relevant": 4, "total_reads": 15, "papers": [{"title": "Agentic RL Scaling Law: Spontaneous Code Execution for Mathematical Problem Solving", "score": 83, "session": "SD-1-306", "pdf_url": "https://openreview.net/pdf/6d8a6cfbc785966d9381495a786eea81de48d681.pdf", "relevant": 4, "reads": 15}], "affiliation": "Fudan University", "role": "Master's Student", "photo_url": null, "profile_url": "https://scholar.google.com/citations?user=6yh67YwAAAAJ"}, {"name": "Haotian Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 83.0, "max_score": 83, "total_relevant": 4, "total_reads": 15, "papers": [{"title": "Agentic RL Scaling Law: Spontaneous Code Execution for Mathematical Problem Solving", "score": 83, "session": "SD-1-306", "pdf_url": "https://openreview.net/pdf/6d8a6cfbc785966d9381495a786eea81de48d681.pdf", "relevant": 4, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenqiang Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 83.0, "max_score": 83, "total_relevant": 4, "total_reads": 15, "papers": [{"title": "Agentic RL Scaling Law: Spontaneous Code Execution for Mathematical Problem Solving", "score": 83, "session": "SD-1-306", "pdf_url": "https://openreview.net/pdf/6d8a6cfbc785966d9381495a786eea81de48d681.pdf", "relevant": 4, "reads": 15}], "affiliation": "Fudan University", "role": "Professor", "photo_url": "https://www.fudanroilab.com/images/people/wenqiangzhangBig.jpg", "profile_url": "https://scholar.google.com/citations?user=vL-VEJYAAAAJ&hl=en"}, {"name": "Chandler Smith", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 83.0, "max_score": 83, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia", "score": 83, "session": "SD-1-3603", "pdf_url": "https://arxiv.org/pdf/2512.03318", "relevant": 1, "reads": 2}], "affiliation": "University of Oxford", "role": "DPhil Student", "photo_url": "https://chandlersmith.me/images/profile_2.jpeg", "profile_url": "https://chandlersmith.me/"}, {"name": "Marwa Abdulhai", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 57.0, "max_score": 83, "total_relevant": 5, "total_reads": 18, "papers": [{"title": "Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia", "score": 83, "session": "SD-1-3603", "pdf_url": "https://arxiv.org/pdf/2512.03318", "relevant": 1, "reads": 2}, {"title": "Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning", "score": 31, "session": "SD-6-1805", "pdf_url": "https://openreview.net/pdf/109c600393cc962e64028e8425eca62778f40ee9.pdf", "relevant": 4, "reads": 16}], "affiliation": "UC Berkeley", "role": "PhD Student", "photo_url": "https://abdulhaim.github.io/images/profile_marwa.png", "profile_url": "https://abdulhaim.github.io/"}, {"name": "Joel Leibo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 83.0, "max_score": 83, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia", "score": 83, "session": "SD-1-3603", "pdf_url": "https://arxiv.org/pdf/2512.03318", "relevant": 1, "reads": 2}], "affiliation": "Google DeepMind", "role": "Senior Staff Research Scientist", "photo_url": "https://lh3.googleusercontent.com/sitesv/AAzXCkd8BBbskt-fdGmMS-p_euw9G2y-4UvW5aTlaqEwtgRmFHMtkou0TdL0Yqc3RLuzDC6onYnHTeG2TFbEQ_XyRc8w8m0JcLzi3IXUi8yADAq9IQd6pFBuPGUP0MdRb3KySHCBDHWJ7cc0gdf6l9f_dGXZb5HtITosOL9ObgFWwclixSTzbLj2cRwRapQ=w1280", "profile_url": "https://www.jzleibo.com/"}, {"name": "Qizheng Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 83.0, "max_score": 83, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "Agentic Plan Caching: Test-Time Memory for Fast and Cost-Efficient LLM Agents", "score": 83, "session": "SD-4-2013", "pdf_url": "https://openreview.net/pdf/2362f2d245503d9d209dde5a45fbfbf3fb6a5990.pdf", "relevant": 3, "reads": 9}], "affiliation": "Stanford University", "role": "PhD Student", "photo_url": "https://alex-q-z.github.io/images/personal_img3.JPG", "profile_url": "https://alex-q-z.github.io/"}, {"name": "Michael Wornow", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 83.0, "max_score": 83, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "Agentic Plan Caching: Test-Time Memory for Fast and Cost-Efficient LLM Agents", "score": 83, "session": "SD-4-2013", "pdf_url": "https://openreview.net/pdf/2362f2d245503d9d209dde5a45fbfbf3fb6a5990.pdf", "relevant": 3, "reads": 9}], "affiliation": "Stanford University", "role": "PhD Student", "photo_url": "https://michaelwornow.net/assets/image/resume/headshot.png", "profile_url": "https://michaelwornow.net/"}, {"name": "Kunle Olukotun", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 83.0, "max_score": 83, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "Agentic Plan Caching: Test-Time Memory for Fast and Cost-Efficient LLM Agents", "score": 83, "session": "SD-4-2013", "pdf_url": "https://openreview.net/pdf/2362f2d245503d9d209dde5a45fbfbf3fb6a5990.pdf", "relevant": 3, "reads": 9}], "affiliation": "Stanford University", "role": "Cadence Design Systems Professor of Electrical Engineering and Computer Science", "photo_url": "https://engineering.stanford.edu/sites/default/files/styles/large_square/public/media/person/kunle-olukotun1553893997357.jpg", "profile_url": "http://arsenalfc.stanford.edu/kunle/"}, {"name": "Kanghua Mo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 82.0, "max_score": 82, "total_relevant": 6, "total_reads": 21, "papers": [{"title": "Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools", "score": 82, "session": "SD-5-1512", "pdf_url": "https://openreview.net/pdf/58fd047ac6ca0bee4f61d85fc98df7a5c5b55e31.pdf", "relevant": 6, "reads": 21}], "affiliation": "Guangzhou University", "role": "PhD Student", "photo_url": null, "profile_url": "https://scholar.google.com/citations?user=pyHD7UQAAAAJ&hl=en"}, {"name": "Li Hu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 82.0, "max_score": 82, "total_relevant": 6, "total_reads": 21, "papers": [{"title": "Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools", "score": 82, "session": "SD-5-1512", "pdf_url": "https://openreview.net/pdf/58fd047ac6ca0bee4f61d85fc98df7a5c5b55e31.pdf", "relevant": 6, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhihao li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 82.0, "max_score": 82, "total_relevant": 6, "total_reads": 21, "papers": [{"title": "Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools", "score": 82, "session": "SD-5-1512", "pdf_url": "https://openreview.net/pdf/58fd047ac6ca0bee4f61d85fc98df7a5c5b55e31.pdf", "relevant": 6, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Bingchen Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 82.0, "max_score": 82, "total_relevant": 5, "total_reads": 19, "papers": [{"title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements", "score": 82, "session": "SD-6-3313", "pdf_url": "https://arxiv.org/pdf/2506.22419", "relevant": 5, "reads": 19}], "affiliation": "University of Edinburgh", "role": "PhD Student", "photo_url": "https://info.zhaobc.me/images/photo1.jpg", "profile_url": "https://bzhao.me/"}, {"name": "Despoina Magka", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 82.0, "max_score": 82, "total_relevant": 5, "total_reads": 19, "papers": [{"title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements", "score": 82, "session": "SD-6-3313", "pdf_url": "https://arxiv.org/pdf/2506.22419", "relevant": 5, "reads": 19}], "affiliation": "Meta AI (FAIR)", "role": "Researcher", "photo_url": "https://media.licdn.com/dms/image/v2/C4E03AQFDpmDOAuSaNw/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1524440381080", "profile_url": "https://scholar.google.co.uk/citations?user=itFfzv0AAAAJ&hl=en"}, {"name": "Yoram Bachrach", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 66.5, "max_score": 82, "total_relevant": 8, "total_reads": 33, "papers": [{"title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements", "score": 82, "session": "SD-6-3313", "pdf_url": "https://arxiv.org/pdf/2506.22419", "relevant": 5, "reads": 19}, {"title": "AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench", "score": 51, "session": "SD-4-2808", "pdf_url": "https://openreview.net/pdf/a22e34894b1de174131d79168e9a32bf5fefd3a5.pdf", "relevant": 3, "reads": 14}], "affiliation": "Meta", "role": "Research Scientist", "photo_url": "https://lh3.googleusercontent.com/sitesv/AAzXCkfaeGB8WHnuovnhnwRrxOwTXX-v_HdgoFw3s2-mHT_mgZP2FxyG6GHjZMFwC8QIhTbXJDM3wKTneSNzgCitYne5J9h61uMddOqvhIU6t3eGFvcu81vlF3qOQ0w3ChIMz16zM2NRZedvCq_7rkq5H5cfBh7eln1mZBfDVYdiyKoZ12UY-L2sxHRONc-Cft8ULXPsM5gaXoO0VKLld4FgBzs50MWc7fW_MnVW5hs=w1280", "profile_url": "https://sites.google.com/view/yoram-bachrach"}, {"name": "Tianyi Ma", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "AutoData: A Multi-Agent System for Open Web Data Collection", "score": 81, "session": "SD-3-2501", "pdf_url": "https://openreview.net/pdf/066a04337fe56fee38b21d8e6b2366fac00856f4.pdf", "relevant": 1, "reads": 2}], "affiliation": "University of Notre Dame", "role": "PhD Student", "photo_url": "https://tianyi-billy-ma.github.io/images/selfie.jpg", "profile_url": "https://tianyi-billy-ma.github.io/"}, {"name": "Yiyue Qian", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "AutoData: A Multi-Agent System for Open Web Data Collection", "score": 81, "session": "SD-3-2501", "pdf_url": "https://openreview.net/pdf/066a04337fe56fee38b21d8e6b2366fac00856f4.pdf", "relevant": 1, "reads": 2}], "affiliation": "Amazon AWS", "role": "Applied Scientist II", "photo_url": "https://yiyueqian.github.io/images/id_photo_1.JPG", "profile_url": "https://yiyueqian.github.io/"}, {"name": "Yanfang Ye", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "AutoData: A Multi-Agent System for Open Web Data Collection", "score": 81, "session": "SD-3-2501", "pdf_url": "https://openreview.net/pdf/066a04337fe56fee38b21d8e6b2366fac00856f4.pdf", "relevant": 1, "reads": 2}], "affiliation": "University of Notre Dame", "role": "Galassi Family Collegiate Professor in Computer Science and Engineering", "photo_url": "https://cse.nd.edu/wp-content/uploads/2021/08/ye-HS-300x300.jpg", "profile_url": "http://yes-lab.org/"}, {"name": "Lan Pan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 14, "total_reads": 24, "papers": [{"title": "Large Language Models Think Too Fast To Explore Effectively", "score": 81, "session": "SD-4-2200", "pdf_url": "https://openreview.net/pdf/84905da6e0006182e87623ee1b532443c3ca840a.pdf", "relevant": 14, "reads": 24}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hanbo Xie", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 14, "total_reads": 24, "papers": [{"title": "Large Language Models Think Too Fast To Explore Effectively", "score": 81, "session": "SD-4-2200", "pdf_url": "https://openreview.net/pdf/84905da6e0006182e87623ee1b532443c3ca840a.pdf", "relevant": 14, "reads": 24}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Robert Wilson", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 14, "total_reads": 24, "papers": [{"title": "Large Language Models Think Too Fast To Explore Effectively", "score": 81, "session": "SD-4-2200", "pdf_url": "https://openreview.net/pdf/84905da6e0006182e87623ee1b532443c3ca840a.pdf", "relevant": 14, "reads": 24}], "affiliation": "Georgia Institute of Technology", "role": "Associate Professor", "photo_url": "https://psychology.gatech.edu/sites/default/files/b.wilson_headshot.png", "profile_url": "https://psychology.gatech.edu/people/robert-wilson"}, {"name": "Zimu Lu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 5, "total_reads": 13, "papers": [{"title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch", "score": 81, "session": "SD-5-1505", "pdf_url": "https://arxiv.org/pdf/2505.03733", "relevant": 5, "reads": 13}], "affiliation": "The Chinese University of Hong Kong", "role": "PhD Student", "photo_url": "https://avatars.githubusercontent.com/u/93258384?v=4", "profile_url": "https://scholar.google.com/citations?user=ewuGUCwAAAAJ&hl=en"}, {"name": "Yunqiao Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 5, "total_reads": 13, "papers": [{"title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch", "score": 81, "session": "SD-5-1505", "pdf_url": "https://arxiv.org/pdf/2505.03733", "relevant": 5, "reads": 13}], "affiliation": "City University of Hong Kong", "role": "PhD Student", "photo_url": null, "profile_url": "https://scholar.google.com/citations?user=LI97oHUAAAAJ&hl=en"}, {"name": "Hongsheng Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 5, "total_reads": 13, "papers": [{"title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch", "score": 81, "session": "SD-5-1505", "pdf_url": "https://arxiv.org/pdf/2505.03733", "relevant": 5, "reads": 13}], "affiliation": "The Chinese University of Hong Kong", "role": "Associate Professor", "photo_url": "https://cpii.hk/wp-content/uploads/2023/01/p1b-04-1024x1024.png", "profile_url": "https://scholar.google.com/citations?user=BN2Ze-QAAAAJ&hl=en"}, {"name": "Hui Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 1, "total_reads": 6, "papers": [{"title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research", "score": 81, "session": "SD-5-2305", "pdf_url": "https://arxiv.org/pdf/2505.19955", "relevant": 1, "reads": 6}], "affiliation": "National University of Singapore", "role": "Research Fellow", "photo_url": "https://chchenhui.github.io/images/profile.jpg", "profile_url": "https://chchenhui.github.io/"}, {"name": "Miao Xiong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 1, "total_reads": 6, "papers": [{"title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research", "score": 81, "session": "SD-5-2305", "pdf_url": "https://arxiv.org/pdf/2505.19955", "relevant": 1, "reads": 6}], "affiliation": "National University of Singapore", "role": "PhD Student", "photo_url": "https://miaoxiong2320.github.io/figures/selfie_new.jpg", "profile_url": "https://miaoxiong2320.github.io/"}, {"name": "Bryan Hooi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 1, "total_reads": 6, "papers": [{"title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research", "score": 81, "session": "SD-5-2305", "pdf_url": "https://arxiv.org/pdf/2505.19955", "relevant": 1, "reads": 6}], "affiliation": "National University of Singapore", "role": "Assistant Professor", "photo_url": "https://bhooi.github.io/photo.jpg", "profile_url": "https://bhooi.github.io/"}, {"name": "Liuhao Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 2, "total_reads": 4, "papers": [{"title": "LTD-Bench: Evaluating Large Language Models by Letting Them Draw", "score": 81, "session": "SD-6-201", "pdf_url": "https://arxiv.org/pdf/2511.02347", "relevant": 2, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ke Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 2, "total_reads": 4, "papers": [{"title": "LTD-Bench: Evaluating Large Language Models by Letting Them Draw", "score": 81, "session": "SD-6-201", "pdf_url": "https://arxiv.org/pdf/2511.02347", "relevant": 2, "reads": 4}], "affiliation": "Tencent Youtu Lab", "role": "Principal Researcher and Team Manager", "photo_url": "http://keli.info/assets/img/DCIM.jpeg", "profile_url": "http://keli.info/"}, {"name": "Rongrong Ji", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 2, "total_reads": 4, "papers": [{"title": "LTD-Bench: Evaluating Large Language Models by Letting Them Draw", "score": 81, "session": "SD-6-201", "pdf_url": "https://arxiv.org/pdf/2511.02347", "relevant": 2, "reads": 4}], "affiliation": "Xiamen University", "role": "Nanqiang Distinguished Professor", "photo_url": "https://mac.xmu.edu.cn/dfiles/9800/rrji/images/profile_20.jpg", "profile_url": "https://mac.xmu.edu.cn/rrji_en/"}, {"name": "Yuxuan Zhu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 5, "total_reads": 9, "papers": [{"title": "Establishing Best Practices in Building Rigorous Agentic Benchmarks", "score": 81, "session": "SD-6-2709", "pdf_url": "https://arxiv.org/pdf/2507.02825", "relevant": 5, "reads": 9}], "affiliation": "University of Illinois at Urbana-Champaign", "role": "PhD Student", "photo_url": "https://yuxuan18.github.io/assets/img/profile_2025.png", "profile_url": "https://yuxuan18.github.io/"}, {"name": "Tengjun Jin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 5, "total_reads": 9, "papers": [{"title": "Establishing Best Practices in Building Rigorous Agentic Benchmarks", "score": 81, "session": "SD-6-2709", "pdf_url": "https://arxiv.org/pdf/2507.02825", "relevant": 5, "reads": 9}], "affiliation": "University of Illinois at Urbana-Champaign", "role": "PhD Student", "photo_url": "https://avatars.githubusercontent.com/u/43330557?s=64&v=4", "profile_url": "https://scholar.google.com/citations?user=LM_VM5gAAAAJ&hl=en"}, {"name": "Daniel Kang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81, "total_relevant": 5, "total_reads": 9, "papers": [{"title": "Establishing Best Practices in Building Rigorous Agentic Benchmarks", "score": 81, "session": "SD-6-2709", "pdf_url": "https://arxiv.org/pdf/2507.02825", "relevant": 5, "reads": 9}], "affiliation": "University of Illinois Urbana-Champaign", "role": "Assistant Professor", "photo_url": "https://ddkang.github.io/assets/images/bio-photo.jpg", "profile_url": "https://ddkang.github.io/"}, {"name": "Emanuele La Malfa", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 80.0, "max_score": 80, "total_relevant": 5, "total_reads": 9, "papers": [{"title": "Large Language Models Miss the Multi-agent Mark", "score": 80, "session": "SD-3-3504", "pdf_url": "https://arxiv.org/pdf/2505.21298", "relevant": 5, "reads": 9}], "affiliation": "University of Oxford", "role": "Postdoctoral Researcher", "photo_url": "https://www.cs.ox.ac.uk/files/14339/photo-id-1.png", "profile_url": "https://emanuelelm.github.io/"}, {"name": "Gabriele La Malfa", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 80.0, "max_score": 80, "total_relevant": 5, "total_reads": 9, "papers": [{"title": "Large Language Models Miss the Multi-agent Mark", "score": 80, "session": "SD-3-3504", "pdf_url": "https://arxiv.org/pdf/2505.21298", "relevant": 5, "reads": 9}], "affiliation": "King's College London", "role": "PhD Student", "photo_url": "https://safeandtrustedai.org/wp-content/uploads/2023/02/Gabriele-La-Malfa.jpg", "profile_url": "https://www.kcl.ac.uk/people/gabriele-la-malfa"}, {"name": "Michael Wooldridge", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 80.0, "max_score": 80, "total_relevant": 5, "total_reads": 9, "papers": [{"title": "Large Language Models Miss the Multi-agent Mark", "score": 80, "session": "SD-3-3504", "pdf_url": "https://arxiv.org/pdf/2505.21298", "relevant": 5, "reads": 9}], "affiliation": "University of Oxford", "role": "Professor", "photo_url": "https://www.cs.ox.ac.uk/files/14482/mjw-pic.jpg", "profile_url": "https://www.cs.ox.ac.uk/people/michael.wooldridge/"}, {"name": "Jack Hopkins", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 80.0, "max_score": 80, "total_relevant": 2, "total_reads": 11, "papers": [{"title": "Factorio Learning Environment", "score": 80, "session": "SD-4-312", "pdf_url": "https://arxiv.org/pdf/2503.09617", "relevant": 2, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mart Bakler", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 80.0, "max_score": 80, "total_relevant": 2, "total_reads": 11, "papers": [{"title": "Factorio Learning Environment", "score": 80, "session": "SD-4-312", "pdf_url": "https://arxiv.org/pdf/2503.09617", "relevant": 2, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Akbir Khan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 80.0, "max_score": 80, "total_relevant": 2, "total_reads": 11, "papers": [{"title": "Factorio Learning Environment", "score": 80, "session": "SD-4-312", "pdf_url": "https://arxiv.org/pdf/2503.09617", "relevant": 2, "reads": 11}], "affiliation": "Anthropic", "role": "Member of Technical Staff", "photo_url": "https://akbir.dev/img/me3.jpeg", "profile_url": "https://akbir.dev/about/"}, {"name": "Andy Zou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 80.0, "max_score": 80, "total_relevant": 8, "total_reads": 18, "papers": [{"title": "Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition", "score": 80, "session": "SD-6-4915", "pdf_url": "https://arxiv.org/pdf/2507.20526", "relevant": 8, "reads": 18}], "affiliation": "Carnegie Mellon University", "role": "PhD Student", "photo_url": "https://andyzoujm.github.io/images/photograph.png", "profile_url": "https://andyzoujm.github.io/"}, {"name": "Maxwell Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 80.0, "max_score": 80, "total_relevant": 8, "total_reads": 18, "papers": [{"title": "Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition", "score": 80, "session": "SD-6-4915", "pdf_url": "https://arxiv.org/pdf/2507.20526", "relevant": 8, "reads": 18}], "affiliation": "AWS", "role": "Software Engineer", "photo_url": "https://maxwellmlin.com/assets/img/favicon/upscaled.jpg", "profile_url": "https://maxwellmlin.com/"}, {"name": "Matt Fredrikson", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 80.0, "max_score": 80, "total_relevant": 8, "total_reads": 18, "papers": [{"title": "Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition", "score": 80, "session": "SD-6-4915", "pdf_url": "https://arxiv.org/pdf/2507.20526", "relevant": 8, "reads": 18}], "affiliation": "Carnegie Mellon University", "role": "Associate Professor", "photo_url": "https://mattfredrikson.com/images/mfredrik.jpg", "profile_url": "https://mattfredrikson.com/"}, {"name": "Yimeng Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 79.0, "max_score": 79, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors", "score": 79, "session": "SD-2-2108", "pdf_url": "https://arxiv.org/pdf/2507.15550", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Piotr Pi√Ñ¬ôkos", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 79.0, "max_score": 79, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors", "score": 79, "session": "SD-2-2108", "pdf_url": "https://arxiv.org/pdf/2507.15550", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "J√É¬ºrgen Schmidhuber", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 79.0, "max_score": 79, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors", "score": 79, "session": "SD-2-2108", "pdf_url": "https://arxiv.org/pdf/2507.15550", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jialong Wu", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 62.5, "max_score": 79, "total_relevant": 12, "total_reads": 37, "papers": [{"title": "WebDancer: Towards Autonomous Information Seeking Agency", "score": 79, "session": "SD-3-5515", "pdf_url": "https://openreview.net/pdf/7c886fbc63b09377d123254d93907b41820d72d7.pdf", "relevant": 1, "reads": 2}, {"title": "RLVR-World: Training World Models with Reinforcement Learning", "score": 46, "session": "SD-4-1301", "pdf_url": "https://openreview.net/pdf/4b0d2935ed4554453743ecdcf099e0d679355328.pdf", "relevant": 11, "reads": 35}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Baixuan Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 79.0, "max_score": 79, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "WebDancer: Towards Autonomous Information Seeking Agency", "score": 79, "session": "SD-3-5515", "pdf_url": "https://openreview.net/pdf/7c886fbc63b09377d123254d93907b41820d72d7.pdf", "relevant": 1, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jingren Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 79.0, "max_score": 79, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "WebDancer: Towards Autonomous Information Seeking Agency", "score": 79, "session": "SD-3-5515", "pdf_url": "https://openreview.net/pdf/7c886fbc63b09377d123254d93907b41820d72d7.pdf", "relevant": 1, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qianyue Hao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 79.0, "max_score": 79, "total_relevant": 0, "total_reads": 18, "papers": [{"title": "LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models", "score": 79, "session": "SD-3-309", "pdf_url": "https://openreview.net/pdf/64ba671c860d8032b21824e28d00916c3e613964.pdf", "relevant": 0, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yiwen Song", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 79.0, "max_score": 79, "total_relevant": 0, "total_reads": 18, "papers": [{"title": "LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models", "score": 79, "session": "SD-3-309", "pdf_url": "https://openreview.net/pdf/64ba671c860d8032b21824e28d00916c3e613964.pdf", "relevant": 0, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yu Yao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 78.0, "max_score": 78, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Can Dependencies Induced by LLM-Agent Workflows Be Trusted?", "score": 78, "session": "SD-2-1904", "pdf_url": "https://openreview.net/pdf/d84e4a86a34dd41d3f1e6d33048f4c075db78d62.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yiliao Song", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 78.0, "max_score": 78, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Can Dependencies Induced by LLM-Agent Workflows Be Trusted?", "score": 78, "session": "SD-2-1904", "pdf_url": "https://openreview.net/pdf/d84e4a86a34dd41d3f1e6d33048f4c075db78d62.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tongliang Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 78.0, "max_score": 78, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Can Dependencies Induced by LLM-Agent Workflows Be Trusted?", "score": 78, "session": "SD-2-1904", "pdf_url": "https://openreview.net/pdf/d84e4a86a34dd41d3f1e6d33048f4c075db78d62.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jingli Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 78.0, "max_score": 78, "total_relevant": 12, "total_reads": 31, "papers": [{"title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding", "score": 78, "session": "SD-5-4601", "pdf_url": "https://arxiv.org/pdf/2507.07984", "relevant": 12, "reads": 31}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chenming Zhu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 78.0, "max_score": 78, "total_relevant": 12, "total_reads": 31, "papers": [{"title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding", "score": 78, "session": "SD-5-4601", "pdf_url": "https://arxiv.org/pdf/2507.07984", "relevant": 12, "reads": 31}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiangmiao Pang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 78.0, "max_score": 78, "total_relevant": 12, "total_reads": 31, "papers": [{"title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding", "score": 78, "session": "SD-5-4601", "pdf_url": "https://arxiv.org/pdf/2507.07984", "relevant": 12, "reads": 31}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiangjie Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 78.0, "max_score": 78, "total_relevant": 1, "total_reads": 11, "papers": [{"title": "Enigmata:  Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles", "score": 78, "session": "SD-6-5104", "pdf_url": "https://openreview.net/pdf/d16d55cfeff749792ae0b093a3f4c0123aa6c09f.pdf", "relevant": 1, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qianyu He", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 78.0, "max_score": 78, "total_relevant": 1, "total_reads": 11, "papers": [{"title": "Enigmata:  Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles", "score": 78, "session": "SD-6-5104", "pdf_url": "https://openreview.net/pdf/d16d55cfeff749792ae0b093a3f4c0123aa6c09f.pdf", "relevant": 1, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mingxuan Wang", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 70.5, "max_score": 78, "total_relevant": 42, "total_reads": 61, "papers": [{"title": "Enigmata:  Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles", "score": 78, "session": "SD-6-5104", "pdf_url": "https://openreview.net/pdf/d16d55cfeff749792ae0b093a3f4c0123aa6c09f.pdf", "relevant": 1, "reads": 11}, {"title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale", "score": 63, "session": "SD-1-316", "pdf_url": "https://openreview.net/pdf/a5ca4684c1debe30e4fde4bd063a262d61e13db7.pdf", "relevant": 41, "reads": 50}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Brown Ebouky", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.0, "max_score": 77, "total_relevant": 9, "total_reads": 28, "papers": [{"title": "Eliciting Reasoning in Language Models with Cognitive Tools", "score": 77, "session": "SD-1-302", "pdf_url": "https://openreview.net/pdf/ea576c6461243d27a8102c9dfa554db950d66757.pdf", "relevant": 9, "reads": 28}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Andrea Bartezzaghi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.0, "max_score": 77, "total_relevant": 9, "total_reads": 28, "papers": [{"title": "Eliciting Reasoning in Language Models with Cognitive Tools", "score": 77, "session": "SD-1-302", "pdf_url": "https://openreview.net/pdf/ea576c6461243d27a8102c9dfa554db950d66757.pdf", "relevant": 9, "reads": 28}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mattia Rigotti", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.0, "max_score": 77, "total_relevant": 9, "total_reads": 28, "papers": [{"title": "Eliciting Reasoning in Language Models with Cognitive Tools", "score": 77, "session": "SD-1-302", "pdf_url": "https://openreview.net/pdf/ea576c6461243d27a8102c9dfa554db950d66757.pdf", "relevant": 9, "reads": 28}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Fan Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.0, "max_score": 77, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem", "score": 77, "session": "SD-3-5516", "pdf_url": "https://openreview.net/pdf/e01a39be0de98c2f808394f7affa2bfb004c68a5.pdf", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhe-Rui Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.0, "max_score": 77, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem", "score": 77, "session": "SD-3-5516", "pdf_url": "https://openreview.net/pdf/e01a39be0de98c2f808394f7affa2bfb004c68a5.pdf", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chuan Guo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.0, "max_score": 77, "total_relevant": 4, "total_reads": 5, "papers": [{"title": "AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents", "score": 77, "session": "SD-4-1304", "pdf_url": "https://arxiv.org/pdf/2503.09780", "relevant": 4, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Akshara Prabhakar", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.0, "max_score": 77, "total_relevant": 1, "total_reads": 8, "papers": [{"title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay", "score": 77, "session": "SD-4-109", "pdf_url": "https://arxiv.org/pdf/2504.03601", "relevant": 1, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zuxin Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.0, "max_score": 77, "total_relevant": 1, "total_reads": 8, "papers": [{"title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay", "score": 77, "session": "SD-4-109", "pdf_url": "https://arxiv.org/pdf/2504.03601", "relevant": 1, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Caiming Xiong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.0, "max_score": 77, "total_relevant": 1, "total_reads": 8, "papers": [{"title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay", "score": 77, "session": "SD-4-109", "pdf_url": "https://arxiv.org/pdf/2504.03601", "relevant": 1, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaokang Ye", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.0, "max_score": 77, "total_relevant": 3, "total_reads": 13, "papers": [{"title": "SimWorld: An Open-ended Simulator for Agents in Physical and Social Worlds", "score": 77, "session": "SD-5-401", "pdf_url": "https://openreview.net/pdf/a98dee23d8b37552151336bbac20c838fd5e9ee1.pdf", "relevant": 3, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiawei Ren", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.0, "max_score": 77, "total_relevant": 3, "total_reads": 13, "papers": [{"title": "SimWorld: An Open-ended Simulator for Agents in Physical and Social Worlds", "score": 77, "session": "SD-5-401", "pdf_url": "https://openreview.net/pdf/a98dee23d8b37552151336bbac20c838fd5e9ee1.pdf", "relevant": 3, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lianhui Qin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.0, "max_score": 77, "total_relevant": 3, "total_reads": 13, "papers": [{"title": "SimWorld: An Open-ended Simulator for Agents in Physical and Social Worlds", "score": 77, "session": "SD-5-401", "pdf_url": "https://openreview.net/pdf/a98dee23d8b37552151336bbac20c838fd5e9ee1.pdf", "relevant": 3, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Gleb Rodionov", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 76.0, "max_score": 76, "total_relevant": 4, "total_reads": 20, "papers": [{"title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention", "score": 76, "session": "SD-1-1906", "pdf_url": "https://openreview.net/pdf/752690b760cb01f226ba630228b97af81df4d1d1.pdf", "relevant": 4, "reads": 20}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Roman Garipov", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 76.0, "max_score": 76, "total_relevant": 4, "total_reads": 20, "papers": [{"title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention", "score": 76, "session": "SD-1-1906", "pdf_url": "https://openreview.net/pdf/752690b760cb01f226ba630228b97af81df4d1d1.pdf", "relevant": 4, "reads": 20}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dan Alistarh", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 76.0, "max_score": 76, "total_relevant": 4, "total_reads": 20, "papers": [{"title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention", "score": 76, "session": "SD-1-1906", "pdf_url": "https://openreview.net/pdf/752690b760cb01f226ba630228b97af81df4d1d1.pdf", "relevant": 4, "reads": 20}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jorge (Zhoujun) Cheng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 76.0, "max_score": 76, "total_relevant": 8, "total_reads": 16, "papers": [{"title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective", "score": 76, "session": "SD-1-107", "pdf_url": "https://arxiv.org/pdf/2506.14965", "relevant": 8, "reads": 16}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shibo Hao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 76.0, "max_score": 76, "total_relevant": 8, "total_reads": 16, "papers": [{"title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective", "score": 76, "session": "SD-1-107", "pdf_url": "https://arxiv.org/pdf/2506.14965", "relevant": 8, "reads": 16}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhiting Hu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 76.0, "max_score": 76, "total_relevant": 8, "total_reads": 16, "papers": [{"title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective", "score": 76, "session": "SD-1-107", "pdf_url": "https://arxiv.org/pdf/2506.14965", "relevant": 8, "reads": 16}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ethan Mendes", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 76.0, "max_score": 76, "total_relevant": 5, "total_reads": 10, "papers": [{"title": "Language Models can Self-Improve at State-Value Estimation for Better Search", "score": 76, "session": "SD-5-1916", "pdf_url": "https://openreview.net/pdf/ea251516f78a77cbc42dd4a50e2ce4aadfc2226f.pdf", "relevant": 5, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Alan Ritter", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 76.0, "max_score": 76, "total_relevant": 5, "total_reads": 10, "papers": [{"title": "Language Models can Self-Improve at State-Value Estimation for Better Search", "score": 76, "session": "SD-5-1916", "pdf_url": "https://openreview.net/pdf/ea251516f78a77cbc42dd4a50e2ce4aadfc2226f.pdf", "relevant": 5, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mengkang Hu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 75.0, "max_score": 75, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation", "score": 75, "session": "SD-1-5409", "pdf_url": "https://openreview.net/pdf/9ea2d3d5cf7f874c7669ab5c3f1270eb3bc794d1.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuhang Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 75.0, "max_score": 75, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation", "score": 75, "session": "SD-1-5409", "pdf_url": "https://openreview.net/pdf/9ea2d3d5cf7f874c7669ab5c3f1270eb3bc794d1.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Guohao Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 75.0, "max_score": 75, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation", "score": 75, "session": "SD-1-5409", "pdf_url": "https://openreview.net/pdf/9ea2d3d5cf7f874c7669ab5c3f1270eb3bc794d1.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shaohang Wei", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 75.0, "max_score": 75, "total_relevant": 0, "total_reads": 1, "papers": [{"title": "TimE: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "score": 75, "session": "SD-1-1806", "pdf_url": "https://arxiv.org/pdf/2505.12891", "relevant": 0, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wei Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 75.0, "max_score": 75, "total_relevant": 0, "total_reads": 1, "papers": [{"title": "TimE: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "score": 75, "session": "SD-1-1806", "pdf_url": "https://arxiv.org/pdf/2505.12891", "relevant": 0, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Houfeng Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 75.0, "max_score": 75, "total_relevant": 0, "total_reads": 1, "papers": [{"title": "TimE: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "score": 75, "session": "SD-1-1806", "pdf_url": "https://arxiv.org/pdf/2505.12891", "relevant": 0, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Joey Hong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 74.0, "max_score": 74, "total_relevant": 7, "total_reads": 22, "papers": [{"title": "Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL", "score": 74, "session": "SD-3-5419", "pdf_url": "https://openreview.net/pdf/a27fd29bd9a99f5624d5362501cf5a6acc4c4e37.pdf", "relevant": 7, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Anca Dragan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 74.0, "max_score": 74, "total_relevant": 7, "total_reads": 22, "papers": [{"title": "Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL", "score": 74, "session": "SD-3-5419", "pdf_url": "https://openreview.net/pdf/a27fd29bd9a99f5624d5362501cf5a6acc4c4e37.pdf", "relevant": 7, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Bin Lei", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 74.0, "max_score": 74, "total_relevant": 2, "total_reads": 7, "papers": [{"title": "InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction", "score": 74, "session": "SD-5-1602", "pdf_url": "https://openreview.net/pdf/ae7bb751a0d72057590b5907d9d4fa86c412be50.pdf", "relevant": 2, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Weitai Kang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 74.0, "max_score": 74, "total_relevant": 2, "total_reads": 7, "papers": [{"title": "InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction", "score": 74, "session": "SD-5-1602", "pdf_url": "https://openreview.net/pdf/ae7bb751a0d72057590b5907d9d4fa86c412be50.pdf", "relevant": 2, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Caiwen Ding", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 74.0, "max_score": 74, "total_relevant": 2, "total_reads": 7, "papers": [{"title": "InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction", "score": 74, "session": "SD-5-1602", "pdf_url": "https://openreview.net/pdf/ae7bb751a0d72057590b5907d9d4fa86c412be50.pdf", "relevant": 2, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hanjun Luo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 74.0, "max_score": 74, "total_relevant": 5, "total_reads": 6, "papers": [{"title": "AgentAuditor: Human-level Safety and Security Evaluation for LLM Agents", "score": 74, "session": "SD-6-1106", "pdf_url": "https://openreview.net/pdf/2ceecf4bf50bffa7cff2d145fadb18c23daf7206.pdf", "relevant": 5, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shenyu Dai", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 74.0, "max_score": 74, "total_relevant": 5, "total_reads": 6, "papers": [{"title": "AgentAuditor: Human-level Safety and Security Evaluation for LLM Agents", "score": 74, "session": "SD-6-1106", "pdf_url": "https://openreview.net/pdf/2ceecf4bf50bffa7cff2d145fadb18c23daf7206.pdf", "relevant": 5, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hanan Salam", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 74.0, "max_score": 74, "total_relevant": 5, "total_reads": 6, "papers": [{"title": "AgentAuditor: Human-level Safety and Security Evaluation for LLM Agents", "score": 74, "session": "SD-6-1106", "pdf_url": "https://openreview.net/pdf/2ceecf4bf50bffa7cff2d145fadb18c23daf7206.pdf", "relevant": 5, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ansong Ni", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 74.0, "max_score": 74, "total_relevant": 4, "total_reads": 18, "papers": [{"title": "Collaborative Reasoner: Self-Improving Social Agents with Synthetic Conversations", "score": 74, "session": "SD-6-1913", "pdf_url": "https://openreview.net/pdf/42a338bca40ea896002753679729eb2240bf62b3.pdf", "relevant": 4, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ruta Desai", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 74.0, "max_score": 74, "total_relevant": 4, "total_reads": 18, "papers": [{"title": "Collaborative Reasoner: Self-Improving Social Agents with Synthetic Conversations", "score": 74, "session": "SD-6-1913", "pdf_url": "https://openreview.net/pdf/42a338bca40ea896002753679729eb2240bf62b3.pdf", "relevant": 4, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Asli Celikyilmaz", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 74.0, "max_score": 74, "total_relevant": 4, "total_reads": 18, "papers": [{"title": "Collaborative Reasoner: Self-Improving Social Agents with Synthetic Conversations", "score": 74, "session": "SD-6-1913", "pdf_url": "https://openreview.net/pdf/42a338bca40ea896002753679729eb2240bf62b3.pdf", "relevant": 4, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zehao Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 73.0, "max_score": 73, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "LogicTree: Improving Complex Reasoning of LLMs via Instantiated Multi-step Synthetic Logical Data", "score": 73, "session": "SD-2-113", "pdf_url": "https://openreview.net/pdf/30059572044fffae201f1e0daf92fc78a5aef218.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lin Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 73.0, "max_score": 73, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "LogicTree: Improving Complex Reasoning of LLMs via Instantiated Multi-step Synthetic Logical Data", "score": 73, "session": "SD-2-113", "pdf_url": "https://openreview.net/pdf/30059572044fffae201f1e0daf92fc78a5aef218.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Enhong Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 73.0, "max_score": 73, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "LogicTree: Improving Complex Reasoning of LLMs via Instantiated Multi-step Synthetic Logical Data", "score": 73, "session": "SD-2-113", "pdf_url": "https://openreview.net/pdf/30059572044fffae201f1e0daf92fc78a5aef218.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ziyuan He", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 73.0, "max_score": 73, "total_relevant": 1, "total_reads": 1, "papers": [{"title": "LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?", "score": 73, "session": "SD-5-5102", "pdf_url": "https://arxiv.org/pdf/2510.22548", "relevant": 1, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuxuan Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 73.0, "max_score": 73, "total_relevant": 1, "total_reads": 1, "papers": [{"title": "LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?", "score": 73, "session": "SD-5-5102", "pdf_url": "https://arxiv.org/pdf/2510.22548", "relevant": 1, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Muhan Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 73.0, "max_score": 73, "total_relevant": 1, "total_reads": 1, "papers": [{"title": "LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?", "score": 73, "session": "SD-5-5102", "pdf_url": "https://arxiv.org/pdf/2510.22548", "relevant": 1, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yining Hong", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 65.5, "max_score": 71, "total_relevant": 12, "total_reads": 35, "papers": [{"title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence", "score": 71, "session": "SD-5-4509", "pdf_url": "https://arxiv.org/pdf/2506.15677", "relevant": 4, "reads": 13}, {"title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model", "score": 60, "session": "SD-4-4806", "pdf_url": "https://openreview.net/pdf/8e3474ff3b680185f73a397352845c1347b77d73.pdf", "relevant": 8, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Rui Sun", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 71.0, "max_score": 71, "total_relevant": 4, "total_reads": 13, "papers": [{"title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence", "score": 71, "session": "SD-5-4509", "pdf_url": "https://arxiv.org/pdf/2506.15677", "relevant": 4, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kai-Wei Chang", "paper_count": 3, "highly_relevant_count": 0, "avg_score": 54.7, "max_score": 71, "total_relevant": 25, "total_reads": 103, "papers": [{"title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence", "score": 71, "session": "SD-5-4509", "pdf_url": "https://arxiv.org/pdf/2506.15677", "relevant": 4, "reads": 13}, {"title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model", "score": 60, "session": "SD-4-4806", "pdf_url": "https://openreview.net/pdf/8e3474ff3b680185f73a397352845c1347b77d73.pdf", "relevant": 8, "reads": 22}, {"title": "OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles", "score": 33, "session": "SD-1-5514", "pdf_url": "https://openreview.net/pdf/62ceb097c643e0416c764c187ebf4f4d6d1ba9c3.pdf", "relevant": 13, "reads": 68}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tianyu Hua", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 71.0, "max_score": 71, "total_relevant": 3, "total_reads": 12, "papers": [{"title": "ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code", "score": 71, "session": "SD-6-2710", "pdf_url": "https://arxiv.org/pdf/2506.02314", "relevant": 3, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Harper Hua", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 71.0, "max_score": 71, "total_relevant": 3, "total_reads": 12, "papers": [{"title": "ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code", "score": 71, "session": "SD-6-2710", "pdf_url": "https://arxiv.org/pdf/2506.02314", "relevant": 3, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Nick Haber", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 71.0, "max_score": 71, "total_relevant": 3, "total_reads": 12, "papers": [{"title": "ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code", "score": 71, "session": "SD-6-2710", "pdf_url": "https://arxiv.org/pdf/2506.02314", "relevant": 3, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shiyi Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 71.0, "max_score": 71, "total_relevant": 3, "total_reads": 8, "papers": [{"title": "ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests", "score": 71, "session": "SD-6-109", "pdf_url": "https://arxiv.org/pdf/2506.04894", "relevant": 3, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hu Yiwen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 71.0, "max_score": 71, "total_relevant": 3, "total_reads": 8, "papers": [{"title": "ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests", "score": 71, "session": "SD-6-109", "pdf_url": "https://arxiv.org/pdf/2506.04894", "relevant": 3, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ji-Rong Wen", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 53.0, "max_score": 71, "total_relevant": 5, "total_reads": 10, "papers": [{"title": "ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests", "score": 71, "session": "SD-6-109", "pdf_url": "https://arxiv.org/pdf/2506.04894", "relevant": 3, "reads": 8}, {"title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants", "score": 35, "session": "SD-4-2015", "pdf_url": "https://openreview.net/pdf/96e1b7b1eeb53a530580aff14cf9527fe3e3d1ac.pdf", "relevant": 2, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuichi Inoue", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.0, "max_score": 70, "total_relevant": 4, "total_reads": 7, "papers": [{"title": "Wider or Deeper?  Scaling LLM Inference-Time Compute with Adaptive Branching Tree Search", "score": 70, "session": "SD-2-3418", "pdf_url": "https://openreview.net/pdf/e6d2b8dcb02f4dadb940b53aefa65032db4fbd89.pdf", "relevant": 4, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kou Misaki", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.0, "max_score": 70, "total_relevant": 4, "total_reads": 7, "papers": [{"title": "Wider or Deeper?  Scaling LLM Inference-Time Compute with Adaptive Branching Tree Search", "score": 70, "session": "SD-2-3418", "pdf_url": "https://openreview.net/pdf/e6d2b8dcb02f4dadb940b53aefa65032db4fbd89.pdf", "relevant": 4, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Takuya Akiba", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 44.5, "max_score": 70, "total_relevant": 5, "total_reads": 9, "papers": [{"title": "Wider or Deeper?  Scaling LLM Inference-Time Compute with Adaptive Branching Tree Search", "score": 70, "session": "SD-2-3418", "pdf_url": "https://openreview.net/pdf/e6d2b8dcb02f4dadb940b53aefa65032db4fbd89.pdf", "relevant": 4, "reads": 7}, {"title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering", "score": 19, "session": "SD-4-702", "pdf_url": "https://arxiv.org/pdf/2506.09050", "relevant": 1, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Siyu Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.0, "max_score": 70, "total_relevant": 4, "total_reads": 32, "papers": [{"title": "WALL-E: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents", "score": 70, "session": "SD-3-2811", "pdf_url": "https://openreview.net/pdf/4a70c0605cac16a41fee061564452113f886243a.pdf", "relevant": 4, "reads": 32}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tianyi Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.0, "max_score": 70, "total_relevant": 4, "total_reads": 32, "papers": [{"title": "WALL-E: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents", "score": 70, "session": "SD-3-2811", "pdf_url": "https://openreview.net/pdf/4a70c0605cac16a41fee061564452113f886243a.pdf", "relevant": 4, "reads": 32}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chengqi Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.0, "max_score": 70, "total_relevant": 4, "total_reads": 32, "papers": [{"title": "WALL-E: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents", "score": 70, "session": "SD-3-2811", "pdf_url": "https://openreview.net/pdf/4a70c0605cac16a41fee061564452113f886243a.pdf", "relevant": 4, "reads": 32}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuchen Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.0, "max_score": 70, "total_relevant": 2, "total_reads": 5, "papers": [{"title": "Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach", "score": 70, "session": "SD-5-1409", "pdf_url": "https://openreview.net/pdf/c98e7220e280f1d8bae43d92944840467237a1e7.pdf", "relevant": 2, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Edward Sun", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.0, "max_score": 70, "total_relevant": 2, "total_reads": 5, "papers": [{"title": "Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach", "score": 70, "session": "SD-5-1409", "pdf_url": "https://openreview.net/pdf/c98e7220e280f1d8bae43d92944840467237a1e7.pdf", "relevant": 2, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jindong Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.0, "max_score": 70, "total_relevant": 2, "total_reads": 5, "papers": [{"title": "Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach", "score": 70, "session": "SD-5-1409", "pdf_url": "https://openreview.net/pdf/c98e7220e280f1d8bae43d92944840467237a1e7.pdf", "relevant": 2, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Isha Puri", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.0, "max_score": 70, "total_relevant": 7, "total_reads": 11, "papers": [{"title": "Rollout Roulette: A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods", "score": 70, "session": "SD-5-5518", "pdf_url": "https://openreview.net/pdf/7ee28149679cef6235a635039365773e8ff846ed.pdf", "relevant": 7, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shivchander Sudalairaj", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.0, "max_score": 70, "total_relevant": 7, "total_reads": 11, "papers": [{"title": "Rollout Roulette: A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods", "score": 70, "session": "SD-5-5518", "pdf_url": "https://openreview.net/pdf/7ee28149679cef6235a635039365773e8ff846ed.pdf", "relevant": 7, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Akash Srivastava", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.0, "max_score": 70, "total_relevant": 7, "total_reads": 11, "papers": [{"title": "Rollout Roulette: A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods", "score": 70, "session": "SD-5-5518", "pdf_url": "https://openreview.net/pdf/7ee28149679cef6235a635039365773e8ff846ed.pdf", "relevant": 7, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dylan Sam", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.0, "max_score": 70, "total_relevant": 10, "total_reads": 22, "papers": [{"title": "Predicting the Performance of Black-box Language Models with Follow-up Queries", "score": 70, "session": "SD-6-1304", "pdf_url": "https://openreview.net/pdf/a9530414a4fd5f326629f3402e2729d04dac678a.pdf", "relevant": 10, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Marc Anton Finzi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.0, "max_score": 70, "total_relevant": 10, "total_reads": 22, "papers": [{"title": "Predicting the Performance of Black-box Language Models with Follow-up Queries", "score": 70, "session": "SD-6-1304", "pdf_url": "https://openreview.net/pdf/a9530414a4fd5f326629f3402e2729d04dac678a.pdf", "relevant": 10, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "J Zico Kolter", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.0, "max_score": 70, "total_relevant": 10, "total_reads": 22, "papers": [{"title": "Predicting the Performance of Black-box Language Models with Follow-up Queries", "score": 70, "session": "SD-6-1304", "pdf_url": "https://openreview.net/pdf/a9530414a4fd5f326629f3402e2729d04dac678a.pdf", "relevant": 10, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kaiwen Zha", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.0, "max_score": 69, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning", "score": 69, "session": "SD-1-4017", "pdf_url": "https://openreview.net/pdf/df2d115dd7fe7763e94fd35d45007ed71cbcebc7.pdf", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhengqi Gao", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 37.0, "max_score": 69, "total_relevant": 5, "total_reads": 13, "papers": [{"title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning", "score": 69, "session": "SD-1-4017", "pdf_url": "https://openreview.net/pdf/df2d115dd7fe7763e94fd35d45007ed71cbcebc7.pdf", "relevant": 3, "reads": 9}, {"title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems", "score": 5, "session": "SD-1-810", "pdf_url": "https://openreview.net/pdf/81561154949bf17e7f12ee6dc0485c10a2415686.pdf", "relevant": 2, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dina Katabi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.0, "max_score": 69, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning", "score": 69, "session": "SD-1-4017", "pdf_url": "https://openreview.net/pdf/df2d115dd7fe7763e94fd35d45007ed71cbcebc7.pdf", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Huacan Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.0, "max_score": 69, "total_relevant": 2, "total_reads": 8, "papers": [{"title": "RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for  Complex Task Solving", "score": 69, "session": "SD-2-1916", "pdf_url": "https://openreview.net/pdf/fea6d1106531a6d10e824159b158dcbffbc07cc2.pdf", "relevant": 2, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ziyi Ni", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.0, "max_score": 69, "total_relevant": 2, "total_reads": 8, "papers": [{"title": "RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for  Complex Task Solving", "score": 69, "session": "SD-2-1916", "pdf_url": "https://openreview.net/pdf/fea6d1106531a6d10e824159b158dcbffbc07cc2.pdf", "relevant": 2, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pin Lyu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.0, "max_score": 69, "total_relevant": 2, "total_reads": 8, "papers": [{"title": "RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for  Complex Task Solving", "score": 69, "session": "SD-2-1916", "pdf_url": "https://openreview.net/pdf/fea6d1106531a6d10e824159b158dcbffbc07cc2.pdf", "relevant": 2, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hwiwon Lee", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.0, "max_score": 69, "total_relevant": 3, "total_reads": 8, "papers": [{"title": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks", "score": 69, "session": "SD-3-2506", "pdf_url": "https://openreview.net/pdf/66d2a1a241b7979f0f8776b51e62c6b1f1bc7db8.pdf", "relevant": 3, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ziqi Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.0, "max_score": 69, "total_relevant": 3, "total_reads": 8, "papers": [{"title": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks", "score": 69, "session": "SD-3-2506", "pdf_url": "https://openreview.net/pdf/66d2a1a241b7979f0f8776b51e62c6b1f1bc7db8.pdf", "relevant": 3, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "LINGMING ZHANG", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.0, "max_score": 69, "total_relevant": 3, "total_reads": 8, "papers": [{"title": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks", "score": 69, "session": "SD-3-2506", "pdf_url": "https://openreview.net/pdf/66d2a1a241b7979f0f8776b51e62c6b1f1bc7db8.pdf", "relevant": 3, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "J Rosser", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.0, "max_score": 69, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement", "score": 69, "session": "SD-4-1201", "pdf_url": "https://openreview.net/pdf/237ca0099a7a95299cd0bae4b495038b19873e08.pdf", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jakob Nicolaus Foerster", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 40.5, "max_score": 69, "total_relevant": 6, "total_reads": 18, "papers": [{"title": "AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement", "score": 69, "session": "SD-4-1201", "pdf_url": "https://openreview.net/pdf/237ca0099a7a95299cd0bae4b495038b19873e08.pdf", "relevant": 3, "reads": 9}, {"title": "LILO: Learning to Reason at the Frontier of Learnability", "score": 12, "session": "SD-6-4905", "pdf_url": "https://openreview.net/pdf/81c8ea42739bd8be2077f7a518eeedc3ad8b7d5c.pdf", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ran Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.0, "max_score": 69, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play", "score": 69, "session": "SD-5-1908", "pdf_url": "https://openreview.net/pdf/d73f4b0dc08ba840cfc15d8e7e9be6ad4a9b52ce.pdf", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuchen Zhuang", "paper_count": 3, "highly_relevant_count": 0, "avg_score": 49.7, "max_score": 69, "total_relevant": 12, "total_reads": 30, "papers": [{"title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play", "score": 69, "session": "SD-5-1908", "pdf_url": "https://openreview.net/pdf/d73f4b0dc08ba840cfc15d8e7e9be6ad4a9b52ce.pdf", "relevant": 1, "reads": 4}, {"title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering", "score": 49, "session": "SD-2-2405", "pdf_url": "https://arxiv.org/pdf/2505.07782", "relevant": 6, "reads": 17}, {"title": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs", "score": 31, "session": "SD-1-3416", "pdf_url": "https://openreview.net/pdf/01546f97c01b32e5c0cf560fc4be7a511b46e042.pdf", "relevant": 5, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Carl Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.0, "max_score": 69, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play", "score": 69, "session": "SD-5-1908", "pdf_url": "https://openreview.net/pdf/d73f4b0dc08ba840cfc15d8e7e9be6ad4a9b52ce.pdf", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Linghao Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.0, "max_score": 68, "total_relevant": 6, "total_reads": 17, "papers": [{"title": "SWE-bench Goes Live!", "score": 68, "session": "SD-1-4009", "pdf_url": "https://arxiv.org/pdf/2505.23419", "relevant": 6, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shilin He", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.0, "max_score": 68, "total_relevant": 6, "total_reads": 17, "papers": [{"title": "SWE-bench Goes Live!", "score": 68, "session": "SD-1-4009", "pdf_url": "https://arxiv.org/pdf/2505.23419", "relevant": 6, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dongmei Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.0, "max_score": 68, "total_relevant": 6, "total_reads": 17, "papers": [{"title": "SWE-bench Goes Live!", "score": 68, "session": "SD-1-4009", "pdf_url": "https://arxiv.org/pdf/2505.23419", "relevant": 6, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zimeng Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.0, "max_score": 68, "total_relevant": 0, "total_reads": 6, "papers": [{"title": "MM-OPERA: Benchmarking Open-ended Association Reasoning for Large Vision-Language Models", "score": 68, "session": "SD-1-4612", "pdf_url": "https://arxiv.org/pdf/2510.26937", "relevant": 0, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jinxin Ke", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.0, "max_score": 68, "total_relevant": 0, "total_reads": 6, "papers": [{"title": "MM-OPERA: Benchmarking Open-ended Association Reasoning for Large Vision-Language Models", "score": 68, "session": "SD-1-4612", "pdf_url": "https://arxiv.org/pdf/2510.26937", "relevant": 0, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ziliang Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.0, "max_score": 68, "total_relevant": 0, "total_reads": 6, "papers": [{"title": "MM-OPERA: Benchmarking Open-ended Association Reasoning for Large Vision-Language Models", "score": 68, "session": "SD-1-4612", "pdf_url": "https://arxiv.org/pdf/2510.26937", "relevant": 0, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Thomas Kwa", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.0, "max_score": 68, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "Measuring AI Ability to Complete Long Software Tasks", "score": 68, "session": "SD-2-5312", "pdf_url": "https://openreview.net/pdf/402f09e70f9d9e6a99edbcaaa360692d05eec7ab.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ben West", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.0, "max_score": 68, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "Measuring AI Ability to Complete Long Software Tasks", "score": 68, "session": "SD-2-5312", "pdf_url": "https://openreview.net/pdf/402f09e70f9d9e6a99edbcaaa360692d05eec7ab.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lawrence Chan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.0, "max_score": 68, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "Measuring AI Ability to Complete Long Software Tasks", "score": 68, "session": "SD-2-5312", "pdf_url": "https://openreview.net/pdf/402f09e70f9d9e6a99edbcaaa360692d05eec7ab.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ziming Wei", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.0, "max_score": 68, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "score": 68, "session": "SD-4-1508", "pdf_url": "https://arxiv.org/pdf/2505.20148", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Bingqian Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.0, "max_score": 68, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "score": 68, "session": "SD-4-1508", "pdf_url": "https://arxiv.org/pdf/2505.20148", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaodan Liang", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 68, "total_relevant": 0, "total_reads": 4, "papers": [{"title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "score": 68, "session": "SD-4-1508", "pdf_url": "https://arxiv.org/pdf/2505.20148", "relevant": 0, "reads": 0}, {"title": "SeePhys:  Does Seeing Help Thinking? √¢¬Ä¬ì Benchmarking Vision-Based Physics Reasoning", "score": 30, "session": "SD-4-4603", "pdf_url": "https://arxiv.org/pdf/2505.19099", "relevant": 0, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chen Xiong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.0, "max_score": 68, "total_relevant": 3, "total_reads": 8, "papers": [{"title": "CoP: Agentic Red-teaming for Large Language Models using Composition of Principles", "score": 68, "session": "SD-6-1307", "pdf_url": "https://openreview.net/pdf/cb30447780175b5ce7f25d0e0277ddcc32156544.pdf", "relevant": 3, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pin-Yu Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.0, "max_score": 68, "total_relevant": 3, "total_reads": 8, "papers": [{"title": "CoP: Agentic Red-teaming for Large Language Models using Composition of Principles", "score": 68, "session": "SD-6-1307", "pdf_url": "https://openreview.net/pdf/cb30447780175b5ce7f25d0e0277ddcc32156544.pdf", "relevant": 3, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tsung-Yi Ho", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.0, "max_score": 68, "total_relevant": 3, "total_reads": 8, "papers": [{"title": "CoP: Agentic Red-teaming for Large Language Models using Composition of Principles", "score": 68, "session": "SD-6-1307", "pdf_url": "https://openreview.net/pdf/cb30447780175b5ce7f25d0e0277ddcc32156544.pdf", "relevant": 3, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Reginald McLean", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.0, "max_score": 67, "total_relevant": 12, "total_reads": 50, "papers": [{"title": "Meta-World+: An Improved, Standardized, RL Benchmark", "score": 67, "session": "SD-1-403", "pdf_url": "https://arxiv.org/pdf/2505.11289", "relevant": 12, "reads": 50}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Evangelos Chatzaroulas", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.0, "max_score": 67, "total_relevant": 12, "total_reads": 50, "papers": [{"title": "Meta-World+: An Improved, Standardized, RL Benchmark", "score": 67, "session": "SD-1-403", "pdf_url": "https://arxiv.org/pdf/2505.11289", "relevant": 12, "reads": 50}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pablo Samuel Castro", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.0, "max_score": 67, "total_relevant": 12, "total_reads": 50, "papers": [{"title": "Meta-World+: An Improved, Standardized, RL Benchmark", "score": 67, "session": "SD-1-403", "pdf_url": "https://arxiv.org/pdf/2505.11289", "relevant": 12, "reads": 50}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Parshin Shojaee", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.0, "max_score": 66, "total_relevant": 22, "total_reads": 41, "papers": [{"title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity", "score": 66, "session": "SD-1-1914", "pdf_url": "https://openreview.net/pdf/b58069a804c5fad686ceb13a131631201748c264.pdf", "relevant": 22, "reads": 41}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Seyed Iman Mirzadeh", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.0, "max_score": 66, "total_relevant": 22, "total_reads": 41, "papers": [{"title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity", "score": 66, "session": "SD-1-1914", "pdf_url": "https://openreview.net/pdf/b58069a804c5fad686ceb13a131631201748c264.pdf", "relevant": 22, "reads": 41}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mehrdad Farajtabar", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.0, "max_score": 66, "total_relevant": 22, "total_reads": 41, "papers": [{"title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity", "score": 66, "session": "SD-1-1914", "pdf_url": "https://openreview.net/pdf/b58069a804c5fad686ceb13a131631201748c264.pdf", "relevant": 22, "reads": 41}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junhong Shen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.0, "max_score": 66, "total_relevant": 4, "total_reads": 8, "papers": [{"title": "Thinking vs. Doing: Improving Agent Reasoning by  Scaling Test-Time Interaction", "score": 66, "session": "SD-2-515", "pdf_url": "https://openreview.net/pdf/20cba89765ebda48a0c183f17ceb68d13f90d0dc.pdf", "relevant": 4, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hao Bai", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.0, "max_score": 66, "total_relevant": 4, "total_reads": 8, "papers": [{"title": "Thinking vs. Doing: Improving Agent Reasoning by  Scaling Test-Time Interaction", "score": 66, "session": "SD-2-515", "pdf_url": "https://openreview.net/pdf/20cba89765ebda48a0c183f17ceb68d13f90d0dc.pdf", "relevant": 4, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Aviral Kumar", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 61.5, "max_score": 66, "total_relevant": 10, "total_reads": 23, "papers": [{"title": "Thinking vs. Doing: Improving Agent Reasoning by  Scaling Test-Time Interaction", "score": 66, "session": "SD-2-515", "pdf_url": "https://openreview.net/pdf/20cba89765ebda48a0c183f17ceb68d13f90d0dc.pdf", "relevant": 4, "reads": 8}, {"title": "Reasoning as an Adaptive Defense for Safety", "score": 57, "session": "SD-1-405", "pdf_url": "https://openreview.net/pdf/c036ebd40ea82ab94b9badc0028faf7e94456ca1.pdf", "relevant": 6, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wei Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.0, "max_score": 66, "total_relevant": 1, "total_reads": 8, "papers": [{"title": "Can Large Language Models Master Complex Card Games?", "score": 66, "session": "SD-2-101", "pdf_url": "https://openreview.net/pdf/1d7991c179addeedf33ce77dc2ffd0e475f69b2c.pdf", "relevant": 1, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Fuqing Bie", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.0, "max_score": 66, "total_relevant": 1, "total_reads": 8, "papers": [{"title": "Can Large Language Models Master Complex Card Games?", "score": 66, "session": "SD-2-101", "pdf_url": "https://openreview.net/pdf/1d7991c179addeedf33ce77dc2ffd0e475f69b2c.pdf", "relevant": 1, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jie Tang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.0, "max_score": 66, "total_relevant": 1, "total_reads": 8, "papers": [{"title": "Can Large Language Models Master Complex Card Games?", "score": 66, "session": "SD-2-101", "pdf_url": "https://openreview.net/pdf/1d7991c179addeedf33ce77dc2ffd0e475f69b2c.pdf", "relevant": 1, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Daniel O&#x27;Malley", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.0, "max_score": 65, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "Benchmarking Large Language Models with Integer Sequence Generation Tasks", "score": 65, "session": "SD-1-2800", "pdf_url": "https://arxiv.org/pdf/2411.04372", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Manish Bhattarai", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.0, "max_score": 65, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "Benchmarking Large Language Models with Integer Sequence Generation Tasks", "score": 65, "session": "SD-1-2800", "pdf_url": "https://arxiv.org/pdf/2411.04372", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Javier E. Santos", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.0, "max_score": 65, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "Benchmarking Large Language Models with Integer Sequence Generation Tasks", "score": 65, "session": "SD-1-2800", "pdf_url": "https://arxiv.org/pdf/2411.04372", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junyan Cheng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.0, "max_score": 65, "total_relevant": 8, "total_reads": 41, "papers": [{"title": "Language Modeling by Language Models", "score": 65, "session": "SD-2-1512", "pdf_url": "https://openreview.net/pdf/1dda401bc2d6f8ee17a263ac3f358eb51e094d8e.pdf", "relevant": 8, "reads": 41}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Peter Clark", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.0, "max_score": 65, "total_relevant": 8, "total_reads": 41, "papers": [{"title": "Language Modeling by Language Models", "score": 65, "session": "SD-2-1512", "pdf_url": "https://openreview.net/pdf/1dda401bc2d6f8ee17a263ac3f358eb51e094d8e.pdf", "relevant": 8, "reads": 41}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kyle Richardson", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.0, "max_score": 65, "total_relevant": 8, "total_reads": 41, "papers": [{"title": "Language Modeling by Language Models", "score": 65, "session": "SD-2-1512", "pdf_url": "https://openreview.net/pdf/1dda401bc2d6f8ee17a263ac3f358eb51e094d8e.pdf", "relevant": 8, "reads": 41}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yi-Kai Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.0, "max_score": 65, "total_relevant": 0, "total_reads": 4, "papers": [{"title": "Let the LLM Stick to Its Strengths: Learning to Route Economical LLM", "score": 65, "session": "SD-2-4203", "pdf_url": "https://openreview.net/pdf/f8ce3f48c335fa784e0e6aef63e636d13c30d93d.pdf", "relevant": 0, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shiyin Lu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.0, "max_score": 65, "total_relevant": 0, "total_reads": 4, "papers": [{"title": "Let the LLM Stick to Its Strengths: Learning to Route Economical LLM", "score": 65, "session": "SD-2-4203", "pdf_url": "https://openreview.net/pdf/f8ce3f48c335fa784e0e6aef63e636d13c30d93d.pdf", "relevant": 0, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Han-Jia Ye", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.0, "max_score": 65, "total_relevant": 0, "total_reads": 4, "papers": [{"title": "Let the LLM Stick to Its Strengths: Learning to Route Economical LLM", "score": 65, "session": "SD-2-4203", "pdf_url": "https://openreview.net/pdf/f8ce3f48c335fa784e0e6aef63e636d13c30d93d.pdf", "relevant": 0, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ori Press", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.0, "max_score": 65, "total_relevant": 10, "total_reads": 18, "papers": [{"title": "AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?", "score": 65, "session": "SD-4-2514", "pdf_url": "https://arxiv.org/pdf/2507.15887", "relevant": 10, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Brandon Amos", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.0, "max_score": 65, "total_relevant": 10, "total_reads": 18, "papers": [{"title": "AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?", "score": 65, "session": "SD-4-2514", "pdf_url": "https://arxiv.org/pdf/2507.15887", "relevant": 10, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ofir Press", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.0, "max_score": 65, "total_relevant": 10, "total_reads": 18, "papers": [{"title": "AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?", "score": 65, "session": "SD-4-2514", "pdf_url": "https://arxiv.org/pdf/2507.15887", "relevant": 10, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yige Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.0, "max_score": 65, "total_relevant": 15, "total_reads": 28, "papers": [{"title": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks and Defenses on Large Language Models", "score": 65, "session": "SD-5-1405", "pdf_url": "https://arxiv.org/pdf/2408.12798", "relevant": 15, "reads": 28}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hanxun Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.0, "max_score": 65, "total_relevant": 15, "total_reads": 28, "papers": [{"title": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks and Defenses on Large Language Models", "score": 65, "session": "SD-5-1405", "pdf_url": "https://arxiv.org/pdf/2408.12798", "relevant": 15, "reads": 28}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jun Sun", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.0, "max_score": 65, "total_relevant": 15, "total_reads": 28, "papers": [{"title": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks and Defenses on Large Language Models", "score": 65, "session": "SD-5-1405", "pdf_url": "https://arxiv.org/pdf/2408.12798", "relevant": 15, "reads": 28}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenxuan Shi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "DeepDiver: Adaptive Web-Search Intensity Scaling via Reinforcement Learning", "score": 64, "session": "SD-1-5301", "pdf_url": "https://openreview.net/pdf/4da779e56e6d170cf4fc07f2af1015f08b35314e.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haochen Tan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "DeepDiver: Adaptive Web-Search Intensity Scaling via Reinforcement Learning", "score": 64, "session": "SD-1-5301", "pdf_url": "https://openreview.net/pdf/4da779e56e6d170cf4fc07f2af1015f08b35314e.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lifeng Shang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "DeepDiver: Adaptive Web-Search Intensity Scaling via Reinforcement Learning", "score": 64, "session": "SD-1-5301", "pdf_url": "https://openreview.net/pdf/4da779e56e6d170cf4fc07f2af1015f08b35314e.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Thomas Kuntz", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64, "total_relevant": 9, "total_reads": 13, "papers": [{"title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents", "score": 64, "session": "SD-2-1312", "pdf_url": "https://arxiv.org/pdf/2506.14866", "relevant": 9, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Agatha Duzan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64, "total_relevant": 9, "total_reads": 13, "papers": [{"title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents", "score": 64, "session": "SD-2-1312", "pdf_url": "https://arxiv.org/pdf/2506.14866", "relevant": 9, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Maksym Andriushchenko", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64, "total_relevant": 9, "total_reads": 13, "papers": [{"title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents", "score": 64, "session": "SD-2-1312", "pdf_url": "https://arxiv.org/pdf/2506.14866", "relevant": 9, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mingyang Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64, "total_relevant": 23, "total_reads": 34, "papers": [{"title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning", "score": 64, "session": "SD-3-1900", "pdf_url": "https://openreview.net/pdf/3a4c411411ec8827c21b081ac099f93878bb8269.pdf", "relevant": 23, "reads": 34}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Linzhuang Sun", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64, "total_relevant": 23, "total_reads": 34, "papers": [{"title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning", "score": 64, "session": "SD-3-1900", "pdf_url": "https://openreview.net/pdf/3a4c411411ec8827c21b081ac099f93878bb8269.pdf", "relevant": 23, "reads": 34}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Weipeng Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64, "total_relevant": 23, "total_reads": 34, "papers": [{"title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning", "score": 64, "session": "SD-3-1900", "pdf_url": "https://openreview.net/pdf/3a4c411411ec8827c21b081ac099f93878bb8269.pdf", "relevant": 23, "reads": 34}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Li Kang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64, "total_relevant": 2, "total_reads": 19, "papers": [{"title": "VIKI√¢¬Ä¬ëR: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning", "score": 64, "session": "SD-4-2310", "pdf_url": "https://arxiv.org/pdf/2506.09049", "relevant": 2, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiufeng Song", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64, "total_relevant": 2, "total_reads": 19, "papers": [{"title": "VIKI√¢¬Ä¬ëR: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning", "score": 64, "session": "SD-4-2310", "pdf_url": "https://arxiv.org/pdf/2506.09049", "relevant": 2, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhenfei Yin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64, "total_relevant": 2, "total_reads": 19, "papers": [{"title": "VIKI√¢¬Ä¬ëR: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning", "score": 64, "session": "SD-4-2310", "pdf_url": "https://arxiv.org/pdf/2506.09049", "relevant": 2, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chenyue Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64, "total_relevant": 1, "total_reads": 7, "papers": [{"title": "AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model for Atmospheric Science", "score": 64, "session": "SD-6-1606", "pdf_url": "https://arxiv.org/pdf/2502.01159", "relevant": 1, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wen Deng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64, "total_relevant": 1, "total_reads": 7, "papers": [{"title": "AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model for Atmospheric Science", "score": 64, "session": "SD-6-1606", "pdf_url": "https://arxiv.org/pdf/2502.01159", "relevant": 1, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Binhang Yuan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64, "total_relevant": 1, "total_reads": 7, "papers": [{"title": "AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model for Atmospheric Science", "score": 64, "session": "SD-6-1606", "pdf_url": "https://arxiv.org/pdf/2502.01159", "relevant": 1, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qiying Yu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 63.0, "max_score": 63, "total_relevant": 41, "total_reads": 50, "papers": [{"title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale", "score": 63, "session": "SD-1-316", "pdf_url": "https://openreview.net/pdf/a5ca4684c1debe30e4fde4bd063a262d61e13db7.pdf", "relevant": 41, "reads": 50}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zheng Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 63.0, "max_score": 63, "total_relevant": 41, "total_reads": 50, "papers": [{"title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale", "score": 63, "session": "SD-1-316", "pdf_url": "https://openreview.net/pdf/a5ca4684c1debe30e4fde4bd063a262d61e13db7.pdf", "relevant": 41, "reads": 50}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junhao Shi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 63.0, "max_score": 63, "total_relevant": 6, "total_reads": 39, "papers": [{"title": "World-aware Planning Narratives Enhance Large Vision-Language Model Planner", "score": 63, "session": "SD-4-2404", "pdf_url": "https://openreview.net/pdf/c4dfdf9e91cc4527c4c879b52d8cc8d352f626ce.pdf", "relevant": 6, "reads": 39}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhaoye Fei", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 63.0, "max_score": 63, "total_relevant": 6, "total_reads": 39, "papers": [{"title": "World-aware Planning Narratives Enhance Large Vision-Language Model Planner", "score": 63, "session": "SD-4-2404", "pdf_url": "https://openreview.net/pdf/c4dfdf9e91cc4527c4c879b52d8cc8d352f626ce.pdf", "relevant": 6, "reads": 39}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xipeng Qiu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 63.0, "max_score": 63, "total_relevant": 6, "total_reads": 39, "papers": [{"title": "World-aware Planning Narratives Enhance Large Vision-Language Model Planner", "score": 63, "session": "SD-4-2404", "pdf_url": "https://openreview.net/pdf/c4dfdf9e91cc4527c4c879b52d8cc8d352f626ce.pdf", "relevant": 6, "reads": 39}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenkai Fang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.0, "max_score": 62, "total_relevant": 13, "total_reads": 34, "papers": [{"title": "SeRL: Self-play Reinforcement Learning for Large Language Models with Limited Data", "score": 62, "session": "SD-2-411", "pdf_url": "https://openreview.net/pdf/f20577e67650da84d5c9396fc93fb971782ca925.pdf", "relevant": 13, "reads": 34}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shunyu Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.0, "max_score": 62, "total_relevant": 13, "total_reads": 34, "papers": [{"title": "SeRL: Self-play Reinforcement Learning for Large Language Models with Limited Data", "score": 62, "session": "SD-2-411", "pdf_url": "https://openreview.net/pdf/f20577e67650da84d5c9396fc93fb971782ca925.pdf", "relevant": 13, "reads": 34}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dacheng Tao", "paper_count": 3, "highly_relevant_count": 0, "avg_score": 41.3, "max_score": 62, "total_relevant": 25, "total_reads": 60, "papers": [{"title": "SeRL: Self-play Reinforcement Learning for Large Language Models with Limited Data", "score": 62, "session": "SD-2-411", "pdf_url": "https://openreview.net/pdf/f20577e67650da84d5c9396fc93fb971782ca925.pdf", "relevant": 13, "reads": 34}, {"title": "Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler", "score": 34, "session": "SD-4-1205", "pdf_url": "https://openreview.net/pdf/2b29a4e872717270eff659096ceb805ef86a1735.pdf", "relevant": 3, "reads": 5}, {"title": "Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning", "score": 28, "session": "SD-6-3404", "pdf_url": "https://openreview.net/pdf/4c696fcdaaa5b6e8b53a1bf9e94c8993ee0cd433.pdf", "relevant": 9, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Anna Sokol", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.0, "max_score": 61, "total_relevant": 5, "total_reads": 9, "papers": [{"title": "BenchmarkCards: Standardized Documentation for Large Language Model Benchmarks", "score": 61, "session": "SD-3-1016", "pdf_url": "https://arxiv.org/pdf/2410.12974", "relevant": 5, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Elizabeth Daly", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.0, "max_score": 61, "total_relevant": 5, "total_reads": 9, "papers": [{"title": "BenchmarkCards: Standardized Documentation for Large Language Model Benchmarks", "score": 61, "session": "SD-3-1016", "pdf_url": "https://arxiv.org/pdf/2410.12974", "relevant": 5, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Nitesh Chawla", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.0, "max_score": 61, "total_relevant": 5, "total_reads": 9, "papers": [{"title": "BenchmarkCards: Standardized Documentation for Large Language Model Benchmarks", "score": 61, "session": "SD-3-1016", "pdf_url": "https://arxiv.org/pdf/2410.12974", "relevant": 5, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yang Yue", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.0, "max_score": 61, "total_relevant": 50, "total_reads": 89, "papers": [{"title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?", "score": 61, "session": "SD-3-1906", "pdf_url": "https://openreview.net/pdf/c3957c2dc397dd6f7bf1e3da21cebaeca53844af.pdf", "relevant": 50, "reads": 89}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhiqi Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.0, "max_score": 61, "total_relevant": 50, "total_reads": 89, "papers": [{"title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?", "score": 61, "session": "SD-3-1906", "pdf_url": "https://openreview.net/pdf/c3957c2dc397dd6f7bf1e3da21cebaeca53844af.pdf", "relevant": 50, "reads": 89}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Gao Huang", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 43.5, "max_score": 61, "total_relevant": 72, "total_reads": 132, "papers": [{"title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?", "score": 61, "session": "SD-3-1906", "pdf_url": "https://openreview.net/pdf/c3957c2dc397dd6f7bf1e3da21cebaeca53844af.pdf", "relevant": 50, "reads": 89}, {"title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data", "score": 26, "session": "SD-6-1908", "pdf_url": "https://openreview.net/pdf/34a1a72a0a54db41248d9ad8862c78e55ac789d9.pdf", "relevant": 22, "reads": 43}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mert Cemri", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.0, "max_score": 61, "total_relevant": 11, "total_reads": 13, "papers": [{"title": "Why Do Multi-Agent LLM Systems Fail?", "score": 61, "session": "SD-3-110", "pdf_url": "https://arxiv.org/pdf/2503.13657", "relevant": 11, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Melissa Z Pan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.0, "max_score": 61, "total_relevant": 11, "total_reads": 13, "papers": [{"title": "Why Do Multi-Agent LLM Systems Fail?", "score": 61, "session": "SD-3-110", "pdf_url": "https://arxiv.org/pdf/2503.13657", "relevant": 11, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ion Stoica", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.0, "max_score": 61, "total_relevant": 11, "total_reads": 13, "papers": [{"title": "Why Do Multi-Agent LLM Systems Fail?", "score": 61, "session": "SD-3-110", "pdf_url": "https://arxiv.org/pdf/2503.13657", "relevant": 11, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jialong Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 60.0, "max_score": 60, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling", "score": 60, "session": "SD-1-1105", "pdf_url": "https://openreview.net/pdf/12984819e947af4200e71748ef8d3b07f7b029cd.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lichao Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 60.0, "max_score": 60, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling", "score": 60, "session": "SD-1-1105", "pdf_url": "https://openreview.net/pdf/12984819e947af4200e71748ef8d3b07f7b029cd.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiao Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 60.0, "max_score": 60, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling", "score": 60, "session": "SD-1-1105", "pdf_url": "https://openreview.net/pdf/12984819e947af4200e71748ef8d3b07f7b029cd.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenbo Hu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 60.0, "max_score": 60, "total_relevant": 8, "total_reads": 22, "papers": [{"title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model", "score": 60, "session": "SD-4-4806", "pdf_url": "https://openreview.net/pdf/8e3474ff3b680185f73a397352845c1347b77d73.pdf", "relevant": 8, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kangrui Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.0, "max_score": 59, "total_relevant": 6, "total_reads": 28, "papers": [{"title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents", "score": 59, "session": "SD-1-5502", "pdf_url": "https://openreview.net/pdf/ed11882b04656406635941c99bb0b5dbf475a088.pdf", "relevant": 6, "reads": 28}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pingyue Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.0, "max_score": 59, "total_relevant": 6, "total_reads": 28, "papers": [{"title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents", "score": 59, "session": "SD-1-5502", "pdf_url": "https://openreview.net/pdf/ed11882b04656406635941c99bb0b5dbf475a088.pdf", "relevant": 6, "reads": 28}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Manling Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.0, "max_score": 59, "total_relevant": 6, "total_reads": 28, "papers": [{"title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents", "score": 59, "session": "SD-1-5502", "pdf_url": "https://openreview.net/pdf/ed11882b04656406635941c99bb0b5dbf475a088.pdf", "relevant": 6, "reads": 28}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hao Kang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.0, "max_score": 59, "total_relevant": 0, "total_reads": 3, "papers": [{"title": "Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs", "score": 59, "session": "SD-2-303", "pdf_url": "https://openreview.net/pdf/13fb7451afdc5c8796e7166332feddd0c8a38f8e.pdf", "relevant": 0, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qingru Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.0, "max_score": 59, "total_relevant": 0, "total_reads": 3, "papers": [{"title": "Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs", "score": 59, "session": "SD-2-303", "pdf_url": "https://openreview.net/pdf/13fb7451afdc5c8796e7166332feddd0c8a38f8e.pdf", "relevant": 0, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tsachy Weissman", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.0, "max_score": 59, "total_relevant": 0, "total_reads": 3, "papers": [{"title": "Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs", "score": 59, "session": "SD-2-303", "pdf_url": "https://openreview.net/pdf/13fb7451afdc5c8796e7166332feddd0c8a38f8e.pdf", "relevant": 0, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhengyu Hu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.0, "max_score": 59, "total_relevant": 2, "total_reads": 11, "papers": [{"title": "Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study", "score": 59, "session": "SD-4-1905", "pdf_url": "https://openreview.net/pdf/0ced283865c2dd5717208f09588d27256b2fa260.pdf", "relevant": 2, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jianxun Lian", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.0, "max_score": 59, "total_relevant": 2, "total_reads": 11, "papers": [{"title": "Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study", "score": 59, "session": "SD-4-1905", "pdf_url": "https://openreview.net/pdf/0ced283865c2dd5717208f09588d27256b2fa260.pdf", "relevant": 2, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hui Xiong", "paper_count": 3, "highly_relevant_count": 0, "avg_score": 28.7, "max_score": 59, "total_relevant": 11, "total_reads": 69, "papers": [{"title": "Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study", "score": 59, "session": "SD-4-1905", "pdf_url": "https://openreview.net/pdf/0ced283865c2dd5717208f09588d27256b2fa260.pdf", "relevant": 2, "reads": 11}, {"title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs", "score": 22, "session": "SD-6-214", "pdf_url": "https://openreview.net/pdf/de9fc962acefe20dd0d80073eadeb19263afeb06.pdf", "relevant": 4, "reads": 21}, {"title": "See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model", "score": 5, "session": "SD-5-5206", "pdf_url": "https://openreview.net/pdf/bcef03fe8da8c459269e00eeb6626d6584a0ba4c.pdf", "relevant": 5, "reads": 37}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qianlan Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.0, "max_score": 59, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Self-Guided Hierarchical Exploration for Generalist Foundation Model Web Agents", "score": 59, "session": "SD-4-5211", "pdf_url": "https://openreview.net/pdf/23935fe967684b7bdc286739474d07c94da9cc76.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiangjun Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.0, "max_score": 59, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Self-Guided Hierarchical Exploration for Generalist Foundation Model Web Agents", "score": 59, "session": "SD-4-5211", "pdf_url": "https://openreview.net/pdf/23935fe967684b7bdc286739474d07c94da9cc76.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yu-Xiong Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.0, "max_score": 59, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Self-Guided Hierarchical Exploration for Generalist Foundation Model Web Agents", "score": 59, "session": "SD-4-5211", "pdf_url": "https://openreview.net/pdf/23935fe967684b7bdc286739474d07c94da9cc76.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiaxin Wen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.0, "max_score": 58, "total_relevant": 6, "total_reads": 10, "papers": [{"title": "Predicting Empirical AI Research Outcomes with Language Models", "score": 58, "session": "SD-1-2413", "pdf_url": "https://openreview.net/pdf/149b9e435472a05ff9bc8f5c3138e681d73132fd.pdf", "relevant": 6, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chenglei Si", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.0, "max_score": 58, "total_relevant": 6, "total_reads": 10, "papers": [{"title": "Predicting Empirical AI Research Outcomes with Language Models", "score": 58, "session": "SD-1-2413", "pdf_url": "https://openreview.net/pdf/149b9e435472a05ff9bc8f5c3138e681d73132fd.pdf", "relevant": 6, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shi Feng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.0, "max_score": 58, "total_relevant": 6, "total_reads": 10, "papers": [{"title": "Predicting Empirical AI Research Outcomes with Language Models", "score": 58, "session": "SD-1-2413", "pdf_url": "https://openreview.net/pdf/149b9e435472a05ff9bc8f5c3138e681d73132fd.pdf", "relevant": 6, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Milena Rmus", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.0, "max_score": 58, "total_relevant": 6, "total_reads": 17, "papers": [{"title": "Generating Computational Cognitive models using Large Language Models", "score": 58, "session": "SD-1-2010", "pdf_url": "https://openreview.net/pdf/71914784b067b8a89e5ba9e648c3f3fe0bef8659.pdf", "relevant": 6, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Akshay Kumar Jagadish", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.0, "max_score": 58, "total_relevant": 6, "total_reads": 17, "papers": [{"title": "Generating Computational Cognitive models using Large Language Models", "score": 58, "session": "SD-1-2010", "pdf_url": "https://openreview.net/pdf/71914784b067b8a89e5ba9e648c3f3fe0bef8659.pdf", "relevant": 6, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Eric Schulz", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.0, "max_score": 58, "total_relevant": 6, "total_reads": 17, "papers": [{"title": "Generating Computational Cognitive models using Large Language Models", "score": 58, "session": "SD-1-2010", "pdf_url": "https://openreview.net/pdf/71914784b067b8a89e5ba9e648c3f3fe0bef8659.pdf", "relevant": 6, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Andy Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.0, "max_score": 58, "total_relevant": 5, "total_reads": 12, "papers": [{"title": "AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration", "score": 58, "session": "SD-4-903", "pdf_url": "https://openreview.net/pdf/e53fb762e883d79a783bab07668988038d9e5162.pdf", "relevant": 5, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kevin Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.0, "max_score": 58, "total_relevant": 5, "total_reads": 12, "papers": [{"title": "AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration", "score": 58, "session": "SD-4-903", "pdf_url": "https://openreview.net/pdf/e53fb762e883d79a783bab07668988038d9e5162.pdf", "relevant": 5, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Bo Li", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 46.5, "max_score": 58, "total_relevant": 7, "total_reads": 21, "papers": [{"title": "AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration", "score": 58, "session": "SD-4-903", "pdf_url": "https://openreview.net/pdf/e53fb762e883d79a783bab07668988038d9e5162.pdf", "relevant": 5, "reads": 12}, {"title": "PolyGuard: Massive Multi-Domain Safety Policy-Grounded Guardrail Dataset", "score": 35, "session": "SD-2-1308", "pdf_url": "https://arxiv.org/pdf/2506.19054", "relevant": 2, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Liyan Tang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.0, "max_score": 57, "total_relevant": 5, "total_reads": 11, "papers": [{"title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "score": 57, "session": "SD-1-4709", "pdf_url": "https://arxiv.org/pdf/2505.13444", "relevant": 5, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Grace Kim", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.0, "max_score": 57, "total_relevant": 5, "total_reads": 11, "papers": [{"title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "score": 57, "session": "SD-1-4709", "pdf_url": "https://arxiv.org/pdf/2505.13444", "relevant": 5, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Greg Durrett", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.0, "max_score": 57, "total_relevant": 5, "total_reads": 11, "papers": [{"title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "score": 57, "session": "SD-1-4709", "pdf_url": "https://arxiv.org/pdf/2505.13444", "relevant": 5, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chenhang Cui", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.0, "max_score": 57, "total_relevant": 9, "total_reads": 17, "papers": [{"title": "Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models", "score": 57, "session": "SD-1-5510", "pdf_url": "https://openreview.net/pdf/8babb592735eb9e46455028c5428dfc816f08161.pdf", "relevant": 9, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Gelei Deng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.0, "max_score": 57, "total_relevant": 9, "total_reads": 17, "papers": [{"title": "Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models", "score": 57, "session": "SD-1-5510", "pdf_url": "https://openreview.net/pdf/8babb592735eb9e46455028c5428dfc816f08161.pdf", "relevant": 9, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tat-Seng Chua", "paper_count": 3, "highly_relevant_count": 0, "avg_score": 44.7, "max_score": 57, "total_relevant": 14, "total_reads": 33, "papers": [{"title": "Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models", "score": 57, "session": "SD-1-5510", "pdf_url": "https://openreview.net/pdf/8babb592735eb9e46455028c5428dfc816f08161.pdf", "relevant": 9, "reads": 17}, {"title": "L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models", "score": 51, "session": "SD-4-1911", "pdf_url": "https://openreview.net/pdf/03edf649a9f42cdf6d921cb59599e7130120540a.pdf", "relevant": 3, "reads": 10}, {"title": "On Reasoning Strength Planning in Large Reasoning Models", "score": 26, "session": "SD-5-5313", "pdf_url": "https://openreview.net/pdf/ec0170f131842f1aaee5993a19df22764b470213.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Taeyoun Kim", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.0, "max_score": 57, "total_relevant": 6, "total_reads": 15, "papers": [{"title": "Reasoning as an Adaptive Defense for Safety", "score": 57, "session": "SD-1-405", "pdf_url": "https://openreview.net/pdf/c036ebd40ea82ab94b9badc0028faf7e94456ca1.pdf", "relevant": 6, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Fahim Tajwar", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.0, "max_score": 57, "total_relevant": 6, "total_reads": 15, "papers": [{"title": "Reasoning as an Adaptive Defense for Safety", "score": 57, "session": "SD-1-405", "pdf_url": "https://openreview.net/pdf/c036ebd40ea82ab94b9badc0028faf7e94456ca1.pdf", "relevant": 6, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junwoo Park", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.0, "max_score": 57, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Delving into Large Language Models for Effective Time-Series Anomaly Detection", "score": 57, "session": "SD-3-2410", "pdf_url": "https://openreview.net/pdf/ff3e7df135cd4038ebbe31199752645c9946fa1e.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kyudan Jung", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.0, "max_score": 57, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Delving into Large Language Models for Effective Time-Series Anomaly Detection", "score": 57, "session": "SD-3-2410", "pdf_url": "https://openreview.net/pdf/ff3e7df135cd4038ebbe31199752645c9946fa1e.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jaewoong Cho", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.0, "max_score": 57, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Delving into Large Language Models for Effective Time-Series Anomaly Detection", "score": 57, "session": "SD-3-2410", "pdf_url": "https://openreview.net/pdf/ff3e7df135cd4038ebbe31199752645c9946fa1e.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Emil Biju", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.0, "max_score": 57, "total_relevant": 3, "total_reads": 4, "papers": [{"title": "SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models", "score": 57, "session": "SD-3-4317", "pdf_url": "https://openreview.net/pdf/2ac09a3b4dda169b90bb239fc45cb980a8bc3efd.pdf", "relevant": 3, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shayan Talaei", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.0, "max_score": 57, "total_relevant": 3, "total_reads": 4, "papers": [{"title": "SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models", "score": 57, "session": "SD-3-4317", "pdf_url": "https://openreview.net/pdf/2ac09a3b4dda169b90bb239fc45cb980a8bc3efd.pdf", "relevant": 3, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Amin Saberi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.0, "max_score": 57, "total_relevant": 3, "total_reads": 4, "papers": [{"title": "SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models", "score": 57, "session": "SD-3-4317", "pdf_url": "https://openreview.net/pdf/2ac09a3b4dda169b90bb239fc45cb980a8bc3efd.pdf", "relevant": 3, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiangqi Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 0, "total_reads": 7, "papers": [{"title": "AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking", "score": 55, "session": "SD-1-5418", "pdf_url": "https://openreview.net/pdf/dd1128b101a8e529cfa219b76424da7861923e32.pdf", "relevant": 0, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yue Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 0, "total_reads": 7, "papers": [{"title": "AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking", "score": 55, "session": "SD-1-5418", "pdf_url": "https://openreview.net/pdf/dd1128b101a8e529cfa219b76424da7861923e32.pdf", "relevant": 0, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiangliang Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 0, "total_reads": 7, "papers": [{"title": "AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking", "score": 55, "session": "SD-1-5418", "pdf_url": "https://openreview.net/pdf/dd1128b101a8e529cfa219b76424da7861923e32.pdf", "relevant": 0, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wanjia Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 5, "total_reads": 7, "papers": [{"title": "SiriuS: Self-improving Multi-agent Systems via Bootstrapped Reasoning", "score": 55, "session": "SD-1-5406", "pdf_url": "https://openreview.net/pdf/856c35f60eccff17ac8726de9ca5f7fbf9bcf3ee.pdf", "relevant": 5, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mert Yuksekgonul", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 5, "total_reads": 7, "papers": [{"title": "SiriuS: Self-improving Multi-agent Systems via Bootstrapped Reasoning", "score": 55, "session": "SD-1-5406", "pdf_url": "https://openreview.net/pdf/856c35f60eccff17ac8726de9ca5f7fbf9bcf3ee.pdf", "relevant": 5, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "James Zou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 5, "total_reads": 7, "papers": [{"title": "SiriuS: Self-improving Multi-agent Systems via Bootstrapped Reasoning", "score": 55, "session": "SD-1-5406", "pdf_url": "https://openreview.net/pdf/856c35f60eccff17ac8726de9ca5f7fbf9bcf3ee.pdf", "relevant": 5, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Fuxiang Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 12, "total_reads": 28, "papers": [{"title": "Incentivizing LLMs to Self-Verify Their Answers", "score": 55, "session": "SD-2-313", "pdf_url": "https://openreview.net/pdf/ea6649a9cfb1c181a137632923e930e4e14e6ad3.pdf", "relevant": 12, "reads": 28}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiacheng Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 12, "total_reads": 28, "papers": [{"title": "Incentivizing LLMs to Self-Verify Their Answers", "score": 55, "session": "SD-2-313", "pdf_url": "https://openreview.net/pdf/ea6649a9cfb1c181a137632923e930e4e14e6ad3.pdf", "relevant": 12, "reads": 28}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Bo An", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 12, "total_reads": 28, "papers": [{"title": "Incentivizing LLMs to Self-Verify Their Answers", "score": 55, "session": "SD-2-313", "pdf_url": "https://openreview.net/pdf/ea6649a9cfb1c181a137632923e930e4e14e6ad3.pdf", "relevant": 12, "reads": 28}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Changlun Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Time Travel is Cheating: Going Live with DeepFund for Real-Time Fund Investment Benchmarking", "score": 55, "session": "SD-2-2407", "pdf_url": "https://arxiv.org/pdf/2505.11065", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yao SHI", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Time Travel is Cheating: Going Live with DeepFund for Real-Time Fund Investment Benchmarking", "score": 55, "session": "SD-2-2407", "pdf_url": "https://arxiv.org/pdf/2505.11065", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuyu Luo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Time Travel is Cheating: Going Live with DeepFund for Real-Time Fund Investment Benchmarking", "score": 55, "session": "SD-2-2407", "pdf_url": "https://arxiv.org/pdf/2505.11065", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haoyang Fang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 6, "total_reads": 10, "papers": [{"title": "MLZero: A Multi-Agent System for End-to-end Machine Learning Automation", "score": 55, "session": "SD-4-1503", "pdf_url": "https://openreview.net/pdf/55f28109c8ee532fe1c950142c23f6efd636a79e.pdf", "relevant": 6, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Boran Han", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 6, "total_reads": 10, "papers": [{"title": "MLZero: A Multi-Agent System for End-to-end Machine Learning Automation", "score": 55, "session": "SD-4-1503", "pdf_url": "https://openreview.net/pdf/55f28109c8ee532fe1c950142c23f6efd636a79e.pdf", "relevant": 6, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "George Karypis", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 6, "total_reads": 10, "papers": [{"title": "MLZero: A Multi-Agent System for End-to-end Machine Learning Automation", "score": 55, "session": "SD-4-1503", "pdf_url": "https://openreview.net/pdf/55f28109c8ee532fe1c950142c23f6efd636a79e.pdf", "relevant": 6, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Daman Arora", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 25, "total_reads": 52, "papers": [{"title": "Training Language Models to Reason Efficiently", "score": 55, "session": "SD-4-3600", "pdf_url": "https://openreview.net/pdf/c169ad531f568624e1f7af8211b9ff6b12391b63.pdf", "relevant": 25, "reads": 52}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Andrea Zanette", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 25, "total_reads": 52, "papers": [{"title": "Training Language Models to Reason Efficiently", "score": 55, "session": "SD-4-3600", "pdf_url": "https://openreview.net/pdf/c169ad531f568624e1f7af8211b9ff6b12391b63.pdf", "relevant": 25, "reads": 52}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haolin Pan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 2, "total_reads": 12, "papers": [{"title": "Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning", "score": 55, "session": "SD-5-410", "pdf_url": "https://openreview.net/pdf/7b8e4f95272ab1e9e3900b30ef9647d058b6b034.pdf", "relevant": 2, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hongyu Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 2, "total_reads": 12, "papers": [{"title": "Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning", "score": 55, "session": "SD-5-410", "pdf_url": "https://openreview.net/pdf/7b8e4f95272ab1e9e3900b30ef9647d058b6b034.pdf", "relevant": 2, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yanjun Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 2, "total_reads": 12, "papers": [{"title": "Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning", "score": 55, "session": "SD-5-410", "pdf_url": "https://openreview.net/pdf/7b8e4f95272ab1e9e3900b30ef9647d058b6b034.pdf", "relevant": 2, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Weijie Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 2, "total_reads": 21, "papers": [{"title": "PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments", "score": 55, "session": "SD-6-3601", "pdf_url": "https://openreview.net/pdf/89b1018647a9f5681e3ad0ed184e80229e3997ee.pdf", "relevant": 2, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xuantang Xiong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 2, "total_reads": 21, "papers": [{"title": "PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments", "score": 55, "session": "SD-6-3601", "pdf_url": "https://openreview.net/pdf/89b1018647a9f5681e3ad0ed184e80229e3997ee.pdf", "relevant": 2, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jinqiao Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55, "total_relevant": 2, "total_reads": 21, "papers": [{"title": "PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments", "score": 55, "session": "SD-6-3601", "pdf_url": "https://openreview.net/pdf/89b1018647a9f5681e3ad0ed184e80229e3997ee.pdf", "relevant": 2, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zihan Zheng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.0, "max_score": 53, "total_relevant": 4, "total_reads": 6, "papers": [{"title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?", "score": 53, "session": "SD-2-2408", "pdf_url": "https://arxiv.org/pdf/2506.11928", "relevant": 4, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zerui Cheng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.0, "max_score": 53, "total_relevant": 4, "total_reads": 6, "papers": [{"title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?", "score": 53, "session": "SD-2-2408", "pdf_url": "https://arxiv.org/pdf/2506.11928", "relevant": 4, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Saining Xie", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.0, "max_score": 53, "total_relevant": 4, "total_reads": 6, "papers": [{"title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?", "score": 53, "session": "SD-2-2408", "pdf_url": "https://arxiv.org/pdf/2506.11928", "relevant": 4, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yedi Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.0, "max_score": 53, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "RvLLM: LLM Runtime Verification with Domain Knowledge", "score": 53, "session": "SD-4-1408", "pdf_url": "https://openreview.net/pdf/e99cb11cda9beb3092445ecb739b48abb0369ac9.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sun Yi Emma", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.0, "max_score": 53, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "RvLLM: LLM Runtime Verification with Domain Knowledge", "score": 53, "session": "SD-4-1408", "pdf_url": "https://openreview.net/pdf/e99cb11cda9beb3092445ecb739b48abb0369ac9.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jin Song Dong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.0, "max_score": 53, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "RvLLM: LLM Runtime Verification with Domain Knowledge", "score": 53, "session": "SD-4-1408", "pdf_url": "https://openreview.net/pdf/e99cb11cda9beb3092445ecb739b48abb0369ac9.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Andy Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.0, "max_score": 53, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems", "score": 53, "session": "SD-5-2302", "pdf_url": "https://arxiv.org/pdf/2505.15216", "relevant": 3, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Joey Ji", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.0, "max_score": 53, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems", "score": 53, "session": "SD-5-2302", "pdf_url": "https://arxiv.org/pdf/2505.15216", "relevant": 3, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Percy Liang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.0, "max_score": 53, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems", "score": 53, "session": "SD-5-2302", "pdf_url": "https://arxiv.org/pdf/2505.15216", "relevant": 3, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Canbin Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.0, "max_score": 53, "total_relevant": 0, "total_reads": 10, "papers": [{"title": "Lookahead Routing for Large Language Models", "score": 53, "session": "SD-6-3603", "pdf_url": "https://openreview.net/pdf/552fece3eff509f39e0f54fafd8aafc36248c8a8.pdf", "relevant": 0, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tianyuan Shi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.0, "max_score": 53, "total_relevant": 0, "total_reads": 10, "papers": [{"title": "Lookahead Routing for Large Language Models", "score": 53, "session": "SD-6-3603", "pdf_url": "https://openreview.net/pdf/552fece3eff509f39e0f54fafd8aafc36248c8a8.pdf", "relevant": 0, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaojun Quan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.0, "max_score": 53, "total_relevant": 0, "total_reads": 10, "papers": [{"title": "Lookahead Routing for Large Language Models", "score": 53, "session": "SD-6-3603", "pdf_url": "https://openreview.net/pdf/552fece3eff509f39e0f54fafd8aafc36248c8a8.pdf", "relevant": 0, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hao Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.0, "max_score": 53, "total_relevant": 3, "total_reads": 5, "papers": [{"title": "DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents", "score": 53, "session": "SD-6-1101", "pdf_url": "https://openreview.net/pdf/7e05f767d463d43be3b045378b14be5760ea2fc1.pdf", "relevant": 3, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaogeng Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.0, "max_score": 53, "total_relevant": 3, "total_reads": 5, "papers": [{"title": "DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents", "score": 53, "session": "SD-6-1101", "pdf_url": "https://openreview.net/pdf/7e05f767d463d43be3b045378b14be5760ea2fc1.pdf", "relevant": 3, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chaowei Xiao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.0, "max_score": 53, "total_relevant": 3, "total_reads": 5, "papers": [{"title": "DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents", "score": 53, "session": "SD-6-1101", "pdf_url": "https://openreview.net/pdf/7e05f767d463d43be3b045378b14be5760ea2fc1.pdf", "relevant": 3, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yukun Jiang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 52, "total_relevant": 2, "total_reads": 11, "papers": [{"title": "Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency", "score": 52, "session": "SD-1-1104", "pdf_url": "https://openreview.net/pdf/7e13bf73f0b1b45f44c038b630279494b0220174.pdf", "relevant": 2, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mingjie Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 52, "total_relevant": 2, "total_reads": 11, "papers": [{"title": "Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency", "score": 52, "session": "SD-1-1104", "pdf_url": "https://openreview.net/pdf/7e13bf73f0b1b45f44c038b630279494b0220174.pdf", "relevant": 2, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yang Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 52, "total_relevant": 2, "total_reads": 11, "papers": [{"title": "Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency", "score": 52, "session": "SD-1-1104", "pdf_url": "https://openreview.net/pdf/7e13bf73f0b1b45f44c038b630279494b0220174.pdf", "relevant": 2, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pengxiang Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 52, "total_relevant": 0, "total_reads": 11, "papers": [{"title": "Iterative Tool Usage Exploration for Multimodal Agents via Step-wise Preference Tuning", "score": 52, "session": "SD-2-4818", "pdf_url": "https://openreview.net/pdf/a3ae43bcdfe712b2361e4ab5254bbce2bcc0dd95.pdf", "relevant": 0, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhi Gao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 52, "total_relevant": 0, "total_reads": 11, "papers": [{"title": "Iterative Tool Usage Exploration for Multimodal Agents via Step-wise Preference Tuning", "score": 52, "session": "SD-2-4818", "pdf_url": "https://openreview.net/pdf/a3ae43bcdfe712b2361e4ab5254bbce2bcc0dd95.pdf", "relevant": 0, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qing Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 52, "total_relevant": 0, "total_reads": 11, "papers": [{"title": "Iterative Tool Usage Exploration for Multimodal Agents via Step-wise Preference Tuning", "score": 52, "session": "SD-2-4818", "pdf_url": "https://openreview.net/pdf/a3ae43bcdfe712b2361e4ab5254bbce2bcc0dd95.pdf", "relevant": 0, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yanbo Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 52, "total_relevant": 0, "total_reads": 1, "papers": [{"title": "DyFlow: Dynamic Workflow Framework for Agentic Reasoning", "score": 52, "session": "SD-3-5207", "pdf_url": "https://openreview.net/pdf/705f64f765b412ab6e17c0dc9c9146763c3e63fe.pdf", "relevant": 0, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zixiang Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 52, "total_relevant": 0, "total_reads": 1, "papers": [{"title": "DyFlow: Dynamic Workflow Framework for Agentic Reasoning", "score": 52, "session": "SD-3-5207", "pdf_url": "https://openreview.net/pdf/705f64f765b412ab6e17c0dc9c9146763c3e63fe.pdf", "relevant": 0, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiuying Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 52, "total_relevant": 0, "total_reads": 1, "papers": [{"title": "DyFlow: Dynamic Workflow Framework for Agentic Reasoning", "score": 52, "session": "SD-3-5207", "pdf_url": "https://openreview.net/pdf/705f64f765b412ab6e17c0dc9c9146763c3e63fe.pdf", "relevant": 0, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mislav Balunovic", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 52, "total_relevant": 7, "total_reads": 10, "papers": [{"title": "MathArena: Evaluating LLMs on Uncontaminated Math Competitions", "score": 52, "session": "SD-3-2508", "pdf_url": "https://arxiv.org/pdf/2505.23281", "relevant": 7, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jasper Dekoninck", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 52, "total_relevant": 7, "total_reads": 10, "papers": [{"title": "MathArena: Evaluating LLMs on Uncontaminated Math Competitions", "score": 52, "session": "SD-3-2508", "pdf_url": "https://arxiv.org/pdf/2505.23281", "relevant": 7, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Martin Vechev", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 52, "total_relevant": 7, "total_reads": 10, "papers": [{"title": "MathArena: Evaluating LLMs on Uncontaminated Math Competitions", "score": 52, "session": "SD-3-2508", "pdf_url": "https://arxiv.org/pdf/2505.23281", "relevant": 7, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junteng Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51, "total_relevant": 4, "total_reads": 7, "papers": [{"title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond", "score": 51, "session": "SD-2-1814", "pdf_url": "https://openreview.net/pdf/b5b93e7b533135e2ad1d72d22e3a481324c2a73d.pdf", "relevant": 4, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuanxiang Fan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51, "total_relevant": 4, "total_reads": 7, "papers": [{"title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond", "score": 51, "session": "SD-2-1814", "pdf_url": "https://openreview.net/pdf/b5b93e7b533135e2ad1d72d22e3a481324c2a73d.pdf", "relevant": 4, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junxian He", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51, "total_relevant": 4, "total_reads": 7, "papers": [{"title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond", "score": 51, "session": "SD-2-1814", "pdf_url": "https://openreview.net/pdf/b5b93e7b533135e2ad1d72d22e3a481324c2a73d.pdf", "relevant": 4, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Felix Chalumeau", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51, "total_relevant": 10, "total_reads": 24, "papers": [{"title": "Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies", "score": 51, "session": "SD-3-311", "pdf_url": "https://openreview.net/pdf/5d59ea40926886752f5ab100ab83a383587e3e1e.pdf", "relevant": 10, "reads": 24}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Daniel Rajaonarivonivelomanantsoa", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51, "total_relevant": 10, "total_reads": 24, "papers": [{"title": "Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies", "score": 51, "session": "SD-3-311", "pdf_url": "https://openreview.net/pdf/5d59ea40926886752f5ab100ab83a383587e3e1e.pdf", "relevant": 10, "reads": 24}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Arnu Pretorius", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51, "total_relevant": 10, "total_reads": 24, "papers": [{"title": "Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies", "score": 51, "session": "SD-3-311", "pdf_url": "https://openreview.net/pdf/5d59ea40926886752f5ab100ab83a383587e3e1e.pdf", "relevant": 10, "reads": 24}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaohao Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51, "total_relevant": 3, "total_reads": 10, "papers": [{"title": "L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models", "score": 51, "session": "SD-4-1911", "pdf_url": "https://openreview.net/pdf/03edf649a9f42cdf6d921cb59599e7130120540a.pdf", "relevant": 3, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaobo Xia", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51, "total_relevant": 3, "total_reads": 10, "papers": [{"title": "L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models", "score": 51, "session": "SD-4-1911", "pdf_url": "https://openreview.net/pdf/03edf649a9f42cdf6d921cb59599e7130120540a.pdf", "relevant": 3, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Edan Toledo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51, "total_relevant": 3, "total_reads": 14, "papers": [{"title": "AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench", "score": 51, "session": "SD-4-2808", "pdf_url": "https://openreview.net/pdf/a22e34894b1de174131d79168e9a32bf5fefd3a5.pdf", "relevant": 3, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Karen Hambardzumyan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51, "total_relevant": 3, "total_reads": 14, "papers": [{"title": "AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench", "score": 51, "session": "SD-4-2808", "pdf_url": "https://openreview.net/pdf/a22e34894b1de174131d79168e9a32bf5fefd3a5.pdf", "relevant": 3, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xeron Du", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51, "total_relevant": 4, "total_reads": 8, "papers": [{"title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines", "score": 51, "session": "SD-4-5103", "pdf_url": "https://arxiv.org/pdf/2502.14739", "relevant": 4, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yifan Yao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51, "total_relevant": 4, "total_reads": 8, "papers": [{"title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines", "score": 51, "session": "SD-4-5103", "pdf_url": "https://arxiv.org/pdf/2502.14739", "relevant": 4, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ge Zhang", "paper_count": 3, "highly_relevant_count": 0, "avg_score": 34.0, "max_score": 51, "total_relevant": 13, "total_reads": 30, "papers": [{"title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines", "score": 51, "session": "SD-4-5103", "pdf_url": "https://arxiv.org/pdf/2502.14739", "relevant": 4, "reads": 8}, {"title": "OmniBench: Towards The Future of  Universal Omni-Language Models", "score": 48, "session": "SD-2-5201", "pdf_url": "https://openreview.net/pdf/c52922a7cb62ce2d159c41efd473976ed5a5693f.pdf", "relevant": 9, "reads": 22}, {"title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation", "score": 3, "session": "SD-4-2014", "pdf_url": "https://openreview.net/pdf/a3f1d4df6324abb0ea9576e5fe2da1d467238283.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Keuntae Kim", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections", "score": 51, "session": "SD-6-3709", "pdf_url": "https://openreview.net/pdf/bcb703c11665128fb39f34dfd53237d4a3431b28.pdf", "relevant": 1, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Eunhye Jeong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections", "score": 51, "session": "SD-6-3709", "pdf_url": "https://openreview.net/pdf/bcb703c11665128fb39f34dfd53237d4a3431b28.pdf", "relevant": 1, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yong Suk Choi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections", "score": 51, "session": "SD-6-3709", "pdf_url": "https://openreview.net/pdf/bcb703c11665128fb39f34dfd53237d4a3431b28.pdf", "relevant": 1, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wasu Top Piriyakulkij", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.0, "max_score": 50, "total_relevant": 14, "total_reads": 39, "papers": [{"title": "PoE-World: Compositional World Modeling with Products of Programmatic Experts", "score": 50, "session": "SD-4-508", "pdf_url": "https://openreview.net/pdf/1983f2216c20adc421975e0092eb41f2ac1d93fa.pdf", "relevant": 14, "reads": 39}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yichao Liang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.0, "max_score": 50, "total_relevant": 14, "total_reads": 39, "papers": [{"title": "PoE-World: Compositional World Modeling with Products of Programmatic Experts", "score": 50, "session": "SD-4-508", "pdf_url": "https://openreview.net/pdf/1983f2216c20adc421975e0092eb41f2ac1d93fa.pdf", "relevant": 14, "reads": 39}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kevin Ellis", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.0, "max_score": 50, "total_relevant": 14, "total_reads": 39, "papers": [{"title": "PoE-World: Compositional World Modeling with Products of Programmatic Experts", "score": 50, "session": "SD-4-508", "pdf_url": "https://openreview.net/pdf/1983f2216c20adc421975e0092eb41f2ac1d93fa.pdf", "relevant": 14, "reads": 39}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hyungjoo Chae", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents", "score": 49, "session": "SD-1-5402", "pdf_url": "https://openreview.net/pdf/db46564195dad40b9b174514d7d03b0336d2a8eb.pdf", "relevant": 3, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sunghwan Kim", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents", "score": 49, "session": "SD-1-5402", "pdf_url": "https://openreview.net/pdf/db46564195dad40b9b174514d7d03b0336d2a8eb.pdf", "relevant": 3, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jinyoung Yeo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents", "score": 49, "session": "SD-1-5402", "pdf_url": "https://openreview.net/pdf/db46564195dad40b9b174514d7d03b0336d2a8eb.pdf", "relevant": 3, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Songhao Han", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 8, "total_reads": 46, "papers": [{"title": "RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation", "score": 49, "session": "SD-2-611", "pdf_url": "https://arxiv.org/pdf/2506.06677", "relevant": 8, "reads": 46}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Boxiang Qiu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 8, "total_reads": 46, "papers": [{"title": "RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation", "score": 49, "session": "SD-2-611", "pdf_url": "https://arxiv.org/pdf/2506.06677", "relevant": 8, "reads": 46}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Si Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 8, "total_reads": 46, "papers": [{"title": "RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation", "score": 49, "session": "SD-2-611", "pdf_url": "https://arxiv.org/pdf/2506.06677", "relevant": 8, "reads": 46}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Rushi Qiang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 6, "total_reads": 17, "papers": [{"title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering", "score": 49, "session": "SD-2-2405", "pdf_url": "https://arxiv.org/pdf/2505.07782", "relevant": 6, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Bo Dai", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 40.0, "max_score": 49, "total_relevant": 11, "total_reads": 26, "papers": [{"title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering", "score": 49, "session": "SD-2-2405", "pdf_url": "https://arxiv.org/pdf/2505.07782", "relevant": 6, "reads": 17}, {"title": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs", "score": 31, "session": "SD-1-3416", "pdf_url": "https://openreview.net/pdf/01546f97c01b32e5c0cf560fc4be7a511b46e042.pdf", "relevant": 5, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuhao Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning", "score": 49, "session": "SD-2-5200", "pdf_url": "https://arxiv.org/pdf/2506.10521", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yiheng Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning", "score": 49, "session": "SD-2-5200", "pdf_url": "https://arxiv.org/pdf/2506.10521", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "LEI BAI", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning", "score": 49, "session": "SD-2-5200", "pdf_url": "https://arxiv.org/pdf/2506.10521", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Rui Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 1, "total_reads": 6, "papers": [{"title": "ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints", "score": 49, "session": "SD-3-5505", "pdf_url": "https://openreview.net/pdf/2bc8b445588504ed12b491f7cc33f033191ae2ad.pdf", "relevant": 1, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dakuan Lu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 1, "total_reads": 6, "papers": [{"title": "ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints", "score": 49, "session": "SD-3-5505", "pdf_url": "https://openreview.net/pdf/2bc8b445588504ed12b491f7cc33f033191ae2ad.pdf", "relevant": 1, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xu Yinghui", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 1, "total_reads": 6, "papers": [{"title": "ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints", "score": 49, "session": "SD-3-5505", "pdf_url": "https://openreview.net/pdf/2bc8b445588504ed12b491f7cc33f033191ae2ad.pdf", "relevant": 1, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pascal Kesseli", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 4, "total_reads": 5, "papers": [{"title": "Logic.py: Bridging the Gap between LLMs and Constraint Solvers", "score": 49, "session": "SD-5-700", "pdf_url": "https://openreview.net/pdf/a811f4bc76e7ad2fcf67bc0ce62afd3123512b8d.pdf", "relevant": 4, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Peter O'Hearn", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 4, "total_reads": 5, "papers": [{"title": "Logic.py: Bridging the Gap between LLMs and Constraint Solvers", "score": 49, "session": "SD-5-700", "pdf_url": "https://openreview.net/pdf/a811f4bc76e7ad2fcf67bc0ce62afd3123512b8d.pdf", "relevant": 4, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ricardo Silveira Cabral", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 4, "total_reads": 5, "papers": [{"title": "Logic.py: Bridging the Gap between LLMs and Constraint Solvers", "score": 49, "session": "SD-5-700", "pdf_url": "https://openreview.net/pdf/a811f4bc76e7ad2fcf67bc0ce62afd3123512b8d.pdf", "relevant": 4, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yiyou Sun", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization", "score": 49, "session": "SD-6-4912", "pdf_url": "https://arxiv.org/pdf/2506.18880", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shawn Hu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 49.0, "max_score": 49, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization", "score": 49, "session": "SD-6-4912", "pdf_url": "https://arxiv.org/pdf/2506.18880", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dawn Song", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 46.0, "max_score": 49, "total_relevant": 17, "total_reads": 39, "papers": [{"title": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization", "score": 49, "session": "SD-6-4912", "pdf_url": "https://arxiv.org/pdf/2506.18880", "relevant": 4, "reads": 9}, {"title": "Scalable Best-of-N Selection for Large Language Models via Self-Certainty", "score": 43, "session": "SD-4-5204", "pdf_url": "https://openreview.net/pdf/141a8955a8007083b8b3068692459c467c444619.pdf", "relevant": 13, "reads": 30}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuxin Zuo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 48.0, "max_score": 48, "total_relevant": 14, "total_reads": 33, "papers": [{"title": "TTRL: Test-Time Reinforcement Learning", "score": 48, "session": "SD-1-5419", "pdf_url": "https://openreview.net/pdf/3ff432e912f7ed9bbbacf9a7a16d7e5af88d721a.pdf", "relevant": 14, "reads": 33}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kaiyan Zhang", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 30.0, "max_score": 48, "total_relevant": 16, "total_reads": 35, "papers": [{"title": "TTRL: Test-Time Reinforcement Learning", "score": 48, "session": "SD-1-5419", "pdf_url": "https://openreview.net/pdf/3ff432e912f7ed9bbbacf9a7a16d7e5af88d721a.pdf", "relevant": 14, "reads": 33}, {"title": "SciArena: An Open Evaluation Platform for Non-Verifiable Scientific Literature-Grounded Tasks", "score": 12, "session": "SD-1-5316", "pdf_url": "", "relevant": 2, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Bowen Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 48.0, "max_score": 48, "total_relevant": 14, "total_reads": 33, "papers": [{"title": "TTRL: Test-Time Reinforcement Learning", "score": 48, "session": "SD-1-5419", "pdf_url": "https://openreview.net/pdf/3ff432e912f7ed9bbbacf9a7a16d7e5af88d721a.pdf", "relevant": 14, "reads": 33}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Fengwei Teng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 48.0, "max_score": 48, "total_relevant": 14, "total_reads": 19, "papers": [{"title": "Atom of Thoughts for Markov LLM Test-Time Scaling", "score": 48, "session": "SD-1-1907", "pdf_url": "https://openreview.net/pdf/aee7e5e0ac85edb676698e634deccc28c92e1407.pdf", "relevant": 14, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Quan Shi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 48.0, "max_score": 48, "total_relevant": 14, "total_reads": 19, "papers": [{"title": "Atom of Thoughts for Markov LLM Test-Time Scaling", "score": 48, "session": "SD-1-1907", "pdf_url": "https://openreview.net/pdf/aee7e5e0ac85edb676698e634deccc28c92e1407.pdf", "relevant": 14, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhijiang Guo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 48.0, "max_score": 48, "total_relevant": 14, "total_reads": 19, "papers": [{"title": "Atom of Thoughts for Markov LLM Test-Time Scaling", "score": 48, "session": "SD-1-1907", "pdf_url": "https://openreview.net/pdf/aee7e5e0ac85edb676698e634deccc28c92e1407.pdf", "relevant": 14, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yizhi LI", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 48.0, "max_score": 48, "total_relevant": 9, "total_reads": 22, "papers": [{"title": "OmniBench: Towards The Future of  Universal Omni-Language Models", "score": 48, "session": "SD-2-5201", "pdf_url": "https://openreview.net/pdf/c52922a7cb62ce2d159c41efd473976ed5a5693f.pdf", "relevant": 9, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chenghua Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 48.0, "max_score": 48, "total_relevant": 9, "total_reads": 22, "papers": [{"title": "OmniBench: Towards The Future of  Universal Omni-Language Models", "score": 48, "session": "SD-2-5201", "pdf_url": "https://openreview.net/pdf/c52922a7cb62ce2d159c41efd473976ed5a5693f.pdf", "relevant": 9, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chaoyou Fu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 48.0, "max_score": 48, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models", "score": 48, "session": "SD-2-4503", "pdf_url": "https://arxiv.org/pdf/2306.13394", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Peixian Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 48.0, "max_score": 48, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models", "score": 48, "session": "SD-2-4503", "pdf_url": "https://arxiv.org/pdf/2306.13394", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ran He", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 48.0, "max_score": 48, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models", "score": 48, "session": "SD-2-4503", "pdf_url": "https://arxiv.org/pdf/2306.13394", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haidong Kang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 48.0, "max_score": 48, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "Revolutionizing Training-Free NAS: Towards Efficient Automatic Proxy Discovery via Large Language Models", "score": 48, "session": "SD-5-3302", "pdf_url": "https://openreview.net/pdf/f5967a55faa0ccebb78581f66e96dad4d59eb767.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lihong Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 48.0, "max_score": 48, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "Revolutionizing Training-Free NAS: Towards Efficient Automatic Proxy Discovery via Large Language Models", "score": 48, "session": "SD-5-3302", "pdf_url": "https://openreview.net/pdf/f5967a55faa0ccebb78581f66e96dad4d59eb767.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hanling Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 48.0, "max_score": 48, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "Revolutionizing Training-Free NAS: Towards Efficient Automatic Proxy Discovery via Large Language Models", "score": 48, "session": "SD-5-3302", "pdf_url": "https://openreview.net/pdf/f5967a55faa0ccebb78581f66e96dad4d59eb767.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhening Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 48.0, "max_score": 48, "total_relevant": 0, "total_reads": 3, "papers": [{"title": "EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths", "score": 48, "session": "SD-6-2410", "pdf_url": "https://openreview.net/pdf/ec254672c2c5013801b6522a08e51c829a7ef814.pdf", "relevant": 0, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Armando Solar-Lezama", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 48.0, "max_score": 48, "total_relevant": 0, "total_reads": 3, "papers": [{"title": "EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths", "score": 48, "session": "SD-6-2410", "pdf_url": "https://openreview.net/pdf/ec254672c2c5013801b6522a08e51c829a7ef814.pdf", "relevant": 0, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Stephan Zheng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 48.0, "max_score": 48, "total_relevant": 0, "total_reads": 3, "papers": [{"title": "EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths", "score": 48, "session": "SD-6-2410", "pdf_url": "https://openreview.net/pdf/ec254672c2c5013801b6522a08e51c829a7ef814.pdf", "relevant": 0, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jie Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 47.0, "max_score": 47, "total_relevant": 8, "total_reads": 9, "papers": [{"title": "RealMath: A Continuous Benchmark for Evaluating Language Models on Research-Level Mathematics", "score": 47, "session": "SD-3-1512", "pdf_url": "https://arxiv.org/pdf/2505.12575", "relevant": 8, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Cezara Petrui", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 47.0, "max_score": 47, "total_relevant": 8, "total_reads": 9, "papers": [{"title": "RealMath: A Continuous Benchmark for Evaluating Language Models on Research-Level Mathematics", "score": 47, "session": "SD-3-1512", "pdf_url": "https://arxiv.org/pdf/2505.12575", "relevant": 8, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Florian Tramer", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 47.0, "max_score": 47, "total_relevant": 8, "total_reads": 9, "papers": [{"title": "RealMath: A Continuous Benchmark for Evaluating Language Models on Research-Level Mathematics", "score": 47, "session": "SD-3-1512", "pdf_url": "https://arxiv.org/pdf/2505.12575", "relevant": 8, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Francis Rhys Ward", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 47.0, "max_score": 47, "total_relevant": 8, "total_reads": 12, "papers": [{"title": "CTRL-ALT-DECEIT Sabotage Evaluations for Automated AI R&D", "score": 47, "session": "SD-3-1208", "pdf_url": "https://openreview.net/pdf/5ba46eb607f50910cda80a0bfc28f6c4f5b9c656.pdf", "relevant": 8, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Teun van der Weij", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 47.0, "max_score": 47, "total_relevant": 8, "total_reads": 12, "papers": [{"title": "CTRL-ALT-DECEIT Sabotage Evaluations for Automated AI R&D", "score": 47, "session": "SD-3-1208", "pdf_url": "https://openreview.net/pdf/5ba46eb607f50910cda80a0bfc28f6c4f5b9c656.pdf", "relevant": 8, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lauren Robson", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 47.0, "max_score": 47, "total_relevant": 8, "total_reads": 12, "papers": [{"title": "CTRL-ALT-DECEIT Sabotage Evaluations for Automated AI R&D", "score": 47, "session": "SD-3-1208", "pdf_url": "https://openreview.net/pdf/5ba46eb607f50910cda80a0bfc28f6c4f5b9c656.pdf", "relevant": 8, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenkai Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 47.0, "max_score": 47, "total_relevant": 8, "total_reads": 13, "papers": [{"title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning", "score": 47, "session": "SD-5-2315", "pdf_url": "https://openreview.net/pdf/a85740fc42f5b0086b031316724925f3d39daa30.pdf", "relevant": 8, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shuming Ma", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 47.0, "max_score": 47, "total_relevant": 8, "total_reads": 13, "papers": [{"title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning", "score": 47, "session": "SD-5-2315", "pdf_url": "https://openreview.net/pdf/a85740fc42f5b0086b031316724925f3d39daa30.pdf", "relevant": 8, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Furu Wei", "paper_count": 3, "highly_relevant_count": 0, "avg_score": 39.7, "max_score": 47, "total_relevant": 14, "total_reads": 44, "papers": [{"title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning", "score": 47, "session": "SD-5-2315", "pdf_url": "https://openreview.net/pdf/a85740fc42f5b0086b031316724925f3d39daa30.pdf", "relevant": 8, "reads": 13}, {"title": "Think Only When You Need with Large Hybrid-Reasoning Models", "score": 37, "session": "SD-2-3713", "pdf_url": "https://openreview.net/pdf/33fb886d1cda050a0c29d0bdee85176c1c3f7f31.pdf", "relevant": 2, "reads": 10}, {"title": "Reward Reasoning Models", "score": 35, "session": "SD-1-4105", "pdf_url": "https://openreview.net/pdf/1b460f75d98849e58bc2a9cf1b6673536d0898d5.pdf", "relevant": 4, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wei Fu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 47.0, "max_score": 47, "total_relevant": 10, "total_reads": 14, "papers": [{"title": "AREAL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning", "score": 47, "session": "SD-6-514", "pdf_url": "https://openreview.net/pdf/1b2829a8cfd93ca52bca9bf2c38c826016159024.pdf", "relevant": 10, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiaxuan Gao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 47.0, "max_score": 47, "total_relevant": 10, "total_reads": 14, "papers": [{"title": "AREAL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning", "score": 47, "session": "SD-6-514", "pdf_url": "https://openreview.net/pdf/1b2829a8cfd93ca52bca9bf2c38c826016159024.pdf", "relevant": 10, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yi Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 47.0, "max_score": 47, "total_relevant": 10, "total_reads": 14, "papers": [{"title": "AREAL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning", "score": 47, "session": "SD-6-514", "pdf_url": "https://openreview.net/pdf/1b2829a8cfd93ca52bca9bf2c38c826016159024.pdf", "relevant": 10, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shaofeng Yin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 46.0, "max_score": 46, "total_relevant": 11, "total_reads": 35, "papers": [{"title": "RLVR-World: Training World Models with Reinforcement Learning", "score": 46, "session": "SD-4-1301", "pdf_url": "https://openreview.net/pdf/4b0d2935ed4554453743ecdcf099e0d679355328.pdf", "relevant": 11, "reads": 35}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mingsheng Long", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 46.0, "max_score": 46, "total_relevant": 11, "total_reads": 35, "papers": [{"title": "RLVR-World: Training World Models with Reinforcement Learning", "score": 46, "session": "SD-4-1301", "pdf_url": "https://openreview.net/pdf/4b0d2935ed4554453743ecdcf099e0d679355328.pdf", "relevant": 11, "reads": 35}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Valentina Pyatkin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 46.0, "max_score": 46, "total_relevant": 6, "total_reads": 17, "papers": [{"title": "Generalizing Verifiable Instruction Following", "score": 46, "session": "SD-5-412", "pdf_url": "https://arxiv.org/pdf/2507.02833", "relevant": 6, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Saumya Malik", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 46.0, "max_score": 46, "total_relevant": 6, "total_reads": 17, "papers": [{"title": "Generalizing Verifiable Instruction Following", "score": 46, "session": "SD-5-412", "pdf_url": "https://arxiv.org/pdf/2507.02833", "relevant": 6, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hanna Hajishirzi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 46.0, "max_score": 46, "total_relevant": 6, "total_reads": 17, "papers": [{"title": "Generalizing Verifiable Instruction Following", "score": 46, "session": "SD-5-412", "pdf_url": "https://arxiv.org/pdf/2507.02833", "relevant": 6, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhi Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 46.0, "max_score": 46, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning", "score": 46, "session": "SD-6-1900", "pdf_url": "https://openreview.net/pdf/73a0607365582aedefc2167e27fb239c96092223.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuhao Tan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 46.0, "max_score": 46, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning", "score": 46, "session": "SD-6-1900", "pdf_url": "https://openreview.net/pdf/73a0607365582aedefc2167e27fb239c96092223.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaoxing Ma", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 46.0, "max_score": 46, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning", "score": 46, "session": "SD-6-1900", "pdf_url": "https://openreview.net/pdf/73a0607365582aedefc2167e27fb239c96092223.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yifu Guo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 46.0, "max_score": 46, "total_relevant": 7, "total_reads": 15, "papers": [{"title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents", "score": 46, "session": "SD-6-1911", "pdf_url": "https://openreview.net/pdf/814cd78232da3150c1f91b29920f4f2e4d70fb3c.pdf", "relevant": 7, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiaye Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 46.0, "max_score": 46, "total_relevant": 7, "total_reads": 15, "papers": [{"title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents", "score": 46, "session": "SD-6-1911", "pdf_url": "https://openreview.net/pdf/814cd78232da3150c1f91b29920f4f2e4d70fb3c.pdf", "relevant": 7, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mingguang Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 46.0, "max_score": 46, "total_relevant": 7, "total_reads": 15, "papers": [{"title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents", "score": 46, "session": "SD-6-1911", "pdf_url": "https://openreview.net/pdf/814cd78232da3150c1f91b29920f4f2e4d70fb3c.pdf", "relevant": 7, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Felipe Maia Polo", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 45, "total_relevant": 11, "total_reads": 19, "papers": [{"title": "Sloth: scaling laws for LLM skills to predict multi-benchmark performance across families", "score": 45, "session": "SD-1-1306", "pdf_url": "https://openreview.net/pdf/82fec2c6ee0cec1161d232d81bfad1d63de8fd54.pdf", "relevant": 4, "reads": 5}, {"title": "Bridging Human and LLM Judgments: Understanding and Narrowing the Gap", "score": 1, "session": "SD-5-1515", "pdf_url": "https://openreview.net/pdf/3b238b3324d5ab3f3e5a672a12a2c7610ee13a48.pdf", "relevant": 7, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Seamus Somerstep", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 45.0, "max_score": 45, "total_relevant": 4, "total_reads": 5, "papers": [{"title": "Sloth: scaling laws for LLM skills to predict multi-benchmark performance across families", "score": 45, "session": "SD-1-1306", "pdf_url": "https://openreview.net/pdf/82fec2c6ee0cec1161d232d81bfad1d63de8fd54.pdf", "relevant": 4, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mikhail Yurochkin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 45.0, "max_score": 45, "total_relevant": 4, "total_reads": 5, "papers": [{"title": "Sloth: scaling laws for LLM skills to predict multi-benchmark performance across families", "score": 45, "session": "SD-1-1306", "pdf_url": "https://openreview.net/pdf/82fec2c6ee0cec1161d232d81bfad1d63de8fd54.pdf", "relevant": 4, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yingxuan Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 45.0, "max_score": 45, "total_relevant": 4, "total_reads": 7, "papers": [{"title": "AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems", "score": 45, "session": "SD-1-1600", "pdf_url": "https://openreview.net/pdf/38bdf6e7191adba7391b1fde1ad37e27887b2bac.pdf", "relevant": 4, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Huacan Chai", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 45.0, "max_score": 45, "total_relevant": 4, "total_reads": 7, "papers": [{"title": "AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems", "score": 45, "session": "SD-1-1600", "pdf_url": "https://openreview.net/pdf/38bdf6e7191adba7391b1fde1ad37e27887b2bac.pdf", "relevant": 4, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Weinan Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 45.0, "max_score": 45, "total_relevant": 4, "total_reads": 7, "papers": [{"title": "AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems", "score": 45, "session": "SD-1-1600", "pdf_url": "https://openreview.net/pdf/38bdf6e7191adba7391b1fde1ad37e27887b2bac.pdf", "relevant": 4, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xueliang Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 45.0, "max_score": 45, "total_relevant": 0, "total_reads": 7, "papers": [{"title": "DynaAct: Large Language Model Reasoning with Dynamic Action Spaces", "score": 45, "session": "SD-4-5304", "pdf_url": "https://openreview.net/pdf/45820087f0186edb228a8b251bb2af4a3cca38b5.pdf", "relevant": 0, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wei Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 45.0, "max_score": 45, "total_relevant": 0, "total_reads": 7, "papers": [{"title": "DynaAct: Large Language Model Reasoning with Dynamic Action Spaces", "score": 45, "session": "SD-4-5304", "pdf_url": "https://openreview.net/pdf/45820087f0186edb228a8b251bb2af4a3cca38b5.pdf", "relevant": 0, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lingpeng Kong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 45.0, "max_score": 45, "total_relevant": 0, "total_reads": 7, "papers": [{"title": "DynaAct: Large Language Model Reasoning with Dynamic Action Spaces", "score": 45, "session": "SD-4-5304", "pdf_url": "https://openreview.net/pdf/45820087f0186edb228a8b251bb2af4a3cca38b5.pdf", "relevant": 0, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chunyu Wei", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 45.0, "max_score": 45, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining", "score": 45, "session": "SD-5-312", "pdf_url": "https://openreview.net/pdf/d184d257f33dec6f932171111d977423bd83f8ef.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenji Hu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 45.0, "max_score": 45, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining", "score": 45, "session": "SD-5-312", "pdf_url": "https://openreview.net/pdf/d184d257f33dec6f932171111d977423bd83f8ef.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yueguo Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 45.0, "max_score": 45, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining", "score": 45, "session": "SD-5-312", "pdf_url": "https://openreview.net/pdf/d184d257f33dec6f932171111d977423bd83f8ef.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qijiong Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 45.0, "max_score": 45, "total_relevant": 4, "total_reads": 8, "papers": [{"title": "Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation", "score": 45, "session": "SD-5-5200", "pdf_url": "https://arxiv.org/pdf/2503.05493", "relevant": 4, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jieming Zhu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 45.0, "max_score": 45, "total_relevant": 4, "total_reads": 8, "papers": [{"title": "Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation", "score": 45, "session": "SD-5-5200", "pdf_url": "https://arxiv.org/pdf/2503.05493", "relevant": 4, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiao-Ming Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 45.0, "max_score": 45, "total_relevant": 4, "total_reads": 8, "papers": [{"title": "Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation", "score": 45, "session": "SD-5-5200", "pdf_url": "https://arxiv.org/pdf/2503.05493", "relevant": 4, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Andreas Auer", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 45.0, "max_score": 45, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning", "score": 45, "session": "SD-6-2404", "pdf_url": "https://openreview.net/pdf/1608428ae182161621e0d5c0fae3d6b608a7acaa.pdf", "relevant": 3, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Patrick Podest", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 45.0, "max_score": 45, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning", "score": 45, "session": "SD-6-2404", "pdf_url": "https://openreview.net/pdf/1608428ae182161621e0d5c0fae3d6b608a7acaa.pdf", "relevant": 3, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sepp Hochreiter", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 45.0, "max_score": 45, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning", "score": 45, "session": "SD-6-2404", "pdf_url": "https://openreview.net/pdf/1608428ae182161621e0d5c0fae3d6b608a7acaa.pdf", "relevant": 3, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xinyu Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 44.0, "max_score": 44, "total_relevant": 1, "total_reads": 11, "papers": [{"title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation", "score": 44, "session": "SD-3-3704", "pdf_url": "https://openreview.net/pdf/5f50a250befd3553dd40112c1a440f86b36737da.pdf", "relevant": 1, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuwei An", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 44.0, "max_score": 44, "total_relevant": 1, "total_reads": 11, "papers": [{"title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation", "score": 44, "session": "SD-3-3704", "pdf_url": "https://openreview.net/pdf/5f50a250befd3553dd40112c1a440f86b36737da.pdf", "relevant": 1, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Beidi Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 44.0, "max_score": 44, "total_relevant": 1, "total_reads": 11, "papers": [{"title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation", "score": 44, "session": "SD-3-3704", "pdf_url": "https://openreview.net/pdf/5f50a250befd3553dd40112c1a440f86b36737da.pdf", "relevant": 1, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junfei Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 44.0, "max_score": 44, "total_relevant": 5, "total_reads": 29, "papers": [{"title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "score": 44, "session": "SD-4-4815", "pdf_url": "https://openreview.net/pdf/0da039d9be67a74526e2ce4408db4cde8bdca6f5.pdf", "relevant": 5, "reads": 29}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jian Guan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 44.0, "max_score": 44, "total_relevant": 5, "total_reads": 29, "papers": [{"title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "score": 44, "session": "SD-4-4815", "pdf_url": "https://openreview.net/pdf/0da039d9be67a74526e2ce4408db4cde8bdca6f5.pdf", "relevant": 5, "reads": 29}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tieniu Tan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 44.0, "max_score": 44, "total_relevant": 5, "total_reads": 29, "papers": [{"title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "score": 44, "session": "SD-4-4815", "pdf_url": "https://openreview.net/pdf/0da039d9be67a74526e2ce4408db4cde8bdca6f5.pdf", "relevant": 5, "reads": 29}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Edoardo Pona", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 43.0, "max_score": 43, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Abstract Counterfactuals for Language Model Agents", "score": 43, "session": "", "pdf_url": "https://openreview.net/pdf/c39308d32f99302b4eb8a97c6d7eae5b2e2c4466.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Milad Kazemi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 43.0, "max_score": 43, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Abstract Counterfactuals for Language Model Agents", "score": 43, "session": "", "pdf_url": "https://openreview.net/pdf/c39308d32f99302b4eb8a97c6d7eae5b2e2c4466.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Nicola Paoletti", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 43.0, "max_score": 43, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Abstract Counterfactuals for Language Model Agents", "score": 43, "session": "", "pdf_url": "https://openreview.net/pdf/c39308d32f99302b4eb8a97c6d7eae5b2e2c4466.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xinyuan Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 43.0, "max_score": 43, "total_relevant": 3, "total_reads": 4, "papers": [{"title": "OpenCUA: Open Foundations for Computer-Use Agents", "score": 43, "session": "SD-3-1509", "pdf_url": "https://openreview.net/pdf/eb1bd0238abbc386303352dba1049a4d5d1fec83.pdf", "relevant": 3, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Bowen Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 43.0, "max_score": 43, "total_relevant": 3, "total_reads": 4, "papers": [{"title": "OpenCUA: Open Foundations for Computer-Use Agents", "score": 43, "session": "SD-3-1509", "pdf_url": "https://openreview.net/pdf/eb1bd0238abbc386303352dba1049a4d5d1fec83.pdf", "relevant": 3, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tao Yu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 43.0, "max_score": 43, "total_relevant": 3, "total_reads": 4, "papers": [{"title": "OpenCUA: Open Foundations for Computer-Use Agents", "score": 43, "session": "SD-3-1509", "pdf_url": "https://openreview.net/pdf/eb1bd0238abbc386303352dba1049a4d5d1fec83.pdf", "relevant": 3, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhewei Kang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 43.0, "max_score": 43, "total_relevant": 13, "total_reads": 30, "papers": [{"title": "Scalable Best-of-N Selection for Large Language Models via Self-Certainty", "score": 43, "session": "SD-4-5204", "pdf_url": "https://openreview.net/pdf/141a8955a8007083b8b3068692459c467c444619.pdf", "relevant": 13, "reads": 30}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xuandong Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 43.0, "max_score": 43, "total_relevant": 13, "total_reads": 30, "papers": [{"title": "Scalable Best-of-N Selection for Large Language Models via Self-Certainty", "score": 43, "session": "SD-4-5204", "pdf_url": "https://openreview.net/pdf/141a8955a8007083b8b3068692459c467c444619.pdf", "relevant": 13, "reads": 30}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wujiang Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 42.0, "max_score": 42, "total_relevant": 16, "total_reads": 24, "papers": [{"title": "A-Mem: Agentic Memory for LLM Agents", "score": 42, "session": "SD-4-210", "pdf_url": "https://openreview.net/pdf/518e506d098a6c821c7e4ae97d1368f374fddac9.pdf", "relevant": 16, "reads": 24}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zujie Liang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 42.0, "max_score": 42, "total_relevant": 16, "total_reads": 24, "papers": [{"title": "A-Mem: Agentic Memory for LLM Agents", "score": 42, "session": "SD-4-210", "pdf_url": "https://openreview.net/pdf/518e506d098a6c821c7e4ae97d1368f374fddac9.pdf", "relevant": 16, "reads": 24}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yongfeng Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 42.0, "max_score": 42, "total_relevant": 16, "total_reads": 24, "papers": [{"title": "A-Mem: Agentic Memory for LLM Agents", "score": 42, "session": "SD-4-210", "pdf_url": "https://openreview.net/pdf/518e506d098a6c821c7e4ae97d1368f374fddac9.pdf", "relevant": 16, "reads": 24}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mingjie Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 42.0, "max_score": 42, "total_relevant": 15, "total_reads": 25, "papers": [{"title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models", "score": 42, "session": "SD-5-1914", "pdf_url": "https://openreview.net/pdf/75e49a843f0b6d00c0584a776fad3bea93496c83.pdf", "relevant": 15, "reads": 25}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shizhe Diao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 42.0, "max_score": 42, "total_relevant": 15, "total_reads": 25, "papers": [{"title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models", "score": 42, "session": "SD-5-1914", "pdf_url": "https://openreview.net/pdf/75e49a843f0b6d00c0584a776fad3bea93496c83.pdf", "relevant": 15, "reads": 25}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yi Dong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 42.0, "max_score": 42, "total_relevant": 15, "total_reads": 25, "papers": [{"title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models", "score": 42, "session": "SD-5-1914", "pdf_url": "https://openreview.net/pdf/75e49a843f0b6d00c0584a776fad3bea93496c83.pdf", "relevant": 15, "reads": 25}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Soumya Suvra Ghosal", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 42.0, "max_score": 42, "total_relevant": 7, "total_reads": 13, "papers": [{"title": "Does Thinking More Always Help? Mirage of Test-Time Scaling in Reasoning Models", "score": 42, "session": "SD-5-5510", "pdf_url": "https://openreview.net/pdf/9a4ddca48299d0fb623da4e9a0093d29392e48a2.pdf", "relevant": 7, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Souradip Chakraborty", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 42.0, "max_score": 42, "total_relevant": 7, "total_reads": 13, "papers": [{"title": "Does Thinking More Always Help? Mirage of Test-Time Scaling in Reasoning Models", "score": 42, "session": "SD-5-5510", "pdf_url": "https://openreview.net/pdf/9a4ddca48299d0fb623da4e9a0093d29392e48a2.pdf", "relevant": 7, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Amrit Singh Bedi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 42.0, "max_score": 42, "total_relevant": 7, "total_reads": 13, "papers": [{"title": "Does Thinking More Always Help? Mirage of Test-Time Scaling in Reasoning Models", "score": 42, "session": "SD-5-5510", "pdf_url": "https://openreview.net/pdf/9a4ddca48299d0fb623da4e9a0093d29392e48a2.pdf", "relevant": 7, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Runzhe Zhan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 42.0, "max_score": 42, "total_relevant": 2, "total_reads": 5, "papers": [{"title": "Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost", "score": 42, "session": "SD-6-1915", "pdf_url": "https://openreview.net/pdf/496a103d9eacba8143b7d8a13098936a351a1a81.pdf", "relevant": 2, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhihong Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 42.0, "max_score": 42, "total_relevant": 2, "total_reads": 5, "papers": [{"title": "Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost", "score": 42, "session": "SD-6-1915", "pdf_url": "https://openreview.net/pdf/496a103d9eacba8143b7d8a13098936a351a1a81.pdf", "relevant": 2, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Derek F. Wong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 42.0, "max_score": 42, "total_relevant": 2, "total_reads": 5, "papers": [{"title": "Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost", "score": 42, "session": "SD-6-1915", "pdf_url": "https://openreview.net/pdf/496a103d9eacba8143b7d8a13098936a351a1a81.pdf", "relevant": 2, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Gongfan Fang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 41.0, "max_score": 41, "total_relevant": 5, "total_reads": 13, "papers": [{"title": "Thinkless: LLM Learns When to Think", "score": 41, "session": "SD-2-4015", "pdf_url": "https://openreview.net/pdf/69d5f7c0fe7ed54d42a4a1908d50a94bc062b721.pdf", "relevant": 5, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xinyin Ma", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 41.0, "max_score": 41, "total_relevant": 5, "total_reads": 13, "papers": [{"title": "Thinkless: LLM Learns When to Think", "score": 41, "session": "SD-2-4015", "pdf_url": "https://openreview.net/pdf/69d5f7c0fe7ed54d42a4a1908d50a94bc062b721.pdf", "relevant": 5, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xinchao Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 41.0, "max_score": 41, "total_relevant": 5, "total_reads": 13, "papers": [{"title": "Thinkless: LLM Learns When to Think", "score": 41, "session": "SD-2-4015", "pdf_url": "https://openreview.net/pdf/69d5f7c0fe7ed54d42a4a1908d50a94bc062b721.pdf", "relevant": 5, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Songjun Tu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 41.0, "max_score": 41, "total_relevant": 7, "total_reads": 14, "papers": [{"title": "Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL", "score": 41, "session": "SD-6-5003", "pdf_url": "https://openreview.net/pdf/dc89a6b0221240f7e120c7bd39c116d57711eec4.pdf", "relevant": 7, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiahao Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 41.0, "max_score": 41, "total_relevant": 7, "total_reads": 14, "papers": [{"title": "Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL", "score": 41, "session": "SD-6-5003", "pdf_url": "https://openreview.net/pdf/dc89a6b0221240f7e120c7bd39c116d57711eec4.pdf", "relevant": 7, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dongbin Zhao", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 28.5, "max_score": 41, "total_relevant": 10, "total_reads": 22, "papers": [{"title": "Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL", "score": 41, "session": "SD-6-5003", "pdf_url": "https://openreview.net/pdf/dc89a6b0221240f7e120c7bd39c116d57711eec4.pdf", "relevant": 7, "reads": 14}, {"title": "Learning and Planning Multi-Agent Tasks via an MoE-based World Model", "score": 16, "session": "SD-4-303", "pdf_url": "https://openreview.net/pdf/09e9ae881c16219816d4e101be5c5634163ae0b6.pdf", "relevant": 3, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qihe Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 41.0, "max_score": 41, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "Many Minds, One Goal: Time Series Forecasting via Sub-task Specialization and Inter-agent Cooperation", "score": 41, "session": "SD-6-2300", "pdf_url": "https://openreview.net/pdf/703b1261fdea6ed1048e2c011db26cfa5a8ecae0.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhengyang Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 41.0, "max_score": 41, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "Many Minds, One Goal: Time Series Forecasting via Sub-task Specialization and Inter-agent Cooperation", "score": 41, "session": "SD-6-2300", "pdf_url": "https://openreview.net/pdf/703b1261fdea6ed1048e2c011db26cfa5a8ecae0.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yang Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 41.0, "max_score": 41, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "Many Minds, One Goal: Time Series Forecasting via Sub-task Specialization and Inter-agent Cooperation", "score": 41, "session": "SD-6-2300", "pdf_url": "https://openreview.net/pdf/703b1261fdea6ed1048e2c011db26cfa5a8ecae0.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Rana Shahout", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 40.0, "max_score": 40, "total_relevant": 3, "total_reads": 12, "papers": [{"title": "Fast Inference for Augmented Large Language Models", "score": 40, "session": "SD-1-208", "pdf_url": "https://openreview.net/pdf/466ae7c6d881581805533176114b2bde5a683e8d.pdf", "relevant": 3, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Cong Liang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 40.0, "max_score": 40, "total_relevant": 3, "total_reads": 12, "papers": [{"title": "Fast Inference for Augmented Large Language Models", "score": 40, "session": "SD-1-208", "pdf_url": "https://openreview.net/pdf/466ae7c6d881581805533176114b2bde5a683e8d.pdf", "relevant": 3, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Michael Mitzenmacher", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 40.0, "max_score": 40, "total_relevant": 3, "total_reads": 12, "papers": [{"title": "Fast Inference for Augmented Large Language Models", "score": 40, "session": "SD-1-208", "pdf_url": "https://openreview.net/pdf/466ae7c6d881581805533176114b2bde5a683e8d.pdf", "relevant": 3, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Minki Kang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 40.0, "max_score": 40, "total_relevant": 10, "total_reads": 14, "papers": [{"title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools", "score": 40, "session": "SD-3-2003", "pdf_url": "https://openreview.net/pdf/c3ead64f6c3fd03156d79bf7aa5185204700b2a2.pdf", "relevant": 10, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jongwon Jeong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 40.0, "max_score": 40, "total_relevant": 10, "total_reads": 14, "papers": [{"title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools", "score": 40, "session": "SD-3-2003", "pdf_url": "https://openreview.net/pdf/c3ead64f6c3fd03156d79bf7aa5185204700b2a2.pdf", "relevant": 10, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sung Ju Hwang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 40.0, "max_score": 40, "total_relevant": 10, "total_reads": 14, "papers": [{"title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools", "score": 40, "session": "SD-3-2003", "pdf_url": "https://openreview.net/pdf/c3ead64f6c3fd03156d79bf7aa5185204700b2a2.pdf", "relevant": 10, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Vishnu Sarukkai", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 40.0, "max_score": 40, "total_relevant": 5, "total_reads": 23, "papers": [{"title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks", "score": 40, "session": "SD-6-3400", "pdf_url": "https://openreview.net/pdf/05d0804df1c396da814a29970ae7d37c40a1b84e.pdf", "relevant": 5, "reads": 23}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhiqiang Xie", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 40.0, "max_score": 40, "total_relevant": 5, "total_reads": 23, "papers": [{"title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks", "score": 40, "session": "SD-6-3400", "pdf_url": "https://openreview.net/pdf/05d0804df1c396da814a29970ae7d37c40a1b84e.pdf", "relevant": 5, "reads": 23}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kayvon Fatahalian", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 40.0, "max_score": 40, "total_relevant": 5, "total_reads": 23, "papers": [{"title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks", "score": 40, "session": "SD-6-3400", "pdf_url": "https://openreview.net/pdf/05d0804df1c396da814a29970ae7d37c40a1b84e.pdf", "relevant": 5, "reads": 23}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Gabrielle Berrada", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 39.0, "max_score": 39, "total_relevant": 2, "total_reads": 12, "papers": [{"title": "Scaling Up Active Testing to Large Language Models", "score": 39, "session": "SD-5-110", "pdf_url": "https://openreview.net/pdf/0e4fdfd137ed152835e93d1c72c71b4217f5dfa0.pdf", "relevant": 2, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jannik Kossen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 39.0, "max_score": 39, "total_relevant": 2, "total_reads": 12, "papers": [{"title": "Scaling Up Active Testing to Large Language Models", "score": 39, "session": "SD-5-110", "pdf_url": "https://openreview.net/pdf/0e4fdfd137ed152835e93d1c72c71b4217f5dfa0.pdf", "relevant": 2, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tom Rainforth", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 39.0, "max_score": 39, "total_relevant": 2, "total_reads": 12, "papers": [{"title": "Scaling Up Active Testing to Large Language Models", "score": 39, "session": "SD-5-110", "pdf_url": "https://openreview.net/pdf/0e4fdfd137ed152835e93d1c72c71b4217f5dfa0.pdf", "relevant": 2, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhenwen Liang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 38.0, "max_score": 38, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation", "score": 38, "session": "SD-2-5401", "pdf_url": "https://openreview.net/pdf/350dc187c2833efc7cd93402a27fd709f790c12b.pdf", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Linfeng Song", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 38.0, "max_score": 38, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation", "score": 38, "session": "SD-2-5401", "pdf_url": "https://openreview.net/pdf/350dc187c2833efc7cd93402a27fd709f790c12b.pdf", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dong Yu", "paper_count": 3, "highly_relevant_count": 0, "avg_score": 18.3, "max_score": 38, "total_relevant": 7, "total_reads": 19, "papers": [{"title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation", "score": 38, "session": "SD-2-5401", "pdf_url": "https://openreview.net/pdf/350dc187c2833efc7cd93402a27fd709f790c12b.pdf", "relevant": 1, "reads": 4}, {"title": "Thoughts Are All Over the Place: On the Underthinking of Long Reasoning Models", "score": 10, "session": "SD-1-5518", "pdf_url": "https://openreview.net/pdf/52cab7bd6214e5b9d63addbfb0411f3ce8f517f4.pdf", "relevant": 4, "reads": 9}, {"title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "score": 7, "session": "SD-5-306", "pdf_url": "https://openreview.net/pdf/f28db2a8d244c8994006bd065afbd5a061c42feb.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tianle Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 38.0, "max_score": 38, "total_relevant": 6, "total_reads": 33, "papers": [{"title": "Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model", "score": 38, "session": "SD-3-4701", "pdf_url": "https://openreview.net/pdf/64ee56c0512b904e0aebcd799494fb96e7bdedba.pdf", "relevant": 6, "reads": 33}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jihai Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 38.0, "max_score": 38, "total_relevant": 6, "total_reads": 33, "papers": [{"title": "Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model", "score": 38, "session": "SD-3-4701", "pdf_url": "https://openreview.net/pdf/64ee56c0512b904e0aebcd799494fb96e7bdedba.pdf", "relevant": 6, "reads": 33}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yu Cheng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 38.0, "max_score": 38, "total_relevant": 6, "total_reads": 33, "papers": [{"title": "Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model", "score": 38, "session": "SD-3-4701", "pdf_url": "https://openreview.net/pdf/64ee56c0512b904e0aebcd799494fb96e7bdedba.pdf", "relevant": 6, "reads": 33}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Khaoula Chehbouni", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 37.0, "max_score": 37, "total_relevant": 4, "total_reads": 15, "papers": [{"title": "Neither Valid nor Reliable? Investigating the Use of LLMs as Judges", "score": 37, "session": "SD-2-1313", "pdf_url": "https://arxiv.org/pdf/2508.18076", "relevant": 4, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mohammed Haddou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 37.0, "max_score": 37, "total_relevant": 4, "total_reads": 15, "papers": [{"title": "Neither Valid nor Reliable? Investigating the Use of LLMs as Judges", "score": 37, "session": "SD-2-1313", "pdf_url": "https://arxiv.org/pdf/2508.18076", "relevant": 4, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Golnoosh Farnadi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 37.0, "max_score": 37, "total_relevant": 4, "total_reads": 15, "papers": [{"title": "Neither Valid nor Reliable? Investigating the Use of LLMs as Judges", "score": 37, "session": "SD-2-1313", "pdf_url": "https://arxiv.org/pdf/2508.18076", "relevant": 4, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lingjie Jiang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 37.0, "max_score": 37, "total_relevant": 2, "total_reads": 10, "papers": [{"title": "Think Only When You Need with Large Hybrid-Reasoning Models", "score": 37, "session": "SD-2-3713", "pdf_url": "https://openreview.net/pdf/33fb886d1cda050a0c29d0bdee85176c1c3f7f31.pdf", "relevant": 2, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xun Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 37.0, "max_score": 37, "total_relevant": 2, "total_reads": 10, "papers": [{"title": "Think Only When You Need with Large Hybrid-Reasoning Models", "score": 37, "session": "SD-2-3713", "pdf_url": "https://openreview.net/pdf/33fb886d1cda050a0c29d0bdee85176c1c3f7f31.pdf", "relevant": 2, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sangwoo Park", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 37.0, "max_score": 37, "total_relevant": 0, "total_reads": 4, "papers": [{"title": "Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees", "score": 37, "session": "SD-2-3402", "pdf_url": "https://openreview.net/pdf/42cf5e1bbaeccd15c151a00a71cb2d9ecef2aa6f.pdf", "relevant": 0, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Matteo Zecchin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 37.0, "max_score": 37, "total_relevant": 0, "total_reads": 4, "papers": [{"title": "Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees", "score": 37, "session": "SD-2-3402", "pdf_url": "https://openreview.net/pdf/42cf5e1bbaeccd15c151a00a71cb2d9ecef2aa6f.pdf", "relevant": 0, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Osvaldo Simeone", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 37.0, "max_score": 37, "total_relevant": 0, "total_reads": 4, "papers": [{"title": "Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees", "score": 37, "session": "SD-2-3402", "pdf_url": "https://openreview.net/pdf/42cf5e1bbaeccd15c151a00a71cb2d9ecef2aa6f.pdf", "relevant": 0, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yijun Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 37.0, "max_score": 37, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "A Controllable Examination for Long-Context Language Models", "score": 37, "session": "SD-3-1911", "pdf_url": "https://arxiv.org/pdf/2506.02921", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zeyu Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 37.0, "max_score": 37, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "A Controllable Examination for Long-Context Language Models", "score": 37, "session": "SD-3-1911", "pdf_url": "https://arxiv.org/pdf/2506.02921", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ivan Titov", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 37.0, "max_score": 37, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "A Controllable Examination for Long-Context Language Models", "score": 37, "session": "SD-3-1911", "pdf_url": "https://arxiv.org/pdf/2506.02921", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chengpeng Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 37.0, "max_score": 37, "total_relevant": 5, "total_reads": 21, "papers": [{"title": "Teaching Language Models to Reason with Tools", "score": 37, "session": "SD-4-1607", "pdf_url": "https://openreview.net/pdf/406d2cc67059647e62684368005578debbc188e9.pdf", "relevant": 5, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhengyang Tang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 37.0, "max_score": 37, "total_relevant": 5, "total_reads": 21, "papers": [{"title": "Teaching Language Models to Reason with Tools", "score": 37, "session": "SD-4-1607", "pdf_url": "https://openreview.net/pdf/406d2cc67059647e62684368005578debbc188e9.pdf", "relevant": 5, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dayiheng Liu", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 36.0, "max_score": 37, "total_relevant": 7, "total_reads": 39, "papers": [{"title": "Teaching Language Models to Reason with Tools", "score": 37, "session": "SD-4-1607", "pdf_url": "https://openreview.net/pdf/406d2cc67059647e62684368005578debbc188e9.pdf", "relevant": 5, "reads": 21}, {"title": "Chain of Execution Supervision Promotes General Reasoning in Large Language Models", "score": 35, "session": "SD-2-4012", "pdf_url": "https://openreview.net/pdf/81876043b2c12b524b3d7cebe4d6ef2b4a940143.pdf", "relevant": 2, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yueh-Han Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 37.0, "max_score": 37, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts", "score": 37, "session": "SD-5-1104", "pdf_url": "https://arxiv.org/pdf/2505.21828", "relevant": 1, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Guy Davidson", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 37.0, "max_score": 37, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts", "score": 37, "session": "SD-5-1104", "pdf_url": "https://arxiv.org/pdf/2505.21828", "relevant": 1, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Brenden Lake", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 37.0, "max_score": 37, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts", "score": 37, "session": "SD-5-1104", "pdf_url": "https://arxiv.org/pdf/2505.21828", "relevant": 1, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yao Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 36.0, "max_score": 36, "total_relevant": 10, "total_reads": 21, "papers": [{"title": "DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios", "score": 36, "session": "SD-1-1111", "pdf_url": "https://arxiv.org/pdf/2510.15501", "relevant": 10, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yitong Sun", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 36.0, "max_score": 36, "total_relevant": 10, "total_reads": 21, "papers": [{"title": "DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios", "score": 36, "session": "SD-1-1111", "pdf_url": "https://arxiv.org/pdf/2510.15501", "relevant": 10, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xingxing Wei", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 36.0, "max_score": 36, "total_relevant": 10, "total_reads": 21, "papers": [{"title": "DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios", "score": 36, "session": "SD-1-1111", "pdf_url": "https://arxiv.org/pdf/2510.15501", "relevant": 10, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Alfin Wijaya Rahardja", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 36.0, "max_score": 36, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Can Agent Fix Agent Issues?", "score": 36, "session": "SD-2-2412", "pdf_url": "https://openreview.net/pdf/ab4c234adba0d835dbb64828e87b465d8983cecd.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junwei Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 36.0, "max_score": 36, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Can Agent Fix Agent Issues?", "score": 36, "session": "SD-2-2412", "pdf_url": "https://openreview.net/pdf/ab4c234adba0d835dbb64828e87b465d8983cecd.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yiling Lou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 36.0, "max_score": 36, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Can Agent Fix Agent Issues?", "score": 36, "session": "SD-2-2412", "pdf_url": "https://openreview.net/pdf/ab4c234adba0d835dbb64828e87b465d8983cecd.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mohammad Shahab Sepehri", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 36.0, "max_score": 36, "total_relevant": 2, "total_reads": 7, "papers": [{"title": "Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs", "score": 36, "session": "SD-4-4605", "pdf_url": "https://arxiv.org/pdf/2507.11932", "relevant": 2, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Berk Tinaz", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 36.0, "max_score": 36, "total_relevant": 2, "total_reads": 7, "papers": [{"title": "Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs", "score": 36, "session": "SD-4-4605", "pdf_url": "https://arxiv.org/pdf/2507.11932", "relevant": 2, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mahdi Soltanolkotabi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 36.0, "max_score": 36, "total_relevant": 2, "total_reads": 7, "papers": [{"title": "Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs", "score": 36, "session": "SD-4-4605", "pdf_url": "https://arxiv.org/pdf/2507.11932", "relevant": 2, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xianda Guo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 15, "total_reads": 52, "papers": [{"title": "SURDS: Benchmarking Spatial Understanding and Reasoning in Driving Scenarios with Vision Language Models", "score": 35, "session": "SD-1-4613", "pdf_url": "https://arxiv.org/pdf/2411.13112", "relevant": 15, "reads": 52}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ruijun Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 15, "total_reads": 52, "papers": [{"title": "SURDS: Benchmarking Spatial Understanding and Reasoning in Driving Scenarios with Vision Language Models", "score": 35, "session": "SD-1-4613", "pdf_url": "https://arxiv.org/pdf/2411.13112", "relevant": 15, "reads": 52}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Long Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 15, "total_reads": 52, "papers": [{"title": "SURDS: Benchmarking Spatial Understanding and Reasoning in Driving Scenarios with Vision Language Models", "score": 35, "session": "SD-1-4613", "pdf_url": "https://arxiv.org/pdf/2411.13112", "relevant": 15, "reads": 52}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiaxin Guo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 4, "total_reads": 21, "papers": [{"title": "Reward Reasoning Models", "score": 35, "session": "SD-1-4105", "pdf_url": "https://openreview.net/pdf/1b460f75d98849e58bc2a9cf1b6673536d0898d5.pdf", "relevant": 4, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zewen Chi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 4, "total_reads": 21, "papers": [{"title": "Reward Reasoning Models", "score": 35, "session": "SD-1-4105", "pdf_url": "https://openreview.net/pdf/1b460f75d98849e58bc2a9cf1b6673536d0898d5.pdf", "relevant": 4, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mintong Kang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 2, "total_reads": 9, "papers": [{"title": "PolyGuard: Massive Multi-Domain Safety Policy-Grounded Guardrail Dataset", "score": 35, "session": "SD-2-1308", "pdf_url": "https://arxiv.org/pdf/2506.19054", "relevant": 2, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhaorun Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 2, "total_reads": 9, "papers": [{"title": "PolyGuard: Massive Multi-Domain Safety Policy-Grounded Guardrail Dataset", "score": 35, "session": "SD-2-1308", "pdf_url": "https://arxiv.org/pdf/2506.19054", "relevant": 2, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Nuo Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 2, "total_reads": 18, "papers": [{"title": "Chain of Execution Supervision Promotes General Reasoning in Large Language Models", "score": 35, "session": "SD-2-4012", "pdf_url": "https://openreview.net/pdf/81876043b2c12b524b3d7cebe4d6ef2b4a940143.pdf", "relevant": 2, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zehua Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 2, "total_reads": 18, "papers": [{"title": "Chain of Execution Supervision Promotes General Reasoning in Large Language Models", "score": 35, "session": "SD-2-4012", "pdf_url": "https://openreview.net/pdf/81876043b2c12b524b3d7cebe4d6ef2b4a940143.pdf", "relevant": 2, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Andrew M. Bean", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 8, "total_reads": 21, "papers": [{"title": "Measuring what Matters: Construct Validity in Large Language Model Benchmarks", "score": 35, "session": "SD-3-107", "pdf_url": "https://arxiv.org/pdf/2511.04703", "relevant": 8, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ryan Othniel Kearns", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 8, "total_reads": 21, "papers": [{"title": "Measuring what Matters: Construct Validity in Large Language Model Benchmarks", "score": 35, "session": "SD-3-107", "pdf_url": "https://arxiv.org/pdf/2511.04703", "relevant": 8, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Adam Mahdi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 8, "total_reads": 21, "papers": [{"title": "Measuring what Matters: Construct Validity in Large Language Model Benchmarks", "score": 35, "session": "SD-3-107", "pdf_url": "https://arxiv.org/pdf/2511.04703", "relevant": 8, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zeyu Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 2, "total_reads": 2, "papers": [{"title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants", "score": 35, "session": "SD-4-2015", "pdf_url": "https://openreview.net/pdf/96e1b7b1eeb53a530580aff14cf9527fe3e3d1ac.pdf", "relevant": 2, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Quanyu Dai", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 2, "total_reads": 2, "papers": [{"title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants", "score": 35, "session": "SD-4-2015", "pdf_url": "https://openreview.net/pdf/96e1b7b1eeb53a530580aff14cf9527fe3e3d1ac.pdf", "relevant": 2, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shihan Dou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 0, "total_reads": 3, "papers": [{"title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving", "score": 35, "session": "SD-4-4110", "pdf_url": "https://openreview.net/pdf/78058118e972a34b3ab22aa5b6b0c000ab083f58.pdf", "relevant": 0, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ming Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 0, "total_reads": 3, "papers": [{"title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving", "score": 35, "session": "SD-4-4110", "pdf_url": "https://openreview.net/pdf/78058118e972a34b3ab22aa5b6b0c000ab083f58.pdf", "relevant": 0, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xuanjing Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 0, "total_reads": 3, "papers": [{"title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving", "score": 35, "session": "SD-4-4110", "pdf_url": "https://openreview.net/pdf/78058118e972a34b3ab22aa5b6b0c000ab083f58.pdf", "relevant": 0, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "ShuHang Xun", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 9, "total_reads": 18, "papers": [{"title": "RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video", "score": 35, "session": "SD-6-4412", "pdf_url": "https://arxiv.org/pdf/2505.02064", "relevant": 9, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sicheng Tao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 9, "total_reads": 18, "papers": [{"title": "RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video", "score": 35, "session": "SD-6-4412", "pdf_url": "https://arxiv.org/pdf/2505.02064", "relevant": 9, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xuming Hu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 35.0, "max_score": 35, "total_relevant": 9, "total_reads": 18, "papers": [{"title": "RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video", "score": 35, "session": "SD-6-4412", "pdf_url": "https://arxiv.org/pdf/2505.02064", "relevant": 9, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Guibin Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 34.0, "max_score": 34, "total_relevant": 6, "total_reads": 9, "papers": [{"title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems", "score": 34, "session": "SD-1-5507", "pdf_url": "https://openreview.net/pdf/52f961783a3212459f228b4ec297f523ba2d0c95.pdf", "relevant": 6, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Muxin Fu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 34.0, "max_score": 34, "total_relevant": 6, "total_reads": 9, "papers": [{"title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems", "score": 34, "session": "SD-1-5507", "pdf_url": "https://openreview.net/pdf/52f961783a3212459f228b4ec297f523ba2d0c95.pdf", "relevant": 6, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shuicheng YAN", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 34.0, "max_score": 34, "total_relevant": 6, "total_reads": 9, "papers": [{"title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems", "score": 34, "session": "SD-1-5507", "pdf_url": "https://openreview.net/pdf/52f961783a3212459f228b4ec297f523ba2d0c95.pdf", "relevant": 6, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haozhen Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 34.0, "max_score": 34, "total_relevant": 4, "total_reads": 11, "papers": [{"title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning", "score": 34, "session": "SD-3-3604", "pdf_url": "https://openreview.net/pdf/363710ed9012131f837da723473810ad47f9d2c9.pdf", "relevant": 4, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tao Feng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 34.0, "max_score": 34, "total_relevant": 4, "total_reads": 11, "papers": [{"title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning", "score": 34, "session": "SD-3-3604", "pdf_url": "https://openreview.net/pdf/363710ed9012131f837da723473810ad47f9d2c9.pdf", "relevant": 4, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiaxuan You", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 34.0, "max_score": 34, "total_relevant": 4, "total_reads": 11, "papers": [{"title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning", "score": 34, "session": "SD-3-3604", "pdf_url": "https://openreview.net/pdf/363710ed9012131f837da723473810ad47f9d2c9.pdf", "relevant": 4, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wei Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 34.0, "max_score": 34, "total_relevant": 2, "total_reads": 4, "papers": [{"title": "LIFEBENCH: Evaluating Length Instruction Following in Large Language Models", "score": 34, "session": "SD-3-3502", "pdf_url": "https://arxiv.org/pdf/2505.16234", "relevant": 2, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhenhong Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 34.0, "max_score": 34, "total_relevant": 2, "total_reads": 4, "papers": [{"title": "LIFEBENCH: Evaluating Length Instruction Following in Large Language Models", "score": 34, "session": "SD-3-3502", "pdf_url": "https://arxiv.org/pdf/2505.16234", "relevant": 2, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sen Su", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 34.0, "max_score": 34, "total_relevant": 2, "total_reads": 4, "papers": [{"title": "LIFEBENCH: Evaluating Length Instruction Following in Large Language Models", "score": 34, "session": "SD-3-3502", "pdf_url": "https://arxiv.org/pdf/2505.16234", "relevant": 2, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zixuan Hu", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 21.5, "max_score": 34, "total_relevant": 6, "total_reads": 9, "papers": [{"title": "Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler", "score": 34, "session": "SD-4-1205", "pdf_url": "https://openreview.net/pdf/2b29a4e872717270eff659096ceb805ef86a1735.pdf", "relevant": 3, "reads": 5}, {"title": "LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for Scientific Embodied Agents", "score": 9, "session": "SD-2-2204", "pdf_url": "https://arxiv.org/pdf/2505.22634", "relevant": 3, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Li Shen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 34.0, "max_score": 34, "total_relevant": 3, "total_reads": 5, "papers": [{"title": "Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler", "score": 34, "session": "SD-4-1205", "pdf_url": "https://openreview.net/pdf/2b29a4e872717270eff659096ceb805ef86a1735.pdf", "relevant": 3, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Cheng Qian", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 33.0, "max_score": 33, "total_relevant": 4, "total_reads": 22, "papers": [{"title": "ToolRL: Reward is All Tool Learning Needs", "score": 33, "session": "SD-1-511", "pdf_url": "https://openreview.net/pdf/097ae4a34c2eb2b82b2bb8fccc279fb0e3585304.pdf", "relevant": 4, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Emre Can Acikgoz", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 33.0, "max_score": 33, "total_relevant": 4, "total_reads": 22, "papers": [{"title": "ToolRL: Reward is All Tool Learning Needs", "score": 33, "session": "SD-1-511", "pdf_url": "https://openreview.net/pdf/097ae4a34c2eb2b82b2bb8fccc279fb0e3585304.pdf", "relevant": 4, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Heng Ji", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 33.0, "max_score": 33, "total_relevant": 4, "total_reads": 22, "papers": [{"title": "ToolRL: Reward is All Tool Learning Needs", "score": 33, "session": "SD-1-511", "pdf_url": "https://openreview.net/pdf/097ae4a34c2eb2b82b2bb8fccc279fb0e3585304.pdf", "relevant": 4, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yihe Deng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 33.0, "max_score": 33, "total_relevant": 13, "total_reads": 68, "papers": [{"title": "OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles", "score": 33, "session": "SD-1-5514", "pdf_url": "https://openreview.net/pdf/62ceb097c643e0416c764c187ebf4f4d6d1ba9c3.pdf", "relevant": 13, "reads": 68}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hritik Bansal", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 33.0, "max_score": 33, "total_relevant": 13, "total_reads": 68, "papers": [{"title": "OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles", "score": 33, "session": "SD-1-5514", "pdf_url": "https://openreview.net/pdf/62ceb097c643e0416c764c187ebf4f4d6d1ba9c3.pdf", "relevant": 13, "reads": 68}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shmuel Berman", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 33.0, "max_score": 33, "total_relevant": 5, "total_reads": 32, "papers": [{"title": "VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs", "score": 33, "session": "SD-1-4911", "pdf_url": "https://openreview.net/pdf/ca79a743ac55c7fce5e100be3b2d695bf84a71ba.pdf", "relevant": 5, "reads": 32}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jia Deng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 33.0, "max_score": 33, "total_relevant": 5, "total_reads": 32, "papers": [{"title": "VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs", "score": 33, "session": "SD-1-4911", "pdf_url": "https://openreview.net/pdf/ca79a743ac55c7fce5e100be3b2d695bf84a71ba.pdf", "relevant": 5, "reads": 32}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zongyuan Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 32.0, "max_score": 32, "total_relevant": 3, "total_reads": 10, "papers": [{"title": "LLM-PySC2: Starcraft II learning environment for Large Language Models", "score": 32, "session": "SD-1-1503", "pdf_url": "https://openreview.net/pdf/ae23b5a1cbb9529359c184cdf3d394c6383add5a.pdf", "relevant": 3, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yanan Ni", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 32.0, "max_score": 32, "total_relevant": 3, "total_reads": 10, "papers": [{"title": "LLM-PySC2: Starcraft II learning environment for Large Language Models", "score": 32, "session": "SD-1-1503", "pdf_url": "https://openreview.net/pdf/ae23b5a1cbb9529359c184cdf3d394c6383add5a.pdf", "relevant": 3, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xuebo Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 32.0, "max_score": 32, "total_relevant": 3, "total_reads": 10, "papers": [{"title": "LLM-PySC2: Starcraft II learning environment for Large Language Models", "score": 32, "session": "SD-1-1503", "pdf_url": "https://openreview.net/pdf/ae23b5a1cbb9529359c184cdf3d394c6383add5a.pdf", "relevant": 3, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Enjun Du", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 32.0, "max_score": 32, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments", "score": 32, "session": "SD-3-5412", "pdf_url": "https://openreview.net/pdf/e15247fbacf803da9beef26d39153bd1afd55fe6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xunkai Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 32.0, "max_score": 32, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments", "score": 32, "session": "SD-3-5412", "pdf_url": "https://openreview.net/pdf/e15247fbacf803da9beef26d39153bd1afd55fe6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Guoren Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 32.0, "max_score": 32, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments", "score": 32, "session": "SD-3-5412", "pdf_url": "https://openreview.net/pdf/e15247fbacf803da9beef26d39153bd1afd55fe6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Boyuan Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 32.0, "max_score": 32, "total_relevant": 2, "total_reads": 5, "papers": [{"title": "InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback", "score": 32, "session": "SD-5-5104", "pdf_url": "https://arxiv.org/pdf/2505.23950", "relevant": 2, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Donghai Hong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 32.0, "max_score": 32, "total_relevant": 2, "total_reads": 5, "papers": [{"title": "InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback", "score": 32, "session": "SD-5-5104", "pdf_url": "https://arxiv.org/pdf/2505.23950", "relevant": 2, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yaodong Yang", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 29.0, "max_score": 32, "total_relevant": 2, "total_reads": 12, "papers": [{"title": "InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback", "score": 32, "session": "SD-5-5104", "pdf_url": "https://arxiv.org/pdf/2505.23950", "relevant": 2, "reads": 5}, {"title": "World Models Should Prioritize the Unification of Physical and Social Dynamics", "score": 26, "session": "SD-3-1009", "pdf_url": "https://arxiv.org/pdf/2510.21219", "relevant": 0, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "ChangHao Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 31.0, "max_score": 31, "total_relevant": 5, "total_reads": 9, "papers": [{"title": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs", "score": 31, "session": "SD-1-3416", "pdf_url": "https://openreview.net/pdf/01546f97c01b32e5c0cf560fc4be7a511b46e042.pdf", "relevant": 5, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Simeng Han", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 31.0, "max_score": 31, "total_relevant": 4, "total_reads": 6, "papers": [{"title": "Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models", "score": 31, "session": "SD-4-1908", "pdf_url": "https://openreview.net/pdf/bc154619c4195b6a62774c122f1706e4d7b1bb7f.pdf", "relevant": 4, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Howard Dai", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 31.0, "max_score": 31, "total_relevant": 4, "total_reads": 6, "papers": [{"title": "Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models", "score": 31, "session": "SD-4-1908", "pdf_url": "https://openreview.net/pdf/bc154619c4195b6a62774c122f1706e4d7b1bb7f.pdf", "relevant": 4, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "R. Thomas McCoy", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 31.0, "max_score": 31, "total_relevant": 4, "total_reads": 6, "papers": [{"title": "Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models", "score": 31, "session": "SD-4-1908", "pdf_url": "https://openreview.net/pdf/bc154619c4195b6a62774c122f1706e4d7b1bb7f.pdf", "relevant": 4, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ryan Cheng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 31.0, "max_score": 31, "total_relevant": 4, "total_reads": 16, "papers": [{"title": "Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning", "score": 31, "session": "SD-6-1805", "pdf_url": "https://openreview.net/pdf/109c600393cc962e64028e8425eca62778f40ee9.pdf", "relevant": 4, "reads": 16}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Natasha Jaques", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 31.0, "max_score": 31, "total_relevant": 4, "total_reads": 16, "papers": [{"title": "Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning", "score": 31, "session": "SD-6-1805", "pdf_url": "https://openreview.net/pdf/109c600393cc962e64028e8425eca62778f40ee9.pdf", "relevant": 4, "reads": 16}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chi-Pin Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 31.0, "max_score": 31, "total_relevant": 31, "total_reads": 99, "papers": [{"title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning", "score": 31, "session": "SD-6-2203", "pdf_url": "https://openreview.net/pdf/b35b0fc70612e191baced400f754db8ff1fae711.pdf", "relevant": 31, "reads": 99}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yueh-Hua Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 31.0, "max_score": 31, "total_relevant": 31, "total_reads": 99, "papers": [{"title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning", "score": 31, "session": "SD-6-2203", "pdf_url": "https://openreview.net/pdf/b35b0fc70612e191baced400f754db8ff1fae711.pdf", "relevant": 31, "reads": 99}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Fu-En Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 31.0, "max_score": 31, "total_relevant": 31, "total_reads": 99, "papers": [{"title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning", "score": 31, "session": "SD-6-2203", "pdf_url": "https://openreview.net/pdf/b35b0fc70612e191baced400f754db8ff1fae711.pdf", "relevant": 31, "reads": 99}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Belinda Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 30.0, "max_score": 30, "total_relevant": 9, "total_reads": 13, "papers": [{"title": "QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?", "score": 30, "session": "SD-4-809", "pdf_url": "https://arxiv.org/pdf/2503.22674", "relevant": 9, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Been Kim", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 30.0, "max_score": 30, "total_relevant": 9, "total_reads": 13, "papers": [{"title": "QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?", "score": 30, "session": "SD-4-809", "pdf_url": "https://arxiv.org/pdf/2503.22674", "relevant": 9, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zi Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 30.0, "max_score": 30, "total_relevant": 9, "total_reads": 13, "papers": [{"title": "QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?", "score": 30, "session": "SD-4-809", "pdf_url": "https://arxiv.org/pdf/2503.22674", "relevant": 9, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kun Xiang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 30.0, "max_score": 30, "total_relevant": 0, "total_reads": 4, "papers": [{"title": "SeePhys:  Does Seeing Help Thinking? √¢¬Ä¬ì Benchmarking Vision-Based Physics Reasoning", "score": 30, "session": "SD-4-4603", "pdf_url": "https://arxiv.org/pdf/2505.19099", "relevant": 0, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Heng Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 30.0, "max_score": 30, "total_relevant": 0, "total_reads": 4, "papers": [{"title": "SeePhys:  Does Seeing Help Thinking? √¢¬Ä¬ì Benchmarking Vision-Based Physics Reasoning", "score": 30, "session": "SD-4-4603", "pdf_url": "https://arxiv.org/pdf/2505.19099", "relevant": 0, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yangyang Guo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 30.0, "max_score": 30, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense", "score": 30, "session": "SD-5-1208", "pdf_url": "https://openreview.net/pdf/85f3e38bd08668ee901051237edc01d1b8d8823e.pdf", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Fangkai Jiao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 30.0, "max_score": 30, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense", "score": 30, "session": "SD-5-1208", "pdf_url": "https://openreview.net/pdf/85f3e38bd08668ee901051237edc01d1b8d8823e.pdf", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mohan Kankanhalli", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 30.0, "max_score": 30, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense", "score": 30, "session": "SD-5-1208", "pdf_url": "https://openreview.net/pdf/85f3e38bd08668ee901051237edc01d1b8d8823e.pdf", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhangdie Yuan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 30.0, "max_score": 30, "total_relevant": 0, "total_reads": 5, "papers": [{"title": "Introducing FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark", "score": 30, "session": "SD-6-401", "pdf_url": "https://arxiv.org/pdf/2502.19676", "relevant": 0, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zifeng Ding", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 30.0, "max_score": 30, "total_relevant": 0, "total_reads": 5, "papers": [{"title": "Introducing FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark", "score": 30, "session": "SD-6-401", "pdf_url": "https://arxiv.org/pdf/2502.19676", "relevant": 0, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Andreas Vlachos", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 30.0, "max_score": 30, "total_relevant": 0, "total_reads": 5, "papers": [{"title": "Introducing FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark", "score": 30, "session": "SD-6-401", "pdf_url": "https://arxiv.org/pdf/2502.19676", "relevant": 0, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuanzhe Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 29.0, "max_score": 29, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve", "score": 29, "session": "SD-4-1608", "pdf_url": "https://openreview.net/pdf/11bc550a62e76a702248f346f36018dce79624fb.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ryan Deng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 29.0, "max_score": 29, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve", "score": 29, "session": "SD-4-1608", "pdf_url": "https://openreview.net/pdf/11bc550a62e76a702248f346f36018dce79624fb.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jie Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 29.0, "max_score": 29, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve", "score": 29, "session": "SD-4-1608", "pdf_url": "https://openreview.net/pdf/11bc550a62e76a702248f346f36018dce79624fb.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dong Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 29.0, "max_score": 29, "total_relevant": 5, "total_reads": 15, "papers": [{"title": "SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search", "score": 29, "session": "SD-4-2011", "pdf_url": "https://openreview.net/pdf/b7f1cf26530dd86e2ae6d7c4a3a2f77a1c33b13b.pdf", "relevant": 5, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xujiang Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 29.0, "max_score": 29, "total_relevant": 5, "total_reads": 15, "papers": [{"title": "SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search", "score": 29, "session": "SD-4-2011", "pdf_url": "https://openreview.net/pdf/b7f1cf26530dd86e2ae6d7c4a3a2f77a1c33b13b.pdf", "relevant": 5, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haifeng Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 29.0, "max_score": 29, "total_relevant": 5, "total_reads": 15, "papers": [{"title": "SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search", "score": 29, "session": "SD-4-2011", "pdf_url": "https://openreview.net/pdf/b7f1cf26530dd86e2ae6d7c4a3a2f77a1c33b13b.pdf", "relevant": 5, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhaowei Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 29.0, "max_score": 29, "total_relevant": 2, "total_reads": 22, "papers": [{"title": "MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly", "score": 29, "session": "SD-5-4507", "pdf_url": "https://arxiv.org/pdf/2505.10610", "relevant": 2, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenhao Yu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 29.0, "max_score": 29, "total_relevant": 2, "total_reads": 22, "papers": [{"title": "MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly", "score": 29, "session": "SD-5-4507", "pdf_url": "https://arxiv.org/pdf/2505.10610", "relevant": 2, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mark Steedman", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 29.0, "max_score": 29, "total_relevant": 2, "total_reads": 22, "papers": [{"title": "MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly", "score": 29, "session": "SD-5-4507", "pdf_url": "https://arxiv.org/pdf/2505.10610", "relevant": 2, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Bo Lv", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 1, "total_reads": 5, "papers": [{"title": "SpecEM: Training-Free LLM Ensembling via Iterative Drafting, Verification, and Online Feedback", "score": 28, "session": "SD-1-3802", "pdf_url": "https://openreview.net/pdf/ce36072cf898e279341e1ca31c51626ddead6950.pdf", "relevant": 1, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Nayu Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 1, "total_reads": 5, "papers": [{"title": "SpecEM: Training-Free LLM Ensembling via Iterative Drafting, Verification, and Online Feedback", "score": 28, "session": "SD-1-3802", "pdf_url": "https://openreview.net/pdf/ce36072cf898e279341e1ca31c51626ddead6950.pdf", "relevant": 1, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ping Luo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 1, "total_reads": 5, "papers": [{"title": "SpecEM: Training-Free LLM Ensembling via Iterative Drafting, Verification, and Online Feedback", "score": 28, "session": "SD-1-3802", "pdf_url": "https://openreview.net/pdf/ce36072cf898e279341e1ca31c51626ddead6950.pdf", "relevant": 1, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Huanyu Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 3, "total_reads": 15, "papers": [{"title": "SATURN: SAT-based Reinforcement Learning to Unleash LLMs Reasoning", "score": 28, "session": "SD-1-4107", "pdf_url": "https://openreview.net/pdf/8c8fdd51e03779f1c757cd48a07127240f2339d4.pdf", "relevant": 3, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jia Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 3, "total_reads": 15, "papers": [{"title": "SATURN: SAT-based Reinforcement Learning to Unleash LLMs Reasoning", "score": 28, "session": "SD-1-4107", "pdf_url": "https://openreview.net/pdf/8c8fdd51e03779f1c757cd48a07127240f2339d4.pdf", "relevant": 3, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ge Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 3, "total_reads": 15, "papers": [{"title": "SATURN: SAT-based Reinforcement Learning to Unleash LLMs Reasoning", "score": 28, "session": "SD-1-4107", "pdf_url": "https://openreview.net/pdf/8c8fdd51e03779f1c757cd48a07127240f2339d4.pdf", "relevant": 3, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shulin Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "ThinkBench: Dynamic Out-of-Distribution Evaluation for Robust LLM Reasoning", "score": 28, "session": "SD-2-109", "pdf_url": "https://arxiv.org/pdf/2502.16268", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Linyi Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "ThinkBench: Dynamic Out-of-Distribution Evaluation for Robust LLM Reasoning", "score": 28, "session": "SD-2-109", "pdf_url": "https://arxiv.org/pdf/2502.16268", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yue Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "ThinkBench: Dynamic Out-of-Distribution Evaluation for Robust LLM Reasoning", "score": 28, "session": "SD-2-109", "pdf_url": "https://arxiv.org/pdf/2502.16268", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Anamika Lochab", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 2, "total_reads": 7, "papers": [{"title": "VERA: Variational Inference Framework for Jailbreaking Large Language Models", "score": 28, "session": "SD-3-5300", "pdf_url": "https://openreview.net/pdf/5d8395fee02384e70cded7bf691df4c9976ebb5a.pdf", "relevant": 2, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lu Yan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 2, "total_reads": 7, "papers": [{"title": "VERA: Variational Inference Framework for Jailbreaking Large Language Models", "score": 28, "session": "SD-3-5300", "pdf_url": "https://openreview.net/pdf/5d8395fee02384e70cded7bf691df4c9976ebb5a.pdf", "relevant": 2, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ruqi Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 2, "total_reads": 7, "papers": [{"title": "VERA: Variational Inference Framework for Jailbreaking Large Language Models", "score": 28, "session": "SD-3-5300", "pdf_url": "https://openreview.net/pdf/5d8395fee02384e70cded7bf691df4c9976ebb5a.pdf", "relevant": 2, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaoyu Zhan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 6, "total_reads": 30, "papers": [{"title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models", "score": 28, "session": "SD-4-4701", "pdf_url": "https://openreview.net/pdf/1321dbcd7c95604245189132069ca5ac16d3058c.pdf", "relevant": 6, "reads": 30}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenxuan Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 6, "total_reads": 30, "papers": [{"title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models", "score": 28, "session": "SD-4-4701", "pdf_url": "https://openreview.net/pdf/1321dbcd7c95604245189132069ca5ac16d3058c.pdf", "relevant": 6, "reads": 30}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yanwen Guo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 6, "total_reads": 30, "papers": [{"title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models", "score": 28, "session": "SD-4-4701", "pdf_url": "https://openreview.net/pdf/1321dbcd7c95604245189132069ca5ac16d3058c.pdf", "relevant": 6, "reads": 30}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shi Qiu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 4, "total_reads": 5, "papers": [{"title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models", "score": 28, "session": "SD-4-2208", "pdf_url": "https://arxiv.org/pdf/2504.16074", "relevant": 4, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shaoyang Guo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 4, "total_reads": 5, "papers": [{"title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models", "score": 28, "session": "SD-4-2208", "pdf_url": "https://arxiv.org/pdf/2504.16074", "relevant": 4, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hua Xing Zhu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 4, "total_reads": 5, "papers": [{"title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models", "score": 28, "session": "SD-4-2208", "pdf_url": "https://arxiv.org/pdf/2504.16074", "relevant": 4, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xinglin Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 5, "total_reads": 14, "papers": [{"title": "Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling", "score": 28, "session": "SD-5-3518", "pdf_url": "https://openreview.net/pdf/639ca4d7460b732c0c9d399142939d60bcdb29d2.pdf", "relevant": 5, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yiwei Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 5, "total_reads": 14, "papers": [{"title": "Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling", "score": 28, "session": "SD-5-3518", "pdf_url": "https://openreview.net/pdf/639ca4d7460b732c0c9d399142939d60bcdb29d2.pdf", "relevant": 5, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kan Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 5, "total_reads": 14, "papers": [{"title": "Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling", "score": 28, "session": "SD-5-3518", "pdf_url": "https://openreview.net/pdf/639ca4d7460b732c0c9d399142939d60bcdb29d2.pdf", "relevant": 5, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Siru Ouyang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 10, "total_reads": 23, "papers": [{"title": "RAST: Reasoning Activation in LLMs via Small-model Transfer", "score": 28, "session": "SD-5-1901", "pdf_url": "https://openreview.net/pdf/4e1cf538a0cee20ba69772f7ede39e11ffddd493.pdf", "relevant": 10, "reads": 23}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xinyu Zhu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 10, "total_reads": 23, "papers": [{"title": "RAST: Reasoning Activation in LLMs via Small-model Transfer", "score": 28, "session": "SD-5-1901", "pdf_url": "https://openreview.net/pdf/4e1cf538a0cee20ba69772f7ede39e11ffddd493.pdf", "relevant": 10, "reads": 23}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiawei Han", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 10, "total_reads": 23, "papers": [{"title": "RAST: Reasoning Activation in LLMs via Small-model Transfer", "score": 28, "session": "SD-5-1901", "pdf_url": "https://openreview.net/pdf/4e1cf538a0cee20ba69772f7ede39e11ffddd493.pdf", "relevant": 10, "reads": 23}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qingyang Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 9, "total_reads": 26, "papers": [{"title": "Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization", "score": 28, "session": "SD-6-3718", "pdf_url": "https://openreview.net/pdf/4beac7313c00cacdbfe88ef717756d49edfe75a1.pdf", "relevant": 9, "reads": 26}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haitao Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 9, "total_reads": 26, "papers": [{"title": "Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization", "score": 28, "session": "SD-6-3718", "pdf_url": "https://openreview.net/pdf/4beac7313c00cacdbfe88ef717756d49edfe75a1.pdf", "relevant": 9, "reads": 26}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yatao Bian", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 9, "total_reads": 26, "papers": [{"title": "Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization", "score": 28, "session": "SD-6-3718", "pdf_url": "https://openreview.net/pdf/4beac7313c00cacdbfe88ef717756d49edfe75a1.pdf", "relevant": 9, "reads": 26}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kongcheng Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 9, "total_reads": 21, "papers": [{"title": "Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning", "score": 28, "session": "SD-6-3404", "pdf_url": "https://openreview.net/pdf/4c696fcdaaa5b6e8b53a1bf9e94c8993ee0cd433.pdf", "relevant": 9, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "QI YAO", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 28.0, "max_score": 28, "total_relevant": 9, "total_reads": 21, "papers": [{"title": "Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning", "score": 28, "session": "SD-6-3404", "pdf_url": "https://openreview.net/pdf/4c696fcdaaa5b6e8b53a1bf9e94c8993ee0cd433.pdf", "relevant": 9, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dacheng Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 27.0, "max_score": 27, "total_relevant": 24, "total_reads": 63, "papers": [{"title": "WorldModelBench: Judging Video Generation Models As World Models", "score": 27, "session": "SD-1-4715", "pdf_url": "https://arxiv.org/pdf/2502.20694", "relevant": 24, "reads": 63}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yunhao Fang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 27.0, "max_score": 27, "total_relevant": 24, "total_reads": 63, "papers": [{"title": "WorldModelBench: Judging Video Generation Models As World Models", "score": 27, "session": "SD-1-4715", "pdf_url": "https://arxiv.org/pdf/2502.20694", "relevant": 24, "reads": 63}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yao Lu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 27.0, "max_score": 27, "total_relevant": 24, "total_reads": 63, "papers": [{"title": "WorldModelBench: Judging Video Generation Models As World Models", "score": 27, "session": "SD-1-4715", "pdf_url": "https://arxiv.org/pdf/2502.20694", "relevant": 24, "reads": 63}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xander Davies", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 27.0, "max_score": 27, "total_relevant": 4, "total_reads": 5, "papers": [{"title": "Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs", "score": 27, "session": "SD-4-5401", "pdf_url": "https://openreview.net/pdf/106f56012866f9c2a8f8d433881447430b77c9df.pdf", "relevant": 4, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Eric Winsor", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 27.0, "max_score": 27, "total_relevant": 4, "total_reads": 5, "papers": [{"title": "Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs", "score": 27, "session": "SD-4-5401", "pdf_url": "https://openreview.net/pdf/106f56012866f9c2a8f8d433881447430b77c9df.pdf", "relevant": 4, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yarin Gal", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 27.0, "max_score": 27, "total_relevant": 4, "total_reads": 5, "papers": [{"title": "Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs", "score": 27, "session": "SD-4-5401", "pdf_url": "https://openreview.net/pdf/106f56012866f9c2a8f8d433881447430b77c9df.pdf", "relevant": 4, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shen Dong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 27.0, "max_score": 27, "total_relevant": 8, "total_reads": 11, "papers": [{"title": "Memory Injection Attacks on LLM Agents via Query-Only Interaction", "score": 27, "session": "SD-5-1412", "pdf_url": "https://openreview.net/pdf/0fa0fce8cc51e3ae2f36e5aef29236b15bcc35bc.pdf", "relevant": 8, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shaochen Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 27.0, "max_score": 27, "total_relevant": 8, "total_reads": 11, "papers": [{"title": "Memory Injection Attacks on LLM Agents via Query-Only Interaction", "score": 27, "session": "SD-5-1412", "pdf_url": "https://openreview.net/pdf/0fa0fce8cc51e3ae2f36e5aef29236b15bcc35bc.pdf", "relevant": 8, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhen Xiang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 27.0, "max_score": 27, "total_relevant": 8, "total_reads": 11, "papers": [{"title": "Memory Injection Attacks on LLM Agents via Query-Only Interaction", "score": 27, "session": "SD-5-1412", "pdf_url": "https://openreview.net/pdf/0fa0fce8cc51e3ae2f36e5aef29236b15bcc35bc.pdf", "relevant": 8, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaoyuan Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 26.0, "max_score": 26, "total_relevant": 0, "total_reads": 7, "papers": [{"title": "World Models Should Prioritize the Unification of Physical and Social Dynamics", "score": 26, "session": "SD-3-1009", "pdf_url": "https://arxiv.org/pdf/2510.21219", "relevant": 0, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chengdong Ma", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 26.0, "max_score": 26, "total_relevant": 0, "total_reads": 7, "papers": [{"title": "World Models Should Prioritize the Unification of Physical and Social Dynamics", "score": 26, "session": "SD-3-1009", "pdf_url": "https://arxiv.org/pdf/2510.21219", "relevant": 0, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Leheng Sheng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 26.0, "max_score": 26, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "On Reasoning Strength Planning in Large Reasoning Models", "score": 26, "session": "SD-5-5313", "pdf_url": "https://openreview.net/pdf/ec0170f131842f1aaee5993a19df22764b470213.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "An Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 26.0, "max_score": 26, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "On Reasoning Strength Planning in Large Reasoning Models", "score": 26, "session": "SD-5-5313", "pdf_url": "https://openreview.net/pdf/ec0170f131842f1aaee5993a19df22764b470213.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Andrew Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 26.0, "max_score": 26, "total_relevant": 22, "total_reads": 43, "papers": [{"title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data", "score": 26, "session": "SD-6-1908", "pdf_url": "https://openreview.net/pdf/34a1a72a0a54db41248d9ad8862c78e55ac789d9.pdf", "relevant": 22, "reads": 43}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yiran Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 26.0, "max_score": 26, "total_relevant": 22, "total_reads": 43, "papers": [{"title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data", "score": 26, "session": "SD-6-1908", "pdf_url": "https://openreview.net/pdf/34a1a72a0a54db41248d9ad8862c78e55ac789d9.pdf", "relevant": 22, "reads": 43}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Weisen Jiang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 25.0, "max_score": 25, "total_relevant": 1, "total_reads": 12, "papers": [{"title": "MetaDefense: Defending Fine-tuning based Jailbreak Attack Before and During Generation", "score": 25, "session": "SD-2-1409", "pdf_url": "https://openreview.net/pdf/006cba1cf9e75c1f5fafd3bea5ee62d71f204085.pdf", "relevant": 1, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sinno Jialin Pan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 25.0, "max_score": 25, "total_relevant": 1, "total_reads": 12, "papers": [{"title": "MetaDefense: Defending Fine-tuning based Jailbreak Attack Before and During Generation", "score": 25, "session": "SD-2-1409", "pdf_url": "https://openreview.net/pdf/006cba1cf9e75c1f5fafd3bea5ee62d71f204085.pdf", "relevant": 1, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ibragim Badertdinov", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 25.0, "max_score": 25, "total_relevant": 6, "total_reads": 9, "papers": [{"title": "SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents", "score": 25, "session": "SD-3-106", "pdf_url": "https://arxiv.org/pdf/2505.20411", "relevant": 6, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Alexander Golubev", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 25.0, "max_score": 25, "total_relevant": 6, "total_reads": 9, "papers": [{"title": "SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents", "score": 25, "session": "SD-3-106", "pdf_url": "https://arxiv.org/pdf/2505.20411", "relevant": 6, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Boris Yangel", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 25.0, "max_score": 25, "total_relevant": 6, "total_reads": 9, "papers": [{"title": "SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents", "score": 25, "session": "SD-3-106", "pdf_url": "https://arxiv.org/pdf/2505.20411", "relevant": 6, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Philippe Wyder", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 25.0, "max_score": 25, "total_relevant": 2, "total_reads": 5, "papers": [{"title": "Common Task Framework For a Critical Evaluation of Scientific Machine Learning Algorithms", "score": 25, "session": "SD-6-2708", "pdf_url": "https://arxiv.org/pdf/2510.23166", "relevant": 2, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Judah Goldfeder", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 25.0, "max_score": 25, "total_relevant": 2, "total_reads": 5, "papers": [{"title": "Common Task Framework For a Critical Evaluation of Scientific Machine Learning Algorithms", "score": 25, "session": "SD-6-2708", "pdf_url": "https://arxiv.org/pdf/2510.23166", "relevant": 2, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Nathan Kutz", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 25.0, "max_score": 25, "total_relevant": 2, "total_reads": 5, "papers": [{"title": "Common Task Framework For a Critical Evaluation of Scientific Machine Learning Algorithms", "score": 25, "session": "SD-6-2708", "pdf_url": "https://arxiv.org/pdf/2510.23166", "relevant": 2, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zihan Ma", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 25.0, "max_score": 25, "total_relevant": 5, "total_reads": 8, "papers": [{"title": "Rethinking Verification for LLM Code Generation: From Generation to Testing", "score": 25, "session": "SD-6-106", "pdf_url": "https://openreview.net/pdf/11dbcf6567a9c2d7cfb82938a0ec215c232e6c80.pdf", "relevant": 5, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Taolin Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 25.0, "max_score": 25, "total_relevant": 5, "total_reads": 8, "papers": [{"title": "Rethinking Verification for LLM Code Generation: From Generation to Testing", "score": 25, "session": "SD-6-106", "pdf_url": "https://openreview.net/pdf/11dbcf6567a9c2d7cfb82938a0ec215c232e6c80.pdf", "relevant": 5, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kai Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 25.0, "max_score": 25, "total_relevant": 5, "total_reads": 8, "papers": [{"title": "Rethinking Verification for LLM Code Generation: From Generation to Testing", "score": 25, "session": "SD-6-106", "pdf_url": "https://openreview.net/pdf/11dbcf6567a9c2d7cfb82938a0ec215c232e6c80.pdf", "relevant": 5, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Riccardo Alberghi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 24.0, "max_score": 24, "total_relevant": 3, "total_reads": 16, "papers": [{"title": "On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study", "score": 24, "session": "SD-5-3307", "pdf_url": "https://openreview.net/pdf/57860c28b21d95678533bc618a0afee1c2a54e47.pdf", "relevant": 3, "reads": 16}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Elizaveta Demyanenko", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 24.0, "max_score": 24, "total_relevant": 3, "total_reads": 16, "papers": [{"title": "On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study", "score": 24, "session": "SD-5-3307", "pdf_url": "https://openreview.net/pdf/57860c28b21d95678533bc618a0afee1c2a54e47.pdf", "relevant": 3, "reads": 16}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Luca Saglietti", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 24.0, "max_score": 24, "total_relevant": 3, "total_reads": 16, "papers": [{"title": "On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study", "score": 24, "session": "SD-5-3307", "pdf_url": "https://openreview.net/pdf/57860c28b21d95678533bc618a0afee1c2a54e47.pdf", "relevant": 3, "reads": 16}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Van Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 24.0, "max_score": 24, "total_relevant": 4, "total_reads": 12, "papers": [{"title": "Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning", "score": 24, "session": "SD-5-4200", "pdf_url": "https://openreview.net/pdf/3aa73c81186850fe236acaa4a81d2af3f1e5c910.pdf", "relevant": 4, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zirui Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 24.0, "max_score": 24, "total_relevant": 4, "total_reads": 12, "papers": [{"title": "Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning", "score": 24, "session": "SD-5-4200", "pdf_url": "https://openreview.net/pdf/3aa73c81186850fe236acaa4a81d2af3f1e5c910.pdf", "relevant": 4, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaotian Han", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 24.0, "max_score": 24, "total_relevant": 4, "total_reads": 12, "papers": [{"title": "Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning", "score": 24, "session": "SD-5-4200", "pdf_url": "https://openreview.net/pdf/3aa73c81186850fe236acaa4a81d2af3f1e5c910.pdf", "relevant": 4, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Parsa Mirtaheri", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 24.0, "max_score": 24, "total_relevant": 6, "total_reads": 9, "papers": [{"title": "Let Me Think! A Long Chain of Thought Can Be Worth Exponentially Many Short Ones", "score": 24, "session": "SD-6-3514", "pdf_url": "https://openreview.net/pdf/3522982c33f5d5199bd558f48909b1bccbf81615.pdf", "relevant": 6, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ezra Edelman", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 24.0, "max_score": 24, "total_relevant": 6, "total_reads": 9, "papers": [{"title": "Let Me Think! A Long Chain of Thought Can Be Worth Exponentially Many Short Ones", "score": 24, "session": "SD-6-3514", "pdf_url": "https://openreview.net/pdf/3522982c33f5d5199bd558f48909b1bccbf81615.pdf", "relevant": 6, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Enric Boix-Adser√É", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 24.0, "max_score": 24, "total_relevant": 6, "total_reads": 9, "papers": [{"title": "Let Me Think! A Long Chain of Thought Can Be Worth Exponentially Many Short Ones", "score": 24, "session": "SD-6-3514", "pdf_url": "https://openreview.net/pdf/3522982c33f5d5199bd558f48909b1bccbf81615.pdf", "relevant": 6, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Adam Pardyl", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 24.0, "max_score": 24, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "FlySearch: Exploring how vision-language models explore", "score": 24, "session": "SD-6-4410", "pdf_url": "https://arxiv.org/pdf/2506.02896", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dominik Matuszek", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 24.0, "max_score": 24, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "FlySearch: Exploring how vision-language models explore", "score": 24, "session": "SD-6-4410", "pdf_url": "https://arxiv.org/pdf/2506.02896", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Maciej Wolczyk", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 24.0, "max_score": 24, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "FlySearch: Exploring how vision-language models explore", "score": 24, "session": "SD-6-4410", "pdf_url": "https://arxiv.org/pdf/2506.02896", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Federico Berto", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "PARCO: Parallel AutoRegressive Models for Multi-Agent Combinatorial Optimization", "score": 23, "session": "SD-2-304", "pdf_url": "https://openreview.net/pdf/672bb4ede45dba28f133a8ebb2cfe40fb1f395c8.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chuanbo Hua", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "PARCO: Parallel AutoRegressive Models for Multi-Agent Combinatorial Optimization", "score": 23, "session": "SD-2-304", "pdf_url": "https://openreview.net/pdf/672bb4ede45dba28f133a8ebb2cfe40fb1f395c8.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jinkyoo Park", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "PARCO: Parallel AutoRegressive Models for Multi-Agent Combinatorial Optimization", "score": 23, "session": "SD-2-304", "pdf_url": "https://openreview.net/pdf/672bb4ede45dba28f133a8ebb2cfe40fb1f395c8.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ruaridh Mon-Williams", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "Partner Modelling Emerges in Recurrent Agents (But Only When It Matters)", "score": 23, "session": "SD-2-301", "pdf_url": "https://openreview.net/pdf/ce9e0e5f6bb446a8103ef0fb8f4c67e47b7972d0.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Max Taylor-Davies", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "Partner Modelling Emerges in Recurrent Agents (But Only When It Matters)", "score": 23, "session": "SD-2-301", "pdf_url": "https://openreview.net/pdf/ce9e0e5f6bb446a8103ef0fb8f4c67e47b7972d0.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Christopher G. Lucas", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "Partner Modelling Emerges in Recurrent Agents (But Only When It Matters)", "score": 23, "session": "SD-2-301", "pdf_url": "https://openreview.net/pdf/ce9e0e5f6bb446a8103ef0fb8f4c67e47b7972d0.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Abhijnan Nath", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "Learning √¢¬Ä¬úPartner-Aware√¢¬Ä¬ù Collaborators in Multi-Party Collaboration", "score": 23, "session": "SD-3-2010", "pdf_url": "https://openreview.net/pdf/40d31b0852235d5e98c6f82c41aa70978247a0dd.pdf", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Nikhil Krishnaswamy", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "Learning √¢¬Ä¬úPartner-Aware√¢¬Ä¬ù Collaborators in Multi-Party Collaboration", "score": 23, "session": "SD-3-2010", "pdf_url": "https://openreview.net/pdf/40d31b0852235d5e98c6f82c41aa70978247a0dd.pdf", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Duy Nguyen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Arm Bandits", "score": 23, "session": "SD-4-4108", "pdf_url": "https://openreview.net/pdf/bf589cfd8356e7e428426d438835ed093caa2e02.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Archiki Prasad", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Arm Bandits", "score": 23, "session": "SD-4-4108", "pdf_url": "https://openreview.net/pdf/bf589cfd8356e7e428426d438835ed093caa2e02.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mohit Bansal", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Arm Bandits", "score": 23, "session": "SD-4-4108", "pdf_url": "https://openreview.net/pdf/bf589cfd8356e7e428426d438835ed093caa2e02.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhangyin Feng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 4, "total_reads": 19, "papers": [{"title": "Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs", "score": 23, "session": "SD-5-3709", "pdf_url": "https://openreview.net/pdf/e7ed5c9a866ddc4710624ed2df37d91bce6455d3.pdf", "relevant": 4, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qianglong Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 4, "total_reads": 19, "papers": [{"title": "Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs", "score": 23, "session": "SD-5-3709", "pdf_url": "https://openreview.net/pdf/e7ed5c9a866ddc4710624ed2df37d91bce6455d3.pdf", "relevant": 4, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhirui Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 4, "total_reads": 19, "papers": [{"title": "Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs", "score": 23, "session": "SD-5-3709", "pdf_url": "https://openreview.net/pdf/e7ed5c9a866ddc4710624ed2df37d91bce6455d3.pdf", "relevant": 4, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Guan Zhe Hong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 13, "total_reads": 28, "papers": [{"title": "A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning", "score": 23, "session": "SD-5-1012", "pdf_url": "https://openreview.net/pdf/edfcc94d23a1796e8903435652cc311e00009492.pdf", "relevant": 13, "reads": 28}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Nishanth Dikkala", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 13, "total_reads": 28, "papers": [{"title": "A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning", "score": 23, "session": "SD-5-1012", "pdf_url": "https://openreview.net/pdf/edfcc94d23a1796e8903435652cc311e00009492.pdf", "relevant": 13, "reads": 28}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Rina Panigrahy", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 13, "total_reads": 28, "papers": [{"title": "A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning", "score": 23, "session": "SD-5-1012", "pdf_url": "https://openreview.net/pdf/edfcc94d23a1796e8903435652cc311e00009492.pdf", "relevant": 13, "reads": 28}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sujun Tang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 1, "total_reads": 13, "papers": [{"title": "REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving", "score": 23, "session": "SD-6-909", "pdf_url": "https://openreview.net/pdf/3e257cd601e943e872f777fd88fbaf2a64c6c6ea.pdf", "relevant": 1, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Christopher Priebe", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 1, "total_reads": 13, "papers": [{"title": "REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving", "score": 23, "session": "SD-6-909", "pdf_url": "https://openreview.net/pdf/3e257cd601e943e872f777fd88fbaf2a64c6c6ea.pdf", "relevant": 1, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hadi Esmaeilzadeh", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 1, "total_reads": 13, "papers": [{"title": "REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving", "score": 23, "session": "SD-6-909", "pdf_url": "https://openreview.net/pdf/3e257cd601e943e872f777fd88fbaf2a64c6c6ea.pdf", "relevant": 1, "reads": 13}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Bao Nguyen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 2, "total_reads": 4, "papers": [{"title": "Reasoning Planning for Language Models", "score": 23, "session": "SD-6-1814", "pdf_url": "https://openreview.net/pdf/4e2c8f0df4fed5d40292835b8448bea21d28648f.pdf", "relevant": 2, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hieu Trung Nguyen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 2, "total_reads": 4, "papers": [{"title": "Reasoning Planning for Language Models", "score": 23, "session": "SD-6-1814", "pdf_url": "https://openreview.net/pdf/4e2c8f0df4fed5d40292835b8448bea21d28648f.pdf", "relevant": 2, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Viet Anh Nguyen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 23.0, "max_score": 23, "total_relevant": 2, "total_reads": 4, "papers": [{"title": "Reasoning Planning for Language Models", "score": 23, "session": "SD-6-1814", "pdf_url": "https://openreview.net/pdf/4e2c8f0df4fed5d40292835b8448bea21d28648f.pdf", "relevant": 2, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Alex Lewandowski", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 22.0, "max_score": 22, "total_relevant": 7, "total_reads": 14, "papers": [{"title": "The World Is Bigger: A Computationally-Embedded Perspective on the Big World Hypothesis", "score": 22, "session": "SD-5-508", "pdf_url": "https://openreview.net/pdf/463a93594b9e688fecce37543cf3bf45e7f91310.pdf", "relevant": 7, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Aditya A. Ramesh", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 22.0, "max_score": 22, "total_relevant": 7, "total_reads": 14, "papers": [{"title": "The World Is Bigger: A Computationally-Embedded Perspective on the Big World Hypothesis", "score": 22, "session": "SD-5-508", "pdf_url": "https://openreview.net/pdf/463a93594b9e688fecce37543cf3bf45e7f91310.pdf", "relevant": 7, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Marlos C. Machado", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 22.0, "max_score": 22, "total_relevant": 7, "total_reads": 14, "papers": [{"title": "The World Is Bigger: A Computationally-Embedded Perspective on the Big World Hypothesis", "score": 22, "session": "SD-5-508", "pdf_url": "https://openreview.net/pdf/463a93594b9e688fecce37543cf3bf45e7f91310.pdf", "relevant": 7, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hengyu Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 22.0, "max_score": 22, "total_relevant": 5, "total_reads": 18, "papers": [{"title": "IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering", "score": 22, "session": "SD-6-4914", "pdf_url": "https://arxiv.org/pdf/2506.23329", "relevant": 5, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chenxin Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 22.0, "max_score": 22, "total_relevant": 5, "total_reads": 18, "papers": [{"title": "IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering", "score": 22, "session": "SD-6-4914", "pdf_url": "https://arxiv.org/pdf/2506.23329", "relevant": 5, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Brandon Feng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 22.0, "max_score": 22, "total_relevant": 5, "total_reads": 18, "papers": [{"title": "IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering", "score": 22, "session": "SD-6-4914", "pdf_url": "https://arxiv.org/pdf/2506.23329", "relevant": 5, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jingyao Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 22.0, "max_score": 22, "total_relevant": 4, "total_reads": 21, "papers": [{"title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs", "score": 22, "session": "SD-6-214", "pdf_url": "https://openreview.net/pdf/de9fc962acefe20dd0d80073eadeb19263afeb06.pdf", "relevant": 4, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenwen Qiang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 22.0, "max_score": 22, "total_relevant": 4, "total_reads": 21, "papers": [{"title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs", "score": 22, "session": "SD-6-214", "pdf_url": "https://openreview.net/pdf/de9fc962acefe20dd0d80073eadeb19263afeb06.pdf", "relevant": 4, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lingyi Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 21.0, "max_score": 21, "total_relevant": 2, "total_reads": 7, "papers": [{"title": "DMWM: Dual-Mind World Model with Long-Term Imagination", "score": 21, "session": "SD-2-5313", "pdf_url": "https://openreview.net/pdf/46a97d474ce914d7520303484925b64a33af1b9b.pdf", "relevant": 2, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Rashed Shelim", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 21.0, "max_score": 21, "total_relevant": 2, "total_reads": 7, "papers": [{"title": "DMWM: Dual-Mind World Model with Long-Term Imagination", "score": 21, "session": "SD-2-5313", "pdf_url": "https://openreview.net/pdf/46a97d474ce914d7520303484925b64a33af1b9b.pdf", "relevant": 2, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Naren Ramakrishnan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 21.0, "max_score": 21, "total_relevant": 2, "total_reads": 7, "papers": [{"title": "DMWM: Dual-Mind World Model with Long-Term Imagination", "score": 21, "session": "SD-2-5313", "pdf_url": "https://openreview.net/pdf/46a97d474ce914d7520303484925b64a33af1b9b.pdf", "relevant": 2, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yanyu Ren", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 21.0, "max_score": 21, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Transcending Cost-Quality Tradeoff in Agent Serving via Session-Awareness", "score": 21, "session": "", "pdf_url": "https://openreview.net/pdf/67186a4229264aede8b786e7d6e259cb157d2aba.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Li Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 21.0, "max_score": 21, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Transcending Cost-Quality Tradeoff in Agent Serving via Session-Awareness", "score": 21, "session": "", "pdf_url": "https://openreview.net/pdf/67186a4229264aede8b786e7d6e259cb157d2aba.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yu Bai", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 21.0, "max_score": 21, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Transcending Cost-Quality Tradeoff in Agent Serving via Session-Awareness", "score": 21, "session": "", "pdf_url": "https://openreview.net/pdf/67186a4229264aede8b786e7d6e259cb157d2aba.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haohan Chi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 21.0, "max_score": 21, "total_relevant": 14, "total_reads": 53, "papers": [{"title": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "score": 21, "session": "SD-4-4612", "pdf_url": "https://arxiv.org/pdf/2505.23757", "relevant": 14, "reads": 53}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Huan-ang Gao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 21.0, "max_score": 21, "total_relevant": 14, "total_reads": 53, "papers": [{"title": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "score": 21, "session": "SD-4-4612", "pdf_url": "https://arxiv.org/pdf/2505.23757", "relevant": 14, "reads": 53}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hao Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 21.0, "max_score": 21, "total_relevant": 14, "total_reads": 53, "papers": [{"title": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "score": 21, "session": "SD-4-4612", "pdf_url": "https://arxiv.org/pdf/2505.23757", "relevant": 14, "reads": 53}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haoyu Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 21.0, "max_score": 21, "total_relevant": 10, "total_reads": 16, "papers": [{"title": "Lifelong Safety Alignment for Language Models", "score": 21, "session": "SD-4-1906", "pdf_url": "https://openreview.net/pdf/8b599899e65f157c99b0a6ac3e8b45afbc4fee1a.pdf", "relevant": 10, "reads": 16}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yifei Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 21.0, "max_score": 21, "total_relevant": 10, "total_reads": 16, "papers": [{"title": "Lifelong Safety Alignment for Language Models", "score": 21, "session": "SD-4-1906", "pdf_url": "https://openreview.net/pdf/8b599899e65f157c99b0a6ac3e8b45afbc4fee1a.pdf", "relevant": 10, "reads": 16}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tianyu Pang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 21.0, "max_score": 21, "total_relevant": 10, "total_reads": 16, "papers": [{"title": "Lifelong Safety Alignment for Language Models", "score": 21, "session": "SD-4-1906", "pdf_url": "https://openreview.net/pdf/8b599899e65f157c99b0a6ac3e8b45afbc4fee1a.pdf", "relevant": 10, "reads": 16}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yiping Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 21.0, "max_score": 21, "total_relevant": 13, "total_reads": 27, "papers": [{"title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example", "score": 21, "session": "SD-6-415", "pdf_url": "https://openreview.net/pdf/6488aab2ea1a4b2423d232a17c9fcd1545659f8c.pdf", "relevant": 13, "reads": 27}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qing Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 21.0, "max_score": 21, "total_relevant": 13, "total_reads": 27, "papers": [{"title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example", "score": 21, "session": "SD-6-415", "pdf_url": "https://openreview.net/pdf/6488aab2ea1a4b2423d232a17c9fcd1545659f8c.pdf", "relevant": 13, "reads": 27}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "yelong shen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 21.0, "max_score": 21, "total_relevant": 13, "total_reads": 27, "papers": [{"title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example", "score": 21, "session": "SD-6-415", "pdf_url": "https://openreview.net/pdf/6488aab2ea1a4b2423d232a17c9fcd1545659f8c.pdf", "relevant": 13, "reads": 27}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhixin Xie", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 20.0, "max_score": 20, "total_relevant": 4, "total_reads": 11, "papers": [{"title": "Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs", "score": 20, "session": "SD-6-1410", "pdf_url": "https://openreview.net/pdf/2a60bb227d356283cc8b6eddcfeda9a0ef466282.pdf", "relevant": 4, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xurui Song", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 20.0, "max_score": 20, "total_relevant": 4, "total_reads": 11, "papers": [{"title": "Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs", "score": 20, "session": "SD-6-1410", "pdf_url": "https://openreview.net/pdf/2a60bb227d356283cc8b6eddcfeda9a0ef466282.pdf", "relevant": 4, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jun Luo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 20.0, "max_score": 20, "total_relevant": 4, "total_reads": 11, "papers": [{"title": "Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs", "score": 20, "session": "SD-6-1410", "pdf_url": "https://openreview.net/pdf/2a60bb227d356283cc8b6eddcfeda9a0ef466282.pdf", "relevant": 4, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sanghyun Ahn", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 20.0, "max_score": 20, "total_relevant": 6, "total_reads": 25, "papers": [{"title": "Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning", "score": 20, "session": "SD-6-2212", "pdf_url": "https://openreview.net/pdf/8a135640e90e68e7dd192021ba6a8fdff76f596f.pdf", "relevant": 6, "reads": 25}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wonje Choi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 20.0, "max_score": 20, "total_relevant": 6, "total_reads": 25, "papers": [{"title": "Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning", "score": 20, "session": "SD-6-2212", "pdf_url": "https://openreview.net/pdf/8a135640e90e68e7dd192021ba6a8fdff76f596f.pdf", "relevant": 6, "reads": 25}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Honguk Woo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 20.0, "max_score": 20, "total_relevant": 6, "total_reads": 25, "papers": [{"title": "Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning", "score": 20, "session": "SD-6-2212", "pdf_url": "https://openreview.net/pdf/8a135640e90e68e7dd192021ba6a8fdff76f596f.pdf", "relevant": 6, "reads": 25}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuning Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 2, "total_reads": 12, "papers": [{"title": "WritingBench: A Comprehensive Benchmark for Generative Writing", "score": 19, "session": "SD-1-1805", "pdf_url": "https://arxiv.org/pdf/2503.05244", "relevant": 2, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiahao Mei", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 2, "total_reads": 12, "papers": [{"title": "WritingBench: A Comprehensive Benchmark for Generative Writing", "score": 19, "session": "SD-1-1805", "pdf_url": "https://arxiv.org/pdf/2503.05244", "relevant": 2, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Fei Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 2, "total_reads": 12, "papers": [{"title": "WritingBench: A Comprehensive Benchmark for Generative Writing", "score": 19, "session": "SD-1-1805", "pdf_url": "https://arxiv.org/pdf/2503.05244", "relevant": 2, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sizhe Tang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning", "score": 19, "session": "SD-1-300", "pdf_url": "https://openreview.net/pdf/8e363493ff9d9b3fa837f8d6bb3198fa13ba65f4.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiayu Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning", "score": 19, "session": "SD-1-300", "pdf_url": "https://openreview.net/pdf/8e363493ff9d9b3fa837f8d6bb3198fa13ba65f4.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tian Lan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning", "score": 19, "session": "SD-1-300", "pdf_url": "https://openreview.net/pdf/8e363493ff9d9b3fa837f8d6bb3198fa13ba65f4.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sohyun An", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 5, "total_reads": 11, "papers": [{"title": "Don√¢¬Ä¬ôt Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models", "score": 19, "session": "SD-1-4203", "pdf_url": "https://openreview.net/pdf/1b9aa688f3d241f228fbd9b8694bdffb938d1d5f.pdf", "relevant": 5, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ruochen Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 5, "total_reads": 11, "papers": [{"title": "Don√¢¬Ä¬ôt Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models", "score": 19, "session": "SD-1-4203", "pdf_url": "https://openreview.net/pdf/1b9aa688f3d241f228fbd9b8694bdffb938d1d5f.pdf", "relevant": 5, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Cho-Jui Hsieh", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 5, "total_reads": 11, "papers": [{"title": "Don√¢¬Ä¬ôt Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models", "score": 19, "session": "SD-1-4203", "pdf_url": "https://openreview.net/pdf/1b9aa688f3d241f228fbd9b8694bdffb938d1d5f.pdf", "relevant": 5, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Narun Krishnamurthi Raman", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 2, "total_reads": 4, "papers": [{"title": "STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models", "score": 19, "session": "SD-1-1510", "pdf_url": "https://openreview.net/pdf/ce44a9975c0bc8b6ace090a98fc92829bdf5fece.pdf", "relevant": 2, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Taylor Lundy", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 2, "total_reads": 4, "papers": [{"title": "STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models", "score": 19, "session": "SD-1-1510", "pdf_url": "https://openreview.net/pdf/ce44a9975c0bc8b6ace090a98fc92829bdf5fece.pdf", "relevant": 2, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jesse Perla", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 2, "total_reads": 4, "papers": [{"title": "STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models", "score": 19, "session": "SD-1-1510", "pdf_url": "https://openreview.net/pdf/ce44a9975c0bc8b6ace090a98fc92829bdf5fece.pdf", "relevant": 2, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiaqi Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 3, "total_reads": 15, "papers": [{"title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning", "score": 19, "session": "SD-4-4007", "pdf_url": "https://openreview.net/pdf/37589115324975c5657483b37546cdd4916aeeed.pdf", "relevant": 3, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Bang Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 3, "total_reads": 15, "papers": [{"title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning", "score": 19, "session": "SD-4-4007", "pdf_url": "https://openreview.net/pdf/37589115324975c5657483b37546cdd4916aeeed.pdf", "relevant": 3, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kwan-Yee K. Wong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 3, "total_reads": 15, "papers": [{"title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning", "score": 19, "session": "SD-4-4007", "pdf_url": "https://openreview.net/pdf/37589115324975c5657483b37546cdd4916aeeed.pdf", "relevant": 3, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pengrui Quan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 0, "total_reads": 1, "papers": [{"title": "Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges", "score": 19, "session": "SD-4-2411", "pdf_url": "https://arxiv.org/pdf/2505.11618", "relevant": 0, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Brian Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 0, "total_reads": 1, "papers": [{"title": "Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges", "score": 19, "session": "SD-4-2411", "pdf_url": "https://arxiv.org/pdf/2505.11618", "relevant": 0, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mani Srivastava", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 0, "total_reads": 1, "papers": [{"title": "Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges", "score": 19, "session": "SD-4-2411", "pdf_url": "https://arxiv.org/pdf/2505.11618", "relevant": 0, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuki Imajuku", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering", "score": 19, "session": "SD-4-702", "pdf_url": "https://arxiv.org/pdf/2506.09050", "relevant": 1, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kohki Horie", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering", "score": 19, "session": "SD-4-702", "pdf_url": "https://arxiv.org/pdf/2506.09050", "relevant": 1, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Guangchen Lan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "Contextual Integrity in LLMs via Reasoning and Reinforcement Learning", "score": 19, "session": "SD-4-416", "pdf_url": "https://openreview.net/pdf/b39de1061902f7e2d23191a93a6d94f945bff28e.pdf", "relevant": 3, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Huseyin A Inan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "Contextual Integrity in LLMs via Reasoning and Reinforcement Learning", "score": 19, "session": "SD-4-416", "pdf_url": "https://openreview.net/pdf/b39de1061902f7e2d23191a93a6d94f945bff28e.pdf", "relevant": 3, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Robert Sim", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "Contextual Integrity in LLMs via Reasoning and Reinforcement Learning", "score": 19, "session": "SD-4-416", "pdf_url": "https://openreview.net/pdf/b39de1061902f7e2d23191a93a6d94f945bff28e.pdf", "relevant": 3, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mircea Tudor Lic√Ñ¬É", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 5, "total_reads": 18, "papers": [{"title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Cultural Learning", "score": 19, "session": "SD-4-401", "pdf_url": "https://openreview.net/pdf/aaff34bc5f2395383d58c98d38975df26262971c.pdf", "relevant": 5, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ojas Shirekar", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 5, "total_reads": 18, "papers": [{"title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Cultural Learning", "score": 19, "session": "SD-4-401", "pdf_url": "https://openreview.net/pdf/aaff34bc5f2395383d58c98d38975df26262971c.pdf", "relevant": 5, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chirag Raman", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 5, "total_reads": 18, "papers": [{"title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Cultural Learning", "score": 19, "session": "SD-4-401", "pdf_url": "https://openreview.net/pdf/aaff34bc5f2395383d58c98d38975df26262971c.pdf", "relevant": 5, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wanxin Tian", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 4, "total_reads": 17, "papers": [{"title": "SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents", "score": 19, "session": "SD-6-303", "pdf_url": "https://openreview.net/pdf/d934e1858b206a0ba8fe1fb5281ee9f117238785.pdf", "relevant": 4, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shijie Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 4, "total_reads": 17, "papers": [{"title": "SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents", "score": 19, "session": "SD-6-303", "pdf_url": "https://openreview.net/pdf/d934e1858b206a0ba8fe1fb5281ee9f117238785.pdf", "relevant": 4, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jian Tang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 19.0, "max_score": 19, "total_relevant": 4, "total_reads": 17, "papers": [{"title": "SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents", "score": 19, "session": "SD-6-303", "pdf_url": "https://openreview.net/pdf/d934e1858b206a0ba8fe1fb5281ee9f117238785.pdf", "relevant": 4, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Alex McKenzie", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 18.0, "max_score": 18, "total_relevant": 7, "total_reads": 10, "papers": [{"title": "Detecting High-Stakes Interactions with Activation Probes", "score": 18, "session": "SD-3-1112", "pdf_url": "https://openreview.net/pdf/6291cf20df7b3579af7a0739c773db78424d18cd.pdf", "relevant": 7, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Urja Pawar", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 18.0, "max_score": 18, "total_relevant": 7, "total_reads": 10, "papers": [{"title": "Detecting High-Stakes Interactions with Activation Probes", "score": 18, "session": "SD-3-1112", "pdf_url": "https://openreview.net/pdf/6291cf20df7b3579af7a0739c773db78424d18cd.pdf", "relevant": 7, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dmitrii Krasheninnikov", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 18.0, "max_score": 18, "total_relevant": 7, "total_reads": 10, "papers": [{"title": "Detecting High-Stakes Interactions with Activation Probes", "score": 18, "session": "SD-3-1112", "pdf_url": "https://openreview.net/pdf/6291cf20df7b3579af7a0739c773db78424d18cd.pdf", "relevant": 7, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Leon Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 18.0, "max_score": 18, "total_relevant": 2, "total_reads": 5, "papers": [{"title": "LLM Generated Persona is a Promise with a Catch", "score": 18, "session": "SD-6-1109", "pdf_url": "https://arxiv.org/pdf/2503.16527", "relevant": 2, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haozhe Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 18.0, "max_score": 18, "total_relevant": 2, "total_reads": 5, "papers": [{"title": "LLM Generated Persona is a Promise with a Catch", "score": 18, "session": "SD-6-1109", "pdf_url": "https://arxiv.org/pdf/2503.16527", "relevant": 2, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tianyi Peng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 18.0, "max_score": 18, "total_relevant": 2, "total_reads": 5, "papers": [{"title": "LLM Generated Persona is a Promise with a Catch", "score": 18, "session": "SD-6-1109", "pdf_url": "https://arxiv.org/pdf/2503.16527", "relevant": 2, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lequan Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 18.0, "max_score": 18, "total_relevant": 0, "total_reads": 5, "papers": [{"title": "ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking", "score": 18, "session": "SD-6-3300", "pdf_url": "https://openreview.net/pdf/2d4818decec8bd80bc1421d0101a796c541e0194.pdf", "relevant": 0, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dai Shi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 18.0, "max_score": 18, "total_relevant": 0, "total_reads": 5, "papers": [{"title": "ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking", "score": 18, "session": "SD-6-3300", "pdf_url": "https://openreview.net/pdf/2d4818decec8bd80bc1421d0101a796c541e0194.pdf", "relevant": 0, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junbin Gao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 18.0, "max_score": 18, "total_relevant": 0, "total_reads": 5, "papers": [{"title": "ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking", "score": 18, "session": "SD-6-3300", "pdf_url": "https://openreview.net/pdf/2d4818decec8bd80bc1421d0101a796c541e0194.pdf", "relevant": 0, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuncong Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 17.0, "max_score": 17, "total_relevant": 14, "total_reads": 36, "papers": [{"title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning", "score": 17, "session": "SD-1-4307", "pdf_url": "https://openreview.net/pdf/5ed386610c8657ff319e5833b8272c6459dd85a4.pdf", "relevant": 14, "reads": 36}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiageng Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 17.0, "max_score": 17, "total_relevant": 14, "total_reads": 36, "papers": [{"title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning", "score": 17, "session": "SD-1-4307", "pdf_url": "https://openreview.net/pdf/5ed386610c8657ff319e5833b8272c6459dd85a4.pdf", "relevant": 14, "reads": 36}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chuang Gan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 17.0, "max_score": 17, "total_relevant": 14, "total_reads": 36, "papers": [{"title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning", "score": 17, "session": "SD-1-4307", "pdf_url": "https://openreview.net/pdf/5ed386610c8657ff319e5833b8272c6459dd85a4.pdf", "relevant": 14, "reads": 36}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Adam Stein", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 17.0, "max_score": 17, "total_relevant": 2, "total_reads": 12, "papers": [{"title": "Once Upon an Input: Reasoning via Per-Instance Program Synthesis", "score": 17, "session": "SD-2-3717", "pdf_url": "https://openreview.net/pdf/7b4a6ca902e7228c2855e6b864e4699825b54723.pdf", "relevant": 2, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Neelay Velingker", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 17.0, "max_score": 17, "total_relevant": 2, "total_reads": 12, "papers": [{"title": "Once Upon an Input: Reasoning via Per-Instance Program Synthesis", "score": 17, "session": "SD-2-3717", "pdf_url": "https://openreview.net/pdf/7b4a6ca902e7228c2855e6b864e4699825b54723.pdf", "relevant": 2, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Eric Wong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 17.0, "max_score": 17, "total_relevant": 2, "total_reads": 12, "papers": [{"title": "Once Upon an Input: Reasoning via Per-Instance Program Synthesis", "score": 17, "session": "SD-2-3717", "pdf_url": "https://openreview.net/pdf/7b4a6ca902e7228c2855e6b864e4699825b54723.pdf", "relevant": 2, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pengrun Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 17.0, "max_score": 17, "total_relevant": 3, "total_reads": 15, "papers": [{"title": "Can We Infer Confidential Properties of Training Data from LLMs?", "score": 17, "session": "SD-4-1313", "pdf_url": "https://openreview.net/pdf/cd0fb86716237f2897625fe1354d6af3d7b0ec5b.pdf", "relevant": 3, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chhavi Yadav", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 17.0, "max_score": 17, "total_relevant": 3, "total_reads": 15, "papers": [{"title": "Can We Infer Confidential Properties of Training Data from LLMs?", "score": 17, "session": "SD-4-1313", "pdf_url": "https://openreview.net/pdf/cd0fb86716237f2897625fe1354d6af3d7b0ec5b.pdf", "relevant": 3, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ruihan Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 17.0, "max_score": 17, "total_relevant": 3, "total_reads": 15, "papers": [{"title": "Can We Infer Confidential Properties of Training Data from LLMs?", "score": 17, "session": "SD-4-1313", "pdf_url": "https://openreview.net/pdf/cd0fb86716237f2897625fe1354d6af3d7b0ec5b.pdf", "relevant": 3, "reads": 15}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jie Ma", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 17.0, "max_score": 17, "total_relevant": 4, "total_reads": 12, "papers": [{"title": "Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs", "score": 17, "session": "SD-6-3616", "pdf_url": "https://openreview.net/pdf/33aae90c6ae97c2a878758d797cf8196f7ca09de.pdf", "relevant": 4, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ning Qu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 17.0, "max_score": 17, "total_relevant": 4, "total_reads": 12, "papers": [{"title": "Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs", "score": 17, "session": "SD-6-3616", "pdf_url": "https://openreview.net/pdf/33aae90c6ae97c2a878758d797cf8196f7ca09de.pdf", "relevant": 4, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "su zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 17.0, "max_score": 17, "total_relevant": 4, "total_reads": 12, "papers": [{"title": "Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs", "score": 17, "session": "SD-6-3616", "pdf_url": "https://openreview.net/pdf/33aae90c6ae97c2a878758d797cf8196f7ca09de.pdf", "relevant": 4, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhida Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 16.0, "max_score": 16, "total_relevant": 5, "total_reads": 33, "papers": [{"title": "From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction", "score": 16, "session": "SD-2-4809", "pdf_url": "https://openreview.net/pdf/dbe78cc8fdf28b0340af74010ec4ad766aca831b.pdf", "relevant": 5, "reads": 33}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Talas Fu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 16.0, "max_score": 16, "total_relevant": 5, "total_reads": 33, "papers": [{"title": "From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction", "score": 16, "session": "SD-2-4809", "pdf_url": "https://openreview.net/pdf/dbe78cc8fdf28b0340af74010ec4ad766aca831b.pdf", "relevant": 5, "reads": 33}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Huchuan Lu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 16.0, "max_score": 16, "total_relevant": 5, "total_reads": 33, "papers": [{"title": "From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction", "score": 16, "session": "SD-2-4809", "pdf_url": "https://openreview.net/pdf/dbe78cc8fdf28b0340af74010ec4ad766aca831b.pdf", "relevant": 5, "reads": 33}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Man Ho LAM", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 16.0, "max_score": 16, "total_relevant": 5, "total_reads": 17, "papers": [{"title": "CodeCrash: Exposing LLM Fragility to Misleading Natural Language in Code Reasoning", "score": 16, "session": "SD-4-2610", "pdf_url": "https://openreview.net/pdf/9b9883a392dca607cfb2b26eccc24136b0e5cf90.pdf", "relevant": 5, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chaozheng Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 16.0, "max_score": 16, "total_relevant": 5, "total_reads": 17, "papers": [{"title": "CodeCrash: Exposing LLM Fragility to Misleading Natural Language in Code Reasoning", "score": 16, "session": "SD-4-2610", "pdf_url": "https://openreview.net/pdf/9b9883a392dca607cfb2b26eccc24136b0e5cf90.pdf", "relevant": 5, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Michael Lyu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 16.0, "max_score": 16, "total_relevant": 5, "total_reads": 17, "papers": [{"title": "CodeCrash: Exposing LLM Fragility to Misleading Natural Language in Code Reasoning", "score": 16, "session": "SD-4-2610", "pdf_url": "https://openreview.net/pdf/9b9883a392dca607cfb2b26eccc24136b0e5cf90.pdf", "relevant": 5, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zijie Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 16.0, "max_score": 16, "total_relevant": 3, "total_reads": 8, "papers": [{"title": "Learning and Planning Multi-Agent Tasks via an MoE-based World Model", "score": 16, "session": "SD-4-303", "pdf_url": "https://openreview.net/pdf/09e9ae881c16219816d4e101be5c5634163ae0b6.pdf", "relevant": 3, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhongyue Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 16.0, "max_score": 16, "total_relevant": 3, "total_reads": 8, "papers": [{"title": "Learning and Planning Multi-Agent Tasks via an MoE-based World Model", "score": 16, "session": "SD-4-303", "pdf_url": "https://openreview.net/pdf/09e9ae881c16219816d4e101be5c5634163ae0b6.pdf", "relevant": 3, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shengtian Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 15.0, "max_score": 15, "total_relevant": 1, "total_reads": 8, "papers": [{"title": "MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection", "score": 15, "session": "", "pdf_url": "https://openreview.net/pdf/12eb1e4189682fe7b8223e53327f9e2ada20bd59.pdf", "relevant": 1, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yue Feng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 15.0, "max_score": 15, "total_relevant": 1, "total_reads": 8, "papers": [{"title": "MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection", "score": 15, "session": "", "pdf_url": "https://openreview.net/pdf/12eb1e4189682fe7b8223e53327f9e2ada20bd59.pdf", "relevant": 1, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jie Qin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 15.0, "max_score": 15, "total_relevant": 1, "total_reads": 8, "papers": [{"title": "MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection", "score": 15, "session": "", "pdf_url": "https://openreview.net/pdf/12eb1e4189682fe7b8223e53327f9e2ada20bd59.pdf", "relevant": 1, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yinghao Zhu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 15.0, "max_score": 15, "total_relevant": 0, "total_reads": 3, "papers": [{"title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks", "score": 15, "session": "SD-4-1809", "pdf_url": "https://arxiv.org/pdf/2505.12371", "relevant": 0, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ziyi He", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 15.0, "max_score": 15, "total_relevant": 0, "total_reads": 3, "papers": [{"title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks", "score": 15, "session": "SD-4-1809", "pdf_url": "https://arxiv.org/pdf/2505.12371", "relevant": 0, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lequan Yu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 15.0, "max_score": 15, "total_relevant": 0, "total_reads": 3, "papers": [{"title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks", "score": 15, "session": "SD-4-1809", "pdf_url": "https://arxiv.org/pdf/2505.12371", "relevant": 0, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zihan Weng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 8, "total_reads": 35, "papers": [{"title": "Caption This, Reason That: VLMs Caught in the Middle", "score": 14, "session": "SD-1-2112", "pdf_url": "https://openreview.net/pdf/756a05a421ed180f3aedfcaa7a55a73f50e83d30.pdf", "relevant": 8, "reads": 35}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lucas Gomez", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 8, "total_reads": 35, "papers": [{"title": "Caption This, Reason That: VLMs Caught in the Middle", "score": 14, "session": "SD-1-2112", "pdf_url": "https://openreview.net/pdf/756a05a421ed180f3aedfcaa7a55a73f50e83d30.pdf", "relevant": 8, "reads": 35}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pouya Bashivan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 8, "total_reads": 35, "papers": [{"title": "Caption This, Reason That: VLMs Caught in the Middle", "score": 14, "session": "SD-1-2112", "pdf_url": "https://openreview.net/pdf/756a05a421ed180f3aedfcaa7a55a73f50e83d30.pdf", "relevant": 8, "reads": 35}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ali Rasekh", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 1, "total_reads": 20, "papers": [{"title": "Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders", "score": 14, "session": "", "pdf_url": "https://openreview.net/pdf/8983a85d88c67728d26c47fa75cdce08379b3561.pdf", "relevant": 1, "reads": 20}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Erfan Bagheri Soula", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 1, "total_reads": 20, "papers": [{"title": "Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders", "score": 14, "session": "", "pdf_url": "https://openreview.net/pdf/8983a85d88c67728d26c47fa75cdce08379b3561.pdf", "relevant": 1, "reads": 20}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mohsen Fayyaz", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 1, "total_reads": 20, "papers": [{"title": "Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders", "score": 14, "session": "", "pdf_url": "https://openreview.net/pdf/8983a85d88c67728d26c47fa75cdce08379b3561.pdf", "relevant": 1, "reads": 20}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Houyi Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 3, "total_reads": 10, "papers": [{"title": "Predictable Scale (Part II) --- Farseer: A Refined Scaling Law in LLMs", "score": 14, "session": "", "pdf_url": "https://openreview.net/pdf/b2d2dae1fc87cdd4510e8c2672fcf585d1c653f2.pdf", "relevant": 3, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenzhen Zheng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 3, "total_reads": 10, "papers": [{"title": "Predictable Scale (Part II) --- Farseer: A Refined Scaling Law in LLMs", "score": 14, "session": "", "pdf_url": "https://openreview.net/pdf/b2d2dae1fc87cdd4510e8c2672fcf585d1c653f2.pdf", "relevant": 3, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Daxin Jiang", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 14, "total_relevant": 6, "total_reads": 18, "papers": [{"title": "Predictable Scale (Part II) --- Farseer: A Refined Scaling Law in LLMs", "score": 14, "session": "", "pdf_url": "https://openreview.net/pdf/b2d2dae1fc87cdd4510e8c2672fcf585d1c653f2.pdf", "relevant": 3, "reads": 10}, {"title": "GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning", "score": 10, "session": "SD-4-415", "pdf_url": "https://openreview.net/pdf/ce35fb684e3b11b9c0f1fcc38598cfb3504c728e.pdf", "relevant": 3, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zekai Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 4, "total_reads": 18, "papers": [{"title": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models", "score": 14, "session": "SD-2-1813", "pdf_url": "https://openreview.net/pdf/2b62626f959a636707dfb61efe094e4dcd7bd32f.pdf", "relevant": 4, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qi Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 4, "total_reads": 18, "papers": [{"title": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models", "score": 14, "session": "SD-2-1813", "pdf_url": "https://openreview.net/pdf/2b62626f959a636707dfb61efe094e4dcd7bd32f.pdf", "relevant": 4, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Biwei Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 4, "total_reads": 18, "papers": [{"title": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models", "score": 14, "session": "SD-2-1813", "pdf_url": "https://openreview.net/pdf/2b62626f959a636707dfb61efe094e4dcd7bd32f.pdf", "relevant": 4, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Saibo Geng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 5, "total_reads": 19, "papers": [{"title": "zip2zip: Inference-Time Adaptive Tokenization via Online Compression", "score": 14, "session": "SD-2-3615", "pdf_url": "https://openreview.net/pdf/0f10815a6be342f3a8137fe6510088e422bd3a6b.pdf", "relevant": 5, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Nathan Ranchin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 5, "total_reads": 19, "papers": [{"title": "zip2zip: Inference-Time Adaptive Tokenization via Online Compression", "score": 14, "session": "SD-2-3615", "pdf_url": "https://openreview.net/pdf/0f10815a6be342f3a8137fe6510088e422bd3a6b.pdf", "relevant": 5, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Robert West", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 5, "total_reads": 19, "papers": [{"title": "zip2zip: Inference-Time Adaptive Tokenization via Online Compression", "score": 14, "session": "SD-2-3615", "pdf_url": "https://openreview.net/pdf/0f10815a6be342f3a8137fe6510088e422bd3a6b.pdf", "relevant": 5, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junting Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 3, "total_reads": 33, "papers": [{"title": "OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis", "score": 14, "session": "SD-2-5507", "pdf_url": "https://openreview.net/pdf/b83bcc6b13bf3bed81ebb73be9bae7cc2be710e7.pdf", "relevant": 3, "reads": 33}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haotian Liang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 3, "total_reads": 33, "papers": [{"title": "OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis", "score": 14, "session": "SD-2-5507", "pdf_url": "https://openreview.net/pdf/b83bcc6b13bf3bed81ebb73be9bae7cc2be710e7.pdf", "relevant": 3, "reads": 33}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lin Shao", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 8.5, "max_score": 14, "total_relevant": 7, "total_reads": 50, "papers": [{"title": "OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis", "score": 14, "session": "SD-2-5507", "pdf_url": "https://openreview.net/pdf/b83bcc6b13bf3bed81ebb73be9bae7cc2be710e7.pdf", "relevant": 3, "reads": 33}, {"title": "VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models", "score": 3, "session": "SD-5-2213", "pdf_url": "https://openreview.net/pdf/05a810d8dce16f520e115b9ee80b8096e6512276.pdf", "relevant": 4, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xuan Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Temporal Logic-Based Multi-Vehicle Backdoor Attacks against Offline RL Agents in End-to-end Autonomous Driving", "score": 14, "session": "SD-2-1201", "pdf_url": "https://openreview.net/pdf/26313d9a67de648ba12a7315cbcaa87c2787a4f5.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shiwei Feng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Temporal Logic-Based Multi-Vehicle Backdoor Attacks against Offline RL Agents in End-to-end Autonomous Driving", "score": 14, "session": "SD-2-1201", "pdf_url": "https://openreview.net/pdf/26313d9a67de648ba12a7315cbcaa87c2787a4f5.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiangyu Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Temporal Logic-Based Multi-Vehicle Backdoor Attacks against Offline RL Agents in End-to-end Autonomous Driving", "score": 14, "session": "SD-2-1201", "pdf_url": "https://openreview.net/pdf/26313d9a67de648ba12a7315cbcaa87c2787a4f5.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qitai Tan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models for Time Series", "score": 14, "session": "SD-2-2304", "pdf_url": "https://arxiv.org/pdf/2510.20273", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yiyun Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models for Time Series", "score": 14, "session": "SD-2-2304", "pdf_url": "https://arxiv.org/pdf/2510.20273", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiao-Ping (Steven) Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models for Time Series", "score": 14, "session": "SD-2-2304", "pdf_url": "https://arxiv.org/pdf/2510.20273", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Bowen Dong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 5, "total_reads": 12, "papers": [{"title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM", "score": 14, "session": "SD-5-4110", "pdf_url": "https://openreview.net/pdf/5c8396fbd51956694870ead9c44afcff4e5253cb.pdf", "relevant": 5, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Minheng Ni", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 5, "total_reads": 12, "papers": [{"title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM", "score": 14, "session": "SD-5-4110", "pdf_url": "https://openreview.net/pdf/5c8396fbd51956694870ead9c44afcff4e5253cb.pdf", "relevant": 5, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lei Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 5, "total_reads": 12, "papers": [{"title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM", "score": 14, "session": "SD-5-4110", "pdf_url": "https://openreview.net/pdf/5c8396fbd51956694870ead9c44afcff4e5253cb.pdf", "relevant": 5, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chaochen Gao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions", "score": 14, "session": "SD-6-4007", "pdf_url": "https://openreview.net/pdf/ed87797f943163aff0337ae4824c26f2e347ea6a.pdf", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xing W", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions", "score": 14, "session": "SD-6-4007", "pdf_url": "https://openreview.net/pdf/ed87797f943163aff0337ae4824c26f2e347ea6a.pdf", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Songlin Hu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions", "score": 14, "session": "SD-6-4007", "pdf_url": "https://openreview.net/pdf/ed87797f943163aff0337ae4824c26f2e347ea6a.pdf", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhengliang Shi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers", "score": 14, "session": "SD-6-4006", "pdf_url": "https://openreview.net/pdf/f6244ed69adb4a0f1c375503deb1e23c39a8ac15.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lingyong Yan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers", "score": 14, "session": "SD-6-4006", "pdf_url": "https://openreview.net/pdf/f6244ed69adb4a0f1c375503deb1e23c39a8ac15.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhaochun Ren", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 14.0, "max_score": 14, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers", "score": 14, "session": "SD-6-4006", "pdf_url": "https://openreview.net/pdf/f6244ed69adb4a0f1c375503deb1e23c39a8ac15.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ken Gu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 13.0, "max_score": 13, "total_relevant": 1, "total_reads": 5, "papers": [{"title": "RADAR: Benchmarking Language Models on Imperfect Tabular Data", "score": 13, "session": "SD-1-2402", "pdf_url": "https://arxiv.org/pdf/2506.08249", "relevant": 1, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhihan Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 13.0, "max_score": 13, "total_relevant": 1, "total_reads": 5, "papers": [{"title": "RADAR: Benchmarking Language Models on Imperfect Tabular Data", "score": 13, "session": "SD-1-2402", "pdf_url": "https://arxiv.org/pdf/2506.08249", "relevant": 1, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xin Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 13.0, "max_score": 13, "total_relevant": 1, "total_reads": 5, "papers": [{"title": "RADAR: Benchmarking Language Models on Imperfect Tabular Data", "score": 13, "session": "SD-1-2402", "pdf_url": "https://arxiv.org/pdf/2506.08249", "relevant": 1, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Guojian Zhan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 13.0, "max_score": 13, "total_relevant": 10, "total_reads": 44, "papers": [{"title": "Bootstrap Off-policy with World Model", "score": 13, "session": "SD-1-304", "pdf_url": "https://openreview.net/pdf/41b2f866d682b3c56d82ba0f291b84901efb52d3.pdf", "relevant": 10, "reads": 44}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Likun Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 13.0, "max_score": 13, "total_relevant": 10, "total_reads": 44, "papers": [{"title": "Bootstrap Off-policy with World Model", "score": 13, "session": "SD-1-304", "pdf_url": "https://openreview.net/pdf/41b2f866d682b3c56d82ba0f291b84901efb52d3.pdf", "relevant": 10, "reads": 44}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shengbo Eben Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 13.0, "max_score": 13, "total_relevant": 10, "total_reads": 44, "papers": [{"title": "Bootstrap Off-policy with World Model", "score": 13, "session": "SD-1-304", "pdf_url": "https://openreview.net/pdf/41b2f866d682b3c56d82ba0f291b84901efb52d3.pdf", "relevant": 10, "reads": 44}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ruihan Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 13.0, "max_score": 13, "total_relevant": 4, "total_reads": 11, "papers": [{"title": "The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement", "score": 13, "session": "SD-2-4009", "pdf_url": "https://openreview.net/pdf/ae3bf83043c88b4e28668b095624b19cc07ed197.pdf", "relevant": 4, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Fanghua Ye", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 13.0, "max_score": 13, "total_relevant": 4, "total_reads": 11, "papers": [{"title": "The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement", "score": 13, "session": "SD-2-4009", "pdf_url": "https://openreview.net/pdf/ae3bf83043c88b4e28668b095624b19cc07ed197.pdf", "relevant": 4, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Deqing Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 13.0, "max_score": 13, "total_relevant": 4, "total_reads": 11, "papers": [{"title": "The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement", "score": 13, "session": "SD-2-4009", "pdf_url": "https://openreview.net/pdf/ae3bf83043c88b4e28668b095624b19cc07ed197.pdf", "relevant": 4, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ahmet H. G√É¬ºzel", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 13.0, "max_score": 13, "total_relevant": 6, "total_reads": 19, "papers": [{"title": "Imagined Autocurricula", "score": 13, "session": "SD-5-300", "pdf_url": "https://openreview.net/pdf/3facb7ccf15168534e07f4990a5bd009d1afea45.pdf", "relevant": 6, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Matthew Thomas Jackson", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 13.0, "max_score": 13, "total_relevant": 6, "total_reads": 19, "papers": [{"title": "Imagined Autocurricula", "score": 13, "session": "SD-5-300", "pdf_url": "https://openreview.net/pdf/3facb7ccf15168534e07f4990a5bd009d1afea45.pdf", "relevant": 6, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jack Parker-Holder", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 13.0, "max_score": 13, "total_relevant": 6, "total_reads": 19, "papers": [{"title": "Imagined Autocurricula", "score": 13, "session": "SD-5-300", "pdf_url": "https://openreview.net/pdf/3facb7ccf15168534e07f4990a5bd009d1afea45.pdf", "relevant": 6, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lisa Alazraki", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "Reverse Engineering Human Preferences with Reinforcement Learning", "score": 12, "session": "SD-1-1909", "pdf_url": "https://openreview.net/pdf/e7f264f17bfbaac2f1357558dddf2cceae14cd6e.pdf", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yi-Chern Tan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "Reverse Engineering Human Preferences with Reinforcement Learning", "score": 12, "session": "SD-1-1909", "pdf_url": "https://openreview.net/pdf/e7f264f17bfbaac2f1357558dddf2cceae14cd6e.pdf", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Max Bartolo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "Reverse Engineering Human Preferences with Reinforcement Learning", "score": 12, "session": "SD-1-1909", "pdf_url": "https://openreview.net/pdf/e7f264f17bfbaac2f1357558dddf2cceae14cd6e.pdf", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yilun Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 2, "total_reads": 2, "papers": [{"title": "SciArena: An Open Evaluation Platform for Non-Verifiable Scientific Literature-Grounded Tasks", "score": 12, "session": "SD-1-5316", "pdf_url": "", "relevant": 2, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Arman Cohan", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 12, "total_relevant": 9, "total_reads": 19, "papers": [{"title": "SciArena: An Open Evaluation Platform for Non-Verifiable Scientific Literature-Grounded Tasks", "score": 12, "session": "SD-1-5316", "pdf_url": "", "relevant": 2, "reads": 2}, {"title": "On Evaluating LLM Alignment by Evaluating LLMs as Judges", "score": 10, "session": "SD-4-2000", "pdf_url": "https://openreview.net/pdf/cefd5dffe2958e9dbfba77cc4764ee04a8cc5a95.pdf", "relevant": 7, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tobias Schnabel", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 10, "total_reads": 22, "papers": [{"title": "Lost in Transmission: When and Why LLMs Fail to Reason Globally", "score": 12, "session": "SD-2-3907", "pdf_url": "https://openreview.net/pdf/93d0795dfa82fdff6b6b121fce6307ed161915ba.pdf", "relevant": 10, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kiran Tomlinson", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 10, "total_reads": 22, "papers": [{"title": "Lost in Transmission: When and Why LLMs Fail to Reason Globally", "score": 12, "session": "SD-2-3907", "pdf_url": "https://openreview.net/pdf/93d0795dfa82fdff6b6b121fce6307ed161915ba.pdf", "relevant": 10, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jennifer Neville", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 10, "total_reads": 22, "papers": [{"title": "Lost in Transmission: When and Why LLMs Fail to Reason Globally", "score": 12, "session": "SD-2-3907", "pdf_url": "https://openreview.net/pdf/93d0795dfa82fdff6b6b121fce6307ed161915ba.pdf", "relevant": 10, "reads": 22}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiankang Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "GRIP: A Graph-Based Reasoning Instruction Producer", "score": 12, "session": "SD-4-1909", "pdf_url": "https://openreview.net/pdf/9ef2967c43fa97f6b4cfdddaf146abf156250794.pdf", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jianjun Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "GRIP: A Graph-Based Reasoning Instruction Producer", "score": 12, "session": "SD-4-1909", "pdf_url": "https://openreview.net/pdf/9ef2967c43fa97f6b4cfdddaf146abf156250794.pdf", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hongtao Xie", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "GRIP: A Graph-Based Reasoning Instruction Producer", "score": 12, "session": "SD-4-1909", "pdf_url": "https://openreview.net/pdf/9ef2967c43fa97f6b4cfdddaf146abf156250794.pdf", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shengda Fan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 0, "total_reads": 3, "papers": [{"title": "Generalizing Experience for Language Agents with Hierarchical MetaFlows", "score": 12, "session": "SD-4-2001", "pdf_url": "https://openreview.net/pdf/e5e7cd3c074c77adae516f8d392136a8fe8657f5.pdf", "relevant": 0, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xin Cong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 0, "total_reads": 3, "papers": [{"title": "Generalizing Experience for Language Agents with Hierarchical MetaFlows", "score": 12, "session": "SD-4-2001", "pdf_url": "https://openreview.net/pdf/e5e7cd3c074c77adae516f8d392136a8fe8657f5.pdf", "relevant": 0, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yankai Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 0, "total_reads": 3, "papers": [{"title": "Generalizing Experience for Language Agents with Hierarchical MetaFlows", "score": 12, "session": "SD-4-2001", "pdf_url": "https://openreview.net/pdf/e5e7cd3c074c77adae516f8d392136a8fe8657f5.pdf", "relevant": 0, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ziwei Deng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 0, "total_reads": 5, "papers": [{"title": "PlanU: Large Language Model Reasoning through Planning under Uncertainty", "score": 12, "session": "SD-6-1906", "pdf_url": "https://openreview.net/pdf/a9771087543396ca7e59296ca5d3d68429e5a708.pdf", "relevant": 0, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mian Deng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 0, "total_reads": 5, "papers": [{"title": "PlanU: Large Language Model Reasoning through Planning under Uncertainty", "score": 12, "session": "SD-6-1906", "pdf_url": "https://openreview.net/pdf/a9771087543396ca7e59296ca5d3d68429e5a708.pdf", "relevant": 0, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Cheng Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 0, "total_reads": 5, "papers": [{"title": "PlanU: Large Language Model Reasoning through Planning under Uncertainty", "score": 12, "session": "SD-6-1906", "pdf_url": "https://openreview.net/pdf/a9771087543396ca7e59296ca5d3d68429e5a708.pdf", "relevant": 0, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Thomas Foster", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "LILO: Learning to Reason at the Frontier of Learnability", "score": 12, "session": "SD-6-4905", "pdf_url": "https://openreview.net/pdf/81c8ea42739bd8be2077f7a518eeedc3ad8b7d5c.pdf", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Anya Sims", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 12.0, "max_score": 12, "total_relevant": 3, "total_reads": 9, "papers": [{"title": "LILO: Learning to Reason at the Frontier of Learnability", "score": 12, "session": "SD-6-4905", "pdf_url": "https://openreview.net/pdf/81c8ea42739bd8be2077f7a518eeedc3ad8b7d5c.pdf", "relevant": 3, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiani Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 2, "total_reads": 10, "papers": [{"title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation", "score": 11, "session": "SD-1-4908", "pdf_url": "https://openreview.net/pdf/32e384d1b7bff0817b05af22b3999df7789e5b8d.pdf", "relevant": 2, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Amish Sethi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 2, "total_reads": 10, "papers": [{"title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation", "score": 11, "session": "SD-1-4908", "pdf_url": "https://openreview.net/pdf/32e384d1b7bff0817b05af22b3999df7789e5b8d.pdf", "relevant": 2, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mayur Naik", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 2, "total_reads": 10, "papers": [{"title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation", "score": 11, "session": "SD-1-4908", "pdf_url": "https://openreview.net/pdf/32e384d1b7bff0817b05af22b3999df7789e5b8d.pdf", "relevant": 2, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiale Ma", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ML4CO-Bench-101: Benchmark Machine Learning for Classic Combinatorial Problems on Graphs", "score": 11, "session": "SD-1-4507", "pdf_url": "", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenzheng Pan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ML4CO-Bench-101: Benchmark Machine Learning for Classic Combinatorial Problems on Graphs", "score": 11, "session": "SD-1-4507", "pdf_url": "", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junchi Yan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ML4CO-Bench-101: Benchmark Machine Learning for Classic Combinatorial Problems on Graphs", "score": 11, "session": "SD-1-4507", "pdf_url": "", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yichao Fu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Efficiently Scaling LLM Reasoning Programs with Certaindex", "score": 11, "session": "SD-2-3706", "pdf_url": "https://openreview.net/pdf/4ca50f6ea693aeda7b95d22f1929d6a5d49cf4ff.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junda Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Efficiently Scaling LLM Reasoning Programs with Certaindex", "score": 11, "session": "SD-2-3706", "pdf_url": "https://openreview.net/pdf/4ca50f6ea693aeda7b95d22f1929d6a5d49cf4ff.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hao Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Efficiently Scaling LLM Reasoning Programs with Certaindex", "score": 11, "session": "SD-2-3706", "pdf_url": "https://openreview.net/pdf/4ca50f6ea693aeda7b95d22f1929d6a5d49cf4ff.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Rui Pan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 4, "total_reads": 14, "papers": [{"title": "SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning", "score": 11, "session": "SD-2-1900", "pdf_url": "https://openreview.net/pdf/7c62b5e4703772251c504c1cdc203fa7a96cd873.pdf", "relevant": 4, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yinwei Dai", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 4, "total_reads": 14, "papers": [{"title": "SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning", "score": 11, "session": "SD-2-1900", "pdf_url": "https://openreview.net/pdf/7c62b5e4703772251c504c1cdc203fa7a96cd873.pdf", "relevant": 4, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ravi Netravali", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 4, "total_reads": 14, "papers": [{"title": "SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning", "score": 11, "session": "SD-2-1900", "pdf_url": "https://openreview.net/pdf/7c62b5e4703772251c504c1cdc203fa7a96cd873.pdf", "relevant": 4, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiangyu Wen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Dependency Matters: Enhancing LLM Reasoning with Explicit Knowledge Grounding", "score": 11, "session": "SD-4-3603", "pdf_url": "https://openreview.net/pdf/88669d564fe12a96458ad3ff7025be1f74506a21.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Min Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Dependency Matters: Enhancing LLM Reasoning with Explicit Knowledge Grounding", "score": 11, "session": "SD-4-3603", "pdf_url": "https://openreview.net/pdf/88669d564fe12a96458ad3ff7025be1f74506a21.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qiang Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Dependency Matters: Enhancing LLM Reasoning with Explicit Knowledge Grounding", "score": 11, "session": "SD-4-3603", "pdf_url": "https://openreview.net/pdf/88669d564fe12a96458ad3ff7025be1f74506a21.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhenyu Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 4, "total_reads": 6, "papers": [{"title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "score": 11, "session": "SD-5-4103", "pdf_url": "https://openreview.net/pdf/311841e491eca81acc49d5288503e9b7819788d6.pdf", "relevant": 4, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tianyi Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 4, "total_reads": 6, "papers": [{"title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "score": 11, "session": "SD-5-4103", "pdf_url": "https://openreview.net/pdf/311841e491eca81acc49d5288503e9b7819788d6.pdf", "relevant": 4, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiaxin Pei", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 4, "total_reads": 6, "papers": [{"title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "score": 11, "session": "SD-5-4103", "pdf_url": "https://openreview.net/pdf/311841e491eca81acc49d5288503e9b7819788d6.pdf", "relevant": 4, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wei Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 3, "total_reads": 7, "papers": [{"title": "Retro-R1: LLM-based Agentic Retrosynthesis", "score": 11, "session": "SD-6-1511", "pdf_url": "https://openreview.net/pdf/470a5ef177e20c474a9e358c1bab0c1cf082f1de.pdf", "relevant": 3, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiangtao Feng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 3, "total_reads": 7, "papers": [{"title": "Retro-R1: LLM-based Agentic Retrosynthesis", "score": 11, "session": "SD-6-1511", "pdf_url": "https://openreview.net/pdf/470a5ef177e20c474a9e358c1bab0c1cf082f1de.pdf", "relevant": 3, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hao Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 3, "total_reads": 7, "papers": [{"title": "Retro-R1: LLM-based Agentic Retrosynthesis", "score": 11, "session": "SD-6-1511", "pdf_url": "https://openreview.net/pdf/470a5ef177e20c474a9e358c1bab0c1cf082f1de.pdf", "relevant": 3, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Young-Jin Park", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "Know What You Don't Know: Uncertainty Calibration of Process Reward Models", "score": 11, "session": "SD-6-1912", "pdf_url": "https://openreview.net/pdf/18533f919a9403854c7955628ad3488bf682b694.pdf", "relevant": 3, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kristjan Greenewald", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "Know What You Don't Know: Uncertainty Calibration of Process Reward Models", "score": 11, "session": "SD-6-1912", "pdf_url": "https://openreview.net/pdf/18533f919a9403854c7955628ad3488bf682b694.pdf", "relevant": 3, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Navid Azizan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 3, "total_reads": 6, "papers": [{"title": "Know What You Don't Know: Uncertainty Calibration of Process Reward Models", "score": 11, "session": "SD-6-1912", "pdf_url": "https://openreview.net/pdf/18533f919a9403854c7955628ad3488bf682b694.pdf", "relevant": 3, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ziang Yan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 8, "total_reads": 47, "papers": [{"title": "VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception", "score": 11, "session": "SD-6-5200", "pdf_url": "https://openreview.net/pdf/7cf4479fc77f583f1cdede042be97389be035f9a.pdf", "relevant": 8, "reads": 47}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yinan He", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 8, "total_reads": 47, "papers": [{"title": "VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception", "score": 11, "session": "SD-6-5200", "pdf_url": "https://openreview.net/pdf/7cf4479fc77f583f1cdede042be97389be035f9a.pdf", "relevant": 8, "reads": 47}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yi Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 11.0, "max_score": 11, "total_relevant": 8, "total_reads": 47, "papers": [{"title": "VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception", "score": 11, "session": "SD-6-5200", "pdf_url": "https://openreview.net/pdf/7cf4479fc77f583f1cdede042be97389be035f9a.pdf", "relevant": 8, "reads": 47}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yue Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 10.0, "max_score": 10, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "Thoughts Are All Over the Place: On the Underthinking of Long Reasoning Models", "score": 10, "session": "SD-1-5518", "pdf_url": "https://openreview.net/pdf/52cab7bd6214e5b9d63addbfb0411f3ce8f517f4.pdf", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qiuzhi Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 10.0, "max_score": 10, "total_relevant": 4, "total_reads": 9, "papers": [{"title": "Thoughts Are All Over the Place: On the Underthinking of Long Reasoning Models", "score": 10, "session": "SD-1-5518", "pdf_url": "https://openreview.net/pdf/52cab7bd6214e5b9d63addbfb0411f3ce8f517f4.pdf", "relevant": 4, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zifeng Cheng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 10.0, "max_score": 10, "total_relevant": 1, "total_reads": 9, "papers": [{"title": "Steering When Necessary: Flexible Steering Large Language Models with Backtracking", "score": 10, "session": "SD-2-1912", "pdf_url": "https://openreview.net/pdf/795a175c3b3b1e9de873069086e754159a4e855b.pdf", "relevant": 1, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jinwei Gan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 10.0, "max_score": 10, "total_relevant": 1, "total_reads": 9, "papers": [{"title": "Steering When Necessary: Flexible Steering Large Language Models with Backtracking", "score": 10, "session": "SD-2-1912", "pdf_url": "https://openreview.net/pdf/795a175c3b3b1e9de873069086e754159a4e855b.pdf", "relevant": 1, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qing Gu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 10.0, "max_score": 10, "total_relevant": 1, "total_reads": 9, "papers": [{"title": "Steering When Necessary: Flexible Steering Large Language Models with Backtracking", "score": 10, "session": "SD-2-1912", "pdf_url": "https://openreview.net/pdf/795a175c3b3b1e9de873069086e754159a4e855b.pdf", "relevant": 1, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junyi Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 10.0, "max_score": 10, "total_relevant": 6, "total_reads": 18, "papers": [{"title": "Reasoning Models Hallucinate More: Factuality-Aware Reinforcement Learning for Large Reasoning Models", "score": 10, "session": "SD-2-314", "pdf_url": "https://openreview.net/pdf/31f57f113b7a0a8d53b00020bbcdabe6ac8a82cf.pdf", "relevant": 6, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hwee Tou Ng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 10.0, "max_score": 10, "total_relevant": 6, "total_reads": 18, "papers": [{"title": "Reasoning Models Hallucinate More: Factuality-Aware Reinforcement Learning for Large Reasoning Models", "score": 10, "session": "SD-2-314", "pdf_url": "https://openreview.net/pdf/31f57f113b7a0a8d53b00020bbcdabe6ac8a82cf.pdf", "relevant": 6, "reads": 18}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haolong Yan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 10.0, "max_score": 10, "total_relevant": 3, "total_reads": 8, "papers": [{"title": "GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning", "score": 10, "session": "SD-4-415", "pdf_url": "https://openreview.net/pdf/ce35fb684e3b11b9c0f1fcc38598cfb3504c728e.pdf", "relevant": 3, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yeqing Shen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 10.0, "max_score": 10, "total_relevant": 3, "total_reads": 8, "papers": [{"title": "GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning", "score": 10, "session": "SD-4-415", "pdf_url": "https://openreview.net/pdf/ce35fb684e3b11b9c0f1fcc38598cfb3504c728e.pdf", "relevant": 3, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yixin Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 10.0, "max_score": 10, "total_relevant": 7, "total_reads": 17, "papers": [{"title": "On Evaluating LLM Alignment by Evaluating LLMs as Judges", "score": 10, "session": "SD-4-2000", "pdf_url": "https://openreview.net/pdf/cefd5dffe2958e9dbfba77cc4764ee04a8cc5a95.pdf", "relevant": 7, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pengfei Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 10.0, "max_score": 10, "total_relevant": 7, "total_reads": 17, "papers": [{"title": "On Evaluating LLM Alignment by Evaluating LLMs as Judges", "score": 10, "session": "SD-4-2000", "pdf_url": "https://openreview.net/pdf/cefd5dffe2958e9dbfba77cc4764ee04a8cc5a95.pdf", "relevant": 7, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yukang Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 10.0, "max_score": 10, "total_relevant": 20, "total_reads": 59, "papers": [{"title": "Scaling RL to Long Videos", "score": 10, "session": "SD-5-4709", "pdf_url": "https://openreview.net/pdf/086f9eed5c2d342130b7d5c3c1f80a2cc8f3594f.pdf", "relevant": 20, "reads": 59}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wei Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 10.0, "max_score": 10, "total_relevant": 20, "total_reads": 59, "papers": [{"title": "Scaling RL to Long Videos", "score": 10, "session": "SD-5-4709", "pdf_url": "https://openreview.net/pdf/086f9eed5c2d342130b7d5c3c1f80a2cc8f3594f.pdf", "relevant": 20, "reads": 59}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Song Han", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 10.0, "max_score": 10, "total_relevant": 20, "total_reads": 59, "papers": [{"title": "Scaling RL to Long Videos", "score": 10, "session": "SD-5-4709", "pdf_url": "https://openreview.net/pdf/086f9eed5c2d342130b7d5c3c1f80a2cc8f3594f.pdf", "relevant": 20, "reads": 59}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuhui Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 19, "total_reads": 26, "papers": [{"title": "EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test", "score": 9, "session": "SD-1-3606", "pdf_url": "https://openreview.net/pdf/28c4c8cf58b0086a2136d73f6059ada87ac33e53.pdf", "relevant": 19, "reads": 26}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Fangyun Wei", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 19, "total_reads": 26, "papers": [{"title": "EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test", "score": 9, "session": "SD-1-3606", "pdf_url": "https://openreview.net/pdf/28c4c8cf58b0086a2136d73f6059ada87ac33e53.pdf", "relevant": 19, "reads": 26}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hongyang Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 19, "total_reads": 26, "papers": [{"title": "EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test", "score": 9, "session": "SD-1-3606", "pdf_url": "https://openreview.net/pdf/28c4c8cf58b0086a2136d73f6059ada87ac33e53.pdf", "relevant": 19, "reads": 26}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Guanhua Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 5, "total_reads": 11, "papers": [{"title": "How Benchmark Prediction from Fewer Data Misses the Mark", "score": 9, "session": "SD-1-207", "pdf_url": "https://openreview.net/pdf/a4097f48740ba588b5ffe5a4fd3f7d88b8eb0a70.pdf", "relevant": 5, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Florian E. Dorner", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 5, "total_reads": 11, "papers": [{"title": "How Benchmark Prediction from Fewer Data Misses the Mark", "score": 9, "session": "SD-1-207", "pdf_url": "https://openreview.net/pdf/a4097f48740ba588b5ffe5a4fd3f7d88b8eb0a70.pdf", "relevant": 5, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Moritz Hardt", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 5, "total_reads": 11, "papers": [{"title": "How Benchmark Prediction from Fewer Data Misses the Mark", "score": 9, "session": "SD-1-207", "pdf_url": "https://openreview.net/pdf/a4097f48740ba588b5ffe5a4fd3f7d88b8eb0a70.pdf", "relevant": 5, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Rishabh Agrawal", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 0, "total_reads": 1, "papers": [{"title": "SpecMAS: A Multi-Agent System for Self-Verifying System Generation via Formal Model Checking", "score": 9, "session": "SD-1-2802", "pdf_url": "https://openreview.net/pdf/d2baf726e03709dc05beda305bd6ede14d1a9b1b.pdf", "relevant": 0, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kaushik Tushar Ranade", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 0, "total_reads": 1, "papers": [{"title": "SpecMAS: A Multi-Agent System for Self-Verifying System Generation via Formal Model Checking", "score": 9, "session": "SD-1-2802", "pdf_url": "https://openreview.net/pdf/d2baf726e03709dc05beda305bd6ede14d1a9b1b.pdf", "relevant": 0, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Apurva Narayan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 0, "total_reads": 1, "papers": [{"title": "SpecMAS: A Multi-Agent System for Self-Verifying System Generation via Formal Model Checking", "score": 9, "session": "SD-1-2802", "pdf_url": "https://openreview.net/pdf/d2baf726e03709dc05beda305bd6ede14d1a9b1b.pdf", "relevant": 0, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Rui Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 3, "total_reads": 4, "papers": [{"title": "LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for Scientific Embodied Agents", "score": 9, "session": "SD-2-2204", "pdf_url": "https://arxiv.org/pdf/2505.22634", "relevant": 3, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "SHIXIANG TANG", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 3, "total_reads": 4, "papers": [{"title": "LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for Scientific Embodied Agents", "score": 9, "session": "SD-2-2204", "pdf_url": "https://arxiv.org/pdf/2505.22634", "relevant": 3, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tianyu Fu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 3, "total_reads": 10, "papers": [{"title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing", "score": 9, "session": "SD-4-5317", "pdf_url": "https://openreview.net/pdf/14de671242c3654992d79919aba5d312e05f7347.pdf", "relevant": 3, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yi Ge", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 3, "total_reads": 10, "papers": [{"title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing", "score": 9, "session": "SD-4-5317", "pdf_url": "https://openreview.net/pdf/14de671242c3654992d79919aba5d312e05f7347.pdf", "relevant": 3, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yu Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 3, "total_reads": 10, "papers": [{"title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing", "score": 9, "session": "SD-4-5317", "pdf_url": "https://openreview.net/pdf/14de671242c3654992d79919aba5d312e05f7347.pdf", "relevant": 3, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Feng Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 4, "total_reads": 14, "papers": [{"title": "Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning", "score": 9, "session": "SD-6-3719", "pdf_url": "https://openreview.net/pdf/d5b025da459325ca88dbc0f0f8c3dc0c23384640.pdf", "relevant": 4, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Allan Raventos", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 4, "total_reads": 14, "papers": [{"title": "Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning", "score": 9, "session": "SD-6-3719", "pdf_url": "https://openreview.net/pdf/d5b025da459325ca88dbc0f0f8c3dc0c23384640.pdf", "relevant": 4, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shaul Druckmann", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 4, "total_reads": 14, "papers": [{"title": "Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning", "score": 9, "session": "SD-6-3719", "pdf_url": "https://openreview.net/pdf/d5b025da459325ca88dbc0f0f8c3dc0c23384640.pdf", "relevant": 4, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lee Jung-Mok", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "Automated Model Discovery via Multi-modal & Multi-step Pipeline", "score": 9, "session": "SD-6-4613", "pdf_url": "https://openreview.net/pdf/9677940f4812871bd634d12cccadc3598687a9ac.pdf", "relevant": 1, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Nam Hyeon-Woo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "Automated Model Discovery via Multi-modal & Multi-step Pipeline", "score": 9, "session": "SD-6-4613", "pdf_url": "https://openreview.net/pdf/9677940f4812871bd634d12cccadc3598687a9ac.pdf", "relevant": 1, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tae-Hyun Oh", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 9.0, "max_score": 9, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "Automated Model Discovery via Multi-modal & Multi-step Pipeline", "score": 9, "session": "SD-6-4613", "pdf_url": "https://openreview.net/pdf/9677940f4812871bd634d12cccadc3598687a9ac.pdf", "relevant": 1, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hadi Khalaf", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 8.0, "max_score": 8, "total_relevant": 11, "total_reads": 27, "papers": [{"title": "Inference-Time Reward Hacking in Large Language Models", "score": 8, "session": "SD-1-1403", "pdf_url": "https://openreview.net/pdf/8d477ae3e38043ed738ea2aa59e110eec3f49a44.pdf", "relevant": 11, "reads": 27}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Claudio Mayrink Verdun", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 8.0, "max_score": 8, "total_relevant": 11, "total_reads": 27, "papers": [{"title": "Inference-Time Reward Hacking in Large Language Models", "score": 8, "session": "SD-1-1403", "pdf_url": "https://openreview.net/pdf/8d477ae3e38043ed738ea2aa59e110eec3f49a44.pdf", "relevant": 11, "reads": 27}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Flavio Calmon", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 8.0, "max_score": 8, "total_relevant": 11, "total_reads": 27, "papers": [{"title": "Inference-Time Reward Hacking in Large Language Models", "score": 8, "session": "SD-1-1403", "pdf_url": "https://openreview.net/pdf/8d477ae3e38043ed738ea2aa59e110eec3f49a44.pdf", "relevant": 11, "reads": 27}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zewei Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 8.0, "max_score": 8, "total_relevant": 24, "total_reads": 55, "papers": [{"title": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "score": 8, "session": "SD-1-2208", "pdf_url": "https://openreview.net/pdf/ac3e0d2216d650bf65be2b1559d68dc79c32c6ed.pdf", "relevant": 24, "reads": 55}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tianhui Cai", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 8.0, "max_score": 8, "total_relevant": 24, "total_reads": 55, "papers": [{"title": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "score": 8, "session": "SD-1-2208", "pdf_url": "https://openreview.net/pdf/ac3e0d2216d650bf65be2b1559d68dc79c32c6ed.pdf", "relevant": 24, "reads": 55}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiaqi Ma", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 8.0, "max_score": 8, "total_relevant": 24, "total_reads": 55, "papers": [{"title": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "score": 8, "session": "SD-1-2208", "pdf_url": "https://openreview.net/pdf/ac3e0d2216d650bf65be2b1559d68dc79c32c6ed.pdf", "relevant": 24, "reads": 55}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Gucongcong Fan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 8.0, "max_score": 8, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs", "score": 8, "session": "SD-2-4111", "pdf_url": "https://openreview.net/pdf/5b9c63b9b600fd293798deb51960a50373bb0faf.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chaoyue Niu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 8.0, "max_score": 8, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs", "score": 8, "session": "SD-2-4111", "pdf_url": "https://openreview.net/pdf/5b9c63b9b600fd293798deb51960a50373bb0faf.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Guihai Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 8.0, "max_score": 8, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs", "score": 8, "session": "SD-2-4111", "pdf_url": "https://openreview.net/pdf/5b9c63b9b600fd293798deb51960a50373bb0faf.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Viktoria Schram", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 8.0, "max_score": 8, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Zero-Shot Performance Prediction for Probabilistic Scaling Laws", "score": 8, "session": "SD-4-608", "pdf_url": "https://openreview.net/pdf/5656a3e0c168ce620e714e942c21cc42662e5dcc.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Markus Hiller", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 8.0, "max_score": 8, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Zero-Shot Performance Prediction for Probabilistic Scaling Laws", "score": 8, "session": "SD-4-608", "pdf_url": "https://openreview.net/pdf/5656a3e0c168ce620e714e942c21cc42662e5dcc.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Trevor Cohn", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 8.0, "max_score": 8, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "Zero-Shot Performance Prediction for Probabilistic Scaling Laws", "score": 8, "session": "SD-4-608", "pdf_url": "https://openreview.net/pdf/5656a3e0c168ce620e714e942c21cc42662e5dcc.pdf", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Christian Fruhwirth-Reisinger", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 5, "total_reads": 21, "papers": [{"title": "STSBench: A Spatio-temporal Scenario Benchmark for Multi-modal Large Language Models in Autonomous Driving", "score": 7, "session": "SD-1-4618", "pdf_url": "https://arxiv.org/pdf/2506.06218", "relevant": 5, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Du√Ö¬°an Mali√Ñ¬á", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 5, "total_reads": 21, "papers": [{"title": "STSBench: A Spatio-temporal Scenario Benchmark for Multi-modal Large Language Models in Autonomous Driving", "score": 7, "session": "SD-1-4618", "pdf_url": "https://arxiv.org/pdf/2506.06218", "relevant": 5, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Horst Possegger", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 5, "total_reads": 21, "papers": [{"title": "STSBench: A Spatio-temporal Scenario Benchmark for Multi-modal Large Language Models in Autonomous Driving", "score": 7, "session": "SD-1-4618", "pdf_url": "https://arxiv.org/pdf/2506.06218", "relevant": 5, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Guiyao Tie", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 4.0, "max_score": 7, "total_relevant": 5, "total_reads": 18, "papers": [{"title": "Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs", "score": 7, "session": "SD-2-5100", "pdf_url": "https://arxiv.org/pdf/2510.16062", "relevant": 4, "reads": 10}, {"title": "BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization", "score": 1, "session": "SD-4-2408", "pdf_url": "https://openreview.net/pdf/9ef98e483d9e324778e7116170aa4848fc6f67e3.pdf", "relevant": 1, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zenghui Yuan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 4, "total_reads": 10, "papers": [{"title": "Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs", "score": 7, "session": "SD-2-5100", "pdf_url": "https://arxiv.org/pdf/2510.16062", "relevant": 4, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lichao Sun", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 4.0, "max_score": 7, "total_relevant": 5, "total_reads": 18, "papers": [{"title": "Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs", "score": 7, "session": "SD-2-5100", "pdf_url": "https://arxiv.org/pdf/2510.16062", "relevant": 4, "reads": 10}, {"title": "BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization", "score": 1, "session": "SD-4-2408", "pdf_url": "https://openreview.net/pdf/9ef98e483d9e324778e7116170aa4848fc6f67e3.pdf", "relevant": 1, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhining Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling", "score": 7, "session": "SD-4-2203", "pdf_url": "https://openreview.net/pdf/97d508cdb6040e0326e1f3e82a7473020de10424.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chuanyang Jin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling", "score": 7, "session": "SD-4-2203", "pdf_url": "https://openreview.net/pdf/97d508cdb6040e0326e1f3e82a7473020de10424.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tianmin Shu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling", "score": 7, "session": "SD-4-2203", "pdf_url": "https://openreview.net/pdf/97d508cdb6040e0326e1f3e82a7473020de10424.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaoyuan Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "score": 7, "session": "SD-5-306", "pdf_url": "https://openreview.net/pdf/f28db2a8d244c8994006bd065afbd5a061c42feb.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tian Liang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "score": 7, "session": "SD-5-306", "pdf_url": "https://openreview.net/pdf/f28db2a8d244c8994006bd065afbd5a061c42feb.pdf", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "John Hughes", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 12, "total_reads": 16, "papers": [{"title": "Best-of-N Jailbreaking", "score": 7, "session": "SD-5-3913", "pdf_url": "https://openreview.net/pdf/214b0cbe5fe5a3a56ddfd1977e1acfb9c721c50a.pdf", "relevant": 12, "reads": 16}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sara Price", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 12, "total_reads": 16, "papers": [{"title": "Best-of-N Jailbreaking", "score": 7, "session": "SD-5-3913", "pdf_url": "https://openreview.net/pdf/214b0cbe5fe5a3a56ddfd1977e1acfb9c721c50a.pdf", "relevant": 12, "reads": 16}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mrinank Sharma", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 12, "total_reads": 16, "papers": [{"title": "Best-of-N Jailbreaking", "score": 7, "session": "SD-5-3913", "pdf_url": "https://openreview.net/pdf/214b0cbe5fe5a3a56ddfd1977e1acfb9c721c50a.pdf", "relevant": 12, "reads": 16}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tianyi Bai", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 7, "total_reads": 21, "papers": [{"title": "Multi-step Visual Reasoning with Visual Tokens Scaling and Verification", "score": 7, "session": "SD-5-4813", "pdf_url": "https://openreview.net/pdf/a48656c1e2d8fb5cee742d21b8b328458f483707.pdf", "relevant": 7, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zengjie Hu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 7, "total_reads": 21, "papers": [{"title": "Multi-step Visual Reasoning with Visual Tokens Scaling and Verification", "score": 7, "session": "SD-5-4813", "pdf_url": "https://openreview.net/pdf/a48656c1e2d8fb5cee742d21b8b328458f483707.pdf", "relevant": 7, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wentao Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 7, "total_reads": 21, "papers": [{"title": "Multi-step Visual Reasoning with Visual Tokens Scaling and Verification", "score": 7, "session": "SD-5-4813", "pdf_url": "https://openreview.net/pdf/a48656c1e2d8fb5cee742d21b8b328458f483707.pdf", "relevant": 7, "reads": 21}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Vlad Sobal", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 26, "total_reads": 80, "papers": [{"title": "Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models", "score": 7, "session": "SD-6-212", "pdf_url": "https://openreview.net/pdf/5233ea1b8598dfd1eed771e087de919731ced5a1.pdf", "relevant": 26, "reads": 80}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wancong Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 26, "total_reads": 80, "papers": [{"title": "Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models", "score": 7, "session": "SD-6-212", "pdf_url": "https://openreview.net/pdf/5233ea1b8598dfd1eed771e087de919731ced5a1.pdf", "relevant": 26, "reads": 80}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yann LeCun", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 7.0, "max_score": 7, "total_relevant": 26, "total_reads": 80, "papers": [{"title": "Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models", "score": 7, "session": "SD-6-212", "pdf_url": "https://openreview.net/pdf/5233ea1b8598dfd1eed771e087de919731ced5a1.pdf", "relevant": 26, "reads": 80}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Runyang You", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 6.0, "max_score": 6, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "R$^2$ec: Towards Large Recommender Models with Reasoning", "score": 6, "session": "SD-1-2412", "pdf_url": "https://openreview.net/pdf/f29b278eccadd4b83c9cca979b6b35476c14e3a8.pdf", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yongqi Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 6.0, "max_score": 6, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "R$^2$ec: Towards Large Recommender Models with Reasoning", "score": 6, "session": "SD-1-2412", "pdf_url": "https://openreview.net/pdf/f29b278eccadd4b83c9cca979b6b35476c14e3a8.pdf", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Liqiang Nie", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 6.0, "max_score": 6, "total_relevant": 1, "total_reads": 4, "papers": [{"title": "R$^2$ec: Towards Large Recommender Models with Reasoning", "score": 6, "session": "SD-1-2412", "pdf_url": "https://openreview.net/pdf/f29b278eccadd4b83c9cca979b6b35476c14e3a8.pdf", "relevant": 1, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yibo Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 6.0, "max_score": 6, "total_relevant": 1, "total_reads": 5, "papers": [{"title": "Triplets Better Than Pairs: Towards Stable and Effective Self-Play Fine-Tuning for LLMs", "score": 6, "session": "SD-5-3608", "pdf_url": "https://openreview.net/pdf/e0596e6e39072aed73b22d2c2c86772c2aae52a0.pdf", "relevant": 1, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hai-Long Sun", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 6.0, "max_score": 6, "total_relevant": 1, "total_reads": 5, "papers": [{"title": "Triplets Better Than Pairs: Towards Stable and Effective Self-Play Fine-Tuning for LLMs", "score": 6, "session": "SD-5-3608", "pdf_url": "https://openreview.net/pdf/e0596e6e39072aed73b22d2c2c86772c2aae52a0.pdf", "relevant": 1, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lijun Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 6.0, "max_score": 6, "total_relevant": 1, "total_reads": 5, "papers": [{"title": "Triplets Better Than Pairs: Towards Stable and Effective Self-Play Fine-Tuning for LLMs", "score": 6, "session": "SD-5-3608", "pdf_url": "https://openreview.net/pdf/e0596e6e39072aed73b22d2c2c86772c2aae52a0.pdf", "relevant": 1, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hancheng Ye", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 2, "total_reads": 4, "papers": [{"title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems", "score": 5, "session": "SD-1-810", "pdf_url": "https://openreview.net/pdf/81561154949bf17e7f12ee6dc0485c10a2415686.pdf", "relevant": 2, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yiran Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 2, "total_reads": 4, "papers": [{"title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems", "score": 5, "session": "SD-1-810", "pdf_url": "https://openreview.net/pdf/81561154949bf17e7f12ee6dc0485c10a2415686.pdf", "relevant": 2, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pranjal Awasthi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Length Generalization via Auxiliary Tasks", "score": 5, "session": "SD-2-5402", "pdf_url": "https://openreview.net/pdf/a5e5e4faf09290faa591d33b29401917374054fa.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Anupam Gupta", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Length Generalization via Auxiliary Tasks", "score": 5, "session": "SD-2-5402", "pdf_url": "https://openreview.net/pdf/a5e5e4faf09290faa591d33b29401917374054fa.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ravi Kumar", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Length Generalization via Auxiliary Tasks", "score": 5, "session": "SD-2-5402", "pdf_url": "https://openreview.net/pdf/a5e5e4faf09290faa591d33b29401917374054fa.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junqi Jiang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 0, "total_reads": 6, "papers": [{"title": "Representation Consistency for Accurate and Coherent LLM Answer Aggregation", "score": 5, "session": "SD-2-4008", "pdf_url": "https://openreview.net/pdf/a3ecdca64c27eaba285b6683681268faca747be5.pdf", "relevant": 0, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tom Bewley", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 0, "total_reads": 6, "papers": [{"title": "Representation Consistency for Accurate and Coherent LLM Answer Aggregation", "score": 5, "session": "SD-2-4008", "pdf_url": "https://openreview.net/pdf/a3ecdca64c27eaba285b6683681268faca747be5.pdf", "relevant": 0, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Francesca Toni", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 0, "total_reads": 6, "papers": [{"title": "Representation Consistency for Accurate and Coherent LLM Answer Aggregation", "score": 5, "session": "SD-2-4008", "pdf_url": "https://openreview.net/pdf/a3ecdca64c27eaba285b6683681268faca747be5.pdf", "relevant": 0, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Penghui Qi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "score": 5, "session": "SD-4-5418", "pdf_url": "https://openreview.net/pdf/8136e4668a09f8c47a2454d9e72728d4fdea055e.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zichen Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "score": 5, "session": "SD-4-5418", "pdf_url": "https://openreview.net/pdf/8136e4668a09f8c47a2454d9e72728d4fdea055e.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Min Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 1, "total_reads": 3, "papers": [{"title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "score": 5, "session": "SD-4-5418", "pdf_url": "https://openreview.net/pdf/8136e4668a09f8c47a2454d9e72728d4fdea055e.pdf", "relevant": 1, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Vijay Viswanathan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 6, "total_reads": 19, "papers": [{"title": "Checklists Are Better Than Reward Models For Aligning Language Models", "score": 5, "session": "SD-4-103", "pdf_url": "https://openreview.net/pdf/490597cf8f353f8b01b8474e2f98c045eba8f5f4.pdf", "relevant": 6, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yanchao Sun", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 6, "total_reads": 19, "papers": [{"title": "Checklists Are Better Than Reward Models For Aligning Language Models", "score": 5, "session": "SD-4-103", "pdf_url": "https://openreview.net/pdf/490597cf8f353f8b01b8474e2f98c045eba8f5f4.pdf", "relevant": 6, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tongshuang Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 6, "total_reads": 19, "papers": [{"title": "Checklists Are Better Than Reward Models For Aligning Language Models", "score": 5, "session": "SD-4-103", "pdf_url": "https://openreview.net/pdf/490597cf8f353f8b01b8474e2f98c045eba8f5f4.pdf", "relevant": 6, "reads": 19}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pengteng Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 5, "total_reads": 37, "papers": [{"title": "See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model", "score": 5, "session": "SD-5-5206", "pdf_url": "https://openreview.net/pdf/bcef03fe8da8c459269e00eeb6626d6584a0ba4c.pdf", "relevant": 5, "reads": 37}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pinhao Song", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 5, "total_reads": 37, "papers": [{"title": "See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model", "score": 5, "session": "SD-5-5206", "pdf_url": "https://openreview.net/pdf/bcef03fe8da8c459269e00eeb6626d6584a0ba4c.pdf", "relevant": 5, "reads": 37}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiao Liang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning", "score": 5, "session": "SD-6-1802", "pdf_url": "https://openreview.net/pdf/c3403842de341324f63358f1732f1518761661f2.pdf", "relevant": 1, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhong-Zhi Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning", "score": 5, "session": "SD-6-1802", "pdf_url": "https://openreview.net/pdf/c3403842de341324f63358f1732f1518761661f2.pdf", "relevant": 1, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Weizhu Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 1, "total_reads": 2, "papers": [{"title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning", "score": 5, "session": "SD-6-1802", "pdf_url": "https://openreview.net/pdf/c3403842de341324f63358f1732f1518761661f2.pdf", "relevant": 1, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhongyi Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 12, "total_reads": 58, "papers": [{"title": "ChatVLA-2: Vision-Language-Action Model with Open-World Reasoning", "score": 5, "session": "SD-6-2201", "pdf_url": "https://openreview.net/pdf/c88d737915ea445cb600d21cb0c7125912b7053b.pdf", "relevant": 12, "reads": 58}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yichen Zhu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 12, "total_reads": 58, "papers": [{"title": "ChatVLA-2: Vision-Language-Action Model with Open-World Reasoning", "score": 5, "session": "SD-6-2201", "pdf_url": "https://openreview.net/pdf/c88d737915ea445cb600d21cb0c7125912b7053b.pdf", "relevant": 12, "reads": 58}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yi Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 5.0, "max_score": 5, "total_relevant": 12, "total_reads": 58, "papers": [{"title": "ChatVLA-2: Vision-Language-Action Model with Open-World Reasoning", "score": 5, "session": "SD-6-2201", "pdf_url": "https://openreview.net/pdf/c88d737915ea445cb600d21cb0c7125912b7053b.pdf", "relevant": 12, "reads": 58}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haonan Duan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 4.0, "max_score": 4, "total_relevant": 2, "total_reads": 8, "papers": [{"title": "Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab", "score": 4, "session": "SD-2-1615", "pdf_url": "https://arxiv.org/pdf/2507.02083", "relevant": 2, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Stephen Lu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 4.0, "max_score": 4, "total_relevant": 2, "total_reads": 8, "papers": [{"title": "Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab", "score": 4, "session": "SD-2-1615", "pdf_url": "https://arxiv.org/pdf/2507.02083", "relevant": 2, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chris Maddison", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 4.0, "max_score": 4, "total_relevant": 2, "total_reads": 8, "papers": [{"title": "Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab", "score": 4, "session": "SD-2-1615", "pdf_url": "https://arxiv.org/pdf/2507.02083", "relevant": 2, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junjie Xing", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 4.0, "max_score": 4, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark", "score": 4, "session": "SD-2-1804", "pdf_url": "https://arxiv.org/pdf/2506.05587", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yeye He", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 4.0, "max_score": 4, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark", "score": 4, "session": "SD-2-1804", "pdf_url": "https://arxiv.org/pdf/2506.05587", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "H. V. Jagadish", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 4.0, "max_score": 4, "total_relevant": 0, "total_reads": 2, "papers": [{"title": "MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark", "score": 4, "session": "SD-2-1804", "pdf_url": "https://arxiv.org/pdf/2506.05587", "relevant": 0, "reads": 2}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhe Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 4.0, "max_score": 4, "total_relevant": 1, "total_reads": 8, "papers": [{"title": "MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?", "score": 4, "session": "SD-6-113", "pdf_url": "https://arxiv.org/pdf/2503.09499", "relevant": 1, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Daoyuan Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 4.0, "max_score": 4, "total_relevant": 1, "total_reads": 8, "papers": [{"title": "MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?", "score": 4, "session": "SD-6-113", "pdf_url": "https://arxiv.org/pdf/2503.09499", "relevant": 1, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ying Shen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 4.0, "max_score": 4, "total_relevant": 1, "total_reads": 8, "papers": [{"title": "MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?", "score": 4, "session": "SD-6-113", "pdf_url": "https://arxiv.org/pdf/2503.09499", "relevant": 1, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ilgee Hong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 4.0, "max_score": 4, "total_relevant": 5, "total_reads": 12, "papers": [{"title": "Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models", "score": 4, "session": "SD-6-3613", "pdf_url": "https://openreview.net/pdf/eec1920da8921f9ebab8a70513b7531b0b8281d3.pdf", "relevant": 5, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Changlong Yu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 4.0, "max_score": 4, "total_relevant": 5, "total_reads": 12, "papers": [{"title": "Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models", "score": 4, "session": "SD-6-3613", "pdf_url": "https://openreview.net/pdf/eec1920da8921f9ebab8a70513b7531b0b8281d3.pdf", "relevant": 5, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tuo Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 4.0, "max_score": 4, "total_relevant": 5, "total_reads": 12, "papers": [{"title": "Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models", "score": 4, "session": "SD-6-3613", "pdf_url": "https://openreview.net/pdf/eec1920da8921f9ebab8a70513b7531b0b8281d3.pdf", "relevant": 5, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kaihang Pan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 3.0, "max_score": 3, "total_relevant": 4, "total_reads": 11, "papers": [{"title": "Janus-Pro-R1: Advancing Collaborative Visual Comprehension and Generation via Reinforcement Learning", "score": 3, "session": "SD-1-5401", "pdf_url": "https://openreview.net/pdf/4aad34acb1fe54ce3d709604ec7d81ff8ce11b11.pdf", "relevant": 4, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yang Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 3.0, "max_score": 3, "total_relevant": 4, "total_reads": 11, "papers": [{"title": "Janus-Pro-R1: Advancing Collaborative Visual Comprehension and Generation via Reinforcement Learning", "score": 3, "session": "SD-1-5401", "pdf_url": "https://openreview.net/pdf/4aad34acb1fe54ce3d709604ec7d81ff8ce11b11.pdf", "relevant": 4, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yueting Zhuang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 3.0, "max_score": 3, "total_relevant": 4, "total_reads": 11, "papers": [{"title": "Janus-Pro-R1: Advancing Collaborative Visual Comprehension and Generation via Reinforcement Learning", "score": 3, "session": "SD-1-5401", "pdf_url": "https://openreview.net/pdf/4aad34acb1fe54ce3d709604ec7d81ff8ce11b11.pdf", "relevant": 4, "reads": 11}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Boyi Wei", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 3.0, "max_score": 3, "total_relevant": 5, "total_reads": 7, "papers": [{"title": "Dynamic Risk Assessments for Offensive Cybersecurity Agents", "score": 3, "session": "SD-2-1109", "pdf_url": "https://arxiv.org/pdf/2505.18384", "relevant": 5, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Benedikt Stroebl", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 3.0, "max_score": 3, "total_relevant": 5, "total_reads": 7, "papers": [{"title": "Dynamic Risk Assessments for Offensive Cybersecurity Agents", "score": 3, "session": "SD-2-1109", "pdf_url": "https://arxiv.org/pdf/2505.18384", "relevant": 5, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Peter Henderson", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 3.0, "max_score": 3, "total_relevant": 5, "total_reads": 7, "papers": [{"title": "Dynamic Risk Assessments for Offensive Cybersecurity Agents", "score": 3, "session": "SD-2-1109", "pdf_url": "https://arxiv.org/pdf/2505.18384", "relevant": 5, "reads": 7}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiajun Shi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 3.0, "max_score": 3, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation", "score": 3, "session": "SD-4-2014", "pdf_url": "https://openreview.net/pdf/a3f1d4df6324abb0ea9576e5fe2da1d467238283.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jian Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 3.0, "max_score": 3, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation", "score": 3, "session": "SD-4-2014", "pdf_url": "https://openreview.net/pdf/a3f1d4df6324abb0ea9576e5fe2da1d467238283.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Satvik Golechha", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 3.0, "max_score": 3, "total_relevant": 12, "total_reads": 16, "papers": [{"title": "Among Us: A Sandbox for Measuring and Detecting Agentic Deception", "score": 3, "session": "SD-5-1517", "pdf_url": "https://openreview.net/pdf/0ff014da0c71915ceb0a84e3977c85eaf1e134dd.pdf", "relevant": 12, "reads": 16}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Adri√É¬† Garriga-Alonso", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 3.0, "max_score": 3, "total_relevant": 12, "total_reads": 16, "papers": [{"title": "Among Us: A Sandbox for Measuring and Detecting Agentic Deception", "score": 3, "session": "SD-5-1517", "pdf_url": "https://openreview.net/pdf/0ff014da0c71915ceb0a84e3977c85eaf1e134dd.pdf", "relevant": 12, "reads": 16}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chongkai Gao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 3.0, "max_score": 3, "total_relevant": 4, "total_reads": 17, "papers": [{"title": "VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models", "score": 3, "session": "SD-5-2213", "pdf_url": "https://openreview.net/pdf/05a810d8dce16f520e115b9ee80b8096e6512276.pdf", "relevant": 4, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zixuan Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 3.0, "max_score": 3, "total_relevant": 4, "total_reads": 17, "papers": [{"title": "VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models", "score": 3, "session": "SD-5-2213", "pdf_url": "https://openreview.net/pdf/05a810d8dce16f520e115b9ee80b8096e6512276.pdf", "relevant": 4, "reads": 17}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Bufang Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 3.0, "max_score": 3, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-world Sensory Perceptions", "score": 3, "session": "SD-6-2515", "pdf_url": "https://openreview.net/pdf/8c61939b607693d9b13cc1df27793d844f3648f5.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lilin Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 3.0, "max_score": 3, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-world Sensory Perceptions", "score": 3, "session": "SD-6-2515", "pdf_url": "https://openreview.net/pdf/8c61939b607693d9b13cc1df27793d844f3648f5.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhenyu Yan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 3.0, "max_score": 3, "total_relevant": 2, "total_reads": 3, "papers": [{"title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-world Sensory Perceptions", "score": 3, "session": "SD-6-2515", "pdf_url": "https://openreview.net/pdf/8c61939b607693d9b13cc1df27793d844f3648f5.pdf", "relevant": 2, "reads": 3}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuante Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 2.0, "max_score": 2, "total_relevant": 0, "total_reads": 1, "papers": [{"title": "R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization", "score": 2, "session": "SD-2-111", "pdf_url": "https://arxiv.org/pdf/2505.15155", "relevant": 0, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xu Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 2.0, "max_score": 2, "total_relevant": 0, "total_reads": 1, "papers": [{"title": "R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization", "score": 2, "session": "SD-2-111", "pdf_url": "https://arxiv.org/pdf/2505.15155", "relevant": 0, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiang Bian", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 2.0, "max_score": 2, "total_relevant": 0, "total_reads": 1, "papers": [{"title": "R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization", "score": 2, "session": "SD-2-111", "pdf_url": "https://arxiv.org/pdf/2505.15155", "relevant": 0, "reads": 1}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yanyuan Qiao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 2.0, "max_score": 2, "total_relevant": 4, "total_reads": 12, "papers": [{"title": "NavBench: Probing Multimodal Large Language Models for Embodied Navigation", "score": 2, "session": "SD-5-5412", "pdf_url": "https://openreview.net/pdf/1ef1a313c6a3eea3eea8cfe4ac568866df673dec.pdf", "relevant": 4, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haodong Hong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 2.0, "max_score": 2, "total_relevant": 4, "total_reads": 12, "papers": [{"title": "NavBench: Probing Multimodal Large Language Models for Embodied Navigation", "score": 2, "session": "SD-5-5412", "pdf_url": "https://openreview.net/pdf/1ef1a313c6a3eea3eea8cfe4ac568866df673dec.pdf", "relevant": 4, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qi Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 2.0, "max_score": 2, "total_relevant": 4, "total_reads": 12, "papers": [{"title": "NavBench: Probing Multimodal Large Language Models for Embodied Navigation", "score": 2, "session": "SD-5-5412", "pdf_url": "https://openreview.net/pdf/1ef1a313c6a3eea3eea8cfe4ac568866df673dec.pdf", "relevant": 4, "reads": 12}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yongsen Mao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 2.0, "max_score": 2, "total_relevant": 19, "total_reads": 36, "papers": [{"title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling", "score": 2, "session": "SD-5-1612", "pdf_url": "https://openreview.net/pdf/22f930c6b44c852e1c53aa7784df786168735e5d.pdf", "relevant": 19, "reads": 36}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junhao Zhong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 2.0, "max_score": 2, "total_relevant": 19, "total_reads": 36, "papers": [{"title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling", "score": 2, "session": "SD-5-1612", "pdf_url": "https://openreview.net/pdf/22f930c6b44c852e1c53aa7784df786168735e5d.pdf", "relevant": 19, "reads": 36}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zihan Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 2.0, "max_score": 2, "total_relevant": 19, "total_reads": 36, "papers": [{"title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling", "score": 2, "session": "SD-5-1612", "pdf_url": "https://openreview.net/pdf/22f930c6b44c852e1c53aa7784df786168735e5d.pdf", "relevant": 19, "reads": 36}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Matthew Landers", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 2.0, "max_score": 2, "total_relevant": 2, "total_reads": 9, "papers": [{"title": "BraVE: Offline Reinforcement Learning for Discrete Combinatorial Action Spaces", "score": 2, "session": "SD-6-306", "pdf_url": "https://openreview.net/pdf/fb6e5db6d0511de2cbf926cf309aa9e0fbe40245.pdf", "relevant": 2, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Taylor W. Killian", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 2.0, "max_score": 2, "total_relevant": 2, "total_reads": 9, "papers": [{"title": "BraVE: Offline Reinforcement Learning for Discrete Combinatorial Action Spaces", "score": 2, "session": "SD-6-306", "pdf_url": "https://openreview.net/pdf/fb6e5db6d0511de2cbf926cf309aa9e0fbe40245.pdf", "relevant": 2, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Afsaneh Doryab", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 2.0, "max_score": 2, "total_relevant": 2, "total_reads": 9, "papers": [{"title": "BraVE: Offline Reinforcement Learning for Discrete Combinatorial Action Spaces", "score": 2, "session": "SD-6-306", "pdf_url": "https://openreview.net/pdf/fb6e5db6d0511de2cbf926cf309aa9e0fbe40245.pdf", "relevant": 2, "reads": 9}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shenghe Zheng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "Scaling Physical Reasoning with the PHYSICS Dataset", "score": 1, "session": "SD-1-2106", "pdf_url": "https://arxiv.org/pdf/2506.00022", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qianjia Cheng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "Scaling Physical Reasoning with the PHYSICS Dataset", "score": 1, "session": "SD-1-2106", "pdf_url": "https://arxiv.org/pdf/2506.00022", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Peng Ye", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 2, "total_reads": 6, "papers": [{"title": "Scaling Physical Reasoning with the PHYSICS Dataset", "score": 1, "session": "SD-1-2106", "pdf_url": "https://arxiv.org/pdf/2506.00022", "relevant": 2, "reads": 6}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Luke Guerdan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 2, "total_reads": 5, "papers": [{"title": "Validating LLM-as-a-Judge Systems under Rating Indeterminacy", "score": 1, "session": "SD-1-1515", "pdf_url": "https://openreview.net/pdf/2ad527d02fe920443fe79a516d7575f417103f99.pdf", "relevant": 2, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Solon Barocas", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 2, "total_reads": 5, "papers": [{"title": "Validating LLM-as-a-Judge Systems under Rating Indeterminacy", "score": 1, "session": "SD-1-1515", "pdf_url": "https://openreview.net/pdf/2ad527d02fe920443fe79a516d7575f417103f99.pdf", "relevant": 2, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Alexandra Chouldechova", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 2, "total_reads": 5, "papers": [{"title": "Validating LLM-as-a-Judge Systems under Rating Indeterminacy", "score": 1, "session": "SD-1-1515", "pdf_url": "https://openreview.net/pdf/2ad527d02fe920443fe79a516d7575f417103f99.pdf", "relevant": 2, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ming Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 0, "total_reads": 4, "papers": [{"title": "To Think or Not To Think: A Study of Thinking in Rule-Based Visual Reinforcement Fine-Tuning", "score": 1, "session": "SD-1-4915", "pdf_url": "https://openreview.net/pdf/8105c1360484fcffb35e03d8f791d0b437aa1589.pdf", "relevant": 0, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jike Zhong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 0, "total_reads": 4, "papers": [{"title": "To Think or Not To Think: A Study of Thinking in Rule-Based Visual Reinforcement Fine-Tuning", "score": 1, "session": "SD-1-4915", "pdf_url": "https://openreview.net/pdf/8105c1360484fcffb35e03d8f791d0b437aa1589.pdf", "relevant": 0, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kaipeng Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 0, "total_reads": 4, "papers": [{"title": "To Think or Not To Think: A Study of Thinking in Rule-Based Visual Reinforcement Fine-Tuning", "score": 1, "session": "SD-1-4915", "pdf_url": "https://openreview.net/pdf/8105c1360484fcffb35e03d8f791d0b437aa1589.pdf", "relevant": 0, "reads": 4}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mengjie Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 4, "total_reads": 10, "papers": [{"title": "Analogy-based Multi-Turn Jailbreak against Large Language Models", "score": 1, "session": "SD-2-1300", "pdf_url": "https://openreview.net/pdf/90ec84387b8d7282640d625e2d28faef32f89000.pdf", "relevant": 4, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yihao Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 4, "total_reads": 10, "papers": [{"title": "Analogy-based Multi-Turn Jailbreak against Large Language Models", "score": 1, "session": "SD-2-1300", "pdf_url": "https://openreview.net/pdf/90ec84387b8d7282640d625e2d28faef32f89000.pdf", "relevant": 4, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lina Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 4, "total_reads": 10, "papers": [{"title": "Analogy-based Multi-Turn Jailbreak against Large Language Models", "score": 1, "session": "SD-2-1300", "pdf_url": "https://openreview.net/pdf/90ec84387b8d7282640d625e2d28faef32f89000.pdf", "relevant": 4, "reads": 10}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hongtao Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 2, "total_reads": 8, "papers": [{"title": "Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling", "score": 1, "session": "SD-4-2002", "pdf_url": "https://openreview.net/pdf/a7327b197b07cb1df3b5e408eecd14e1276acc6d.pdf", "relevant": 2, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenting Shen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 2, "total_reads": 8, "papers": [{"title": "Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling", "score": 1, "session": "SD-4-2002", "pdf_url": "https://openreview.net/pdf/a7327b197b07cb1df3b5e408eecd14e1276acc6d.pdf", "relevant": 2, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Weile Jia", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 2, "total_reads": 8, "papers": [{"title": "Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling", "score": 1, "session": "SD-4-2002", "pdf_url": "https://openreview.net/pdf/a7327b197b07cb1df3b5e408eecd14e1276acc6d.pdf", "relevant": 2, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xueyang Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 1, "total_reads": 8, "papers": [{"title": "BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization", "score": 1, "session": "SD-4-2408", "pdf_url": "https://openreview.net/pdf/9ef98e483d9e324778e7116170aa4848fc6f67e3.pdf", "relevant": 1, "reads": 8}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jusheng Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 0, "total_reads": 5, "papers": [{"title": "GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning", "score": 1, "session": "SD-4-5314", "pdf_url": "https://openreview.net/pdf/55c4606303cc86c81ca99b5d5743cfe40d7fb140.pdf", "relevant": 0, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yijia Fan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 0, "total_reads": 5, "papers": [{"title": "GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning", "score": 1, "session": "SD-4-5314", "pdf_url": "https://openreview.net/pdf/55c4606303cc86c81ca99b5d5743cfe40d7fb140.pdf", "relevant": 0, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Keze Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 0, "total_reads": 5, "papers": [{"title": "GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning", "score": 1, "session": "SD-4-5314", "pdf_url": "https://openreview.net/pdf/55c4606303cc86c81ca99b5d5743cfe40d7fb140.pdf", "relevant": 0, "reads": 5}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kanghui Ning", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 2, "total_reads": 14, "papers": [{"title": "TS-RAG: Retrieval-Augmented Generation based Time Series Foundation Models are Stronger Zero-Shot Forecaster", "score": 1, "session": "SD-5-2308", "pdf_url": "https://openreview.net/pdf/e686d5b872f3c42cd96442359b22e23319fe0acb.pdf", "relevant": 2, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zijie Pan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 2, "total_reads": 14, "papers": [{"title": "TS-RAG: Retrieval-Augmented Generation based Time Series Foundation Models are Stronger Zero-Shot Forecaster", "score": 1, "session": "SD-5-2308", "pdf_url": "https://openreview.net/pdf/e686d5b872f3c42cd96442359b22e23319fe0acb.pdf", "relevant": 2, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dongjin Song", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 2, "total_reads": 14, "papers": [{"title": "TS-RAG: Retrieval-Augmented Generation based Time Series Foundation Models are Stronger Zero-Shot Forecaster", "score": 1, "session": "SD-5-2308", "pdf_url": "https://openreview.net/pdf/e686d5b872f3c42cd96442359b22e23319fe0acb.pdf", "relevant": 2, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xinhe Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 7, "total_reads": 14, "papers": [{"title": "Bridging Human and LLM Judgments: Understanding and Narrowing the Gap", "score": 1, "session": "SD-5-1515", "pdf_url": "https://openreview.net/pdf/3b238b3324d5ab3f3e5a672a12a2c7610ee13a48.pdf", "relevant": 7, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuekai Sun", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 1.0, "max_score": 1, "total_relevant": 7, "total_reads": 14, "papers": [{"title": "Bridging Human and LLM Judgments: Understanding and Narrowing the Gap", "score": 1, "session": "SD-5-1515", "pdf_url": "https://openreview.net/pdf/3b238b3324d5ab3f3e5a672a12a2c7610ee13a48.pdf", "relevant": 7, "reads": 14}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}];

        // Available categories
        const allCategories = ["Agent Benchmarking and Evaluation", "Web and Computer-Use Agents", "Reasoning and Test-Time Compute", "Multi-Agent Systems and Collaboration", "Reinforcement Learning for LLMs", "Agent Safety and Security", "Tool Use and Code Generation", "Planning and Decision Making", "Mathematical and Logical Reasoning", "Vision-Language-Action Models", "Domain-Specific Applications", "Memory and Context Management", "Self-Improvement and Meta-Learning", "Spatial and Physical Reasoning", "Model Efficiency and Optimization"];

        // Pagination state
        let currentAuthorsPage = 1;
        const authorsPerPage = 10;
        let currentPapersPage = 1;
        const papersPerPage = 20;

        function displayStats() {
            const scores = papers.map(p => parseInt(p.score));
            const totalPapers = papers.length;
            const avgScore = (scores.reduce((a, b) => a + b, 0) / totalPapers).toFixed(1);
            const topScore = Math.max(...scores);

            // Find most popular paper (by relevant_to_users)
            const mostPopular = papers.reduce((max, p) => {
                const relevant = parseInt(p.relevant_to_users) || 0;
                const maxRelevant = parseInt(max.relevant_to_users) || 0;
                return relevant > maxRelevant ? p : max;
            }, papers[0]);

            document.getElementById('totalPapers').textContent = totalPapers;
            document.getElementById('avgScore').textContent = avgScore;
            document.getElementById('topScore').textContent = topScore;
            document.getElementById('mostPopular').textContent = `${parseInt(mostPopular.relevant_to_users) || 0} üëç`;
        }

        function displayChart() {
            const scores = papers.map(p => parseInt(p.score));

            // Create histogram bins
            const bins = {};
            const binSize = 10;
            for (let i = 0; i < 100; i += binSize) {
                bins[i] = 0;
            }

            scores.forEach(score => {
                const bin = Math.floor(score / binSize) * binSize;
                bins[bin] = (bins[bin] || 0) + 1;
            });

            const ctx = document.getElementById('scoreChart').getContext('2d');
            new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: Object.keys(bins).map(b => `${b}-${parseInt(b) + binSize - 1}`),
                    datasets: [{
                        label: 'Number of Papers',
                        data: Object.values(bins),
                        backgroundColor: 'rgba(102, 126, 234, 0.8)',
                        borderColor: 'rgba(102, 126, 234, 1)',
                        borderWidth: 2,
                        borderRadius: 8
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    plugins: {
                        legend: {
                            display: false
                        }
                    },
                    scales: {
                        y: {
                            beginAtZero: true,
                            ticks: {
                                precision: 0
                            }
                        }
                    }
                }
            });
        }

        let selectedCategories = new Set();

        function renderCategoryFilters() {
            const filtersDiv = document.getElementById('categoryFilters');
            if (!allCategories || allCategories.length === 0) {
                filtersDiv.style.display = 'none';
                return;
            }

            let html = '<div style="margin-bottom: 10px;"><strong>Filter by Category:</strong></div><div>';
            allCategories.forEach(category => {
                html += `<div class="category-pill" onclick="toggleCategory('${category.replace(/'/g, "\'")}')">${category}</div>`;
            });
            html += '</div>';
            filtersDiv.innerHTML = html;
        }

        function toggleCategory(category) {
            if (selectedCategories.has(category)) {
                selectedCategories.delete(category);
            } else {
                selectedCategories.add(category);
            }

            // Update UI
            document.querySelectorAll('.category-pill').forEach(pill => {
                if (pill.textContent === category) {
                    pill.classList.toggle('selected');
                }
            });

            // Re-display papers with filter
            displayPapers(document.getElementById('sortBy').value, 1);
        }

        function togglePaperCard(index) {
            const card = document.querySelectorAll('.paper-card')[index];
            card.classList.toggle('expanded');
        }

        function openPaperModal(paperIndex, currentPapers) {
            const paper = currentPapers[paperIndex];
            const modal = document.getElementById('paperModal');
            const modalContent = document.getElementById('modalContent');

            let html = `
                <h2 style="margin-top: 0; color: #667eea;">${paper.title || 'Untitled'}</h2>
                ${paper.ai_categories && paper.ai_categories.length > 0 ? `
                    <div style="margin-bottom: 20px;">
                        ${paper.ai_categories.map(cat => `<span class="paper-category-badge">${cat}</span>`).join('')}
                    </div>
                ` : ''}
                ${paper.authors ? `<p><strong>Authors:</strong> ${paper.authors}</p>` : ''}
                <p><strong>Relevance Score:</strong> <span style="font-size: 1.2em; color: #667eea; font-weight: bold;">${paper.score}</span></p>
                ${paper.session_type ? `<p><strong>Session:</strong> ${paper.session_type}</p>` : ''}
                ${paper.session_location ? `<p><strong>Location:</strong> ${paper.session_location}</p>` : ''}

                ${paper.description ? `
                    <div style="margin-top: 25px;">
                        <h3 style="color: #667eea; margin-bottom: 10px;">üìñ What is this paper about?</h3>
                        <p style="line-height: 1.6;">${paper.description}</p>
                    </div>
                ` : ''}

                ${paper.novelty ? `
                    <div style="margin-top: 25px; padding: 20px; background: linear-gradient(135deg, #fff5e6 0%, #ffe6f0 100%); border-radius: 10px; border-left: 4px solid #f59e0b;">
                        <h3 style="color: #d97706; margin-top: 0; margin-bottom: 10px;">üí° What Makes This Novel?</h3>
                        <p style="line-height: 1.6; margin-bottom: 0;">${paper.novelty}</p>
                    </div>
                ` : ''}

                ${paper.key_contribution ? `
                    <div style="margin-top: 25px;">
                        <h3 style="color: #667eea; margin-bottom: 10px;">üéØ Key Contribution</h3>
                        <p style="line-height: 1.6;">${paper.key_contribution}</p>
                    </div>
                ` : ''}

                ${paper.key_findings ? `
                    <div style="margin-top: 25px;">
                        <h3 style="color: #667eea; margin-bottom: 10px;">üîç Key Findings</h3>
                        <p style="line-height: 1.6;">${paper.key_findings}</p>
                    </div>
                ` : ''}

                <div style="margin-top: 30px;">
                    ${paper.relevant_to_users ? `<span style="margin-right: 15px;">üëç <strong>${paper.relevant_to_users}</strong> found relevant</span>` : ''}
                    ${paper.read_by_users ? `<span>üìñ <strong>${paper.read_by_users}</strong> reads</span>` : ''}
                </div>

                ${paper.pdf_url ? `
                    <div style="margin-top: 25px;">
                        <a href="${paper.pdf_url}" target="_blank" style="display: inline-block; padding: 12px 24px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; text-decoration: none; border-radius: 8px; font-weight: 600;">
                            üìÑ View Full PDF
                        </a>
                    </div>
                ` : ''}
            `;

            modalContent.innerHTML = html;
            modal.classList.add('active');
        }

        function closePaperModal() {
            document.getElementById('paperModal').classList.remove('active');
        }

        function displayPapers(sortBy = 'score', page = 1) {
            currentPapersPage = page;
            let sortedPapers = [...papers];

            // Filter by selected categories
            if (selectedCategories.size > 0) {
                sortedPapers = sortedPapers.filter(paper => {
                    if (!paper.ai_categories || paper.ai_categories.length === 0) {
                        return false;
                    }
                    return paper.ai_categories.some(cat => selectedCategories.has(cat));
                });
            }

            switch(sortBy) {
                case 'score':
                    sortedPapers.sort((a, b) => parseInt(b.score) - parseInt(a.score));
                    break;
                case 'relevant':
                    sortedPapers.sort((a, b) => (parseInt(b.relevant_to_users) || 0) - (parseInt(a.relevant_to_users) || 0));
                    break;
                case 'reads':
                    sortedPapers.sort((a, b) => (parseInt(b.read_by_users) || 0) - (parseInt(a.read_by_users) || 0));
                    break;
                case 'title':
                    sortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
            }

            const totalPapers = sortedPapers.length;
            const totalPages = Math.ceil(totalPapers / papersPerPage);
            const startIdx = (page - 1) * papersPerPage;
            const endIdx = startIdx + papersPerPage;
            const pagePapers = sortedPapers.slice(startIdx, endIdx);

            const papersList = document.getElementById('papersList');
            papersList.innerHTML = pagePapers.map((paper, idx) => `
                <div class="paper-card" onclick="togglePaperCard(${idx})">
                    <div class="paper-header">
                        <div class="paper-title">${paper.title || 'Untitled'}</div>
                        <div class="paper-score">${paper.score}</div>
                    </div>
                    ${paper.ai_categories && paper.ai_categories.length > 0 ? `
                        <div style="margin: 10px 0;">
                            ${paper.ai_categories.map(cat => `<span class="paper-category-badge">${cat}</span>`).join('')}
                        </div>
                    ` : ''}
                    ${paper.authors ? `<div class="paper-authors">üë• ${paper.authors}</div>` : ''}
                    <div class="paper-details">
                        ${paper.session_type ? `<div class="detail-item"><strong>Session:</strong> ${paper.session_type}</div>` : ''}
                        ${paper.session_location ? `<div class="detail-item"><strong>Location:</strong> ${paper.session_location}</div>` : ''}
                    </div>
                    <div class="paper-stats">
                        ${paper.relevant_to_users ? `<div class="stat-badge">üëç <strong>${paper.relevant_to_users}</strong> found relevant</div>` : ''}
                        ${paper.read_by_users ? `<div class="stat-badge">üìñ <strong>${paper.read_by_users}</strong> reads</div>` : ''}
                    </div>

                    <div class="paper-expandable">
                        ${paper.novelty ? `
                            <div class="paper-key-info" style="background: linear-gradient(135deg, #fff5e6 0%, #ffe6f0 100%); border-left: 3px solid #f59e0b;">
                                <strong style="color: #d97706;">üí° What's Novel:</strong>
                                <p style="margin: 8px 0; line-height: 1.5;">${paper.novelty.substring(0, 200)}${paper.novelty.length > 200 ? '...' : ''}</p>
                            </div>
                        ` : ''}
                        ${paper.key_contribution ? `
                            <div class="paper-key-info">
                                <strong style="color: #667eea;">üéØ Key Contribution:</strong>
                                <p style="margin: 8px 0; line-height: 1.5;">${paper.key_contribution.substring(0, 150)}${paper.key_contribution.length > 150 ? '...' : ''}</p>
                            </div>
                        ` : ''}
                        ${paper.key_findings ? `
                            <div class="paper-key-info">
                                <strong style="color: #667eea;">üîç Key Findings:</strong>
                                <p style="margin: 8px 0; line-height: 1.5;">${paper.key_findings.substring(0, 150)}${paper.key_findings.length > 150 ? '...' : ''}</p>
                            </div>
                        ` : ''}
                        <button onclick="event.stopPropagation(); openPaperModal(${idx}, ${JSON.stringify(pagePapers).replace(/"/g, '&quot;')})"
                                style="margin-top: 15px; padding: 8px 16px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border: none; border-radius: 6px; cursor: pointer; font-weight: 600;">
                            View Full Details
                        </button>
                    </div>

                    ${paper.pdf_url ? `<div class="paper-link"><a href="${paper.pdf_url}" target="_blank" onclick="event.stopPropagation()">üìÑ View PDF ‚Üí</a></div>` : ''}
                </div>
            `).join('');

            // Render pagination controls
            const paginationDiv = document.getElementById('papersPagination');
            if (totalPages > 1) {
                let paginationHTML = '';

                // Previous button
                paginationHTML += `<button class="pagination-btn" onclick="displayPapers('${sortBy}', ${page - 1})" ${page === 1 ? 'disabled' : ''}>‚Üê Previous</button>`;

                // Page info
                paginationHTML += `<span class="pagination-info">Page ${page} of ${totalPages} (${totalPapers} papers)</span>`;

                // Next button
                paginationHTML += `<button class="pagination-btn" onclick="displayPapers('${sortBy}', ${page + 1})" ${page === totalPages ? 'disabled' : ''}>Next ‚Üí</button>`;

                paginationDiv.innerHTML = paginationHTML;
            } else {
                paginationDiv.innerHTML = '';
            }

            // Scroll to top of papers list
            if (page > 1) {
                document.getElementById('papersList').scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }

        // Initialize on load
        document.addEventListener('DOMContentLoaded', () => {
            displayStats();
            displayChart();
            renderCategoryFilters();
            displayPapers();

            document.getElementById('sortBy').addEventListener('change', (e) => {
                displayPapers(e.target.value);
            });

            // Initialize authors display
            displayAffiliationChart();
            displayAuthors();

            // Close modal when clicking outside
            document.getElementById('paperModal').addEventListener('click', (e) => {
                if (e.target.id === 'paperModal') {
                    closePaperModal();
                }
            });
        });

        function switchTab(tabName) {
            // Hide all tabs
            document.getElementById('papersTab').classList.remove('active');
            document.getElementById('authorsTab').classList.remove('active');
            document.getElementById('synthesisTab').classList.remove('active');

            // Remove active from all tab buttons
            document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));

            // Show selected tab
            if (tabName === 'papers') {
                document.getElementById('papersTab').classList.add('active');
                document.querySelectorAll('.tab')[0].classList.add('active');
            } else if (tabName === 'authors') {
                document.getElementById('authorsTab').classList.add('active');
                document.querySelectorAll('.tab')[1].classList.add('active');
            } else if (tabName === 'synthesis') {
                document.getElementById('synthesisTab').classList.add('active');
                document.querySelectorAll('.tab')[2].classList.add('active');
            }
        }

        function displayAffiliationChart() {
            // Get authors with highly relevant papers and known affiliations
            const qualifyingAuthors = authors.filter(a =>
                a.highly_relevant_count >= 1 &&
                a.affiliation &&
                a.affiliation !== 'Unknown'
            );

            // Count affiliations
            const affiliationCounts = {};
            qualifyingAuthors.forEach(author => {
                const affiliation = author.affiliation;
                affiliationCounts[affiliation] = (affiliationCounts[affiliation] || 0) + 1;
            });

            // Sort by count and take top 15
            const sortedAffiliations = Object.entries(affiliationCounts)
                .sort((a, b) => b[1] - a[1])
                .slice(0, 15);

            const labels = sortedAffiliations.map(a => a[0]);
            const data = sortedAffiliations.map(a => a[1]);

            const ctx = document.getElementById('affiliationChart').getContext('2d');
            new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: labels,
                    datasets: [{
                        label: 'Number of Researchers',
                        data: data,
                        backgroundColor: 'rgba(102, 126, 234, 0.8)',
                        borderColor: 'rgba(102, 126, 234, 1)',
                        borderWidth: 1
                    }]
                },
                options: {
                    indexAxis: 'y',
                    responsive: true,
                    maintainAspectRatio: false,
                    plugins: {
                        legend: {
                            display: false
                        },
                        title: {
                            display: true,
                            text: `Top Institutions (${qualifyingAuthors.length} of ${authors.filter(a => a.highly_relevant_count >= 1).length} authors have known affiliations)`,
                            font: {
                                size: 13
                            },
                            color: '#666'
                        }
                    },
                    scales: {
                        x: {
                            beginAtZero: true,
                            ticks: {
                                stepSize: 1
                            },
                            title: {
                                display: true,
                                text: 'Number of Researchers'
                            }
                        }
                    }
                }
            });
        }

        function displayAuthors(page = 1) {
            currentAuthorsPage = page;

            // Filter to authors with at least 1 highly relevant paper (score >= 85)
            let sortedAuthors = authors
                .filter(a => a.highly_relevant_count >= 1)
                .sort((a, b) => {
                    // Sort by highly relevant count first, then total papers as tiebreaker
                    if (b.highly_relevant_count !== a.highly_relevant_count) {
                        return b.highly_relevant_count - a.highly_relevant_count;
                    }
                    return b.paper_count - a.paper_count;
                });

            const totalAuthors = sortedAuthors.length;
            const totalPages = Math.ceil(totalAuthors / authorsPerPage);
            const startIdx = (page - 1) * authorsPerPage;
            const endIdx = startIdx + authorsPerPage;
            const pageAuthors = sortedAuthors.slice(startIdx, endIdx);

            const authorsList = document.getElementById('authorsList');
            authorsList.innerHTML = pageAuthors.map(author => `
                <div class="author-card">
                    <div class="author-header">
                        ${author.photo_url ? `
                            <img src="${author.photo_url}" alt="${author.name}" class="author-photo" onerror="this.style.display='none'; this.nextElementSibling.style.display='flex';">
                            <div class="author-photo-placeholder" style="display: none;">${author.name.charAt(0)}</div>
                        ` : `
                            <div class="author-photo-placeholder">${author.name.charAt(0)}</div>
                        `}
                        <div class="author-info">
                            <div class="author-name">
                                ${author.name}
                                ${author.profile_url ? `<a href="${author.profile_url}" target="_blank" class="author-profile-link" onclick="event.stopPropagation()">üîó Profile</a>` : ''}
                            </div>
                            ${author.affiliation && author.affiliation !== 'Unknown' ? `
                                <div class="author-affiliation">
                                    <span class="affiliation-badge" title="Current affiliation">${author.affiliation}</span>
                                    ${author.role && author.role !== 'Unknown' ? `<span class="role-badge" title="Academic/professional role">${author.role}</span>` : ''}
                                </div>
                            ` : ''}
                            <div class="author-stats-badges">
                                <div class="author-badge" title="Total number of papers by this author that align with your interests">
                                    üìÑ <strong>${author.paper_count}</strong> total
                                </div>
                                <div class="author-badge" title="Average relevance score across all their papers (higher = better alignment with your interests)">
                                    üìä <strong>${author.avg_score}</strong> avg
                                </div>
                                ${author.total_relevant > 0 ? `<div class="author-badge" title="Total times their papers were marked relevant by other users">üëç <strong>${author.total_relevant}</strong></div>` : ''}
                                ${author.total_reads > 0 ? `<div class="author-badge" title="Total times their papers were read by other users">üìñ <strong>${author.total_reads}</strong></div>` : ''}
                            </div>
                        </div>
                    </div>
                    <div class="author-papers-list">
                        ${author.papers.map(paper => `
                            <div class="author-paper-item">
                                <div class="author-paper-score" title="Relevance score: how well this paper aligns with your research interests (higher = stronger alignment)">${paper.score}</div>
                                <div class="author-paper-title">${paper.title}</div>
                            </div>
                        `).join('')}
                    </div>
                </div>
            `).join('');

            // Render pagination controls
            const paginationDiv = document.getElementById('authorsPagination');
            if (totalPages > 1) {
                let paginationHTML = '';

                // Previous button
                paginationHTML += `<button class="pagination-btn" onclick="displayAuthors(${page - 1})" ${page === 1 ? 'disabled' : ''}>‚Üê Previous</button>`;

                // Page info
                paginationHTML += `<span class="pagination-info">Page ${page} of ${totalPages} (${totalAuthors} authors)</span>`;

                // Next button
                paginationHTML += `<button class="pagination-btn" onclick="displayAuthors(${page + 1})" ${page === totalPages ? 'disabled' : ''}>Next ‚Üí</button>`;

                paginationDiv.innerHTML = paginationHTML;
            } else {
                paginationDiv.innerHTML = '';
            }

            // Scroll to top of authors list
            if (page > 1) {
                document.getElementById('authorsList').scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
    </script>
</body>
</html>