<div style="font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; line-height: 1.8; color: #2c3e50;">
<div style="text-align: center; margin-bottom: 30px; padding: 30px; background: linear-gradient(135deg, #1c3664 0%, #0a1f44 100%); color: white; border-radius: 8px;">
<h1 style="margin: 0; color: white; font-weight: 600;">NeurIPS 2025: Agent Systems Research Synthesis</h1>
<p style="margin: 10px 0 0 0; color: #b8c5d6;">Analysis of 367 papers across 15 research areas</p>
</div>
<h1 style="color: #1c3664; font-weight: 600; margin-top: 20px;">NeurIPS 2025: A Critical Synthesis of Agent Systems, Benchmarking, and Reasoning Research</h1>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">Executive Summary</h2>

<p>NeurIPS 2025 represents a pivotal moment in AI agent research, marked by a dramatic shift from capability demonstration to critical evaluation, from monolithic models to collaborative systems, and from static benchmarks to dynamic, contamination-resistant evaluation frameworks. Across 367 papers spanning agent benchmarking, tool use, reasoning, multi-agent collaboration, and safety, several patterns emerge: <strong>test-time compute scaling is reaching theoretical and practical limits</strong>, <strong>multi-agent systems show promise but lack fundamental social intelligence</strong>, <strong>current benchmarks systematically overestimate real-world capabilities</strong>, and <strong>the safety-capability tension in autonomous systems remains fundamentally unresolved</strong>. Perhaps most critically, the conference reveals a field grappling with reproducibility crises, evaluation validity concerns, and the sobering realization that scaling alone cannot solve reasoning limitations.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">I. The Evaluation Crisis: Benchmarking Under Scrutiny</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">1.1 Contamination, Validity, and the Death of Static Benchmarks</h3>

<p>The conference mounted a comprehensive assault on traditional evaluation methodologies, with over 30 papers directly challenging benchmark validity. <strong>The contamination problem has reached crisis levels</strong>: <span class="paper-ref" data-paper-id="127" data-tooltip="MathArena: Evaluating LLMs on Uncontaminated Math Competitions">[Paper 127]</span> demonstrates that models show declining performance on newer test sets despite high scores on established benchmarks, suggesting widespread memorization rather than genuine reasoning. <span class="paper-ref" data-paper-id="218" data-tooltip="ThinkBench: Dynamic Out-of-Distribution Evaluation for Robust LLM Reasoning">[Paper 218]</span> introduces dynamic out-of-distribution generation specifically to combat this, while <span class="paper-ref" data-paper-id="72" data-tooltip="SWE-bench Goes Live!">[Paper 72]</span> launches "SWE-bench Goes Live" with continuous automated issue collection to prevent dataset staleness.</p>

<p>More fundamentally, multiple papers expose that <strong>benchmark performance dramatically overestimates real-world capabilities</strong>. <span class="paper-ref" data-paper-id="2" data-tooltip="REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites">[Paper 2]</span> shows that deterministic simulation reveals capabilities invisible in live-environment testing, while <span class="paper-ref" data-paper-id="4" data-tooltip="TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks">[Paper 4]</span> demonstrates success rates below 50% on realistic business tasks despite models achieving 80%+ on academic benchmarks. <span class="paper-ref" data-paper-id="74" data-tooltip="Measuring AI Ability to Complete Long Software Tasks">[Paper 74]</span> reveals that even advanced models take 4-10× longer to complete real software tasks than benchmark metrics suggest. The gap between "laboratory performance" and "production readiness" has never been more apparent.</p>

<p><strong>The psychometric critique</strong> introduced by <span class="paper-ref" data-paper-id="178" data-tooltip="Neither Valid nor Reliable? Investigating the Use of LLMs as Judges">[Paper 178]</span> fundamentally questions whether LLM-as-judge has construct validity, revealing that position bias, prompt sensitivity, and systematic artifacts persist despite optimization attempts. <span class="paper-ref" data-paper-id="191" data-tooltip="Measuring what Matters: Construct Validity in Large Language Model Benchmarks">[Paper 191]</span> provides the first rigorous framework for assessing construct validity in LLM benchmarks, while <span class="paper-ref" data-paper-id="94" data-tooltip="BenchmarkCards: Standardized Documentation for Large Language Model Benchmarks">[Paper 94]</span> establishes standardized documentation requirements through BenchmarkCards. These methodological papers signal a field maturing beyond raw performance chasing toward scientific rigor.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">1.2 Task-Specific Limitations and Domain Gaps</h3>

<p>Domain-specific evaluation reveals systematic blind spots. <strong>Agent systems struggle with consequential, long-horizon tasks</strong>: <span class="paper-ref" data-paper-id="4" data-tooltip="TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks">[Paper 4]</span> shows leading models fail on authentic business workflows, <span class="paper-ref" data-paper-id="16" data-tooltip="MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?">[Paper 16]</span> demonstrates near-random performance on ML research challenges, <span class="paper-ref" data-paper-id="32" data-tooltip="MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research">[Paper 32]</span> reveals struggles with open-ended research methodology, and <span class="paper-ref" data-paper-id="75" data-tooltip="MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents">[Paper 75]</span> exposes severe limitations in spatial planning for embodied tasks. The pattern is clear: <strong>models excel at isolated, short-horizon tasks with clear success criteria but collapse when facing open-ended, multi-step challenges requiring sustained reasoning and adaptation</strong>.</p>

<p><strong>Spatial and temporal reasoning remain fundamental bottlenecks</strong>: <span class="paper-ref" data-paper-id="33" data-tooltip="LTD-Bench: Evaluating Large Language Models by Letting Them Draw">[Paper 33]</span> shows models achieve only 30-60% accuracy on spatial reasoning despite strong general performance, <span class="paper-ref" data-paper-id="41" data-tooltip="OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding">[Paper 41]</span> demonstrates performance degradation as context length increases in video understanding, <span class="paper-ref" data-paper-id="138" data-tooltip="Scientists&#39; First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning">[Paper 138]</span> reveals that even state-of-the-art models fail at basic mental visualization tasks, and <span class="paper-ref" data-paper-id="262" data-tooltip="Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges">[Paper 262]</span> shows models struggling with combined spatiotemporal reasoning despite handling either dimension independently. <span class="paper-ref" data-paper-id="23" data-tooltip="LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language">[Paper 23]</span> introduces temporal constraints to planning evaluation and finds current models fundamentally unprepared for time-bounded reasoning.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">1.3 The Multi-Modal Reality Check</h3>

<p>Vision-language-action models face particularly harsh scrutiny. <span class="paper-ref" data-paper-id="59" data-tooltip="Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence">[Paper 59]</span> reveals 66.6% of failures stem from cross-domain integration (transitioning between physical and digital contexts) rather than individual domain incompetence, achieving only 6.4% on cooking tasks versus 77% human performance. <span class="paper-ref" data-paper-id="106" data-tooltip="ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models">[Paper 106]</span> exposes dramatic 35-55% performance drops on visual reasoning questions compared to text-heavy questions, while <span class="paper-ref" data-paper-id="201" data-tooltip="VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs">[Paper 201]</span> demonstrates that leading VLMs fail at fundamental nonlocal visual reasoning tasks that are trivial for humans (99.5-100% accuracy).</p>

<p>The <strong>vision-language capability tension</strong> identified by <span class="paper-ref" data-paper-id="279" data-tooltip="Caption This, Reason That: VLMs Caught in the Middle">[Paper 279]</span> reveals that architectural choices optimal for captioning degrade reasoning performance and vice versa—suggesting current unified architectures may be fundamentally limited. <span class="paper-ref" data-paper-id="280" data-tooltip="Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders">[Paper 280]</span> shows this extends to temporal understanding, where models struggle despite architectural claims of video processing capability.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">II. Test-Time Compute Scaling: Promise and Limits</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">2.1 The Scaling Paradigm and Its Discontents</h3>

<p>Over 50 papers explore test-time compute scaling, revealing both potential and fundamental limitations. <strong>The basic premise works</strong>: <span class="paper-ref" data-paper-id="10" data-tooltip="$\texttt{G1}$: Teaching LLMs to Reason on Graphs with Reinforcement Learning">[Paper 10]</span> demonstrates 3B models outperforming 72B models through RL on synthetic graph tasks, <span class="paper-ref" data-paper-id="24" data-tooltip="Agentic RL Scaling Law: Spontaneous Code Execution for Mathematical Problem Solving">[Paper 24]</span> shows spontaneous code execution emerging through pure RL, and <span class="paper-ref" data-paper-id="18" data-tooltip="Bag of Tricks for Inference-time Computation of LLM Reasoning">[Paper 18]</span> provides comprehensive empirical validation across 1,000+ experiments. <span class="paper-ref" data-paper-id="50" data-tooltip="Language Models can Self-Improve at State-Value Estimation for Better Search">[Paper 50]</span> enables models to learn value estimation without ground-truth rewards, achieving 39% success rate improvements.</p>

<p>However, <strong>critical failure modes emerge at scale</strong>. <span class="paper-ref" data-paper-id="30" data-tooltip="Large Language Models Think Too Fast To Explore Effectively">[Paper 30]</span> reveals LLMs "think too fast," making premature decisions before empowerment considerations can influence behavior—test-time compute helps only when models use extended reasoning at inference (like o1). <span class="paper-ref" data-paper-id="78" data-tooltip="The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity">[Paper 78]</span> exposes performance collapse beyond specific complexity thresholds despite available token budgets, while <span class="paper-ref" data-paper-id="168" data-tooltip="Does Thinking More Always Help? Mirage of Test-Time Scaling in Reasoning Models">[Paper 168]</span> demonstrates "overthinking" where extended reasoning actually degrades performance, with drops up to 17 percentage points.</p>

<p><strong>The efficiency crisis</strong> is stark: <span class="paper-ref" data-paper-id="170" data-tooltip="Thinkless: LLM Learns When to Think">[Paper 170]</span> shows models use 2× more tokens than needed, <span class="paper-ref" data-paper-id="149" data-tooltip="Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning">[Paper 149]</span> finds optimal CoT length varies by domain with excessive computation hurting performance, and <span class="paper-ref" data-paper-id="303" data-tooltip="Efficiently Scaling LLM Reasoning Programs with Certaindex">[Paper 303]</span> reveals models generate 4.5× more tokens than necessary due to "self-doubt." <span class="paper-ref" data-paper-id="259" data-tooltip="Donât Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models">[Paper 259]</span> cuts token usage by 47% with only 4.9% accuracy loss, demonstrating massive waste in current approaches.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">2.2 Theoretical Limits and Architectural Constraints</h3>

<p>Several papers provide theoretical grounding for observed limitations. <span class="paper-ref" data-paper-id="238" data-tooltip="Let Me Think! A Long Chain of Thought Can Be Worth Exponentially Many Short Ones">[Paper 238]</span> proves formal separations between sequential and parallel test-time compute strategies, establishing that certain reasoning tasks require long chains that cannot be efficiently substituted by ensembles of short chains. <span class="paper-ref" data-paper-id="130" data-tooltip="L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models">[Paper 130]</span> demonstrates that longer reasoning without structure doesn't help—leap-based prediction patterns that break from adjacent tokens achieve superior results.</p>

<p><strong>The "thinking vs. doing" distinction</strong> introduced by <span class="paper-ref" data-paper-id="79" data-tooltip="Thinking vs. Doing: Improving Agent Reasoning by  Scaling Test-Time Interaction">[Paper 79]</span> reveals that scaling interaction horizons (environmental feedback loops) provides orthogonal benefits to per-step reasoning compute, with curriculum-based horizon scheduling enabling models to learn when to think versus when to act. <span class="paper-ref" data-paper-id="270" data-tooltip="MindJourney: Test-Time Scaling with World Models for Spatial Reasoning">[Paper 270]</span> shows controllable video generation enables test-time spatial reasoning improvements without fine-tuning, achieving 7-8% gains by granting VLMs missing 3D reasoning capabilities through external world modeling.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">2.3 Adaptive and Efficient Scaling Strategies</h3>

<p>The field is converging on <strong>adaptive, difficulty-aware compute allocation</strong> rather than uniform scaling. <span class="paper-ref" data-paper-id="13" data-tooltip="AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks">[Paper 13]</span> introduces agents that autonomously discover optimal model-budget allocations across heterogeneous subtasks, <span class="paper-ref" data-paper-id="111" data-tooltip="AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking">[Paper 111]</span> trains models to adaptively decide when to use chain-of-thought versus direct inference, and <span class="paper-ref" data-paper-id="179" data-tooltip="Think Only When You Need with Large Hybrid-Reasoning Models">[Paper 179]</span> enables reasoning models to dynamically adjust thinking effort based on problem difficulty. <span class="paper-ref" data-paper-id="308" data-tooltip="Know What You Don&#39;t Know: Uncertainty Calibration of Process Reward Models">[Paper 308]</span> achieves 60-75% computational savings through instance-adaptive scaling using calibrated uncertainty estimates.</p>

<p><strong>Speculative and parallel approaches</strong> offer practical efficiency gains: <span class="paper-ref" data-paper-id="48" data-tooltip="Hogwild! Inference: Parallel LLM Generation via Concurrent Attention">[Paper 48]</span> enables multiple LLM instances to run in parallel with shared attention cache, <span class="paper-ref" data-paper-id="161" data-tooltip="Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation">[Paper 161]</span> breaks autoregressive constraints through learned parallelization decisions achieving 2× speedup, and <span class="paper-ref" data-paper-id="304" data-tooltip="SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning">[Paper 304]</span> operates at semantic similarity level rather than token-level for reasoning-specific speculation achieving 1.5-2.5× speedup.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">III. The Multi-Agent Reality: Capability and Coordination Challenges</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">3.1 The Social Intelligence Gap</h3>

<p>A sobering theme emerges: <strong>multi-agent systems built from capable individual LLMs systematically fail at coordination</strong>. <span class="paper-ref" data-paper-id="35" data-tooltip="Large Language Models Miss the Multi-agent Mark">[Paper 35]</span> provides the definitive diagnosis: LLMs fundamentally lack native social intelligence capabilities required for multi-agent collaboration, struggling with communication protocols, theory of mind, and cooperative behavior despite strong individual performance. <span class="paper-ref" data-paper-id="25" data-tooltip="Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia">[Paper 25]</span> shows agents exhibit 90%+ agreement rates in collaborative reasoning, limiting their ability to challenge incorrect solutions—precisely the opposite of effective collaboration.</p>

<p><strong>Communication architecture matters profoundly</strong>: <span class="paper-ref" data-paper-id="29" data-tooltip="AutoData: A Multi-Agent System for Open Web Data Collection">[Paper 29]</span> reduces costs by 77% through oriented message hypergraph communication that enables targeted multi-recipient messaging rather than broadcast, <span class="paper-ref" data-paper-id="156" data-tooltip="AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems">[Paper 156]</span> introduces decentralized evolutionary coordination eliminating centralized bottlenecks, and <span class="paper-ref" data-paper-id="19" data-tooltip="Multi-Agent Collaboration via Evolving Orchestration">[Paper 19]</span> uses reinforcement learning to train centralized orchestrators that dynamically direct collaboration, achieving simultaneous effectiveness and efficiency improvements.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">3.2 Coordination Patterns and Specialization</h3>

<p>The <strong>puppeteer-style paradigm</strong> <span class="paper-ref" data-paper-id="19" data-tooltip="Multi-Agent Collaboration via Evolving Orchestration">[Paper 19]</span> achieves superior performance with reduced computational costs by using centralized orchestrators trained via RL to dynamically direct agents, evolving toward compact, cyclic graph structures. <span class="paper-ref" data-paper-id="17" data-tooltip="AI-Researcher: Autonomous Scientific Innovation">[Paper 17]</span> demonstrates complete pipeline automation from literature review to publication through hierarchical mentor-student collaboration patterns, achieving 93.8% implementation success with Claude models.</p>

<p>However, <span class="paper-ref" data-paper-id="96" data-tooltip="Why Do Multi-Agent LLM Systems Fail?">[Paper 96]</span> reveals systematic failure modes across diverse scenarios: communication breakdowns, role confusion, premature termination, and conversation history loss plague current systems. <span class="paper-ref" data-paper-id="70" data-tooltip="AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement">[Paper 70]</span> demonstrates that safety optimization must occur at the architectural level—multi-agent systems harbor previously unexplored vulnerabilities that emerge during capability optimization.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">3.3 Practical Multi-Agent Applications</h3>

<p>Domain-specific applications show promise. <span class="paper-ref" data-paper-id="12" data-tooltip="AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems">[Paper 12]</span> establishes that agentic recommenders substantially outperform traditional methods when workflows integrate user history, candidate items, and platform-specific features. <span class="paper-ref" data-paper-id="51" data-tooltip="OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation">[Paper 51]</span> achieves 69.70% on GAIA by training domain-agnostic planners through RL on real-world feedback, demonstrating cross-domain transferability. <span class="paper-ref" data-paper-id="242" data-tooltip="Learning âPartner-Awareâ Collaborators in Multi-Party Collaboration">[Paper 242]</span> learns partner-awareness through prompt-based counterfactual construction, achieving 47% improvement on collaborative decision-making tasks.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">IV. Reasoning: Progress, Limitations, and Fundamental Questions</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">4.1 The Reinforcement Learning Revolution</h3>

<p><strong>RLVR (Reinforcement Learning with Verifiable Rewards) dominates mathematical and logical reasoning improvements</strong>. <span class="paper-ref" data-paper-id="95" data-tooltip="Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?">[Paper 95]</span> enables generalization beyond mathematics to diverse domains, achieving GPT-4o-level performance while being 12× faster. <span class="paper-ref" data-paper-id="167" data-tooltip="ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models">[Paper 167]</span> shows extended RL training (2000+ steps) genuinely expands capabilities beyond base models, achieving perfect performance on tasks where base models completely fail. <span class="paper-ref" data-paper-id="254" data-tooltip="Reinforcement Learning for Reasoning in Large Language Models with One Training Example">[Paper 254]</span> demonstrates one-shot RL can boost performance from 36% to 73.6% using just a single training example.</p>

<p>However, <strong>fundamental questions about what RL actually learns remain</strong>. <span class="paper-ref" data-paper-id="95" data-tooltip="Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?">[Paper 95]</span> challenges the assumption that RLVR develops genuinely new reasoning strategies, showing base models achieve higher pass@k at large k values, suggesting RL optimizes existing capacities rather than discovering novel reasoning patterns. <span class="paper-ref" data-paper-id="230" data-tooltip="On Reasoning Strength Planning in Large Reasoning Models">[Paper 230]</span> reveals RL activates latent reasoning by adjusting only ~4% of reasoning-critical tokens, with these adjustments transferable across model scales—suggesting reasoning capabilities already exist from pretraining.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">4.2 Search, Planning, and World Models</h3>

<p><strong>Classical AI techniques make surprising comebacks</strong>. <span class="paper-ref" data-paper-id="20" data-tooltip="Classical Planning with LLM-Generated Heuristics: Challenging the State of the Art with Python Code">[Paper 20]</span> shows LLM-generated heuristics for classical planning outperform traditional approaches, solving 373/720 test tasks versus 243 for widely-used heuristics while generating reusable domain-level knowledge. <span class="paper-ref" data-paper-id="63" data-tooltip="WALL-E: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents">[Paper 63]</span> proves world model alignment without retraining through executable code constraints achieves 16-98% higher success rates than RL-based methods like PPO.</p>

<p><strong>Model-based planning struggles with long horizons</strong>: <span class="paper-ref" data-paper-id="14" data-tooltip="Wide-Horizon Thinking and Simulation-Based Evaluation for Real-World LLM Planning with Multifaceted Constraints">[Paper 14]</span> introduces "wide-horizon thinking" for parallel constraint consideration, achieving 5-40% improvements over sequential methods but revealing that current approaches fail when plans exceed certain complexity thresholds. <span class="paper-ref" data-paper-id="251" data-tooltip="DMWM: Dual-Mind World Model with Long-Term Imagination">[Paper 251]</span> shows dual-mind architectures combining neural dynamics with logical reasoning address error accumulation in long-term imagination, achieving 120% improvement on extended horizons.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">4.3 The Self-Verification Challenge</h3>

<p>Multiple papers reveal <strong>models struggle to self-verify their reasoning</strong>. <span class="paper-ref" data-paper-id="329" data-tooltip="Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards">[Paper 329]</span> exposes "superficial self-reflection" where models trained with traditional RL can generate correct answers but cannot reliably verify outputs, achieving only 26.8% verification accuracy without explicit verification training. <span class="paper-ref" data-paper-id="113" data-tooltip="Incentivizing LLMs to Self-Verify Their Answers">[Paper 113]</span> shows unified RL training of both generation and verification achieves 83.60% accuracy, but models still lag behind external verifiers.</p>

<p><strong>The calibration problem</strong> persists: <span class="paper-ref" data-paper-id="308" data-tooltip="Know What You Don&#39;t Know: Uncertainty Calibration of Process Reward Models">[Paper 308]</span> reveals state-of-the-art process reward models systematically overestimate success probabilities, requiring quantile regression calibration to achieve reliable confidence bounds. <span class="paper-ref" data-paper-id="165" data-tooltip="Scalable Best-of-N Selection for Large Language Models via Self-Certainty">[Paper 165]</span> demonstrates lightweight probes can match medium-sized LLM monitors for safety monitoring at million-fold lower computational cost, but generalization across domains remains limited.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">V. Tool Use, Code Generation, and Executable Reasoning</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">5.1 The Code-as-Reasoning Paradigm</h3>

<p><strong>Code generation emerges as a powerful reasoning modality</strong>: <span class="paper-ref" data-paper-id="24" data-tooltip="Agentic RL Scaling Law: Spontaneous Code Execution for Mathematical Problem Solving">[Paper 24]</span> demonstrates agents can spontaneously learn to use code execution tools through pure RL without supervised examples, achieving 52.3% on challenging math benchmarks. <span class="paper-ref" data-paper-id="182" data-tooltip="Teaching Language Models to Reason with Tools">[Paper 182]</span> shows teaching LLMs to strategically integrate tools at optimal reasoning points achieves 4-8% accuracy improvements while reducing token usage by 30-50%. <span class="paper-ref" data-paper-id="43" data-tooltip="Eliciting Reasoning in Language Models with Cognitive Tools">[Paper 43]</span> reveals cognitive tools (modular reasoning operations executed by the LLM itself) increase GPT-4's performance from 32% to 53% on AIME2024.</p>

<p>However, <strong>code reasoning faces severe robustness challenges</strong>: <span class="paper-ref" data-paper-id="275" data-tooltip="CodeCrash: Exposing LLM Fragility to Misleading Natural Language in Code Reasoning">[Paper 275]</span> exposes that LLMs suffer 23.2% performance degradation when code contains misleading comments or documentation, demonstrating over-reliance on textual cues rather than genuine execution tracing. <span class="paper-ref" data-paper-id="235" data-tooltip="Rethinking Verification for LLM Code Generation: From Generation to Testing">[Paper 235]</span> challenges existing benchmarks as insufficiently rigorous, showing that limited test coverage artificially inflates performance metrics while missing subtle bugs.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">5.2 Agentic Tool Selection and Learning</h3>

<p><strong>Tool learning benefits from reinforcement over supervision</strong>: <span class="paper-ref" data-paper-id="199" data-tooltip="ToolRL: Reward is All Tool Learning Needs">[Paper 199]</span> demonstrates RL-based tool learning outperforms supervised fine-tuning by 15%, with fine-grained reward decomposition providing richer learning signals than binary rewards. <span class="paper-ref" data-paper-id="125" data-tooltip="Iterative Tool Usage Exploration for Multimodal Agents via Step-wise Preference Tuning">[Paper 125]</span> enables agents to learn effective tool-usage strategies through completely autonomous exploration without pre-collected annotations, using step-level preference optimization.</p>

<p><strong>The instruction-following gap</strong> identified by <span class="paper-ref" data-paper-id="6" data-tooltip="AGENTIF: Benchmarking Large Language Models Instruction Following Ability in Agentic Scenarios">[Paper 6]</span> shows current LLMs perform poorly at following instructions in agentic contexts (below 50% on novel constraints), particularly struggling with complex constraint structures typical of real-world agent applications. <span class="paper-ref" data-paper-id="152" data-tooltip="Generalizing Verifiable Instruction Following">[Paper 152]</span> extends this finding to show models trained on existing benchmarks cannot generalize to novel constraint types, achieving below 50% despite 80%+ performance on training distributions.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">5.3 Planning with Executable Verification</h3>

<p><strong>Executable verification enables reliable evaluation</strong>: <span class="paper-ref" data-paper-id="11" data-tooltip="Self-Challenging Language Model Agents">[Paper 11]</span> introduces Code-as-Task formulation with automatic verification, achieving 2× performance improvement through self-challenging frameworks. <span class="paper-ref" data-paper-id="63" data-tooltip="WALL-E: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents">[Paper 63]</span> shows training-free world alignment through executable code rules outperforms expensive RL fine-tuning. <span class="paper-ref" data-paper-id="256" data-tooltip="Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning">[Paper 256]</span> combines symbolic verification with interactive exploration, improving success rates by 46.2% through exploratory code generation.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">VI. Safety, Security, and Alignment: Fundamental Tensions</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">6.1 The Attack Surface Expands</h3>

<p><strong>Agent systems introduce qualitatively new vulnerabilities</strong>. <span class="paper-ref" data-paper-id="7" data-tooltip="RiOSWorld: Benchmarking the Risk of Multimodal Computer-Use Agents">[Paper 7]</span> reveals 84.93% unsafe rates across 492 risky tasks in real VM environments, with 99.2% unsafe rate for phishing websites and 89.8% for induced text attacks—demonstrating that safety alignment from dialogue scenarios doesn't transfer to autonomous computer-use environments. <span class="paper-ref" data-paper-id="8" data-tooltip="WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks">[Paper 8]</span> shows state-of-the-art web agents achieve intermediate attack success rates up to 86% despite low end-to-end goal completion (0-17%), revealing "security by incompetence" rather than robust defenses.</p>

<p><strong>Prompt injection remains effective despite defenses</strong>: <span class="paper-ref" data-paper-id="27" data-tooltip="Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools">[Paper 27]</span> introduces metadata manipulation attacks achieving 81-95% success rates that remain effective even under existing prompt-level defenses and structured tool-selection protocols. <span class="paper-ref" data-paper-id="228" data-tooltip="Memory Injection Attacks on LLM Agents via Query-Only Interaction">[Paper 228]</span> demonstrates query-only memory injection attacks achieving 98.2% injection success without privileged access, revealing critical supply-chain vulnerabilities. <span class="paper-ref" data-paper-id="124" data-tooltip="Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency">[Paper 124]</span> shows task concurrency can be exploited through word-level interleaving of harmful and benign content.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">6.2 Training-Time and Fine-Tuning Attacks</h3>

<p><strong>Fine-tuning emerges as a critical attack vector</strong>: <span class="paper-ref" data-paper-id="255" data-tooltip="Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs">[Paper 255]</span> demonstrates jailbreak attacks achieving 94.84% success rate using only 10 benign QA pairs, with 100% benign content making attacks undetectable by content moderation. <span class="paper-ref" data-paper-id="198" data-tooltip="Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler">[Paper 198]</span> introduces Bayesian data scheduling achieving 74.4% improvement in defending against harmful fine-tuning with 50%+ average improvement across diverse models.</p>

<p><strong>Backdoor attacks</strong> reveal deep vulnerabilities: <span class="paper-ref" data-paper-id="85" data-tooltip="BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks and Defenses on Large Language Models">[Paper 85]</span> provides the first comprehensive benchmark showing backdoor attacks remain feasible across various LLM architectures, with even low-success-rate backdoors significantly amplifying jailbreak vulnerabilities. <span class="paper-ref" data-paper-id="361" data-tooltip="BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization">[Paper 361]</span> demonstrates first systematic backdoor attacks on VLA models achieving near-100% success rates while maintaining clean task accuracy.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">6.3 Defense Mechanisms and Fundamental Limits</h3>

<p><strong>Promising defense directions emerge</strong>: <span class="paper-ref" data-paper-id="64" data-tooltip="Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach">[Paper 64]</span> achieves 31.6% safety improvement through strategic information acquisition using LLM-guided MCTS with only 2.7 queries on average. <span class="paper-ref" data-paper-id="108" data-tooltip="Reasoning as an Adaptive Defense for Safety">[Paper 108]</span> introduces TARS showing smaller reasoning-enabled models can be safer than larger models through adaptive test-time compute for safety evaluation. <span class="paper-ref" data-paper-id="123" data-tooltip="DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents">[Paper 123]</span> reduces attack success from 30.7% to 1.3% through dynamic privilege-based validation with memory stream isolation.</p>

<p>However, <strong>fundamental attribution and detection challenges persist</strong>: <span class="paper-ref" data-paper-id="66" data-tooltip="Predicting the Performance of Black-box Language Models with Follow-up Queries">[Paper 66]</span> shows predicting model correctness from internal activations achieves 0.95-0.96 AUROC but depends heavily on activation selection. <span class="paper-ref" data-paper-id="163" data-tooltip="Abstract Counterfactuals for Language Model Agents">[Paper 163]</span> introduces "abstract counterfactuals" recognizing token-level approaches are inadequate for agent safety evaluation. <span class="paper-ref" data-paper-id="330" data-tooltip="Best-of-N Jailbreaking">[Paper 330]</span> demonstrates Best-of-N jailbreaking achieves 89% ASR on GPT-4o by exploiting sampling randomness, revealing systematic biases that scaling defenses alone cannot address.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">VII. Memory, Context, and Knowledge Management</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">7.1 The Long-Context Challenge</h3>

<p><strong>Extended context reveals fundamental limitations</strong>: <span class="paper-ref" data-paper-id="58" data-tooltip="LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?">[Paper 58]</span> shows performance gaps persist despite expanded context windows, with degradation across lengthy passages indicating context size alone doesn't guarantee effective reasoning. <span class="paper-ref" data-paper-id="237" data-tooltip="Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning">[Paper 237]</span> demonstrates long-context capacity is foundational for reasoning ability, with models showing substantial improvements (85.04% to 88.70%) when context window is extended before reasoning fine-tuning.</p>

<p><strong>Memory architectures evolve toward structure</strong>: <span class="paper-ref" data-paper-id="166" data-tooltip="A-Mem: Agentic Memory for LLM Agents">[Paper 166]</span> introduces agentic memory systems where LLMs autonomously decide memory organization, achieving 80% improvement on multi-hop reasoning with 13× fewer tokens. <span class="paper-ref" data-paper-id="98" data-tooltip="3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model">[Paper 98]</span> uses dense 3D representations for episodic memory in embodied agents, achieving 37.6% success rate with 16.5% improvements over baselines. <span class="paper-ref" data-paper-id="195" data-tooltip="G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems">[Paper 195]</span> employs hierarchical graph architecture enabling agent-specific customization, achieving 20.89% increase in embodied action success.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">7.2 Retrieval and Context Integration</h3>

<p><strong>Retrieval-augmented approaches</strong> show mixed results: <span class="paper-ref" data-paper-id="22" data-tooltip="WebThinker: Empowering Large Reasoning Models with Deep Research Capability">[Paper 22]</span> reveals visual reasoning models achieve 60.7 percentage points improvement through world-aware planning narratives, while <span class="paper-ref" data-paper-id="88" data-tooltip="ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning">[Paper 88]</span> demonstrates search-augmented reasoning benefits from RL-learned search policies rather than heuristic triggers. <span class="paper-ref" data-paper-id="289" data-tooltip="Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers">[Paper 289]</span> achieves +7.8% exact match improvement through iterative self-incentivization where LLMs learn from search trajectories.</p>

<p>However, <span class="paper-ref" data-paper-id="335" data-tooltip="KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems">[Paper 335]</span> reveals KV-cache sharing across multi-agent contexts faces offset variance problems, with KVCOMM achieving up to 7.8× speedup through anchor-based online learning. <span class="paper-ref" data-paper-id="288" data-tooltip="LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions">[Paper 288]</span> introduces self-synthesis for long-context instructions achieving 10-13× token efficiency versus existing methods.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">VIII. Domain-Specific Applications and Transfer</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">8.1 Scientific and Technical Domains</h3>

<p><strong>Specialized domains reveal severe capability gaps</strong>: <span class="paper-ref" data-paper-id="138" data-tooltip="Scientists&#39; First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning">[Paper 138]</span> shows only 63.0% accuracy for best models on scientific reasoning versus 93% human performance, <span class="paper-ref" data-paper-id="260" data-tooltip="STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models">[Paper 260]</span> exposes that models achieve near-random accuracy on atmospheric science tasks, and <span class="paper-ref" data-paper-id="221" data-tooltip="PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models">[Paper 221]</span> demonstrates frontier models achieve only 36.9% on physics problems versus 61.9% human expert performance.</p>

<p><strong>Theorem proving and formal reasoning</strong> show promise: <span class="paper-ref" data-paper-id="176" data-tooltip="MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation">[Paper 176]</span> introduces step-wise theorem proving with multi-perspective search achieving state-of-the-art performance, while <span class="paper-ref" data-paper-id="140" data-tooltip="Logic.py: Bridging the Gap between LLMs and Constraint Solvers">[Paper 140]</span> bridges LLMs and constraint solvers through specialized DSLs achieving 91.4% accuracy on logic grid puzzles (65% absolute improvement).</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">8.2 Robotics and Embodied AI</h3>

<p><strong>Vision-language-action models face reality</strong>: <span class="paper-ref" data-paper-id="54" data-tooltip="InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction">[Paper 54]</span> achieves 35.3% success on OSWorld through modular architecture with specialized models for different stages, but <span class="paper-ref" data-paper-id="318" data-tooltip="LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for Scientific Embodied Agents">[Paper 318]</span> reveals models achieve >70% success on atomic actions with sharp degradation on long-horizon tasks and near-zero out-of-domain generalization. <span class="paper-ref" data-paper-id="284" data-tooltip="OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis">[Paper 284]</span> demonstrates unified VLM architecture for mobile manipulation achieves 97.85% decision-making success in simulation but faces substantial sim-to-real gaps.</p>

<p><strong>Spatial reasoning remains fundamental bottleneck</strong>: <span class="paper-ref" data-paper-id="187" data-tooltip="SURDS: Benchmarking Spatial Understanding and Reasoning in Driving Scenarios with Vision Language Models">[Paper 187]</span> shows models struggle dramatically with fine-grained spatial reasoning in driving scenarios despite safety-critical requirements, while <span class="paper-ref" data-paper-id="220" data-tooltip="Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models">[Paper 220]</span> introduces activation-based spatial reasoning achieving 88.5% on CV-Bench versus GPT-4o's 62.7%.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">8.3 Business and Production Systems</h3>

<p><strong>The deployment readiness gap yawns wide</strong>: <span class="paper-ref" data-paper-id="4" data-tooltip="TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks">[Paper 4]</span> shows success rates below 50% on realistic business workflows, <span class="paper-ref" data-paper-id="185" data-tooltip="Can Agent Fix Agent Issues?">[Paper 185]</span> demonstrates agents achieve only 0.67-4.67% resolution rates on agent system issues (versus 23-50% on traditional software), and <span class="paper-ref" data-paper-id="59" data-tooltip="Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence">[Paper 59]</span> reveals only 6.4% success on cooking tasks requiring physical-digital integration.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">IX. Model Efficiency, Architecture, and Training</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">9.1 The Efficiency Imperative</h3>

<p><strong>Computational waste pervades current systems</strong>: <span class="paper-ref" data-paper-id="170" data-tooltip="Thinkless: LLM Learns When to Think">[Paper 170]</span> identifies models use 2× necessary compute, <span class="paper-ref" data-paper-id="259" data-tooltip="Donât Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models">[Paper 259]</span> achieves 47% attention FLOPs reduction while improving accuracy, and <span class="paper-ref" data-paper-id="303" data-tooltip="Efficiently Scaling LLM Reasoning Programs with Certaindex">[Paper 303]</span> reveals 4.5× unnecessary token generation. <span class="paper-ref" data-paper-id="367" data-tooltip="Transcending Cost-Quality Tradeoff in Agent Serving via Session-Awareness">[Paper 367]</span> reduces costs by 83.5% through session-aware serving, while <span class="paper-ref" data-paper-id="172" data-tooltip="Fast Inference for Augmented Large Language Models">[Paper 172]</span> achieves 10× inference speedup through token-level routing between small and large models.</p>

<p><strong>Speculative approaches</strong> offer practical gains: <span class="paper-ref" data-paper-id="315" data-tooltip="EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test">[Paper 315]</span> achieves 6.5× speedup through training-time test enabling multi-step prediction alignment, <span class="paper-ref" data-paper-id="216" data-tooltip="SpecEM: Training-Free LLM Ensembling via Iterative Drafting, Verification, and Online Feedback">[Paper 216]</span> provides 15-40% sequence reduction through online compression-based tokenization, and <span class="paper-ref" data-paper-id="283" data-tooltip="zip2zip: Inference-Time Adaptive Tokenization via Online Compression">[Paper 283]</span> reduces token consumption by 48-70% in specialized domains through dynamic vocabulary adaptation.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">9.2 Scaling Laws and Predictability</h3>

<p><strong>Scaling behavior proves more nuanced than expected</strong>: <span class="paper-ref" data-paper-id="155" data-tooltip="Sloth: scaling laws for LLM skills to predict multi-benchmark performance across families">[Paper 155]</span> introduces latent skills framework showing reasoning scales primarily with model size while knowledge depends heavily on both size and training tokens. <span class="paper-ref" data-paper-id="281" data-tooltip="Predictable Scale (Part II) --- Farseer: A Refined Scaling Law in LLMs">[Paper 281]</span> provides refined scaling laws improving extrapolation accuracy by 433%, predicting optimal data-to-parameter ratios increase with compute budgets (contradicting Chinchilla's static recommendations).</p>

<p><strong>Architecture and training insights</strong>: <span class="paper-ref" data-paper-id="82" data-tooltip="Language Modeling by Language Models">[Paper 82]</span> demonstrates LLMs can autonomously discover competitive architectures through genetic programming and adversarial self-play, while <span class="paper-ref" data-paper-id="146" data-tooltip="Revolutionizing Training-Free NAS: Towards Efficient Automatic Proxy Discovery via Large Language Models">[Paper 146]</span> shows training-free NAS via LLM-guided discovery outperforms manually-designed proxies. <span class="paper-ref" data-paper-id="325" data-tooltip="Zero-Shot Performance Prediction for Probabilistic Scaling Laws">[Paper 325]</span> enables zero-shot learning curve prediction using multi-output Gaussian Processes, reducing scaling law determination costs substantially.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">9.3 Training Dynamics and Data Quality</h3>

<p><strong>Data quality trumps quantity</strong>: <span class="paper-ref" data-paper-id="269" data-tooltip="ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking">[Paper 269]</span> reduces performance gap to <2% while saving 70-90% annotation costs through human-LLM collaboration, <span class="paper-ref" data-paper-id="340" data-tooltip="SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning">[Paper 340]</span> reveals weakness-driven problem synthesis achieves 10% (7B) and 7.7% (32B) improvements through targeted self-improvement, and <span class="paper-ref" data-paper-id="297" data-tooltip="GRIP: A Graph-Based Reasoning Instruction Producer">[Paper 297]</span> generates 2.1M instructions from 7.5K seeds through graph-based systematic exploration.</p>

<p><strong>Training stability and optimization</strong> receive attention: <span class="paper-ref" data-paper-id="91" data-tooltip="DAPO: An Open-Source LLM Reinforcement Learning System at Scale">[Paper 91]</span> introduces decoupled clipping bounds and dynamic sampling addressing entropy collapse in large-scale RL, while <span class="paper-ref" data-paper-id="150" data-tooltip="AREAL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning">[Paper 150]</span> demonstrates asynchronous RL eliminates straggler delays achieving 2.77× speedup. <span class="paper-ref" data-paper-id="320" data-tooltip="Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning">[Paper 320]</span> reveals standard cross-entropy fine-tuning misaligns with test-time compute scaling, requiring direct coverage optimization.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">X. Emerging Directions and Future Challenges</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">10.1 Reasoning About Reasoning</h3>

<p><strong>Meta-level reasoning capabilities emerge</strong>: <span class="paper-ref" data-paper-id="21" data-tooltip="ReMA: Learning to Meta-Think for LLMs with Multi-agent Reinforcement Learning">[Paper 21]</span> introduces hierarchical decomposition with strategic meta-thinking agents showing improved generalization through learned collaboration. <span class="paper-ref" data-paper-id="328" data-tooltip="AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling">[Paper 328]</span> achieves 82.43% accuracy on Theory of Mind benchmarks through automated agent modeling, substantially outperforming reasoning models. <span class="paper-ref" data-paper-id="230" data-tooltip="On Reasoning Strength Planning in Large Reasoning Models">[Paper 230]</span> reveals models pre-plan reasoning strengths in activations, with directional vectors causally controlling reasoning allocation.</p>

<p><strong>Self-improvement without supervision</strong> shows promise: <span class="paper-ref" data-paper-id="224" data-tooltip="Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization">[Paper 224]</span> minimizes entropy in latent semantic space achieving 48.1% accuracy on MATH (from 30.7%) without labeled data, <span class="paper-ref" data-paper-id="231" data-tooltip="Absolute Zero: Reinforced Self-play Reasoning with Zero Data">[Paper 231]</span> achieves 50.4% combined code+math performance through self-play without expert examples, and <span class="paper-ref" data-paper-id="225" data-tooltip="Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning">[Paper 225]</span> uses trajectory consistency patterns for fully self-supervised learning.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">10.2 Multimodal Integration and Grounding</h3>

<p><strong>Vision-language reasoning</strong> advances slowly: <span class="paper-ref" data-paper-id="287" data-tooltip="MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM">[Paper 287]</span> introduces first metric for reasoning-induced hallucinations versus perception errors, revealing spatial hallucinations remain unaffected by scaling. <span class="paper-ref" data-paper-id="309" data-tooltip="VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception">[Paper 309]</span> demonstrates visual test-time scaling through iterative perception achieves 5%+ improvements, while <span class="paper-ref" data-paper-id="331" data-tooltip="Multi-step Visual Reasoning with Visual Tokens Scaling and Verification">[Paper 331]</span> enables models to dynamically generate visual information during reasoning achieving 6.9% improvements on compositional tasks.</p>

<p><strong>Grounding challenges persist</strong>: <span class="paper-ref" data-paper-id="301" data-tooltip="ESCA: Contextualizing Embodied Agents via Scene-Graph Generation">[Paper 301]</span> achieves spatial-temporal scene understanding through graph generation but requires 87K+ training videos. <span class="paper-ref" data-paper-id="341" data-tooltip="ChatVLA-2: Vision-Language-Action Model with Open-World Reasoning">[Paper 341]</span> preserves VLM capabilities during robotics fine-tuning achieving 82.7% on open-world reasoning versus 0% for baselines, demonstrating path toward genuine transfer.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">10.3 Unresolved Fundamental Questions</h3>

<p><strong>What is reasoning?</strong> <span class="paper-ref" data-paper-id="78" data-tooltip="The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity">[Paper 78]</span> exposes performance collapse beyond complexity thresholds suggesting fundamental computational limits. <span class="paper-ref" data-paper-id="365" data-tooltip="Thoughts Are All Over the Place: On the Underthinking of Long Reasoning Models">[Paper 365]</span> reveals longer reasoning chains can be exponentially better than ensembles for certain task structures (formal proof via computational complexity theory). <span class="paper-ref" data-paper-id="101" data-tooltip="Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study">[Paper 101]</span> challenges whether LLMs possess "general learning ability" versus task-specific memorization.</p>

<p><strong>The generalization puzzle</strong>: <span class="paper-ref" data-paper-id="141" data-tooltip="OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization">[Paper 141]</span> disaggregates mathematical generalization into exploratory, compositional, and transformative types, revealing brittleness across all three dimensions. <span class="paper-ref" data-paper-id="52" data-tooltip="TimE: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios">[Paper 52]</span> introduces time-based evaluation showing temporal retrieval correlates with all higher-level temporal reasoning capabilities. <span class="paper-ref" data-paper-id="236" data-tooltip="On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study">[Paper 236]</span> demonstrates training-free methods severely underperform on research-level sequential reasoning.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">XI. Critical Assessment and Future Directions</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">11.1 The Field's Maturation Crisis</h3>

<p>The conference reveals a field in transition from <strong>capability demonstration to scientific rigor</strong>. The proliferation of benchmarks (30+ papers), systematic evaluation critiques [Papers 34, 94, 178, 191], and reproducibility concerns [Papers 28, 72, 233] signal growing pains. <strong>Publication incentives misalign with progress</strong>: researchers chase benchmark metrics while real-world deployment remains distant [Papers 4, 74, 185].</p>

<p><strong>The contamination problem</strong> threatens scientific validity. With models trained on web-scale data inevitably including benchmark solutions, the field faces a fundamental measurement challenge. Dynamic benchmarks [Papers 72, 127, 218] and contamination-resistant evaluation [Papers 148, 212] offer partial solutions, but <strong>the cat-and-mouse game between dataset creation and model training continues</strong>.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">11.2 What Actually Works?</h3>

<p><strong>High-confidence findings from convergent evidence</strong>:</p>

<p>1. <strong>Reinforcement learning with verifiable rewards consistently improves reasoning</strong> across mathematics, coding, and logic [Papers 10, 24, 95, 167, 254] - but primarily activates latent capabilities rather than teaching new strategies [Papers 95, 230].</p>

<p>2. <strong>Test-time compute scaling helps but faces diminishing returns</strong> beyond problem-specific thresholds [Papers 78, 168, 170] - adaptive allocation based on difficulty is critical [Papers 13, 111, 179, 308].</p>

<p>3. <strong>Multi-agent coordination fails systematically due to social intelligence gaps</strong> [Papers 35, 96] - specialized communication architectures [Papers 29, 156] and trained orchestration <span class="paper-ref" data-paper-id="19" data-tooltip="Multi-Agent Collaboration via Evolving Orchestration">[Paper 19]</span> partially address this.</p>

<p>4. <strong>Current benchmarks systematically overestimate capabilities</strong> [Papers 2, 4, 7, 74] - contamination, distribution shift, and task simplification create false confidence.</p>

<p>5. <strong>Safety alignment doesn't transfer to agentic contexts</strong> [Papers 7, 8, 228] - new paradigms required for autonomous systems with tool access and environmental interaction.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">11.3 What Remains Uncertain?</h3>

<p><strong>Unresolved tensions</strong>:</p>

<p>- <strong>Does extended thinking actually improve reasoning or just redistribute errors?</strong> [Papers 78, 168, 365] provide contradictory evidence depending on task structure.</p>

<p>- <strong>Can multi-agent systems overcome social intelligence limitations through architecture alone</strong>, or do fundamental training paradigm shifts [Papers 35, 242] require new approaches?</p>

<p>- <strong>Will adaptive test-time compute</strong> [Papers 111, 179, 308] solve efficiency problems or merely shift waste to meta-decision overhead?</p>

<p>- <strong>Can evaluation keep pace with capability growth</strong>, or will contamination and benchmark gaming [Papers 127, 178, 218] permanently undermine progress measurement?</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">11.4 Overhyped Approaches</h3>

<p><strong>Critical skepticism warranted for</strong>:</p>

<p>1. <strong>Chain-of-thought as universal solution</strong> - [Papers 78, 168, 358] show it can hurt performance; task-appropriate reasoning matters more than uniform verbosity.</p>

<p>2. <strong>Multi-agent systems without coordination mechanisms</strong> - [Papers 35, 96] demonstrate systematic failures; throwing more agents at problems without addressing social intelligence gaps wastes resources.</p>

<p>3. <strong>Benchmark-driven development without deployment validation</strong> - [Papers 4, 74, 185] reveal massive capability gaps between benchmark and production performance.</p>

<p>4. <strong>Vision-language models as reasoning systems</strong> - [Papers 201, 279, 287] expose fundamental limitations in spatial reasoning and modality integration that architectural unification hasn't solved.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">XII. Practical Recommendations</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">12.1 For Researchers</h3>

<p><strong>Prioritize</strong>:<br>- Contamination-resistant evaluation using continuous benchmarks [Papers 72, 218] and out-of-distribution testing [Papers 127, 141]<br>- Task-appropriate reasoning strategies rather than uniform CoT application [Papers 111, 179, 358]<br>- Systematic ablation studies isolating architectural choices from training effects [Papers 3, 350]<br>- Multi-dimensional evaluation capturing generalization breadth [Papers 52, 141, 148]</p>

<p><strong>Avoid</strong>:<br>- Static benchmarks without contamination monitoring<br>- Claiming reasoning improvements without verifying latent capability activation [Papers 95, 230]<br>- Multi-agent systems without explicit coordination mechanisms [Papers 35, 156]<br>- Aggregate performance metrics obscuring critical failure modes [Papers 78, 287]</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">12.2 For Practitioners</h3>

<p><strong>Deploy with confidence</strong>:<br>- Reinforcement learning for domains with verifiable rewards [Papers 10, 24, 95, 167]<br>- Adaptive test-time compute with difficulty-aware allocation [Papers 13, 179, 308]<br>- Structured memory architectures for long-context applications [Papers 98, 166, 195]<br>- Session-aware serving for cost-efficient agent deployment <span class="paper-ref" data-paper-id="367" data-tooltip="Transcending Cost-Quality Tradeoff in Agent Serving via Session-Awareness">[Paper 367]</span></p>

<p><strong>Approach cautiously</strong>:<br>- Multi-agent systems without proven coordination (expect systematic failures [Papers 35, 96])<br>- Vision-language agents for spatial reasoning tasks (fundamental gaps remain [Papers 33, 201, 220])<br>- Autonomous agents with tool access in safety-critical contexts (alignment gaps persist [Papers 7, 8])<br>- Long-horizon planning without intermediate verification (error accumulation inevitable [Papers 14, 251])</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">12.3 For the Field</h3>

<p><strong>The path forward requires</strong>:</p>

<p>1. <strong>Evaluation reform</strong> - Abandon static benchmarks; embrace continuous evaluation <span class="paper-ref" data-paper-id="72" data-tooltip="SWE-bench Goes Live!">[Paper 72]</span>, contamination-resistant generation [Papers 127, 218], and multi-dimensional metrics [Papers 52, 141].</p>

<p>2. <strong>Architectural innovation beyond scaling</strong> - Papers demonstrate fundamental limits to test-time compute [Papers 78, 168], suggesting qualitative architectural changes needed rather than quantitative scaling.</p>

<p>3. <strong>Social intelligence as first-class problem</strong> - Multi-agent success requires addressing coordination fundamentally [Papers 35, 156, 242], not treating it as emergent from capability scaling.</p>

<p>4. <strong>Safety paradigms for agentic systems</strong> - Dialogue safety doesn't transfer [Papers 7, 8]; new frameworks needed for autonomous tool-using agents [Papers 123, 228, 264].</p>

<p>5. <strong>Bridging simulation and reality</strong> - The deployment gap [Papers 4, 59, 74] demands systematic sim-to-real transfer validation rather than assuming benchmark performance generalizes.</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">Conclusion</h2>

<p>NeurIPS 2025 marks a <strong>critical inflection point</strong>: the field confronts uncomfortable truths about evaluation validity, capability generalization, and safety alignment while making genuine progress on reasoning, efficiency, and specialized applications. The conference reveals both <strong>extraordinary capability emergence</strong> (self-play learning <span class="paper-ref" data-paper-id="231" data-tooltip="Absolute Zero: Reinforced Self-play Reasoning with Zero Data">[Paper 231]</span>, spontaneous tool use <span class="paper-ref" data-paper-id="24" data-tooltip="Agentic RL Scaling Law: Spontaneous Code Execution for Mathematical Problem Solving">[Paper 24]</span>, autonomous architecture discovery <span class="paper-ref" data-paper-id="82" data-tooltip="Language Modeling by Language Models">[Paper 82]</span>) and <strong>sobering limitations</strong> (social intelligence gaps <span class="paper-ref" data-paper-id="35" data-tooltip="Large Language Models Miss the Multi-agent Mark">[Paper 35]</span>, spatial reasoning failures [Papers 201, 220], deployment readiness gaps [Papers 4, 74]).</p>

<p>The next phase demands <strong>scientific rigor over capability claims</strong>, <strong>deployment validation over benchmark optimization</strong>, and <strong>fundamental innovation over incremental scaling</strong>. Researchers equipped with insights from these 367 papers have unprecedented understanding of both what works and what remains fundamentally unsolved—the critical question is whether incentive structures will enable the field to address root causes rather than pursuing incremental improvements on saturated benchmarks.</p>

<p>The research community stands at a crossroads: continue the benchmark-capability arms race or pivot toward generalizable, deployable, safe agent systems. The evidence from NeurIPS 2025 suggests only the latter path leads to genuine progress.</p>

<details style="margin-top: 40px; padding: 20px; background: #f8f9fa; border-radius: 8px;">
<summary style="cursor: pointer; font-weight: bold; font-size: 1.1em; color: #1c3664;">📚 Paper Reference Index (367 papers)</summary>
<div style="margin-top: 20px;">
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 1]</strong> Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents<br><small style="color: #666;">Score: 95 | Agent Benchmarking and Evaluation, Web and Computer-Use Agents, Vision-Language-Action Models</small> <a href="https://arxiv.org/pdf/2505.24878" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 2]</strong> REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites<br><small style="color: #666;">Score: 93 | Agent Benchmarking and Evaluation, Web and Computer-Use Agents</small> <a href="https://arxiv.org/pdf/2504.11543" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 3]</strong> How to Train Your LLM Web Agent: A Statistical Diagnosis<br><small style="color: #666;">Score: 92 | Agent Benchmarking and Evaluation, Web and Computer-Use Agents, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/0252f1f5697c0153740a7438e473e45964e85102.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 4]</strong> TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks<br><small style="color: #666;">Score: 92 | Agent Benchmarking and Evaluation, Web and Computer-Use Agents, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2412.14161" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 5]</strong> Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge<br><small style="color: #666;">Score: 92 | Agent Benchmarking and Evaluation, Web and Computer-Use Agents, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2506.21506" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 6]</strong> AGENTIF: Benchmarking Large Language Models Instruction Following Ability in Agentic Scenarios<br><small style="color: #666;">Score: 92 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 7]</strong> RiOSWorld: Benchmarking the Risk of Multimodal Computer-Use Agents<br><small style="color: #666;">Score: 91 | Agent Safety and Security, Agent Benchmarking and Evaluation, Web and Computer-Use Agents</small> <a href="https://openreview.net/pdf/11ac207fff4f7439cb345f2e79ef2a6fb5611910.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 8]</strong> WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks<br><small style="color: #666;">Score: 91 | Agent Safety and Security, Web and Computer-Use Agents, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2504.18575" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 9]</strong> T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning<br><small style="color: #666;">Score: 90 | Tool Use and Code Generation, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2505.16986" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 10]</strong> $\texttt{G1}$: Teaching LLMs to Reason on Graphs with Reinforcement Learning<br><small style="color: #666;">Score: 89 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/eb95dfb11f1b0c705f2e368d650f0af1afbef02b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 11]</strong> Self-Challenging Language Model Agents<br><small style="color: #666;">Score: 89 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/d7856a8a4e991dab71814a3f39f28fe17b6b93bd.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 12]</strong> AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems<br><small style="color: #666;">Score: 89 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Domain-Specific Agents and Applications</small> <a href="https://arxiv.org/pdf/2505.19623" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 13]</strong> AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks<br><small style="color: #666;">Score: 89 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/13ea5abf7135c2269571122e956e24009565de1f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 14]</strong> Wide-Horizon Thinking and Simulation-Based Evaluation for Real-World LLM Planning with Multifaceted Constraints<br><small style="color: #666;">Score: 88 | Planning and Decision Making, Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/4fe14c361104913e172e118fa05d358d3b030840.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 15]</strong> General-Reasoner: Advancing LLM Reasoning Across All Domains<br><small style="color: #666;">Score: 87 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/59ca2ebad16c9f4c1d622f0bcd9007d31e1874bc.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 16]</strong> MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?<br><small style="color: #666;">Score: 87 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2504.09702" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 17]</strong> AI-Researcher: Autonomous Scientific Innovation<br><small style="color: #666;">Score: 86 | Multi-Agent Systems and Collaboration, Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/a1c63cdd0495de94664b1513f7d95a3aedcb483a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 18]</strong> Bag of Tricks for Inference-time Computation of LLM Reasoning<br><small style="color: #666;">Score: 86 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2502.07191" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 19]</strong> Multi-Agent Collaboration via Evolving Orchestration<br><small style="color: #666;">Score: 85 | Multi-Agent Systems and Collaboration, Reinforcement Learning for LLMs, Planning and Decision Making</small> <a href="https://openreview.net/pdf/9727f658d788c52f49f12ae4b230baf4cf0d4007.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 20]</strong> Classical Planning with LLM-Generated Heuristics: Challenging the State of the Art with Python Code<br><small style="color: #666;">Score: 85 | Tool Use and Code Generation, World Models and Planning</small> <a href="https://openreview.net/pdf/924624fad0f2d9ea4637686b275593407c20b753.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 21]</strong> ReMA: Learning to Meta-Think for LLMs with Multi-agent Reinforcement Learning<br><small style="color: #666;">Score: 84 | Multi-Agent Systems and Collaboration, Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/4ef88667f9d6e59455e6805cd91df2e8e519f9d0.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 22]</strong> WebThinker: Empowering Large Reasoning Models with Deep Research Capability<br><small style="color: #666;">Score: 84 | Web and Computer-Use Agents, Reasoning and Test-Time Compute, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/4e3ecfb1260a3ed3a6f051cf994b3be14a8f904e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 23]</strong> LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language<br><small style="color: #666;">Score: 84 | Agent Benchmarking and Evaluation, World Models and Planning, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2510.05972" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 24]</strong> Agentic RL Scaling Law: Spontaneous Code Execution for Mathematical Problem Solving<br><small style="color: #666;">Score: 83 | Tool Use and Code Generation, Reasoning and Test-Time Compute, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/6d8a6cfbc785966d9381495a786eea81de48d681.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 25]</strong> Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia<br><small style="color: #666;">Score: 83 | Agent Benchmarking and Evaluation, Multi-Agent Systems and Collaboration</small> <a href="https://arxiv.org/pdf/2512.03318" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 26]</strong> Agentic Plan Caching: Test-Time Memory for Fast and Cost-Efficient LLM Agents<br><small style="color: #666;">Score: 83 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/2362f2d245503d9d209dde5a45fbfbf3fb6a5990.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 27]</strong> Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools<br><small style="color: #666;">Score: 82 | Agent Safety and Security, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/58fd047ac6ca0bee4f61d85fc98df7a5c5b55e31.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 28]</strong> The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements<br><small style="color: #666;">Score: 82 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2506.22419" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 29]</strong> AutoData: A Multi-Agent System for Open Web Data Collection<br><small style="color: #666;">Score: 81 | Multi-Agent Systems and Collaboration, Tool Use and Code Generation, Web and Computer-Use Agents</small> <a href="https://openreview.net/pdf/066a04337fe56fee38b21d8e6b2366fac00856f4.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 30]</strong> Large Language Models Think Too Fast To Explore Effectively<br><small style="color: #666;">Score: 81 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation, Planning and Decision Making</small> <a href="https://openreview.net/pdf/84905da6e0006182e87623ee1b532443c3ca840a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 31]</strong> WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch<br><small style="color: #666;">Score: 81 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Web and Computer-Use Agents</small> <a href="https://arxiv.org/pdf/2505.03733" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 32]</strong> MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research<br><small style="color: #666;">Score: 81 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2505.19955" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 33]</strong> LTD-Bench: Evaluating Large Language Models by Letting Them Draw<br><small style="color: #666;">Score: 81 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2511.02347" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 34]</strong> Establishing Best Practices in Building Rigorous Agentic Benchmarks<br><small style="color: #666;">Score: 81 | Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2507.02825" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 35]</strong> Large Language Models Miss the Multi-agent Mark<br><small style="color: #666;">Score: 80 | Multi-Agent Systems and Collaboration, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2505.21298" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 36]</strong> Factorio Learning Environment<br><small style="color: #666;">Score: 80 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Planning and Decision Making</small> <a href="https://arxiv.org/pdf/2503.09617" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 37]</strong> Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition<br><small style="color: #666;">Score: 80 | Agent Safety and Security, Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2507.20526" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 38]</strong> PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors<br><small style="color: #666;">Score: 79 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2507.15550" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 39]</strong> WebDancer: Towards Autonomous Information Seeking Agency<br><small style="color: #666;">Score: 79 | Web and Computer-Use Agents, Agent Benchmarking and Evaluation, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/7c886fbc63b09377d123254d93907b41820d72d7.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 40]</strong> LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models<br><small style="color: #666;">Score: 79 | Reinforcement Learning for LLMs, Planning and Decision Making, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/64ba671c860d8032b21824e28d00916c3e613964.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 41]</strong> OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding<br><small style="color: #666;">Score: 78 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2507.07984" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 42]</strong> Enigmata:  Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles<br><small style="color: #666;">Score: 78 | Reasoning and Test-Time Compute, Mathematical and Logical Reasoning, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/d16d55cfeff749792ae0b093a3f4c0123aa6c09f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 43]</strong> Eliciting Reasoning in Language Models with Cognitive Tools<br><small style="color: #666;">Score: 77 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/ea576c6461243d27a8102c9dfa554db950d66757.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 44]</strong> MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem<br><small style="color: #666;">Score: 77 | Tool Use and Code Generation, Mathematical and Logical Reasoning, Planning and Decision Making</small> <a href="https://openreview.net/pdf/e01a39be0de98c2f808394f7affa2bfb004c68a5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 45]</strong> AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents<br><small style="color: #666;">Score: 77 | Agent Safety and Security, Agent Benchmarking and Evaluation, Web and Computer-Use Agents</small> <a href="https://arxiv.org/pdf/2503.09780" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 46]</strong> APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay<br><small style="color: #666;">Score: 77 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2504.03601" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 47]</strong> SimWorld: An Open-ended Simulator for Agents in Physical and Social Worlds<br><small style="color: #666;">Score: 77 | Agent Benchmarking and Evaluation, Multi-Agent Systems and Collaboration, Planning and Decision Making</small> <a href="https://openreview.net/pdf/a98dee23d8b37552151336bbac20c838fd5e9ee1.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 48]</strong> Hogwild! Inference: Parallel LLM Generation via Concurrent Attention<br><small style="color: #666;">Score: 76 | Reasoning and Test-Time Compute, Multi-Agent Systems and Collaboration</small> <a href="https://openreview.net/pdf/752690b760cb01f226ba630228b97af81df4d1d1.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 49]</strong> Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective<br><small style="color: #666;">Score: 76 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2506.14965" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 50]</strong> Language Models can Self-Improve at State-Value Estimation for Better Search<br><small style="color: #666;">Score: 76 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation, Web and Computer-Use Agents</small> <a href="https://openreview.net/pdf/ea251516f78a77cbc42dd4a50e2ce4aadfc2226f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 51]</strong> OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation<br><small style="color: #666;">Score: 75 | Multi-Agent Systems and Collaboration, Reinforcement Learning for LLMs, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/9ea2d3d5cf7f874c7669ab5c3f1270eb3bc794d1.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 52]</strong> TimE: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios<br><small style="color: #666;">Score: 75 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2505.12891" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 53]</strong> Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL<br><small style="color: #666;">Score: 74 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/a27fd29bd9a99f5624d5362501cf5a6acc4c4e37.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 54]</strong> InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction<br><small style="color: #666;">Score: 74 | Web and Computer-Use Agents, Tool Use and Code Generation, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/ae7bb751a0d72057590b5907d9d4fa86c412be50.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 55]</strong> AgentAuditor: Human-level Safety and Security Evaluation for LLM Agents<br><small style="color: #666;">Score: 74 | Agent Safety and Security, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/2ceecf4bf50bffa7cff2d145fadb18c23daf7206.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 56]</strong> Collaborative Reasoner: Self-Improving Social Agents with Synthetic Conversations<br><small style="color: #666;">Score: 74 | Multi-Agent Systems and Collaboration, Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/42a338bca40ea896002753679729eb2240bf62b3.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 57]</strong> LogicTree: Improving Complex Reasoning of LLMs via Instantiated Multi-step Synthetic Logical Data<br><small style="color: #666;">Score: 73 | Reasoning and Test-Time Compute, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/30059572044fffae201f1e0daf92fc78a5aef218.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 58]</strong> LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?<br><small style="color: #666;">Score: 73 | Agent Benchmarking and Evaluation, Memory and Context Management</small> <a href="https://arxiv.org/pdf/2510.22548" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 59]</strong> Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence<br><small style="color: #666;">Score: 71 | Web and Computer-Use Agents, Agent Benchmarking and Evaluation, Vision-Language-Action Models</small> <a href="https://arxiv.org/pdf/2506.15677" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 60]</strong> ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code<br><small style="color: #666;">Score: 71 | Tool Use and Code Generation, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2506.02314" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 61]</strong> ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests<br><small style="color: #666;">Score: 71 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2506.04894" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 62]</strong> Wider or Deeper?  Scaling LLM Inference-Time Compute with Adaptive Branching Tree Search<br><small style="color: #666;">Score: 70 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/e6d2b8dcb02f4dadb940b53aefa65032db4fbd89.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 63]</strong> WALL-E: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents<br><small style="color: #666;">Score: 70 | World Models and Planning, Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/4a70c0605cac16a41fee061564452113f886243a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 64]</strong> Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach<br><small style="color: #666;">Score: 70 | Agent Safety and Security, Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/c98e7220e280f1d8bae43d92944840467237a1e7.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 65]</strong> Rollout Roulette: A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods<br><small style="color: #666;">Score: 70 | Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/7ee28149679cef6235a635039365773e8ff846ed.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 66]</strong> Predicting the Performance of Black-box Language Models with Follow-up Queries<br><small style="color: #666;">Score: 70 | Agent Benchmarking and Evaluation, Agent Safety and Security</small> <a href="https://openreview.net/pdf/a9530414a4fd5f326629f3402e2729d04dac678a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 67]</strong> RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning<br><small style="color: #666;">Score: 69 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/df2d115dd7fe7763e94fd35d45007ed71cbcebc7.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 68]</strong> RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for  Complex Task Solving<br><small style="color: #666;">Score: 69 | Tool Use and Code Generation, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/fea6d1106531a6d10e824159b158dcbffbc07cc2.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 69]</strong> SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks<br><small style="color: #666;">Score: 69 | Agent Benchmarking and Evaluation, Agent Safety and Security, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/66d2a1a241b7979f0f8776b51e62c6b1f1bc7db8.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 70]</strong> AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement<br><small style="color: #666;">Score: 69 | Multi-Agent Systems and Collaboration, Agent Safety and Security, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/237ca0099a7a95299cd0bae4b495038b19873e08.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 71]</strong> AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play<br><small style="color: #666;">Score: 69 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/d73f4b0dc08ba840cfc15d8e7e9be6ad4a9b52ce.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 72]</strong> SWE-bench Goes Live!<br><small style="color: #666;">Score: 68 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2505.23419" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 73]</strong> MM-OPERA: Benchmarking Open-ended Association Reasoning for Large Vision-Language Models<br><small style="color: #666;">Score: 68 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2510.26937" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 74]</strong> Measuring AI Ability to Complete Long Software Tasks<br><small style="color: #666;">Score: 68 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/402f09e70f9d9e6a99edbcaaa360692d05eec7ab.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 75]</strong> MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents<br><small style="color: #666;">Score: 68 | Agent Benchmarking and Evaluation, Spatial and Physical Reasoning, Planning and Decision Making</small> <a href="https://arxiv.org/pdf/2505.20148" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 76]</strong> CoP: Agentic Red-teaming for Large Language Models using Composition of Principles<br><small style="color: #666;">Score: 68 | Agent Safety and Security, Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/cb30447780175b5ce7f25d0e0277ddcc32156544.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 77]</strong> Meta-World+: An Improved, Standardized, RL Benchmark<br><small style="color: #666;">Score: 67 | Agent Benchmarking and Evaluation, Reinforcement Learning for LLMs</small> <a href="https://arxiv.org/pdf/2505.11289" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 78]</strong> The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity<br><small style="color: #666;">Score: 66 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/b58069a804c5fad686ceb13a131631201748c264.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 79]</strong> Thinking vs. Doing: Improving Agent Reasoning by  Scaling Test-Time Interaction<br><small style="color: #666;">Score: 66 | Reasoning and Test-Time Compute, Web and Computer-Use Agents, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/20cba89765ebda48a0c183f17ceb68d13f90d0dc.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 80]</strong> Can Large Language Models Master Complex Card Games?<br><small style="color: #666;">Score: 66 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Domain-Specific Agents and Applications</small> <a href="https://openreview.net/pdf/1d7991c179addeedf33ce77dc2ffd0e475f69b2c.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 81]</strong> Benchmarking Large Language Models with Integer Sequence Generation Tasks<br><small style="color: #666;">Score: 65 | Agent Benchmarking and Evaluation, Mathematical and Logical Reasoning</small> <a href="https://arxiv.org/pdf/2411.04372" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 82]</strong> Language Modeling by Language Models<br><small style="color: #666;">Score: 65 | Self-Improvement and Meta-Learning, Multi-Agent Systems and Collaboration</small> <a href="https://openreview.net/pdf/1dda401bc2d6f8ee17a263ac3f358eb51e094d8e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 83]</strong> Let the LLM Stick to Its Strengths: Learning to Route Economical LLM<br><small style="color: #666;">Score: 65 | Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/f8ce3f48c335fa784e0e6aef63e636d13c30d93d.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 84]</strong> AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?<br><small style="color: #666;">Score: 65 | Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2507.15887" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 85]</strong> BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks and Defenses on Large Language Models<br><small style="color: #666;">Score: 65 | Agent Safety and Security, Domain-Specific Agents and Applications</small> <a href="https://arxiv.org/pdf/2408.12798" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 86]</strong> DeepDiver: Adaptive Web-Search Intensity Scaling via Reinforcement Learning<br><small style="color: #666;">Score: 64 | Web and Computer-Use Agents, Reinforcement Learning for LLMs, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/4da779e56e6d170cf4fc07f2af1015f08b35314e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 87]</strong> OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents<br><small style="color: #666;">Score: 64 | Agent Safety and Security, Agent Benchmarking and Evaluation, Web and Computer-Use Agents</small> <a href="https://arxiv.org/pdf/2506.14866" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 88]</strong> ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning<br><small style="color: #666;">Score: 64 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/3a4c411411ec8827c21b081ac099f93878bb8269.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 89]</strong> VIKIâR: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning<br><small style="color: #666;">Score: 64 | Multi-Agent Systems and Collaboration, Reinforcement Learning for LLMs, Vision-Language-Action Models</small> <a href="https://arxiv.org/pdf/2506.09049" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 90]</strong> AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model for Atmospheric Science<br><small style="color: #666;">Score: 64 | Agent Benchmarking and Evaluation, Domain-Specific Agents and Applications</small> <a href="https://arxiv.org/pdf/2502.01159" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 91]</strong> DAPO: An Open-Source LLM Reinforcement Learning System at Scale<br><small style="color: #666;">Score: 63 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/a5ca4684c1debe30e4fde4bd063a262d61e13db7.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 92]</strong> World-aware Planning Narratives Enhance Large Vision-Language Model Planner<br><small style="color: #666;">Score: 63 | Planning and Decision Making, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/c4dfdf9e91cc4527c4c879b52d8cc8d352f626ce.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 93]</strong> SeRL: Self-play Reinforcement Learning for Large Language Models with Limited Data<br><small style="color: #666;">Score: 62 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/f20577e67650da84d5c9396fc93fb971782ca925.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 94]</strong> BenchmarkCards: Standardized Documentation for Large Language Model Benchmarks<br><small style="color: #666;">Score: 61 | Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2410.12974" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 95]</strong> Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?<br><small style="color: #666;">Score: 61 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/c3957c2dc397dd6f7bf1e3da21cebaeca53844af.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 96]</strong> Why Do Multi-Agent LLM Systems Fail?<br><small style="color: #666;">Score: 61 | Multi-Agent Systems and Collaboration, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2503.13657" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 97]</strong> GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling<br><small style="color: #666;">Score: 60 | Agent Safety and Security, Multi-Agent Systems and Collaboration</small> <a href="https://openreview.net/pdf/12984819e947af4200e71748ef8d3b07f7b029cd.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 98]</strong> 3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model<br><small style="color: #666;">Score: 60 | Memory and Context Management, Vision-Language-Action Models, Spatial and Physical Reasoning</small> <a href="https://openreview.net/pdf/8e3474ff3b680185f73a397352845c1347b77d73.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 99]</strong> VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents<br><small style="color: #666;">Score: 59 | Vision-Language-Action Models, Planning and Decision Making, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/ed11882b04656406635941c99bb0b5dbf475a088.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 100]</strong> Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs<br><small style="color: #666;">Score: 59 | Agent Benchmarking and Evaluation, Domain-Specific Agents and Applications</small> <a href="https://openreview.net/pdf/13fb7451afdc5c8796e7166332feddd0c8a38f8e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 101]</strong> Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study<br><small style="color: #666;">Score: 59 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/0ced283865c2dd5717208f09588d27256b2fa260.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 102]</strong> Self-Guided Hierarchical Exploration for Generalist Foundation Model Web Agents<br><small style="color: #666;">Score: 59 | Web and Computer-Use Agents, Self-Improvement and Meta-Learning, Planning and Decision Making</small> <a href="https://openreview.net/pdf/23935fe967684b7bdc286739474d07c94da9cc76.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 103]</strong> Predicting Empirical AI Research Outcomes with Language Models<br><small style="color: #666;">Score: 58 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/149b9e435472a05ff9bc8f5c3138e681d73132fd.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 104]</strong> Generating Computational Cognitive models using Large Language Models<br><small style="color: #666;">Score: 58 | Tool Use and Code Generation, Domain-Specific Applications</small> <a href="https://openreview.net/pdf/71914784b067b8a89e5ba9e648c3f3fe0bef8659.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 105]</strong> AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration<br><small style="color: #666;">Score: 58 | Agent Safety and Security, Multi-Agent Systems and Collaboration, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/e53fb762e883d79a783bab07668988038d9e5162.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 106]</strong> ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models<br><small style="color: #666;">Score: 57 | Agent Benchmarking and Evaluation, Vision-Language-Action Models</small> <a href="https://arxiv.org/pdf/2505.13444" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 107]</strong> Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models<br><small style="color: #666;">Score: 57 | Agent Safety and Security, Vision-Language-Action Models, Multi-Agent Systems and Collaboration</small> <a href="https://openreview.net/pdf/8babb592735eb9e46455028c5428dfc816f08161.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 108]</strong> Reasoning as an Adaptive Defense for Safety<br><small style="color: #666;">Score: 57 | Agent Safety and Security, Reasoning and Test-Time Compute, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/c036ebd40ea82ab94b9badc0028faf7e94456ca1.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 109]</strong> Delving into Large Language Models for Effective Time-Series Anomaly Detection<br><small style="color: #666;">Score: 57 | Domain-Specific Agents and Applications, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/ff3e7df135cd4038ebbe31199752645c9946fa1e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 110]</strong> SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models<br><small style="color: #666;">Score: 57 | Reasoning and Test-Time Compute, Tool Use and Code Generation, World Models and Planning</small> <a href="https://openreview.net/pdf/2ac09a3b4dda169b90bb239fc45cb980a8bc3efd.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 111]</strong> AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking<br><small style="color: #666;">Score: 55 | Reasoning and Test-Time Compute, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/dd1128b101a8e529cfa219b76424da7861923e32.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 112]</strong> SiriuS: Self-improving Multi-agent Systems via Bootstrapped Reasoning<br><small style="color: #666;">Score: 55 | Multi-Agent Systems and Collaboration, Reasoning and Test-Time Compute, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/856c35f60eccff17ac8726de9ca5f7fbf9bcf3ee.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 113]</strong> Incentivizing LLMs to Self-Verify Their Answers<br><small style="color: #666;">Score: 55 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/ea6649a9cfb1c181a137632923e930e4e14e6ad3.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 114]</strong> Time Travel is Cheating: Going Live with DeepFund for Real-Time Fund Investment Benchmarking<br><small style="color: #666;">Score: 55 | Agent Benchmarking and Evaluation, Multi-Agent Systems and Collaboration, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2505.11065" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 115]</strong> MLZero: A Multi-Agent System for End-to-end Machine Learning Automation<br><small style="color: #666;">Score: 55 | Multi-Agent Systems and Collaboration, Tool Use and Code Generation, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/55f28109c8ee532fe1c950142c23f6efd636a79e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 116]</strong> Training Language Models to Reason Efficiently<br><small style="color: #666;">Score: 55 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/c169ad531f568624e1f7af8211b9ff6b12391b63.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 117]</strong> Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning<br><small style="color: #666;">Score: 55 | Tool Use and Code Generation, Reinforcement Learning for LLMs, Domain-Specific Agents and Applications</small> <a href="https://openreview.net/pdf/7b8e4f95272ab1e9e3900b30ef9647d058b6b034.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 118]</strong> PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments<br><small style="color: #666;">Score: 55 | Vision-Language-Action Models, Reasoning and Test-Time Compute, Planning and Decision Making</small> <a href="https://openreview.net/pdf/89b1018647a9f5681e3ad0ed184e80229e3997ee.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 119]</strong> LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?<br><small style="color: #666;">Score: 53 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2506.11928" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 120]</strong> RvLLM: LLM Runtime Verification with Domain Knowledge<br><small style="color: #666;">Score: 53 | Agent Safety and Security, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/e99cb11cda9beb3092445ecb739b48abb0369ac9.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 121]</strong> BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems<br><small style="color: #666;">Score: 53 | Agent Benchmarking and Evaluation, Agent Safety and Security, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2505.15216" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 122]</strong> Lookahead Routing for Large Language Models<br><small style="color: #666;">Score: 53 | Model Efficiency and Optimization, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/552fece3eff509f39e0f54fafd8aafc36248c8a8.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 123]</strong> DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents<br><small style="color: #666;">Score: 53 | Agent Safety and Security, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/7e05f767d463d43be3b045378b14be5760ea2fc1.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 124]</strong> Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency<br><small style="color: #666;">Score: 52 | Agent Safety and Security, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/7e13bf73f0b1b45f44c038b630279494b0220174.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 125]</strong> Iterative Tool Usage Exploration for Multimodal Agents via Step-wise Preference Tuning<br><small style="color: #666;">Score: 52 | Tool Use and Code Generation, Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/a3ae43bcdfe712b2361e4ab5254bbce2bcc0dd95.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 126]</strong> DyFlow: Dynamic Workflow Framework for Agentic Reasoning<br><small style="color: #666;">Score: 52 | Reasoning and Test-Time Compute, Multi-Agent Systems and Collaboration, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/705f64f765b412ab6e17c0dc9c9146763c3e63fe.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 127]</strong> MathArena: Evaluating LLMs on Uncontaminated Math Competitions<br><small style="color: #666;">Score: 52 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2505.23281" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 128]</strong> SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond<br><small style="color: #666;">Score: 51 | Reasoning and Test-Time Compute, Mathematical and Logical Reasoning, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/b5b93e7b533135e2ad1d72d22e3a481324c2a73d.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 129]</strong> Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies<br><small style="color: #666;">Score: 51 | Reasoning and Test-Time Compute, Multi-Agent Systems and Collaboration, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/5d59ea40926886752f5ab100ab83a383587e3e1e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 130]</strong> L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models<br><small style="color: #666;">Score: 51 | Model Efficiency and Optimization, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/03edf649a9f42cdf6d921cb59599e7130120540a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 131]</strong> AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench<br><small style="color: #666;">Score: 51 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/a22e34894b1de174131d79168e9a32bf5fefd3a5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 132]</strong> SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines<br><small style="color: #666;">Score: 51 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2502.14739" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 133]</strong> CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections<br><small style="color: #666;">Score: 51 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/bcb703c11665128fb39f34dfd53237d4a3431b28.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 134]</strong> PoE-World: Compositional World Modeling with Products of Programmatic Experts<br><small style="color: #666;">Score: 50 | World Models and Planning, Reasoning and Test-Time Compute, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/1983f2216c20adc421975e0092eb41f2ac1d93fa.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 135]</strong> Web-Shepherd: Advancing PRMs for Reinforcing Web Agents<br><small style="color: #666;">Score: 49 | Web and Computer-Use Agents, Agent Benchmarking and Evaluation, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/db46564195dad40b9b174514d7d03b0336d2a8eb.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 136]</strong> RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation<br><small style="color: #666;">Score: 49 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Planning and Decision Making</small> <a href="https://arxiv.org/pdf/2506.06677" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 137]</strong> MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering<br><small style="color: #666;">Score: 49 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2505.07782" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 138]</strong> Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning<br><small style="color: #666;">Score: 49 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2506.10521" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 139]</strong> ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints<br><small style="color: #666;">Score: 49 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/2bc8b445588504ed12b491f7cc33f033191ae2ad.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 140]</strong> Logic.py: Bridging the Gap between LLMs and Constraint Solvers<br><small style="color: #666;">Score: 49 | Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/a811f4bc76e7ad2fcf67bc0ce62afd3123512b8d.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 141]</strong> OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization<br><small style="color: #666;">Score: 49 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://arxiv.org/pdf/2506.18880" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 142]</strong> TTRL: Test-Time Reinforcement Learning<br><small style="color: #666;">Score: 48 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/3ff432e912f7ed9bbbacf9a7a16d7e5af88d721a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 143]</strong> Atom of Thoughts for Markov LLM Test-Time Scaling<br><small style="color: #666;">Score: 48 | Reasoning and Test-Time Compute, Planning and Decision Making, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/aee7e5e0ac85edb676698e634deccc28c92e1407.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 144]</strong> OmniBench: Towards The Future of  Universal Omni-Language Models<br><small style="color: #666;">Score: 48 | Agent Benchmarking and Evaluation, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/c52922a7cb62ce2d159c41efd473976ed5a5693f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 145]</strong> MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models<br><small style="color: #666;">Score: 48 | Agent Benchmarking and Evaluation, Vision-Language-Action Models</small> <a href="https://arxiv.org/pdf/2306.13394" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 146]</strong> Revolutionizing Training-Free NAS: Towards Efficient Automatic Proxy Discovery via Large Language Models<br><small style="color: #666;">Score: 48 | Self-Improvement and Meta-Learning, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/f5967a55faa0ccebb78581f66e96dad4d59eb767.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 147]</strong> EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths<br><small style="color: #666;">Score: 48 | Tool Use and Code Generation, Planning and Decision Making, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/ec254672c2c5013801b6522a08e51c829a7ef814.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 148]</strong> RealMath: A Continuous Benchmark for Evaluating Language Models on Research-Level Mathematics<br><small style="color: #666;">Score: 47 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2505.12575" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 149]</strong> Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning<br><small style="color: #666;">Score: 47 | Reasoning and Test-Time Compute, Self-Improvement and Meta-Learning, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/a85740fc42f5b0086b031316724925f3d39daa30.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 150]</strong> AREAL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning<br><small style="color: #666;">Score: 47 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/1b2829a8cfd93ca52bca9bf2c38c826016159024.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 151]</strong> RLVR-World: Training World Models with Reinforcement Learning<br><small style="color: #666;">Score: 46 | Reinforcement Learning for LLMs, Planning and Decision Making, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/4b0d2935ed4554453743ecdcf099e0d679355328.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 152]</strong> Generalizing Verifiable Instruction Following<br><small style="color: #666;">Score: 46 | Agent Benchmarking and Evaluation, Reinforcement Learning for LLMs, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2507.02833" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 153]</strong> A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning<br><small style="color: #666;">Score: 46 | Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/73a0607365582aedefc2167e27fb239c96092223.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 154]</strong> SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents<br><small style="color: #666;">Score: 46 | Reasoning and Test-Time Compute, Self-Improvement and Meta-Learning, Planning and Decision Making</small> <a href="https://openreview.net/pdf/814cd78232da3150c1f91b29920f4f2e4d70fb3c.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 155]</strong> Sloth: scaling laws for LLM skills to predict multi-benchmark performance across families<br><small style="color: #666;">Score: 45 | Agent Benchmarking and Evaluation, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/82fec2c6ee0cec1161d232d81bfad1d63de8fd54.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 156]</strong> AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems<br><small style="color: #666;">Score: 45 | Multi-Agent Systems and Collaboration, Tool Use and Code Generation, Memory and Context Management</small> <a href="https://openreview.net/pdf/38bdf6e7191adba7391b1fde1ad37e27887b2bac.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 157]</strong> DynaAct: Large Language Model Reasoning with Dynamic Action Spaces<br><small style="color: #666;">Score: 45 | Planning and Decision Making, Tool Use and Code Generation, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/45820087f0186edb228a8b251bb2af4a3cca38b5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 158]</strong> GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining<br><small style="color: #666;">Score: 45 | Tool Use and Code Generation, Agent Benchmarking and Evaluation, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/d184d257f33dec6f932171111d977423bd83f8ef.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 159]</strong> Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation<br><small style="color: #666;">Score: 45 | Agent Benchmarking and Evaluation, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2503.05493" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 160]</strong> TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning<br><small style="color: #666;">Score: 45 | Model Efficiency and Optimization, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/1608428ae182161621e0d5c0fae3d6b608a7acaa.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 161]</strong> Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation<br><small style="color: #666;">Score: 44 | Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/5f50a250befd3553dd40112c1a440f86b36737da.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 162]</strong> Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing<br><small style="color: #666;">Score: 44 | Spatial and Physical Reasoning, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/0da039d9be67a74526e2ce4408db4cde8bdca6f5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 163]</strong> Abstract Counterfactuals for Language Model Agents<br><small style="color: #666;">Score: 43 | Agent Benchmarking and Evaluation, Agent Safety and Security</small> <a href="https://openreview.net/pdf/c39308d32f99302b4eb8a97c6d7eae5b2e2c4466.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 164]</strong> OpenCUA: Open Foundations for Computer-Use Agents<br><small style="color: #666;">Score: 43 | Web and Computer-Use Agents, Agent Benchmarking and Evaluation, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/eb1bd0238abbc386303352dba1049a4d5d1fec83.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 165]</strong> Scalable Best-of-N Selection for Large Language Models via Self-Certainty<br><small style="color: #666;">Score: 43 | Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/141a8955a8007083b8b3068692459c467c444619.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 166]</strong> A-Mem: Agentic Memory for LLM Agents<br><small style="color: #666;">Score: 42 | Memory and Context Management, Planning and Decision Making, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/518e506d098a6c821c7e4ae97d1368f374fddac9.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 167]</strong> ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models<br><small style="color: #666;">Score: 42 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/75e49a843f0b6d00c0584a776fad3bea93496c83.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 168]</strong> Does Thinking More Always Help? Mirage of Test-Time Scaling in Reasoning Models<br><small style="color: #666;">Score: 42 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/9a4ddca48299d0fb623da4e9a0093d29392e48a2.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 169]</strong> Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost<br><small style="color: #666;">Score: 42 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/496a103d9eacba8143b7d8a13098936a351a1a81.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 170]</strong> Thinkless: LLM Learns When to Think<br><small style="color: #666;">Score: 41 | Reasoning and Test-Time Compute, Model Efficiency and Optimization, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/69d5f7c0fe7ed54d42a4a1908d50a94bc062b721.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 171]</strong> Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL<br><small style="color: #666;">Score: 41 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/dc89a6b0221240f7e120c7bd39c116d57711eec4.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 172]</strong> Fast Inference for Augmented Large Language Models<br><small style="color: #666;">Score: 40 | Tool Use and Code Generation, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/466ae7c6d881581805533176114b2bde5a683e8d.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 173]</strong> Distilling LLM Agent into Small Models with Retrieval and Code Tools<br><small style="color: #666;">Score: 40 | Tool Use and Code Generation, Model Efficiency and Optimization, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/c3ead64f6c3fd03156d79bf7aa5185204700b2a2.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 174]</strong> Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks<br><small style="color: #666;">Score: 40 | Planning and Decision Making, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/05d0804df1c396da814a29970ae7d37c40a1b84e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 175]</strong> Scaling Up Active Testing to Large Language Models<br><small style="color: #666;">Score: 39 | Agent Benchmarking and Evaluation, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/0e4fdfd137ed152835e93d1c72c71b4217f5dfa0.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 176]</strong> MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation<br><small style="color: #666;">Score: 38 | Mathematical and Logical Reasoning, Reasoning and Test-Time Compute, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/350dc187c2833efc7cd93402a27fd709f790c12b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 177]</strong> Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model<br><small style="color: #666;">Score: 38 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/64ee56c0512b904e0aebcd799494fb96e7bdedba.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 178]</strong> Neither Valid nor Reliable? Investigating the Use of LLMs as Judges<br><small style="color: #666;">Score: 37 | Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2508.18076" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 179]</strong> Think Only When You Need with Large Hybrid-Reasoning Models<br><small style="color: #666;">Score: 37 | Reasoning and Test-Time Compute, Model Efficiency and Optimization, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/33fb886d1cda050a0c29d0bdee85176c1c3f7f31.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 180]</strong> Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees<br><small style="color: #666;">Score: 37 | Agent Benchmarking and Evaluation, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/42cf5e1bbaeccd15c151a00a71cb2d9ecef2aa6f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 181]</strong> A Controllable Examination for Long-Context Language Models<br><small style="color: #666;">Score: 37 | Agent Benchmarking and Evaluation, Memory and Context Management</small> <a href="https://arxiv.org/pdf/2506.02921" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 182]</strong> Teaching Language Models to Reason with Tools<br><small style="color: #666;">Score: 37 | Tool Use and Code Generation, Mathematical and Logical Reasoning, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/406d2cc67059647e62684368005578debbc188e9.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 183]</strong> SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts<br><small style="color: #666;">Score: 37 | Agent Safety and Security, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2505.21828" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 184]</strong> DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios<br><small style="color: #666;">Score: 36 | Agent Safety and Security, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2510.15501" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 185]</strong> Can Agent Fix Agent Issues?<br><small style="color: #666;">Score: 36 | Agent Benchmarking and Evaluation, Self-Improvement and Meta-Learning, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/ab4c234adba0d835dbb64828e87b465d8983cecd.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 186]</strong> Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs<br><small style="color: #666;">Score: 36 | Agent Benchmarking and Evaluation, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2507.11932" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 187]</strong> SURDS: Benchmarking Spatial Understanding and Reasoning in Driving Scenarios with Vision Language Models<br><small style="color: #666;">Score: 35 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2411.13112" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 188]</strong> Reward Reasoning Models<br><small style="color: #666;">Score: 35 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/1b460f75d98849e58bc2a9cf1b6673536d0898d5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 189]</strong> PolyGuard: Massive Multi-Domain Safety Policy-Grounded Guardrail Dataset<br><small style="color: #666;">Score: 35 | Agent Safety and Security, Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2506.19054" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 190]</strong> Chain of Execution Supervision Promotes General Reasoning in Large Language Models<br><small style="color: #666;">Score: 35 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/81876043b2c12b524b3d7cebe4d6ef2b4a940143.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 191]</strong> Measuring what Matters: Construct Validity in Large Language Model Benchmarks<br><small style="color: #666;">Score: 35 | Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2511.04703" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 192]</strong> MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants<br><small style="color: #666;">Score: 35 | Agent Benchmarking and Evaluation, Memory and Context Management</small> <a href="https://openreview.net/pdf/96e1b7b1eeb53a530580aff14cf9527fe3e3d1ac.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 193]</strong> EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving<br><small style="color: #666;">Score: 35 | Agent Benchmarking and Evaluation, Self-Improvement and Meta-Learning, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/78058118e972a34b3ab22aa5b6b0c000ab083f58.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 194]</strong> RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video<br><small style="color: #666;">Score: 35 | Agent Benchmarking and Evaluation, Vision-Language-Action Models</small> <a href="https://arxiv.org/pdf/2505.02064" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 195]</strong> G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems<br><small style="color: #666;">Score: 34 | Multi-Agent Systems and Collaboration, Memory and Context Management</small> <a href="https://openreview.net/pdf/52f961783a3212459f228b4ec297f523ba2d0c95.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 196]</strong> Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning<br><small style="color: #666;">Score: 34 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute, Multi-Agent Systems and Collaboration</small> <a href="https://openreview.net/pdf/363710ed9012131f837da723473810ad47f9d2c9.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 197]</strong> LIFEBENCH: Evaluating Length Instruction Following in Large Language Models<br><small style="color: #666;">Score: 34 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2505.16234" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 198]</strong> Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler<br><small style="color: #666;">Score: 34 | Agent Safety and Security, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/2b29a4e872717270eff659096ceb805ef86a1735.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 199]</strong> ToolRL: Reward is All Tool Learning Needs<br><small style="color: #666;">Score: 33 | Tool Use and Code Generation, Reinforcement Learning for LLMs, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/097ae4a34c2eb2b82b2bb8fccc279fb0e3585304.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 200]</strong> OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles<br><small style="color: #666;">Score: 33 | Reasoning and Test-Time Compute, Vision-Language-Action Models, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/62ceb097c643e0416c764c187ebf4f4d6d1ba9c3.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 201]</strong> VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs<br><small style="color: #666;">Score: 33 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/ca79a743ac55c7fce5e100be3b2d695bf84a71ba.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 202]</strong> LLM-PySC2: Starcraft II learning environment for Large Language Models<br><small style="color: #666;">Score: 32 | Agent Benchmarking and Evaluation, Multi-Agent Systems and Collaboration, Planning and Decision Making</small> <a href="https://openreview.net/pdf/ae23b5a1cbb9529359c184cdf3d394c6383add5a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 203]</strong> GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments<br><small style="color: #666;">Score: 32 | Multi-Agent Systems and Collaboration, Tool Use and Code Generation, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/e15247fbacf803da9beef26d39153bd1afd55fe6.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 204]</strong> InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback<br><small style="color: #666;">Score: 32 | Reinforcement Learning for LLMs, Memory and Context Management</small> <a href="https://arxiv.org/pdf/2505.23950" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 205]</strong> Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs<br><small style="color: #666;">Score: 31 | Agent Benchmarking and Evaluation, Planning and Decision Making, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/01546f97c01b32e5c0cf560fc4be7a511b46e042.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 206]</strong> Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models<br><small style="color: #666;">Score: 31 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/bc154619c4195b6a62774c122f1706e4d7b1bb7f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 207]</strong> Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning<br><small style="color: #666;">Score: 31 | Reinforcement Learning for LLMs, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/109c600393cc962e64028e8425eca62778f40ee9.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 208]</strong> ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning<br><small style="color: #666;">Score: 31 | Vision-Language-Action Models, Planning and Decision Making, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/b35b0fc70612e191baced400f754db8ff1fae711.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 209]</strong> QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?<br><small style="color: #666;">Score: 30 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Planning and Decision Making</small> <a href="https://arxiv.org/pdf/2503.22674" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 210]</strong> SeePhys:  Does Seeing Help Thinking? â Benchmarking Vision-Based Physics Reasoning<br><small style="color: #666;">Score: 30 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2505.19099" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 211]</strong> The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense<br><small style="color: #666;">Score: 30 | Agent Safety and Security, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/85f3e38bd08668ee901051237edc01d1b8d8823e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 212]</strong> Introducing FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark<br><small style="color: #666;">Score: 30 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2502.19676" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 213]</strong> Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve<br><small style="color: #666;">Score: 29 | Multi-Agent Systems and Collaboration, Tool Use and Code Generation, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/11bc550a62e76a702248f346f36018dce79624fb.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 214]</strong> SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search<br><small style="color: #666;">Score: 29 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/b7f1cf26530dd86e2ae6d7c4a3a2f77a1c33b13b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 215]</strong> MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly<br><small style="color: #666;">Score: 29 | Agent Benchmarking and Evaluation, Vision-Language-Action Models</small> <a href="https://arxiv.org/pdf/2505.10610" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 216]</strong> SpecEM: Training-Free LLM Ensembling via Iterative Drafting, Verification, and Online Feedback<br><small style="color: #666;">Score: 28 | Model Efficiency and Optimization, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/ce36072cf898e279341e1ca31c51626ddead6950.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 217]</strong> SATURN: SAT-based Reinforcement Learning to Unleash LLMs Reasoning<br><small style="color: #666;">Score: 28 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/8c8fdd51e03779f1c757cd48a07127240f2339d4.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 218]</strong> ThinkBench: Dynamic Out-of-Distribution Evaluation for Robust LLM Reasoning<br><small style="color: #666;">Score: 28 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://arxiv.org/pdf/2502.16268" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 219]</strong> VERA: Variational Inference Framework for Jailbreaking Large Language Models<br><small style="color: #666;">Score: 28 | Agent Safety and Security, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/5d8395fee02384e70cded7bf691df4c9976ebb5a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 220]</strong> Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models<br><small style="color: #666;">Score: 28 | Spatial and Physical Reasoning, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/1321dbcd7c95604245189132069ca5ac16d3058c.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 221]</strong> PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models<br><small style="color: #666;">Score: 28 | Agent Benchmarking and Evaluation, Mathematical and Logical Reasoning, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2504.16074" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 222]</strong> Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling<br><small style="color: #666;">Score: 28 | Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/639ca4d7460b732c0c9d399142939d60bcdb29d2.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 223]</strong> RAST: Reasoning Activation in LLMs via Small-model Transfer<br><small style="color: #666;">Score: 28 | Reasoning and Test-Time Compute, Model Efficiency and Optimization, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/4e1cf538a0cee20ba69772f7ede39e11ffddd493.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 224]</strong> Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization<br><small style="color: #666;">Score: 28 | Reasoning and Test-Time Compute, Self-Improvement and Meta-Learning, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/4beac7313c00cacdbfe88ef717756d49edfe75a1.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 225]</strong> Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning<br><small style="color: #666;">Score: 28 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/4c696fcdaaa5b6e8b53a1bf9e94c8993ee0cd433.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 226]</strong> WorldModelBench: Judging Video Generation Models As World Models<br><small style="color: #666;">Score: 27 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2502.20694" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 227]</strong> Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs<br><small style="color: #666;">Score: 27 | Agent Safety and Security</small> <a href="https://openreview.net/pdf/106f56012866f9c2a8f8d433881447430b77c9df.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 228]</strong> Memory Injection Attacks on LLM Agents via Query-Only Interaction<br><small style="color: #666;">Score: 27 | Agent Safety and Security, Memory and Context Management</small> <a href="https://openreview.net/pdf/0fa0fce8cc51e3ae2f36e5aef29236b15bcc35bc.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 229]</strong> World Models Should Prioritize the Unification of Physical and Social Dynamics<br><small style="color: #666;">Score: 26 | Multi-Agent Systems and Collaboration, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2510.21219" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 230]</strong> On Reasoning Strength Planning in Large Reasoning Models<br><small style="color: #666;">Score: 26 | Reasoning and Test-Time Compute, Model Efficiency and Optimization, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/ec0170f131842f1aaee5993a19df22764b470213.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 231]</strong> Absolute Zero: Reinforced Self-play Reasoning with Zero Data<br><small style="color: #666;">Score: 26 | Self-Improvement and Meta-Learning, Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/34a1a72a0a54db41248d9ad8862c78e55ac789d9.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 232]</strong> MetaDefense: Defending Fine-tuning based Jailbreak Attack Before and During Generation<br><small style="color: #666;">Score: 25 | Agent Safety and Security, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/006cba1cf9e75c1f5fafd3bea5ee62d71f204085.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 233]</strong> SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents<br><small style="color: #666;">Score: 25 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2505.20411" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 234]</strong> Common Task Framework For a Critical Evaluation of Scientific Machine Learning Algorithms<br><small style="color: #666;">Score: 25 | Agent Benchmarking and Evaluation, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2510.23166" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 235]</strong> Rethinking Verification for LLM Code Generation: From Generation to Testing<br><small style="color: #666;">Score: 25 | Tool Use and Code Generation, Agent Benchmarking and Evaluation, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/11dbcf6567a9c2d7cfb82938a0ec215c232e6c80.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 236]</strong> On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study<br><small style="color: #666;">Score: 24 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation, Planning and Decision Making</small> <a href="https://openreview.net/pdf/57860c28b21d95678533bc618a0afee1c2a54e47.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 237]</strong> Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning<br><small style="color: #666;">Score: 24 | Reasoning and Test-Time Compute, Memory and Context Management</small> <a href="https://openreview.net/pdf/3aa73c81186850fe236acaa4a81d2af3f1e5c910.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 238]</strong> Let Me Think! A Long Chain of Thought Can Be Worth Exponentially Many Short Ones<br><small style="color: #666;">Score: 24 | Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/3522982c33f5d5199bd558f48909b1bccbf81615.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 239]</strong> FlySearch: Exploring how vision-language models explore<br><small style="color: #666;">Score: 24 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Planning and Decision Making</small> <a href="https://arxiv.org/pdf/2506.02896" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 240]</strong> PARCO: Parallel AutoRegressive Models for Multi-Agent Combinatorial Optimization<br><small style="color: #666;">Score: 23 | Multi-Agent Systems and Collaboration, Planning and Decision Making, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/672bb4ede45dba28f133a8ebb2cfe40fb1f395c8.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 241]</strong> Partner Modelling Emerges in Recurrent Agents (But Only When It Matters)<br><small style="color: #666;">Score: 23 | Multi-Agent Systems and Collaboration, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/ce9e0e5f6bb446a8103ef0fb8f4c67e47b7972d0.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 242]</strong> Learning âPartner-Awareâ Collaborators in Multi-Party Collaboration<br><small style="color: #666;">Score: 23 | Multi-Agent Systems and Collaboration, Reinforcement Learning for LLMs, Agent Safety and Security</small> <a href="https://openreview.net/pdf/40d31b0852235d5e98c6f82c41aa70978247a0dd.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 243]</strong> LASeR: Learning to Adaptively Select Reward Models with Multi-Arm Bandits<br><small style="color: #666;">Score: 23 | Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/bf589cfd8356e7e428426d438835ed093caa2e02.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 244]</strong> Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs<br><small style="color: #666;">Score: 23 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/e7ed5c9a866ddc4710624ed2df37d91bce6455d3.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 245]</strong> A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning<br><small style="color: #666;">Score: 23 | Mathematical and Logical Reasoning, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/edfcc94d23a1796e8903435652cc311e00009492.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 246]</strong> REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving<br><small style="color: #666;">Score: 23 | Tool Use and Code Generation, Model Efficiency and Optimization, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/3e257cd601e943e872f777fd88fbaf2a64c6c6ea.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 247]</strong> Reasoning Planning for Language Models<br><small style="color: #666;">Score: 23 | Reasoning and Test-Time Compute, Planning and Decision Making, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/4e2c8f0df4fed5d40292835b8448bea21d28648f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 248]</strong> The World Is Bigger: A Computationally-Embedded Perspective on the Big World Hypothesis<br><small style="color: #666;">Score: 22 | Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/463a93594b9e688fecce37543cf3bf45e7f91310.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 249]</strong> IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering<br><small style="color: #666;">Score: 22 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2506.23329" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 250]</strong> Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs<br><small style="color: #666;">Score: 22 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/de9fc962acefe20dd0d80073eadeb19263afeb06.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 251]</strong> DMWM: Dual-Mind World Model with Long-Term Imagination<br><small style="color: #666;">Score: 21 | Planning and Decision Making, Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/46a97d474ce914d7520303484925b64a33af1b9b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 252]</strong> Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models<br><small style="color: #666;">Score: 21 | Vision-Language-Action Models, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2505.23757" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 253]</strong> Lifelong Safety Alignment for Language Models<br><small style="color: #666;">Score: 21 | Agent Safety and Security, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/8b599899e65f157c99b0a6ac3e8b45afbc4fee1a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 254]</strong> Reinforcement Learning for Reasoning in Large Language Models with One Training Example<br><small style="color: #666;">Score: 21 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/6488aab2ea1a4b2423d232a17c9fcd1545659f8c.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 255]</strong> Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs<br><small style="color: #666;">Score: 20 | Agent Safety and Security, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/2a60bb227d356283cc8b6eddcfeda9a0ef466282.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 256]</strong> Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning<br><small style="color: #666;">Score: 20 | Tool Use and Code Generation, Planning and Decision Making, Spatial and Physical Reasoning</small> <a href="https://openreview.net/pdf/8a135640e90e68e7dd192021ba6a8fdff76f596f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 257]</strong> WritingBench: A Comprehensive Benchmark for Generative Writing<br><small style="color: #666;">Score: 19 | Agent Benchmarking and Evaluation, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2503.05244" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 258]</strong> MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning<br><small style="color: #666;">Score: 19 | Multi-Agent Systems and Collaboration, Planning and Decision Making, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/8e363493ff9d9b3fa837f8d6bb3198fa13ba65f4.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 259]</strong> Donât Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models<br><small style="color: #666;">Score: 19 | Reasoning and Test-Time Compute, Model Efficiency and Optimization, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/1b9aa688f3d241f228fbd9b8694bdffb938d1d5f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 260]</strong> STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models<br><small style="color: #666;">Score: 19 | Agent Benchmarking and Evaluation, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/ce44a9975c0bc8b6ace090a98fc92829bdf5fece.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 261]</strong> SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning<br><small style="color: #666;">Score: 19 | Reasoning and Test-Time Compute, Self-Improvement and Meta-Learning, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/37589115324975c5657483b37546cdd4916aeeed.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 262]</strong> Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges<br><small style="color: #666;">Score: 19 | Agent Benchmarking and Evaluation, Spatial and Physical Reasoning, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2505.11618" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 263]</strong> ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering<br><small style="color: #666;">Score: 19 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2506.09050" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 264]</strong> Contextual Integrity in LLMs via Reasoning and Reinforcement Learning<br><small style="color: #666;">Score: 19 | Agent Safety and Security, Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/b39de1061902f7e2d23191a93a6d94f945bff28e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 265]</strong> MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Cultural Learning<br><small style="color: #666;">Score: 19 | Multi-Agent Systems and Collaboration, Reasoning and Test-Time Compute, Memory and Context Management</small> <a href="https://openreview.net/pdf/aaff34bc5f2395383d58c98d38975df26262971c.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 266]</strong> SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents<br><small style="color: #666;">Score: 19 | Self-Improvement and Meta-Learning, Reinforcement Learning for LLMs, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/d934e1858b206a0ba8fe1fb5281ee9f117238785.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 267]</strong> Detecting High-Stakes Interactions with Activation Probes<br><small style="color: #666;">Score: 18 | Agent Safety and Security, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/6291cf20df7b3579af7a0739c773db78424d18cd.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 268]</strong> LLM Generated Persona is a Promise with a Catch<br><small style="color: #666;">Score: 18 | Agent Benchmarking and Evaluation, Agent Safety and Security</small> <a href="https://arxiv.org/pdf/2503.16527" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 269]</strong> ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking<br><small style="color: #666;">Score: 18 | Agent Benchmarking and Evaluation, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/2d4818decec8bd80bc1421d0101a796c541e0194.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 270]</strong> MindJourney: Test-Time Scaling with World Models for Spatial Reasoning<br><small style="color: #666;">Score: 17 | Reasoning and Test-Time Compute, Spatial and Physical Reasoning, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/5ed386610c8657ff319e5833b8272c6459dd85a4.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 271]</strong> Once Upon an Input: Reasoning via Per-Instance Program Synthesis<br><small style="color: #666;">Score: 17 | Tool Use and Code Generation, Reasoning and Test-Time Compute, Planning and Decision Making</small> <a href="https://openreview.net/pdf/7b4a6ca902e7228c2855e6b864e4699825b54723.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 272]</strong> Can We Infer Confidential Properties of Training Data from LLMs?<br><small style="color: #666;">Score: 17 | Agent Safety and Security, Domain-Specific Applications</small> <a href="https://openreview.net/pdf/cd0fb86716237f2897625fe1354d6af3d7b0ec5b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 273]</strong> Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs<br><small style="color: #666;">Score: 17 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Agent Safety and Security</small> <a href="https://openreview.net/pdf/33aae90c6ae97c2a878758d797cf8196f7ca09de.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 274]</strong> From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction<br><small style="color: #666;">Score: 16 | Planning and Decision Making, Vision-Language-Action Models, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/dbe78cc8fdf28b0340af74010ec4ad766aca831b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 275]</strong> CodeCrash: Exposing LLM Fragility to Misleading Natural Language in Code Reasoning<br><small style="color: #666;">Score: 16 | Tool Use and Code Generation, Agent Safety and Security, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/9b9883a392dca607cfb2b26eccc24136b0e5cf90.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 276]</strong> Learning and Planning Multi-Agent Tasks via an MoE-based World Model<br><small style="color: #666;">Score: 16 | Multi-Agent Systems and Collaboration, Planning and Decision Making, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/09e9ae881c16219816d4e101be5c5634163ae0b6.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 277]</strong> MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection<br><small style="color: #666;">Score: 15 | Vision-Language-Action Models, Domain-Specific Applications</small> <a href="https://openreview.net/pdf/12eb1e4189682fe7b8223e53327f9e2ada20bd59.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 278]</strong> MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks<br><small style="color: #666;">Score: 15 | Agent Benchmarking and Evaluation, Multi-Agent Systems and Collaboration, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2505.12371" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 279]</strong> Caption This, Reason That: VLMs Caught in the Middle<br><small style="color: #666;">Score: 14 | Vision-Language-Action Models, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/756a05a421ed180f3aedfcaa7a55a73f50e83d30.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 280]</strong> Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders<br><small style="color: #666;">Score: 14 | Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/8983a85d88c67728d26c47fa75cdce08379b3561.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 281]</strong> Predictable Scale (Part II) --- Farseer: A Refined Scaling Law in LLMs<br><small style="color: #666;">Score: 14 | Model Efficiency and Optimization, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/b2d2dae1fc87cdd4510e8c2672fcf585d1c653f2.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 282]</strong> Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models<br><small style="color: #666;">Score: 14 | Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/2b62626f959a636707dfb61efe094e4dcd7bd32f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 283]</strong> zip2zip: Inference-Time Adaptive Tokenization via Online Compression<br><small style="color: #666;">Score: 14 | Model Efficiency and Optimization, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/0f10815a6be342f3a8137fe6510088e422bd3a6b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 284]</strong> OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis<br><small style="color: #666;">Score: 14 | Vision-Language-Action Models, Tool Use and Code Generation, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/b83bcc6b13bf3bed81ebb73be9bae7cc2be710e7.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 285]</strong> Temporal Logic-Based Multi-Vehicle Backdoor Attacks against Offline RL Agents in End-to-end Autonomous Driving<br><small style="color: #666;">Score: 14 | Agent Safety and Security, Reinforcement Learning for LLMs, Planning and Decision Making</small> <a href="https://openreview.net/pdf/26313d9a67de648ba12a7315cbcaa87c2787a4f5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 286]</strong> SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models for Time Series<br><small style="color: #666;">Score: 14 | Agent Benchmarking and Evaluation</small> <a href="https://arxiv.org/pdf/2510.20273" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 287]</strong> MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM<br><small style="color: #666;">Score: 14 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/5c8396fbd51956694870ead9c44afcff4e5253cb.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 288]</strong> LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions<br><small style="color: #666;">Score: 14 | Memory and Context Management, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/ed87797f943163aff0337ae4824c26f2e347ea6a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 289]</strong> Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers<br><small style="color: #666;">Score: 14 | Web and Computer-Use Agents, Self-Improvement and Meta-Learning, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/f6244ed69adb4a0f1c375503deb1e23c39a8ac15.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 290]</strong> RADAR: Benchmarking Language Models on Imperfect Tabular Data<br><small style="color: #666;">Score: 13 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2506.08249" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 291]</strong> Bootstrap Off-policy with World Model<br><small style="color: #666;">Score: 13 | Planning and Decision Making, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/41b2f866d682b3c56d82ba0f291b84901efb52d3.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 292]</strong> The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement<br><small style="color: #666;">Score: 13 | Self-Improvement and Meta-Learning, Planning and Decision Making, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/ae3bf83043c88b4e28668b095624b19cc07ed197.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 293]</strong> Imagined Autocurricula<br><small style="color: #666;">Score: 13 | Self-Improvement and Meta-Learning, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/3facb7ccf15168534e07f4990a5bd009d1afea45.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 294]</strong> Reverse Engineering Human Preferences with Reinforcement Learning<br><small style="color: #666;">Score: 12 | Agent Safety and Security, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/e7f264f17bfbaac2f1357558dddf2cceae14cd6e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 295]</strong> SciArena: An Open Evaluation Platform for Non-Verifiable Scientific Literature-Grounded Tasks<br><small style="color: #666;">Score: 12 | Agent Benchmarking and Evaluation, Domain-Specific Applications</small></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 296]</strong> Lost in Transmission: When and Why LLMs Fail to Reason Globally<br><small style="color: #666;">Score: 12 | Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/93d0795dfa82fdff6b6b121fce6307ed161915ba.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 297]</strong> GRIP: A Graph-Based Reasoning Instruction Producer<br><small style="color: #666;">Score: 12 | Reasoning and Test-Time Compute, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/9ef2967c43fa97f6b4cfdddaf146abf156250794.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 298]</strong> Generalizing Experience for Language Agents with Hierarchical MetaFlows<br><small style="color: #666;">Score: 12 | Memory and Context Management, Self-Improvement and Meta-Learning, Planning and Decision Making</small> <a href="https://openreview.net/pdf/e5e7cd3c074c77adae516f8d392136a8fe8657f5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 299]</strong> PlanU: Large Language Model Reasoning through Planning under Uncertainty<br><small style="color: #666;">Score: 12 | Planning and Decision Making, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/a9771087543396ca7e59296ca5d3d68429e5a708.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 300]</strong> LILO: Learning to Reason at the Frontier of Learnability<br><small style="color: #666;">Score: 12 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/81c8ea42739bd8be2077f7a518eeedc3ad8b7d5c.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 301]</strong> ESCA: Contextualizing Embodied Agents via Scene-Graph Generation<br><small style="color: #666;">Score: 11 | Vision-Language-Action Models, Planning and Decision Making, Spatial and Physical Reasoning</small> <a href="https://openreview.net/pdf/32e384d1b7bff0817b05af22b3999df7789e5b8d.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 302]</strong> ML4CO-Bench-101: Benchmark Machine Learning for Classic Combinatorial Problems on Graphs<br><small style="color: #666;">Score: 11 | Agent Benchmarking and Evaluation, Planning and Decision Making</small></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 303]</strong> Efficiently Scaling LLM Reasoning Programs with Certaindex<br><small style="color: #666;">Score: 11 | Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/4ca50f6ea693aeda7b95d22f1929d6a5d49cf4ff.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 304]</strong> SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning<br><small style="color: #666;">Score: 11 | Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/7c62b5e4703772251c504c1cdc203fa7a96cd873.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 305]</strong> Dependency Matters: Enhancing LLM Reasoning with Explicit Knowledge Grounding<br><small style="color: #666;">Score: 11 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Agent Safety and Security</small> <a href="https://openreview.net/pdf/88669d564fe12a96458ad3ff7025be1f74506a21.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 306]</strong> ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents<br><small style="color: #666;">Score: 11 | Planning and Decision Making, Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/311841e491eca81acc49d5288503e9b7819788d6.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 307]</strong> Retro-R1: LLM-based Agentic Retrosynthesis<br><small style="color: #666;">Score: 11 | Tool Use and Code Generation, Reinforcement Learning for LLMs, Domain-Specific Applications</small> <a href="https://openreview.net/pdf/470a5ef177e20c474a9e358c1bab0c1cf082f1de.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 308]</strong> Know What You Don't Know: Uncertainty Calibration of Process Reward Models<br><small style="color: #666;">Score: 11 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/18533f919a9403854c7955628ad3488bf682b694.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 309]</strong> VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception<br><small style="color: #666;">Score: 11 | Reasoning and Test-Time Compute, Vision-Language-Action Models, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/7cf4479fc77f583f1cdede042be97389be035f9a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 310]</strong> Steering When Necessary: Flexible Steering Large Language Models with Backtracking<br><small style="color: #666;">Score: 10 | Agent Safety and Security, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/795a175c3b3b1e9de873069086e754159a4e855b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 311]</strong> Reasoning Models Hallucinate More: Factuality-Aware Reinforcement Learning for Large Reasoning Models<br><small style="color: #666;">Score: 10 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Agent Safety and Security</small> <a href="https://openreview.net/pdf/31f57f113b7a0a8d53b00020bbcdabe6ac8a82cf.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 312]</strong> GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning<br><small style="color: #666;">Score: 10 | Web and Computer-Use Agents, Reinforcement Learning for LLMs, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/ce35fb684e3b11b9c0f1fcc38598cfb3504c728e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 313]</strong> On Evaluating LLM Alignment by Evaluating LLMs as Judges<br><small style="color: #666;">Score: 10 | Agent Benchmarking and Evaluation, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/cefd5dffe2958e9dbfba77cc4764ee04a8cc5a95.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 314]</strong> Scaling RL to Long Videos<br><small style="color: #666;">Score: 10 | Reinforcement Learning for LLMs, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/086f9eed5c2d342130b7d5c3c1f80a2cc8f3594f.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 315]</strong> EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test<br><small style="color: #666;">Score: 9 | Model Efficiency and Optimization, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/28c4c8cf58b0086a2136d73f6059ada87ac33e53.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 316]</strong> How Benchmark Prediction from Fewer Data Misses the Mark<br><small style="color: #666;">Score: 9 | Agent Benchmarking and Evaluation, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/a4097f48740ba588b5ffe5a4fd3f7d88b8eb0a70.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 317]</strong> SpecMAS: A Multi-Agent System for Self-Verifying System Generation via Formal Model Checking<br><small style="color: #666;">Score: 9 | Multi-Agent Systems and Collaboration, Tool Use and Code Generation, Agent Safety and Security</small> <a href="https://openreview.net/pdf/d2baf726e03709dc05beda305bd6ede14d1a9b1b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 318]</strong> LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for Scientific Embodied Agents<br><small style="color: #666;">Score: 9 | Agent Benchmarking and Evaluation, Domain-Specific Applications, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2505.22634" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 319]</strong> R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing<br><small style="color: #666;">Score: 9 | Reasoning and Test-Time Compute, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/14de671242c3654992d79919aba5d312e05f7347.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 320]</strong> Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning<br><small style="color: #666;">Score: 9 | Reasoning and Test-Time Compute, Mathematical and Logical Reasoning, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/d5b025da459325ca88dbc0f0f8c3dc0c23384640.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 321]</strong> Automated Model Discovery via Multi-modal & Multi-step Pipeline<br><small style="color: #666;">Score: 9 | Tool Use and Code Generation, Self-Improvement and Meta-Learning, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/9677940f4812871bd634d12cccadc3598687a9ac.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 322]</strong> Inference-Time Reward Hacking in Large Language Models<br><small style="color: #666;">Score: 8 | Reinforcement Learning for LLMs, Agent Safety and Security, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/8d477ae3e38043ed738ea2aa59e110eec3f49a44.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 323]</strong> AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning<br><small style="color: #666;">Score: 8 | Vision-Language-Action Models, Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/ac3e0d2216d650bf65be2b1559d68dc79c32c6ed.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 324]</strong> CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs<br><small style="color: #666;">Score: 8 | Agent Safety and Security, Multi-Agent Systems and Collaboration, Web and Computer-Use Agents</small> <a href="https://openreview.net/pdf/5b9c63b9b600fd293798deb51960a50373bb0faf.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 325]</strong> Zero-Shot Performance Prediction for Probabilistic Scaling Laws<br><small style="color: #666;">Score: 8 | Model Efficiency and Optimization, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/5656a3e0c168ce620e714e942c21cc42662e5dcc.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 326]</strong> STSBench: A Spatio-temporal Scenario Benchmark for Multi-modal Large Language Models in Autonomous Driving<br><small style="color: #666;">Score: 7 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2506.06218" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 327]</strong> Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs<br><small style="color: #666;">Score: 7 | Agent Benchmarking and Evaluation, Self-Improvement and Meta-Learning, Reasoning and Test-Time Compute</small> <a href="https://arxiv.org/pdf/2510.16062" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 328]</strong> AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling<br><small style="color: #666;">Score: 7 | Multi-Agent Systems and Collaboration, Reasoning and Test-Time Compute, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/97d508cdb6040e0326e1f3e82a7473020de10424.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 329]</strong> Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards<br><small style="color: #666;">Score: 7 | Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/f28db2a8d244c8994006bd065afbd5a061c42feb.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 330]</strong> Best-of-N Jailbreaking<br><small style="color: #666;">Score: 7 | Agent Safety and Security, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/214b0cbe5fe5a3a56ddfd1977e1acfb9c721c50a.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 331]</strong> Multi-step Visual Reasoning with Visual Tokens Scaling and Verification<br><small style="color: #666;">Score: 7 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/a48656c1e2d8fb5cee742d21b8b328458f483707.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 332]</strong> Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models<br><small style="color: #666;">Score: 7 | Planning and Decision Making, Reinforcement Learning for LLMs, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/5233ea1b8598dfd1eed771e087de919731ced5a1.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 333]</strong> R$^2$ec: Towards Large Recommender Models with Reasoning<br><small style="color: #666;">Score: 6 | Reasoning and Test-Time Compute, Tool Use and Code Generation, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/f29b278eccadd4b83c9cca979b6b35476c14e3a8.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 334]</strong> Triplets Better Than Pairs: Towards Stable and Effective Self-Play Fine-Tuning for LLMs<br><small style="color: #666;">Score: 6 | Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/e0596e6e39072aed73b22d2c2c86772c2aae52a0.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 335]</strong> KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems<br><small style="color: #666;">Score: 5 | Multi-Agent Systems and Collaboration, Memory and Context Management, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/81561154949bf17e7f12ee6dc0485c10a2415686.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 336]</strong> Representation Consistency for Accurate and Coherent LLM Answer Aggregation<br><small style="color: #666;">Score: 5 | Reasoning and Test-Time Compute, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/a3ecdca64c27eaba285b6683681268faca747be5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 337]</strong> Optimizing Anytime Reasoning via Budget Relative Policy Optimization<br><small style="color: #666;">Score: 5 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/8136e4668a09f8c47a2454d9e72728d4fdea055e.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 338]</strong> Checklists Are Better Than Reward Models For Aligning Language Models<br><small style="color: #666;">Score: 5 | Reinforcement Learning for LLMs, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/490597cf8f353f8b01b8474e2f98c045eba8f5f4.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 339]</strong> See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model<br><small style="color: #666;">Score: 5 | Vision-Language-Action Models, Spatial and Physical Reasoning, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/bcef03fe8da8c459269e00eeb6626d6584a0ba4c.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 340]</strong> SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning<br><small style="color: #666;">Score: 5 | Reinforcement Learning for LLMs, Self-Improvement and Meta-Learning, Mathematical and Logical Reasoning</small> <a href="https://openreview.net/pdf/c3403842de341324f63358f1732f1518761661f2.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 341]</strong> ChatVLA-2: Vision-Language-Action Model with Open-World Reasoning<br><small style="color: #666;">Score: 5 | Vision-Language-Action Models, Reasoning and Test-Time Compute, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/c88d737915ea445cb600d21cb0c7125912b7053b.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 342]</strong> Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab<br><small style="color: #666;">Score: 4 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2507.02083" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 343]</strong> MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark<br><small style="color: #666;">Score: 4 | Agent Benchmarking and Evaluation, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2506.05587" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 344]</strong> MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?<br><small style="color: #666;">Score: 4 | Reasoning and Test-Time Compute, Self-Improvement and Meta-Learning</small> <a href="https://arxiv.org/pdf/2503.09499" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 345]</strong> Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models<br><small style="color: #666;">Score: 4 | Reinforcement Learning for LLMs, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/eec1920da8921f9ebab8a70513b7531b0b8281d3.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 346]</strong> Janus-Pro-R1: Advancing Collaborative Visual Comprehension and Generation via Reinforcement Learning<br><small style="color: #666;">Score: 3 | Reinforcement Learning for LLMs, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/4aad34acb1fe54ce3d709604ec7d81ff8ce11b11.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 347]</strong> Dynamic Risk Assessments for Offensive Cybersecurity Agents<br><small style="color: #666;">Score: 3 | Agent Safety and Security, Agent Benchmarking and Evaluation, Domain-Specific Applications</small> <a href="https://arxiv.org/pdf/2505.18384" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 348]</strong> KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation<br><small style="color: #666;">Score: 3 | Agent Benchmarking and Evaluation, Reasoning and Test-Time Compute, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/a3f1d4df6324abb0ea9576e5fe2da1d467238283.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 349]</strong> Among Us: A Sandbox for Measuring and Detecting Agentic Deception<br><small style="color: #666;">Score: 3 | Agent Benchmarking and Evaluation, Agent Safety and Security, Multi-Agent Systems and Collaboration</small> <a href="https://openreview.net/pdf/0ff014da0c71915ceb0a84e3977c85eaf1e134dd.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 350]</strong> VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models<br><small style="color: #666;">Score: 3 | Vision-Language-Action Models, Planning and Decision Making, Agent Benchmarking and Evaluation</small> <a href="https://openreview.net/pdf/05a810d8dce16f520e115b9ee80b8096e6512276.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 351]</strong> ContextAgent: Context-Aware Proactive LLM Agents with Open-world Sensory Perceptions<br><small style="color: #666;">Score: 3 | Agent Benchmarking and Evaluation, Tool Use and Code Generation, Memory and Context Management</small> <a href="https://openreview.net/pdf/8c61939b607693d9b13cc1df27793d844f3648f5.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 352]</strong> R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization<br><small style="color: #666;">Score: 2 | Multi-Agent Systems and Collaboration, Domain-Specific Applications, Tool Use and Code Generation</small> <a href="https://arxiv.org/pdf/2505.15155" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 353]</strong> NavBench: Probing Multimodal Large Language Models for Embodied Navigation<br><small style="color: #666;">Score: 2 | Agent Benchmarking and Evaluation, Vision-Language-Action Models, Spatial and Physical Reasoning</small> <a href="https://openreview.net/pdf/1ef1a313c6a3eea3eea8cfe4ac568866df673dec.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 354]</strong> SpatialLM: Training Large Language Models for Structured Indoor Modeling<br><small style="color: #666;">Score: 2 | Vision-Language-Action Models, Spatial and Physical Reasoning, Domain-Specific Applications</small> <a href="https://openreview.net/pdf/22f930c6b44c852e1c53aa7784df786168735e5d.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 355]</strong> BraVE: Offline Reinforcement Learning for Discrete Combinatorial Action Spaces<br><small style="color: #666;">Score: 2 | Planning and Decision Making, Reinforcement Learning for LLMs</small> <a href="https://openreview.net/pdf/fb6e5db6d0511de2cbf926cf309aa9e0fbe40245.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 356]</strong> Scaling Physical Reasoning with the PHYSICS Dataset<br><small style="color: #666;">Score: 1 | Reasoning and Test-Time Compute, Spatial and Physical Reasoning</small> <a href="https://arxiv.org/pdf/2506.00022" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 357]</strong> Validating LLM-as-a-Judge Systems under Rating Indeterminacy<br><small style="color: #666;">Score: 1 | Agent Benchmarking and Evaluation, Agent Safety and Security</small> <a href="https://openreview.net/pdf/2ad527d02fe920443fe79a516d7575f417103f99.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 358]</strong> To Think or Not To Think: A Study of Thinking in Rule-Based Visual Reinforcement Fine-Tuning<br><small style="color: #666;">Score: 1 | Reasoning and Test-Time Compute, Reinforcement Learning for LLMs, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/8105c1360484fcffb35e03d8f791d0b437aa1589.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 359]</strong> Analogy-based Multi-Turn Jailbreak against Large Language Models<br><small style="color: #666;">Score: 1 | Agent Safety and Security, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/90ec84387b8d7282640d625e2d28faef32f89000.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 360]</strong> Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling<br><small style="color: #666;">Score: 1 | Model Efficiency and Optimization, Memory and Context Management</small> <a href="https://openreview.net/pdf/a7327b197b07cb1df3b5e408eecd14e1276acc6d.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 361]</strong> BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization<br><small style="color: #666;">Score: 1 | Agent Safety and Security, Vision-Language-Action Models</small> <a href="https://openreview.net/pdf/9ef98e483d9e324778e7116170aa4848fc6f67e3.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 362]</strong> GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning<br><small style="color: #666;">Score: 1 | Multi-Agent Systems and Collaboration, Vision-Language-Action Models, Reasoning and Test-Time Compute</small> <a href="https://openreview.net/pdf/55c4606303cc86c81ca99b5d5743cfe40d7fb140.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 363]</strong> TS-RAG: Retrieval-Augmented Generation based Time Series Foundation Models are Stronger Zero-Shot Forecaster<br><small style="color: #666;">Score: 1 | Domain-Specific Applications, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/e686d5b872f3c42cd96442359b22e23319fe0acb.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 364]</strong> Bridging Human and LLM Judgments: Understanding and Narrowing the Gap<br><small style="color: #666;">Score: 1 | Agent Benchmarking and Evaluation, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/3b238b3324d5ab3f3e5a672a12a2c7610ee13a48.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 365]</strong> Thoughts Are All Over the Place: On the Underthinking of Long Reasoning Models<br><small style="color: #666;">Score: 10 | Reasoning and Test-Time Compute, Mathematical and Logical Reasoning, Model Efficiency and Optimization</small> <a href="https://openreview.net/pdf/52cab7bd6214e5b9d63addbfb0411f3ce8f517f4.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 366]</strong> Length Generalization via Auxiliary Tasks<br><small style="color: #666;">Score: 5 | Reasoning and Test-Time Compute, Self-Improvement and Meta-Learning</small> <a href="https://openreview.net/pdf/a5e5e4faf09290faa591d33b29401917374054fa.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
<p style="margin: 10px 0; padding: 10px; background: white; border-radius: 5px;"><strong>[Paper 367]</strong> Transcending Cost-Quality Tradeoff in Agent Serving via Session-Awareness<br><small style="color: #666;">Score: 21 | Model Efficiency and Optimization, Tool Use and Code Generation</small> <a href="https://openreview.net/pdf/67186a4229264aede8b786e7d6e259cb157d2aba.pdf" target="_blank" style="color: #00c781; text-decoration: none;">📄 PDF</a></p>
</div>
</details>
</div>